"id","title","role","means","ends","highlight","kind","subkind","severity","false_positive"
18354,"As a developer, I'd like to have a central place to manage external properties for applications across all the environments, so I can provide server and client side support for externalized configuration for XD Admin and XD Container servers. ","As a developer",", I'd like to have a central place to manage external properties for applications across all the environments,","so I can provide server and client side support for externalized configuration for XD Admin and XD Container servers.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18475,"The current build ships everything that is found in the modules directory, including build artifacts such as build or IDEA .iml files. Restrict the build to only include config , lib at the moment.",NULL,"The current build ships everything that is found in the modules directory, including build artifacts such as build or IDEA .iml files. Restrict the build to only include config , lib at the moment.",NULL,"Add for who this story is","well_formed","no_role","high",False
18290,"Currently there is no ModuleInstanceStatus returned. This issue will fill in the details.",NULL,"Currently there is no ModuleInstanceStatus returned. This issue will fill in the details.",NULL,"Currently there is no ModuleInstanceStatus returned<span class='highlight-text severity-high'>. This issue will fill in the details.</span>","minimal","punctuation","high",False
18284,"As a s c d developer, I'd like to investigate how to include exclude msg bus binding jars, so I can decide the binding selection and fallback mechanism when there is none setup.","As a s c d developer",", I'd like to investigate how to include exclude msg bus binding jars,","so I can decide the binding selection and fallback mechanism when there is none setup.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18952,"INFO main zookeeper.ZooKeeper 100 Client environment java.class.path produces a huge amount of output and is rather distracting, if that specific entry could be at a log level of debug it would make the startup of the server look cleaner.",NULL,"INFO main zookeeper.ZooKeeper 100 Client environment java.class.path produces a huge amount of output and is rather distracting, if that specific entry could be at a log level of debug it would make the startup of the server look cleaner.",NULL,"Add for who this story is","well_formed","no_role","high",False
18537,"To be added in AbstractMetricsController, as well as the various shell commands counter all delete , etc... ",NULL,"To be added in AbstractMetricsController, as well as the various shell commands counter all delete , etc... ",NULL,"Add for who this story is","well_formed","no_role","high",False
18534,"Since the refactoring of the module registry that does not look inside a module, it can t know that the scripts directory is not a module. Everything that is a direct child of source, processor, sink, job should be a module archive. Everything else supporting that should be moved out, e.g. in modules common",NULL,"Since the refactoring of the module registry that does not look inside a module, it can t know that the scripts directory is not a module. Everything that is a direct child of source, processor, sink, job should be a module archive. Everything else supporting that should be moved out, e.g. in modules common",NULL,"Add for who this story is","well_formed","no_role","high",False
18575,"See RedisSingleNodeStreamDeploymentIntegrationTests etc.",NULL,"See RedisSingleNodeStreamDeploymentIntegrationTests etc.",NULL,"Add for who this story is","well_formed","no_role","high",False
18564,"As a scala developer, someone could easily deploy the spark streaming module developed using scala. ","As a scala developer, someon","e could easily deploy the spark streaming module developed using scala.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18566,"As a developer, I'd like to upgrade to SHDP GA release so that I can sync up with the latest bits. ","As a developer",", I'd like to upgrade to SHDP GA release","so that I can sync up with the latest bits.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18601,"As a user, I'd like to have the option to store the custom module uber jar in HDFS so that I can rely on the HA feature to reliably read and reinstall under failure scenarios. ","As a user",", I'd like to have the option to store the custom module uber jar in HDFS","so that I can rely on the HA feature to reliably read and reinstall under failure scenarios.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18591,"Run a clean gradle build to identify all warnings.",NULL,"Run a clean gradle build to identify all warnings.",NULL,"Add for who this story is","well_formed","no_role","high",False
18648,"As a developer, I'd like to include the following improvements as part of the EC2 CI infrastructure, so that we can reliably run the CI builds and also assert over feature functionalities. Scope Enable distributed jvm test Change from using artifactory gradle task to a command task that calls . gradlew Test w embedded hadoop off Turn on maxParallelForks ","As a developer",", I'd like to include the following improvements as part of the EC2 CI infrastructure,","so that we can reliably run the CI builds and also assert over feature functionalities. Scope Enable distributed jvm test Change from using artifactory gradle task to a command task that calls . gradlew Test w embedded hadoop off Turn on maxParallelForks","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18368,"Need acceptance tests to run on the 1.2.X branch. Needs to be setup as a child of the Publish 1.2.x",NULL,"Need acceptance tests to run on the 1.2.X branch. Needs to be setup as a child of the Publish 1.2.x",NULL,"Need acceptance tests to run on the 1<span class='highlight-text severity-high'>.2.X branch. Needs to be setup as a child of the Publish 1.2.x</span>","minimal","punctuation","high",False
18369,"Sort alphabetically, nest Available modules section appropriately. Optionally, move to a whole different PART in reference doc",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
18369,"Sort alphabetically, nest Available modules section appropriately. Optionally, move to a whole different PART in reference doc",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
18649,"As a build master, I'd like to research CI options so that I can improve CI build stability and reliability. Potential Option npm cache https github.com swarajban npm cache Caching previously installed dependencies, npm cache doesn t require downloading the internet each time we build.",NULL,"As a build master, I'd like to research CI options","so that I can improve CI build stability and reliability. Potential Option npm cache https github.com swarajban npm cache Caching previously installed dependencies, npm cache doesn t require downloading the internet each time we build.","Add for who this story is","well_formed","no_role","high",False
18728,"XD 1.0.2 Release PHD 2.1 Upgrade Action Items Update to SHDP 2.0.3 Add Hadoop 2.5 hadoop25 Change PHD 2.x from phd20 to phd21 Test PHD 2.0 with phd21 Document that both PHD 2.1 and PHD 2.0 is supported with phd21 ",NULL,"XD 1.0.2 Release PHD 2.1 Upgrade Action Items Update to SHDP 2.0.3 Add Hadoop 2.5 hadoop25 Change PHD 2.x from phd20 to phd21 Test PHD 2.0 with phd21 Document that both PHD 2.1 and PHD 2.0 is supported with phd21 ",NULL,"Add for who this story is","well_formed","no_role","high",False
18727,"As a user, I'd like to have the option to supply data partitioning strategy so that I can parallelize ingest of data from RDBMS to HDFS.","As a user",", I'd like to have the option to supply data partitioning strategy","so that I can parallelize ingest of data from RDBMS to HDFS.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18729,"As a user, I'd like to evaluate Spring Boot dependency upgrades so that I can make sure there aren t any side effects or impacts to existing functionalities. ","As a user",", I'd like to evaluate Spring Boot dependency upgrades","so that I can make sure there aren t any side effects or impacts to existing functionalities.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18730,"As a user, I need a document covering our recommendations for deploying a XD cluster using Mesos with the Marathon Framework. ","As a user",", I need a document covering our recommendations for deploying a XD cluster using Mesos with the Marathon Framework.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18718,"To enrich acceptance test, I'd like to lazy initialize DB for JDBC Sink acceptance tests. Check to see if it is already initialized and decide. ",NULL,"To enrich acceptance test, I'd like to lazy initialize DB for JDBC Sink acceptance tests. Check to see if it is already initialized and decide. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19088,"Create a first pass at an acceptance test app for a stream definition of http log. This will involve creating two new projects in xd 1. spring xd integration test 2. spring xd acceptance tests 1 will contain generally useful utility methods for acceptance test, such as sending data over http, obtaining and asserting JMX values of specific modules. 2 will contain tests that use 1 to test the various out of the box modules provides in XD.",NULL,"Create a first pass at an acceptance test app for a stream definition of http log. This will involve creating two new projects in xd 1. spring xd integration test 2. spring xd acceptance tests 1 will contain generally useful utility methods for acceptance test, such as sending data over http, obtaining and asserting JMX values of specific modules. 2 will contain tests that use 1 to test the various out of the box modules provides in XD.",NULL,"Add for who this story is","well_formed","no_role","high",False
18141,"As a s c d developer, I'd like to explore options to bootstrap and setup Lattice based infrastructure for s c d s bare metal deployment.","As a s c d developer",", I'd like to explore options to bootstrap and setup Lattice based infrastructure for s c d s bare metal deployment.",NULL,"As a s c d developer, I'd like to explore options to bootstrap<span class='highlight-text severity-high'> and </span>setup Lattice based infrastructure for s c d s bare metal deployment.","atomic","conjunctions","high",False
18141,"As a s c d developer, I'd like to explore options to bootstrap and setup Lattice based infrastructure for s c d s bare metal deployment.","As a s c d developer",", I'd like to explore options to bootstrap and setup Lattice based infrastructure for s c d s bare metal deployment.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18144,"As an XD user, I'd like to be able to visually differentiate between job composition workflow and single job.","As an XD user",", I'd like to be able to visually differentiate between job composition workflow and single job.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18146,"As an XD user, I'd like to have a REST endpoint that returns job composition flag , so I can use it to differentiate visual representation between parent child relationship and standalone jobs. ","As an XD user",", I'd like to have a REST endpoint that returns job composition flag ,","so I can use it to differentiate visual representation between parent child relationship and standalone jobs.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18148,"As a Spring XD developer, I'd like to move cassandra module from XD to s c s repo, so I can use it as sink to build streaming pipeline.","As a Spring XD developer",", I'd like to move cassandra module from XD to s c s repo,","so I can use it as sink to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18147,"As a Spring XD developer, I'd like to port analytic pmml module from XD to s c s repo, so I can use it as processor module to build streaming pipeline.","As a Spring XD developer",", I'd like to port analytic pmml module from XD to s c s repo,","so I can use it as processor module to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18170,"As a Spring XD developer, I'd like to move splunk module from XD to s c s repo, so I can use it as sink to build streaming pipeline. ","As a Spring XD developer",", I'd like to move splunk module from XD to s c s repo,","so I can use it as sink to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18169,"As a Spring XD developer, I'd like to move tcp module from XD to s c s repo, so I can use it as sink to build streaming pipeline. ","As a Spring XD developer",", I'd like to move tcp module from XD to s c s repo,","so I can use it as sink to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18175,"As a Spring XD developer, I'd like to move mail module from XD to s c s repo, so I can use it as sink to build streaming pipeline. ","As a Spring XD developer",", I'd like to move mail module from XD to s c s repo,","so I can use it as sink to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18172,"As a Spring XD developer, I'd like to move null module from XD to s c s repo, so I can use it as sink to build streaming pipeline. ","As a Spring XD developer",", I'd like to move null module from XD to s c s repo,","so I can use it as sink to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18173,"As a Spring XD developer, I'd like to move mqtt module from XD to s c s repo, so I can use it as sink to build streaming pipeline. ","As a Spring XD developer",", I'd like to move mqtt module from XD to s c s repo,","so I can use it as sink to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18176,"As a Spring XD developer, I'd like to move hdfs dataset module from XD to s c s repo, so I can use it as sink to build streaming pipeline. ","As a Spring XD developer",", I'd like to move hdfs dataset module from XD to s c s repo,","so I can use it as sink to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18177,"As a Spring XD developer, I'd like to move gpfdist module from XD to s c s repo, so I can use it as sink to build streaming pipeline. ","As a Spring XD developer",", I'd like to move gpfdist module from XD to s c s repo,","so I can use it as sink to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18171,"As a Spring XD developer, I'd like to move shell module from XD to s c s repo, so I can use it as sink to build streaming pipeline. ","As a Spring XD developer",", I'd like to move shell module from XD to s c s repo,","so I can use it as sink to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18178,"As a Spring XD developer, I'd like to move tcp client module from XD to s c s repo, so I can use it as source to build streaming pipeline. ","As a Spring XD developer",", I'd like to move tcp client module from XD to s c s repo,","so I can use it as source to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18180,"As a Spring XD developer, I'd like to move mail module from XD to s c s repo, so I can use it as source to build streaming pipeline.","As a Spring XD developer",", I'd like to move mail module from XD to s c s repo,","so I can use it as source to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18184,"As a Spring XD developer, I'd like to move mqtt module from XD to s c s repo, so I can use it as source to build streaming pipeline. ","As a Spring XD developer",", I'd like to move mqtt module from XD to s c s repo,","so I can use it as source to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18181,"As a Spring XD developer, I'd like to move syslog module from XD to s c s repo, so I can use it as source to build streaming pipeline. ","As a Spring XD developer",", I'd like to move syslog module from XD to s c s repo,","so I can use it as source to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18182,"As a Spring XD developer, I'd like to move stdout module from XD to s c s repo, so I can use it as source to build streaming pipeline.","As a Spring XD developer",", I'd like to move stdout module from XD to s c s repo,","so I can use it as source to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18183,"As a Spring XD developer, I'd like to move reactor ip module from XD to s c s repo, so I can use it as source to build streaming pipeline.","As a Spring XD developer",", I'd like to move reactor ip module from XD to s c s repo,","so I can use it as source to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18185,"As a Spring XD developer, I'd like to move mongo module from XD to s c s m repo, so I can use it as source to build streaming pipeline. ","As a Spring XD developer",", I'd like to move mongo module from XD to s c s m repo,","so I can use it as source to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18186,"As a Spring XD developer, I'd like to move mail module from XD to s c s m repo, so I can use it as source to build streaming pipeline. ","As a Spring XD developer",", I'd like to move mail module from XD to s c s m repo,","so I can use it as source to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18187,"As a Spring XD developer, I'd like to move jms module from XD to s c s m repo, so I can use it as source to build streaming pipeline. ","As a Spring XD developer",", I'd like to move jms module from XD to s c s m repo,","so I can use it as source to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18884,"Add an option to destroy the stream job definitions. Also add confirm action that asks for user to confirm to proceed with destroy.",NULL,"Add an option to destroy the stream job definitions. Also add confirm action that asks for user to confirm to proceed with destroy.",NULL,"Add for who this story is","well_formed","no_role","high",False
19134,"Create a script to sanity check JMS and MQTT",NULL,"Create a script to sanity check JMS and MQTT",NULL,"Add for who this story is","well_formed","no_role","high",False
18884,"Add an option to destroy the stream job definitions. Also add confirm action that asks for user to confirm to proceed with destroy.",NULL,"Add an option to destroy the stream job definitions. Also add confirm action that asks for user to confirm to proceed with destroy.",NULL,"Add an option to destroy the stream job definitions<span class='highlight-text severity-high'>. Also add confirm action that asks for user to confirm to proceed with destroy.</span>","minimal","punctuation","high",False
18086,"Make it more clear what drivers need to be copied where. See https github.com spring projects spring xd issues 1653",NULL,"Make it more clear what drivers need to be copied where. See https github.com spring projects spring xd issues 1653",NULL,"Add for who this story is","well_formed","no_role","high",False
19173,"BatchJobsController.listForJob should be executionsForJob BatchJobsController.jobInstances should be instancesForJob The JavaDoc for the class and each method should be more descriptive about their functionality ",NULL,"BatchJobsController.listForJob should be executionsForJob BatchJobsController.jobInstances should be instancesForJob The JavaDoc for the class and each method should be more descriptive about their functionality ",NULL,"Add for who this story is","well_formed","no_role","high",False
19226,"Some tests esp. ModuleClasspathTests.testModuleWithClasspathAfterServerStarted seem to fail because of a race condition. Add a Hamcrest matcher that knows how to read the content of a FileSink Source and refactor those to read like e.g. assertThat fileSink, eventually hasContent foo ",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19226,"Some tests esp. ModuleClasspathTests.testModuleWithClasspathAfterServerStarted seem to fail because of a race condition. Add a Hamcrest matcher that knows how to read the content of a FileSink Source and refactor those to read like e.g. assertThat fileSink, eventually hasContent foo ",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
18086,"Make it more clear what drivers need to be copied where. See https github.com spring projects spring xd issues 1653",NULL,"Make it more clear what drivers need to be copied where. See https github.com spring projects spring xd issues 1653",NULL,"Make it more clear what drivers need to be copied where<span class='highlight-text severity-high'>. See https github.com spring projects spring xd issues 1653</span>","minimal","punctuation","high",False
18094,"As a developer, I'd like to create separate repo for Lattice SPI, so I'don t have to bundle all SPI variants under one admin project.","As a developer",", I'd like to create separate repo for Lattice SPI,","so I'don t have to bundle all SPI variants under one admin project.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18096,"As a user, I'd like to have direct shell commands to scale up down a given module instance, so I can avoid SPI specific CLI commands that needs run outside of data flow.","As a user",", I'd like to have direct shell commands to scale up down a given module instance,","so I can avoid SPI specific CLI commands that needs run outside of data flow.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18092,"As a developer, I'd like to add undeployed status for Lattice SPI, so I can represent the correct status instead of the current unknown state.","As a developer",", I'd like to add undeployed status for Lattice SPI,","so I can represent the correct status instead of the current unknown state.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18090,"As a developer, I'd like to add undeployed status for k8s SPI, so I can represent the correct status instead of the current unknown state.","As a developer",", I'd like to add undeployed status for k8s SPI,","so I can represent the correct status instead of the current unknown state.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18285,"The current implementation makes use of cf java client, which is relatively heavy for our needs. It should be removed in favour of a bespoke RestOperations wrapper. See https github.com Zteve test cc oauth for sample code.",NULL,"The current implementation makes use of cf java client, which is relatively heavy for our needs. It should be removed in favour of a bespoke RestOperations wrapper. See https github.com Zteve test cc oauth for sample code.",NULL,"Add for who this story is","well_formed","no_role","high",False
19256,"Currently, ModulesController creates the ModuleDefinitionRepository instance with ModuleRegistry. Instead, we should inject the moduleDefinitionRepository into ModulesController directly.",NULL,"Currently, ModulesController creates the ModuleDefinitionRepository instance with ModuleRegistry. Instead, we should inject the moduleDefinitionRepository into ModulesController directly.",NULL,"Add for who this story is","well_formed","no_role","high",False
18644,"Would be nice to have the shell zip the contents of a directory if not already in zipped form. This way, the development cycle if one decided to use upload is quicker and edits can be done in place.",NULL,"Would be nice to have the shell zip the contents of a directory if not already in zipped form. This way, the development cycle if one decided to use upload is quicker and edits can be done in place.",NULL,"Would be nice to have the shell zip the contents of a directory if not already in zipped form<span class='highlight-text severity-high'>. This way, the development cycle if one decided to use upload is quicker and edits can be done in place.</span>","minimal","punctuation","high",False
18682,"Create a dedicated Launch Page for Jobs. Currently we create a launch form underneath the deployments table. That is a bit unwieldy when many deployed jobs are shown in the table.",NULL,"Create a dedicated Launch Page for Jobs. Currently we create a launch form underneath the deployments table. That is a bit unwieldy when many deployed jobs are shown in the table.",NULL,"Add for who this story is","well_formed","no_role","high",False
18682,"Create a dedicated Launch Page for Jobs. Currently we create a launch form underneath the deployments table. That is a bit unwieldy when many deployed jobs are shown in the table.",NULL,"Create a dedicated Launch Page for Jobs. Currently we create a launch form underneath the deployments table. That is a bit unwieldy when many deployed jobs are shown in the table.",NULL,"Create a dedicated Launch Page for Jobs<span class='highlight-text severity-high'>. Currently we create a launch form underneath the deployments table. That is a bit unwieldy when many deployed jobs are shown in the table.</span>","minimal","punctuation","high",False
18280,"Apply the same strategy for the Module Command Tests also to all other Shell integration tests.",NULL,"Apply the same strategy for the Module Command Tests also to all other Shell integration tests.",NULL,"Add for who this story is","well_formed","no_role","high",False
18301,"As a s c d developer, I'd like to invoke REST APIs via shell, so I can validate StreamController operations.","As a s c d developer",", I'd like to invoke REST APIs via shell,","so I can validate StreamController operations.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18703,"As a follow up action from module registry refactoring we would have to clean up deprecated functions ex download of module definitions within our codebase. It may also be necessary to clean up Shell and Admin UI modules. ",NULL,"As a follow up action from module registry refactoring we would have to clean up deprecated functions ex download of module definitions within our codebase. It may also be necessary to clean up Shell and Admin UI modules. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18703,"As a follow up action from module registry refactoring we would have to clean up deprecated functions ex download of module definitions within our codebase. It may also be necessary to clean up Shell and Admin UI modules. ",NULL,"As a follow up action from module registry refactoring we would have to clean up deprecated functions ex download of module definitions within our codebase. It may also be necessary to clean up Shell and Admin UI modules. ",NULL,"As a follow up action from module registry refactoring we would have to clean up deprecated functions ex download of module definitions within our codebase<span class='highlight-text severity-high'>. It may also be necessary to clean up Shell and Admin UI modules. </span>","minimal","punctuation","high",False
18704,"As a developer, I'd like to have a maintenance branch so that I can commit MINOR release ex 1.0.2 code changes instead of committing to MASTER.","As a developer",", I'd like to have a maintenance branch","so that I can commit MINOR release ex 1.0.2 code changes instead of committing to MASTER.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18706,"Pre requisite for Rabbit MQ Benchmarks Infrastructure setup Configuration changes Tool chain setup",NULL,"Pre requisite for Rabbit MQ Benchmarks Infrastructure setup Configuration changes Tool chain setup",NULL,"Add for who this story is","well_formed","no_role","high",False
18707,"Please refer to the GH Issue reported here https github.com spring projects spring xd issues 1218",NULL,"Please refer to the GH Issue reported here https github.com spring projects spring xd issues 1218",NULL,"Add for who this story is","well_formed","no_role","high",False
18809,"http localhost 9393 management jolokia search xd. type , for singlenode in M7 returned a value..... on master it returns 404 error... ",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
18809,"http localhost 9393 management jolokia search xd. type , for singlenode in M7 returned a value..... on master it returns 404 error... ",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
18809,"http localhost 9393 management jolokia search xd. type , for singlenode in M7 returned a value..... on master it returns 404 error... ",NULL,NULL,NULL,"http localhost 9393 management jolokia search xd<span class='highlight-text severity-high'>. type , for singlenode in M7 returned a value..... on master it returns 404 error... </span>","minimal","punctuation","high",False
19009,"A reactor based TCP module that would support some basic CODECS. Should evaluate if this new TCP module would subsume the current reactor syslog module functionality or if the reactor syslog module should be enhanced upgradted.",NULL,"A reactor based TCP module that would support some basic CODECS. Should evaluate if this new TCP module would subsume the current reactor syslog module functionality or if the reactor syslog module should be enhanced upgradted.",NULL,"Add for who this story is","well_formed","no_role","high",False
19009,"A reactor based TCP module that would support some basic CODECS. Should evaluate if this new TCP module would subsume the current reactor syslog module functionality or if the reactor syslog module should be enhanced upgradted.",NULL,"A reactor based TCP module that would support some basic CODECS. Should evaluate if this new TCP module would subsume the current reactor syslog module functionality or if the reactor syslog module should be enhanced upgradted.",NULL,"A reactor based TCP module that would support some basic CODECS. Should evaluate if this new TCP module would subsume the current reactor syslog module functionality<span class='highlight-text severity-high'> or </span>if the reactor syslog module should be enhanced upgradted.","atomic","conjunctions","high",False
18732,"Small follow up story to XD 2094 to improve tooltip handling.",NULL,"Small follow up story to XD 2094 to improve tooltip handling.",NULL,"Add for who this story is","well_formed","no_role","high",False
19666,"Need to create a basic client library to provide easier access to the XD REST API. ",NULL,"Need to create a basic client library to provide easier access to the XD REST API. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18105,"As a user, I'd like to refer to job orchestration documentation, so I can use it as guideline for building batch workflows. ","As a user",", I'd like to refer to job orchestration documentation,","so I can use it as guideline for building batch workflows.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19668,"Running on Cloud Foundry and other managed environments we need to be able to specify a Redis password in addition to host and port.",NULL,"Running on Cloud Foundry and other managed environments we need to be able to specify a Redis password in addition to host and port.",NULL,"Add for who this story is","well_formed","no_role","high",False
19668,"Running on Cloud Foundry and other managed environments we need to be able to specify a Redis password in addition to host and port.",NULL,"Running on Cloud Foundry and other managed environments we need to be able to specify a Redis password in addition to host and port.",NULL,"Running on Cloud Foundry<span class='highlight-text severity-high'> and </span>other managed environments we need to be able to specify a Redis password in addition to host and port.","atomic","conjunctions","high",False
19670,"As a developer, I need a way to deploy job configurations as well as the related custom code to XD. ","As a developer",", I need a way to deploy job configurations as well as the related custom code to XD.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18106,"As a user, I'd like to use SpEL expressions inline at the stream definition level, so I can operate on the payload consistently while using any OOTB, including the custom modules. ","As a user",", I'd like to use SpEL expressions inline at the stream definition level,","so I can operate on the payload consistently while using any OOTB, including the custom modules.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18109,"See https github.com spring cloud spring cloud dataflow issues 128 This is needed to support channel channel type constructs",NULL,"See https github.com spring cloud spring cloud dataflow issues 128 This is needed to support channel channel type constructs",NULL,"Add for who this story is","well_formed","no_role","high",False
19736,"The syslog source currently is hard coded to use udp on port 11111. Need to parameterize the port and provide an option to use TCP.",NULL,"The syslog source currently is hard coded to use udp on port 11111. Need to parameterize the port and provide an option to use TCP.",NULL,"Add for who this story is","well_formed","no_role","high",False
19736,"The syslog source currently is hard coded to use udp on port 11111. Need to parameterize the port and provide an option to use TCP.",NULL,"The syslog source currently is hard coded to use udp on port 11111. Need to parameterize the port and provide an option to use TCP.",NULL,"The syslog source currently is hard coded to use udp on port 11111. Need to parameterize the port<span class='highlight-text severity-high'> and </span>provide an option to use TCP.","atomic","conjunctions","high",False
19736,"The syslog source currently is hard coded to use udp on port 11111. Need to parameterize the port and provide an option to use TCP.",NULL,"The syslog source currently is hard coded to use udp on port 11111. Need to parameterize the port and provide an option to use TCP.",NULL,"The syslog source currently is hard coded to use udp on port 11111<span class='highlight-text severity-high'>. Need to parameterize the port and provide an option to use TCP.</span>","minimal","punctuation","high",False
18111,"As a Flo user, I'd like to have timeout and pollInterval as global options at the DSL level, so I can override the defaults at will. ","As a Flo user",", I'd like to have timeout and pollInterval as global options at the DSL level,","so I can override the defaults at will.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18245,"Build SCS and SCD projects upon change in github repo. Push docker image for SCD Admin to docker hub",NULL,"Build SCS and SCD projects upon change in github repo. Push docker image for SCD Admin to docker hub",NULL,"Add for who this story is","well_formed","no_role","high",False
18245,"Build SCS and SCD projects upon change in github repo. Push docker image for SCD Admin to docker hub",NULL,"Build SCS and SCD projects upon change in github repo. Push docker image for SCD Admin to docker hub",NULL,"Build SCS and SCD projects upon change in github repo<span class='highlight-text severity-high'>. Push docker image for SCD Admin to docker hub</span>","minimal","punctuation","high",False
18246,"As a s c s developer, I'd like to create auto configuration for singlenode binder configuration properties, so I can automatically configure the Spring application based on the dependencies.","As a s c","s developer, I'd like to create auto configuration for singlenode binder configuration properties,","so I can automatically configure the Spring application based on the dependencies.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18247,"As a user I should be able to use the existing admin UI client for spring cloud data admin with the appropriate server configurations.","As a user","I should be able to use the existing admin UI client for spring cloud data admin with the appropriate server configurations.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19683,"The asciidoc wiki should have a section included in the Sidebar.asciidoc as well that describes the general usage of the DSL syntax.",NULL,"The asciidoc wiki should have a section included in the Sidebar.asciidoc as well that describes the general usage of the DSL syntax.",NULL,"Add for who this story is","well_formed","no_role","high",False
19738,"Pointers to other documentation on how to install hadoop. ",NULL,"Pointers to other documentation on how to install hadoop. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19739,"Show overall flow of data in a stream, the server components admin and container . How modules are deployed.",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19739,"Show overall flow of data in a stream, the server components admin and container . How modules are deployed.",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19739,"Show overall flow of data in a stream, the server components admin and container . How modules are deployed.",NULL,NULL,NULL,"Show overall flow of data in a stream, the server components admin and container <span class='highlight-text severity-high'>. How modules are deployed.</span>","minimal","punctuation","high",False
19741,"Similar to what would show up on structure101 reports.",NULL,"Similar to what would show up on structure101 reports.",NULL,"Add for who this story is","well_formed","no_role","high",False
19740,"Asciidoc doctor might have one as part of it toolchain",NULL,"Asciidoc doctor might have one as part of it toolchain",NULL,"Add for who this story is","well_formed","no_role","high",False
18112,"As a developer, I'd like to resolve remaining gaps wrt CI pipelines for Data Flow and the family, so I can continuously evaluate functionalities on every commit.","As a developer",", I'd like to re","solve remaining gaps wrt CI pipelines for Data Flow and the family, so I can continuously evaluate functionalities on every commit.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18257,"As a Spring XD user, I'd like to use the latest releases of HDP PHD distros, so I can leverage the latest features to create pipelines involving HDFS .","As a Spring XD user",", I'd like to use the latest releases of HDP PHD distros,","so I can leverage the latest features to create pipelines involving HDFS .","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18644,"Would be nice to have the shell zip the contents of a directory if not already in zipped form. This way, the development cycle if one decided to use upload is quicker and edits can be done in place.",NULL,"Would be nice to have the shell zip the contents of a directory if not already in zipped form. This way, the development cycle if one decided to use upload is quicker and edits can be done in place.",NULL,"Add for who this story is","well_formed","no_role","high",False
18644,"Would be nice to have the shell zip the contents of a directory if not already in zipped form. This way, the development cycle if one decided to use upload is quicker and edits can be done in place.",NULL,"Would be nice to have the shell zip the contents of a directory if not already in zipped form. This way, the development cycle if one decided to use upload is quicker and edits can be done in place.",NULL,"Would be nice to have the shell zip the contents of a directory if not already in zipped form. This way, the development cycle if one decided to use upload is quicker<span class='highlight-text severity-high'> and </span>edits can be done in place.","atomic","conjunctions","high",False
19009,"A reactor based TCP module that would support some basic CODECS. Should evaluate if this new TCP module would subsume the current reactor syslog module functionality or if the reactor syslog module should be enhanced upgradted.",NULL,"A reactor based TCP module that would support some basic CODECS. Should evaluate if this new TCP module would subsume the current reactor syslog module functionality or if the reactor syslog module should be enhanced upgradted.",NULL,"A reactor based TCP module that would support some basic CODECS<span class='highlight-text severity-high'>. Should evaluate if this new TCP module would subsume the current reactor syslog module functionality or if the reactor syslog module should be enhanced upgradted.</span>","minimal","punctuation","high",False
19017,"Need to be able to test the following sources TCP, HTTP, Time,",NULL,"Need to be able to test the following sources TCP, HTTP, Time,",NULL,"Add for who this story is","well_formed","no_role","high",False
18989,"1. Get listing of job modules 2. Remove version and action column 3. Text to say creating definitions from available modules in the UI is forthcoming, link to https github.com spring projects spring xd wiki Batch Jobs creating a job for how to do this in the command line. 4. Hardcode an association between spring xd out of the box module names and a description. 5. Add button to display the XML file that defines the job module ",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
18989,"1. Get listing of job modules 2. Remove version and action column 3. Text to say creating definitions from available modules in the UI is forthcoming, link to https github.com spring projects spring xd wiki Batch Jobs creating a job for how to do this in the command line. 4. Hardcode an association between spring xd out of the box module names and a description. 5. Add button to display the XML file that defines the job module ",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
18989,"1. Get listing of job modules 2. Remove version and action column 3. Text to say creating definitions from available modules in the UI is forthcoming, link to https github.com spring projects spring xd wiki Batch Jobs creating a job for how to do this in the command line. 4. Hardcode an association between spring xd out of the box module names and a description. 5. Add button to display the XML file that defines the job module ",NULL,NULL,NULL,"1<span class='highlight-text severity-high'>. Get listing of job modules 2. Remove version and action column 3. Text to say creating definitions from available modules in the UI is forthcoming, link to https github.com spring projects spring xd wiki Batch Jobs creating a job for how to do this in the command line. 4. Hardcode an association between spring xd out of the box module names and a description. 5. Add button to display the XML file that defines the job module </span>","minimal","punctuation","high",False
18990,"The standard SimpleHealthIndicator that boot performs a database test that fails in xd container since it does not require the use of a database. ",NULL,"The standard SimpleHealthIndicator that boot performs a database test that fails in xd container since it does not require the use of a database. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18993,"The grunt build for karma unit tests is currently broken with requireJS support on the XD admin app. ",NULL,"The grunt build for karma unit tests is currently broken with requireJS support on the XD admin app. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18302,"As a spring cloud data developer, I'd like to use an in memory stream definition repository, so I'don t have to spin up a store; obviously, this will not persist between application executions, but it will be useful for a simplified development experience. ","As a spring cloud data developer",", I'd like to use an in memory stream definition repository,","so I'don t have to spin up a store; obviously, this will not persist between application executions, but it will be useful for a simplified development experience.","As a spring cloud data developer, I'd like to use an in memory stream definition repository, so I'don t have to spin up a store<span class='highlight-text severity-high'>; obviously, this will not persist between application executions, but it will be useful for a simplified development experience. </span>","minimal","punctuation","high",False
18302,"As a spring cloud data developer, I'd like to use an in memory stream definition repository, so I'don t have to spin up a store; obviously, this will not persist between application executions, but it will be useful for a simplified development experience. ","As a spring cloud data developer",", I'd like to use an in memory stream definition repository,","so I'don t have to spin up a store; obviously, this will not persist between application executions, but it will be useful for a simplified development experience.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18304,"As a s c d developer, I'd like to create ModuleRegistry stubs, so I can create mock streams by interacting with the registry APIs.","As a s c d developer",", I'd like to create ModuleRegistry stubs,","so I can create mock streams by interacting with the registry APIs.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18305,"As a s c d developer, I'd like to establish the foundation to expose REST APIs to interact with the xd admin and likewise perform CRUD operations to maneuver streaming and batch pipelines. ","As a s c d developer",", I'd like to establish the foundation to expose REST APIs to interact with the xd admin and likewise perform CRUD operations to maneuver streaming and batch pipelines.",NULL,"As a s c d developer, I'd like to establish the foundation to expose REST APIs to interact with the xd admin<span class='highlight-text severity-high'> and </span>likewise perform CRUD operations to maneuver streaming and batch pipelines. ","atomic","conjunctions","high",False
18305,"As a s c d developer, I'd like to establish the foundation to expose REST APIs to interact with the xd admin and likewise perform CRUD operations to maneuver streaming and batch pipelines. ","As a s c d developer",", I'd like to establish the foundation to expose REST APIs to interact with the xd admin and likewise perform CRUD operations to maneuver streaming and batch pipelines.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18308,"As Spring XD, I will be able to launch Spring Boot jar files as Diego Tasks. ","As Spring XD",", I will be able to launch Spring Boot jar files as Diego Tasks.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18307,"As a developer, I need to be able to run batch jobs that use the centrally configured job repository to store job state. ","As a developer",", I need to be able to run batch jobs that use the centrally configured job repository to store job state.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18309,"As a developer, I'd like to be able to run a boot jar as a task on CF and obtain the result reliably. ","As a developer",", I'd like to be able to run a boot jar as a task on CF and obtain the result reliably.",NULL,"As a developer, I'd like to be able to run a boot jar as a task on CF<span class='highlight-text severity-high'> and </span>obtain the result reliably. ","atomic","conjunctions","high",False
18309,"As a developer, I'd like to be able to run a boot jar as a task on CF and obtain the result reliably. ","As a developer",", I'd like to be able to run a boot jar as a task on CF and obtain the result reliably.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18739,"stream create foo definition label bar xxxx stream deploy foo properties module.label.yyy zzz seems to work but it does not. The pre validation is correct, but downstream, deployment logic still looks for module.bar instead of module.label ",NULL,"stream create foo definition label bar xxxx stream deploy foo properties module.label.yyy zzz seems to work but it does not. The pre validation is correct, but downstream, deployment logic still looks for module.bar instead of module.label ",NULL,"Add for who this story is","well_formed","no_role","high",False
18739,"stream create foo definition label bar xxxx stream deploy foo properties module.label.yyy zzz seems to work but it does not. The pre validation is correct, but downstream, deployment logic still looks for module.bar instead of module.label ",NULL,"stream create foo definition label bar xxxx stream deploy foo properties module.label.yyy zzz seems to work but it does not. The pre validation is correct, but downstream, deployment logic still looks for module.bar instead of module.label ",NULL,"stream create foo definition label bar xxxx stream deploy foo properties module<span class='highlight-text severity-high'>.label.yyy zzz seems to work but it does not. The pre validation is correct, but downstream, deployment logic still looks for module.bar instead of module.label </span>","minimal","punctuation","high",False
18373,"As a developer, I'd like to update to the 4.1.5 SI release, so I can pickup the latest improvements to message channels.","As a developer",", I'd like to update to the 4.1.5 SI release,","so I can pickup the latest improvements to message channels.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18374,"Some info is obsolete and add more content re. dependency management",NULL,"Some info is obsolete and add more content re. dependency management",NULL,"Add for who this story is","well_formed","no_role","high",False
18374,"Some info is obsolete and add more content re. dependency management",NULL,"Some info is obsolete and add more content re. dependency management",NULL,"Some info is obsolete<span class='highlight-text severity-high'> and </span>add more content re. dependency management","atomic","conjunctions","high",False
18374,"Some info is obsolete and add more content re. dependency management",NULL,"Some info is obsolete and add more content re. dependency management",NULL,"Some info is obsolete and add more content re<span class='highlight-text severity-high'>. dependency management</span>","minimal","punctuation","high",False
18376,"There is an hadoop core 2.5.0 mr1 cdh5.3.3.jar in the lib cdh5 directory we need to remove that from the dist",NULL,"There is an hadoop core 2.5.0 mr1 cdh5.3.3.jar in the lib cdh5 directory we need to remove that from the dist",NULL,"Add for who this story is","well_formed","no_role","high",False
18379,"As a user, I'd like to refer to the analytics tab docs, so I can understand how to use various widgets from streaming pipeline. ","As a user",", I'd like to refer to the analytics tab docs,","so I can understand how to use various widgets from streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18380,"This is in reference to the investigation done as part of XD 2548",NULL,"This is in reference to the investigation done as part of XD 2548",NULL,"Add for who this story is","well_formed","no_role","high",False
18381,"This addresses The plugin issue https www.jfrog.com jira browse GAP 172 to disable spring xd pom.xml",NULL,"This addresses The plugin issue https www.jfrog.com jira browse GAP 172 to disable spring xd pom.xml",NULL,"Add for who this story is","well_formed","no_role","high",False
18382,"As a user, I'd like to refer to OOTB batch jobs and the documentation, so I can jump to the right job section and review details. ","As a user",", I'd like to refer to OOTB batch jobs and the documentation,","so I can jump to the right job section and review details.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18407,"This is a source module for video ingestion the modules captures video frames from a camera or from a video file. For camera, the frames are grabbed from the rtsp video stream. This module will generate message with the frame image encoded with JPEG as the payload. ",NULL,"This is a source module for video ingestion the modules captures video frames from a camera or from a video file. For camera, the frames are grabbed from the rtsp video stream. This module will generate message with the frame image encoded with JPEG as the payload. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18407,"This is a source module for video ingestion the modules captures video frames from a camera or from a video file. For camera, the frames are grabbed from the rtsp video stream. This module will generate message with the frame image encoded with JPEG as the payload. ",NULL,"This is a source module for video ingestion the modules captures video frames from a camera or from a video file. For camera, the frames are grabbed from the rtsp video stream. This module will generate message with the frame image encoded with JPEG as the payload. ",NULL,"This is a source module for video ingestion the modules captures video frames from a camera or from a video file<span class='highlight-text severity-high'>. For camera, the frames are grabbed from the rtsp video stream. This module will generate message with the frame image encoded with JPEG as the payload. </span>","minimal","punctuation","high",False
18444,"As a developer, I'd like to refactor the programmatic means by which the MessageBus transforms the Message so throughput performance can be optimized. ","As a developer",", I'd like to refactor the programmatic means by which the MessageBus transforms the Message","so throughput performance can be optimized.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18410,"Identify and report hotspots while running the load generator source and the throughput sink on Singlenode In Memory Transport Singlenode Kafka Transport Admin Container Kafka Transport",NULL,"Identify and report hotspots while running the load generator source and the throughput sink on Singlenode In Memory Transport Singlenode Kafka Transport Admin Container Kafka Transport",NULL,"Add for who this story is","well_formed","no_role","high",False
18410,"Identify and report hotspots while running the load generator source and the throughput sink on Singlenode In Memory Transport Singlenode Kafka Transport Admin Container Kafka Transport",NULL,"Identify and report hotspots while running the load generator source and the throughput sink on Singlenode In Memory Transport Singlenode Kafka Transport Admin Container Kafka Transport",NULL,"Identify<span class='highlight-text severity-high'> and </span>report hotspots while running the load generator source and the throughput sink on Singlenode In Memory Transport Singlenode Kafka Transport Admin Container Kafka Transport","atomic","conjunctions","high",False
18411,"Complete and submit DEBS 2015 paper as described here http www.debs2015.org camera ready instructions.html",NULL,"Complete and submit DEBS 2015 paper as described here http www.debs2015.org camera ready instructions.html",NULL,"Add for who this story is","well_formed","no_role","high",False
18445,"Code that is in there could be moved to the SparkStreamingModule. Then, as part of a later refactoring, that plugin should be made part of the module and loaded by the module classloader ",NULL,"Code that is in there could be moved to the SparkStreamingModule. Then, as part of a later refactoring, that plugin should be made part of the module and loaded by the module classloader ",NULL,"Add for who this story is","well_formed","no_role","high",False
18445,"Code that is in there could be moved to the SparkStreamingModule. Then, as part of a later refactoring, that plugin should be made part of the module and loaded by the module classloader ",NULL,"Code that is in there could be moved to the SparkStreamingModule. Then, as part of a later refactoring, that plugin should be made part of the module and loaded by the module classloader ",NULL,"Code that is in there could be moved to the SparkStreamingModule. Then, as part of a later refactoring, that plugin should be made part of the module<span class='highlight-text severity-high'> and </span>loaded by the module classloader ","atomic","conjunctions","high",False
18445,"Code that is in there could be moved to the SparkStreamingModule. Then, as part of a later refactoring, that plugin should be made part of the module and loaded by the module classloader ",NULL,"Code that is in there could be moved to the SparkStreamingModule. Then, as part of a later refactoring, that plugin should be made part of the module and loaded by the module classloader ",NULL,"Code that is in there could be moved to the SparkStreamingModule<span class='highlight-text severity-high'>. Then, as part of a later refactoring, that plugin should be made part of the module and loaded by the module classloader </span>","minimal","punctuation","high",False
18474,"Similar to the DetailedModuleDefinitionResource that is returned when querying a single module, but would be returned when listing provided a ?full flag has been turned on ",NULL,"Similar to the DetailedModuleDefinitionResource that is returned when querying a single module, but would be returned when listing provided a ?full flag has been turned on ",NULL,"Add for who this story is","well_formed","no_role","high",False
18741,"As a user, I'd like to have the ability to mass ingest data from various database systems so that I m not restricted with the current approach jdbchdfs that is dependent on JDBC drivers. Spike Scope Identify integration options Collaborate to determine the design Document outcome design specs ","As a user",", I'd like to have the ability to mass ingest data from various database systems","so that I m not restricted with the current approach jdbchdfs that is dependent on JDBC drivers. Spike Scope Identify integration options Collaborate to determine the design Document outcome design specs","As a user, I'd like to have the ability to mass ingest data from various database systems so that I m not restricted with the current approach jdbchdfs that is dependent on JDBC drivers<span class='highlight-text severity-high'>. Spike Scope Identify integration options Collaborate to determine the design Document outcome design specs </span>","minimal","punctuation","high",False
18741,"As a user, I'd like to have the ability to mass ingest data from various database systems so that I m not restricted with the current approach jdbchdfs that is dependent on JDBC drivers. Spike Scope Identify integration options Collaborate to determine the design Document outcome design specs ","As a user",", I'd like to have the ability to mass ingest data from various database systems","so that I m not restricted with the current approach jdbchdfs that is dependent on JDBC drivers. Spike Scope Identify integration options Collaborate to determine the design Document outcome design specs","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18743,"As a user, I'd like to have the option to configure default access control for endpoints so that I can grant access by Admin or Viewer roles.","As a user",", I'd like to have the option to configure default access control for endpoints","so that I can grant access by Admin or Viewer roles.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18742,"As a user, I'd like to have the option of kerberized HDFS sink so that I can leverage Kerberos open source distributed authentication system for secured data writes into Hadoop.","As a user",", I'd like to have the option of kerberized HDFS sink","so that I can leverage Kerberos open source distributed authentication system for secured data writes into Hadoop.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18352,"As a developer, I'd like to use spring cloud config server for spring bus modules, so I can centrally manage external properties.","As a developer",", I'd like to use spring cloud config server for spring bus modules,","so I can centrally manage external properties.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18353,"As a developer, I'd like to migrate the current MASTER branch CI builds to EC2 instances, so I can manage them all in one place reliably.","As a developer",", I'd like to migrate the current MASTER branch CI builds to EC2 instances,","so I can manage them all in one place reliably.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18475,"The current build ships everything that is found in the modules directory, including build artifacts such as build or IDEA .iml files. Restrict the build to only include config , lib at the moment.",NULL,"The current build ships everything that is found in the modules directory, including build artifacts such as build or IDEA .iml files. Restrict the build to only include config , lib at the moment.",NULL,"The current build ships everything that is found in the modules directory, including build artifacts such as build or IDEA <span class='highlight-text severity-high'>.iml files. Restrict the build to only include config , lib at the moment.</span>","minimal","punctuation","high",False
18446,"When using the rest interface to create a Job with an empty description, used to generate the following exception, Definition can not be empty . Now generates XD112E pos 0 Unexpectedly ran out of input^ . The correct error should be, definition cannot be blank or null ",NULL,"When using the rest interface to create a Job with an empty description, used to generate the following exception, Definition can not be empty . Now generates XD112E pos 0 Unexpectedly ran out of input^ . The correct error should be, definition cannot be blank or null ",NULL,"Add for who this story is","well_formed","no_role","high",False
18446,"When using the rest interface to create a Job with an empty description, used to generate the following exception, Definition can not be empty . Now generates XD112E pos 0 Unexpectedly ran out of input^ . The correct error should be, definition cannot be blank or null ",NULL,"When using the rest interface to create a Job with an empty description, used to generate the following exception, Definition can not be empty . Now generates XD112E pos 0 Unexpectedly ran out of input^ . The correct error should be, definition cannot be blank or null ",NULL,"When using the rest interface to create a Job with an empty description, used to generate the following exception, Definition can not be empty <span class='highlight-text severity-high'>. Now generates XD112E pos 0 Unexpectedly ran out of input^ . The correct error should be, definition cannot be blank or null </span>","minimal","punctuation","high",False
18448,"As a user, I'd like to parameterize Merge Options, so I can incrementally consume the delta with the help of megastore. ","As a user",", I'd like to parameterize Merge Options,","so I can incrementally consume the delta with the help of megastore.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18449,"As a user, I'd like to parameterize CodeGen Options, so I can generate code on the fly as needed. ","As a user",", I'd like to parameterize CodeGen Options,","so I can generate code on the fly as needed.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18451,"As a developer, I'd like to document how to nest batch jobs and workflows in XD, so it will be easy for end users to use it as reference. ","As a developer",", I'd like to document how to nest batch jobs and workflows in XD,","so it will be easy for end users to use it as reference.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18467,"As a developer, I'd like to upgrade to SI milestone GA release, so I can synchronize with JMX improvements. This is dependent on SI Milestone and GA release timelines.","As a developer",", I'd like to upgrade to SI milestone GA release,","so I can synchronize with JMX improvements. This is dependent on SI Milestone and GA release timelines.","As a developer, I'd like to upgrade to SI milestone GA release, so I can synchronize with JMX improvements<span class='highlight-text severity-high'>. This is dependent on SI Milestone and GA release timelines.</span>","minimal","punctuation","high",False
18467,"As a developer, I'd like to upgrade to SI milestone GA release, so I can synchronize with JMX improvements. This is dependent on SI Milestone and GA release timelines.","As a developer",", I'd like to upgrade to SI milestone GA release,","so I can synchronize with JMX improvements. This is dependent on SI Milestone and GA release timelines.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18469,"As a user, I'd like to have the configuration option to use an alternative DLQ, so I can publish the message this time with additional headers, including one that contains the exception and stack trace . ","As a user",", I'd like to have the configuration option to use an alternative DLQ,","so I can publish the message this time with additional headers, including one that contains the exception and stack trace .","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18470,"While creation sqoop and providing the password for the sqoop jobs the guid does not mask the password with a .",NULL,"While creation sqoop and providing the password for the sqoop jobs the guid does not mask the password with a .",NULL,"Add for who this story is","well_formed","no_role","high",False
18471,"Characters line t, n, etc. should be either escaped, or rendered as human readable variants in module info eg newline ",NULL,"Characters line t, n, etc. should be either escaped, or rendered as human readable variants in module info eg newline ",NULL,"Add for who this story is","well_formed","no_role","high",False
18471,"Characters line t, n, etc. should be either escaped, or rendered as human readable variants in module info eg newline ",NULL,"Characters line t, n, etc. should be either escaped, or rendered as human readable variants in module info eg newline ",NULL,"Characters line t, n, etc. should be either escaped,<span class='highlight-text severity-high'> or </span>rendered as human readable variants in module info eg newline ","atomic","conjunctions","high",False
18471,"Characters line t, n, etc. should be either escaped, or rendered as human readable variants in module info eg newline ",NULL,"Characters line t, n, etc. should be either escaped, or rendered as human readable variants in module info eg newline ",NULL,"Characters line t, n, etc<span class='highlight-text severity-high'>. should be either escaped, or rendered as human readable variants in module info eg newline </span>","minimal","punctuation","high",False
18472,"As a developer, I'd like to certify Spring XD against PHD 3.0, so I can synchronize with the latest ODP based bits. ","As a developer",", I'd like to certify Spring XD against PHD 3.0,","so I can synchronize with the latest ODP based bits.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18113,"As a user, I'd like to use SFTP source module, so I can create streaming pipeline with it. However, I cannot see SFTP as OOTB module listed on module list and as well as the module bits are not available in maven repo http repo.spring.io libs snapshot org springframework cloud stream module . ","As a user",", I'd like to use SFTP","source module, so I can create streaming pipeline with it. However, I cannot see SFTP as OOTB module listed on module list and as well as the module bits are not available in maven repo http repo.spring.io libs snapshot org springframework cloud stream module .","As a user, I'd like to use SFTP source module, so I can create streaming pipeline with it<span class='highlight-text severity-high'>. However, I cannot see SFTP as OOTB module listed on module list and as well as the module bits are not available in maven repo http repo.spring.io libs snapshot org springframework cloud stream module . </span>","minimal","punctuation","high",False
18113,"As a user, I'd like to use SFTP source module, so I can create streaming pipeline with it. However, I cannot see SFTP as OOTB module listed on module list and as well as the module bits are not available in maven repo http repo.spring.io libs snapshot org springframework cloud stream module . ","As a user",", I'd like to use SFTP","source module, so I can create streaming pipeline with it. However, I cannot see SFTP as OOTB module listed on module list and as well as the module bits are not available in maven repo http repo.spring.io libs snapshot org springframework cloud stream module .","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18115,"As a developer, I'd like to get rid off XDRuntimeException from XD.","As a developer",", I'd like to get rid off XDRuntimeException from XD.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18114,"As a user, I'd like to use the latest release of gemfire sink, so I can create a streaming pipeline to land data in gemfire. ","As a user",", I'd like to use the latest release of gemfire sink,","so I can create a streaming pipeline to land data in gemfire.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18118,"As a s c d user, I'd like to deploy data flow on YARN, so I can reuse the existing Hadoop cluster and leverage data flow features to build streaming or batch pipelines.","As a s c d user",", I'd like to deploy data flow on YARN,","so I can reuse the existing Hadoop cluster and leverage data flow features to build streaming or batch pipelines.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18117,"As a developer, I'd like to port file module from XD to s c s repo, so I can use it as source module to build streaming pipeline.","As a developer",", I'd like to port file module from XD to s c s repo,","so I can use it as source module to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18121,"As a developer, I'd like to port rich gauge module from XD to s c s repo, so I can use it as sink module to build streaming pipeline.","As a developer",", I'd like to port rich gauge module from XD to s c s repo,","so I can use it as sink module to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18119,"As a s c d user, I'd like to have runtime info as shell command, so I can use this to list the details about the module such as host , port and the like.","As a s c d user",", I'd like to have runtime info as shell command,","so I can use this to list the details about the module such as host , port and the like.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18125,"As a XD user, I'd like to have job DSL as an option, so I can leverage the DSL to create comprehensive workflows and orchestrate jobs. ","As a XD user",", I'd like to have job DSL as an option,","so I can leverage the DSL to create comprehensive workflows and orchestrate jobs.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18122,"As a developer, I'd like to port gauge module from XD to s c s repo, so I can use it as sink module to build streaming pipeline.","As a developer",", I'd like to port gauge module from XD to s c s repo,","so I can use it as sink module to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18123,"As a developer, I'd like to port aggregate counter module from XD to s c s repo, so I can use it as sink module to build streaming pipeline.","As a developer",", I'd like to port aggregate counter module from XD to s c s repo,","so I can use it as sink module to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18124,"As a developer, I'd like to port field value counter module from XD to s c s repo, so I can use it as sink module to build streaming pipeline.","As a developer",", I'd like to port field value counter module from XD to s c s repo,","so I can use it as sink module to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18126,"As a developer, I'd like to port Log module from XD to s c s repo, so I can use it as sink modules to build streaming pipeline.","As a developer",", I'd like to port Log module from XD to s c s repo,","so I can use it as sink modules to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18127,"We need to make sure that JMX MBean names are unique, even in the case of labeled modules. The following stream fails for example http filter filter2 filter log A good candidate could be stream name group module label.",NULL,"We need to make sure that JMX MBean names are unique, even in the case of labeled modules. The following stream fails for example http filter filter2 filter log A good candidate could be stream name group module label.",NULL,"Add for who this story is","well_formed","no_role","high",False
18127,"We need to make sure that JMX MBean names are unique, even in the case of labeled modules. The following stream fails for example http filter filter2 filter log A good candidate could be stream name group module label.",NULL,"We need to make sure that JMX MBean names are unique, even in the case of labeled modules. The following stream fails for example http filter filter2 filter log A good candidate could be stream name group module label.",NULL,"We need to make sure that JMX MBean names are unique, even in the case of labeled modules<span class='highlight-text severity-high'>. The following stream fails for example http filter filter2 filter log A good candidate could be stream name group module label.</span>","minimal","punctuation","high",False
18128,"Described in https github.com spring cloud spring cloud stream issues 144 As a developer, I want Input enpoints to be started after all the beans in the context, so that received messages can be delivered to components. ",NULL,"Described in https github.com spring cloud spring cloud stream issues 144 As a developer, I want Input enpoints to be started after all the beans in the context,","so that received messages can be delivered to components.","Add for who this story is","well_formed","no_role","high",False
18128,"Described in https github.com spring cloud spring cloud stream issues 144 As a developer, I want Input enpoints to be started after all the beans in the context, so that received messages can be delivered to components. ",NULL,"Described in https github.com spring cloud spring cloud stream issues 144 As a developer, I want Input enpoints to be started after all the beans in the context,","so that received messages can be delivered to components.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18131,"As an s c d user, I'd like to have the option to use named channels , so I can create streaming pipelines without source or sink modules. ","As an s c d user",", I'd like to have the option to use named channels ,","so I can create streaming pipelines without source or sink modules.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18130,"As a s c d developer, I'd like to add test coverage for StreamController , so I can verify API contracts at build time. ","As a s c d developer",", I'd like to add test coverage for StreamController ,","so I can verify API contracts at build time.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18137,"As an s c d developer, I'd like to move redis Rule to a separate repo, so I can consume the test fixtures in different projects.","As an s c d developer",", I'd like to move redis Rule to a separate repo,","so I can consume the test fixtures in different projects.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18139,"As an s c d user, I'd like to have tab completion on shell, so I can interact with the modules and its available options.","As an s c d user",", I'd like to have tab completion on shell,","so I can interact with the modules and its available options.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18135,"As an s c d developer, I'd like to move rabbit Rule to a separate repo, so I can consume the test fixtures in different projects.","As an s c d developer",", I'd like to move rabbit Rule to a separate repo,","so I can consume the test fixtures in different projects.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18136,"As an s c d developer, I'd like to move kafka Rule to a separate repo, so I can consume the test fixtures in different projects.","As an s c d developer",", I'd like to move kafka Rule to a separate repo,","so I can consume the test fixtures in different projects.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18140,"As a spring cloud stream user, I'd like to build stream definitions using dot delimited syntax for resolving properties for Tuple and JSON.","As a spring cloud stream user",", I'd like to build stream definitions using dot delimited syntax for resolving properties for Tuple and JSON.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18133,"As an s c d user, I'd like to take advantage of admin running on variety of platforms such as Lattice, YARN or CF. I'd like to access REST APIs consistently across these platforms.","As an s c d user",", I'd like to take advantage of admin running on variety of platforms such as Lattice, YARN or CF. I'd like to access REST APIs consistently across these platforms.",NULL,"As an s c d user, I'd like to take advantage of admin running on variety of platforms such as Lattice, YARN<span class='highlight-text severity-high'> or </span>CF. I'd like to access REST APIs consistently across these platforms.","atomic","conjunctions","high",False
18133,"As an s c d user, I'd like to take advantage of admin running on variety of platforms such as Lattice, YARN or CF. I'd like to access REST APIs consistently across these platforms.","As an s c d user",", I'd like to take advantage of admin running on variety of platforms such as Lattice, YARN or CF. I'd like to access REST APIs consistently across these platforms.",NULL,"As an s c d user, I'd like to take advantage of admin running on variety of platforms such as Lattice, YARN or CF<span class='highlight-text severity-high'>. I'd like to access REST APIs consistently across these platforms.</span>","minimal","punctuation","high",False
18133,"As an s c d user, I'd like to take advantage of admin running on variety of platforms such as Lattice, YARN or CF. I'd like to access REST APIs consistently across these platforms.","As an s c d user",", I'd like to take advantage of admin running on variety of platforms such as Lattice, YARN or CF. I'd like to access REST APIs consistently across these platforms.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18138,"As an s c d user, I'd like to deploy s c d on Mesos.","As an s c d user",", I'd like to deploy s c d on Mesos.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18142,"As an XD user, I'd like have support restart an existing composed job, so I could re launch it at will.","As an XD user",", I'd like have support restart an existing composed job,","so I could re launch it at will.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18145,"As an XD user, I'd like to have a REST endpoint that returns job composition graph, so I can use it to build visual representation of parent child relationship. ","As an XD user",", I'd like to have a REST endpoint that returns job composition graph,","so I can use it to build visual representation of parent child relationship.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18143,"As an XD user, I'd like to click to go the detail page of the job page whether or not the selected entity is singular or part of a composed job.","As an XD user",", I'd like to click to go the detail page of the job page whether or not the selected entity is singular or part of a composed job.",NULL,"As an XD user, I'd like to click to go the detail page of the job page whether<span class='highlight-text severity-high'> or </span>not the selected entity is singular or part of a composed job.","atomic","conjunctions","high",False
18143,"As an XD user, I'd like to click to go the detail page of the job page whether or not the selected entity is singular or part of a composed job.","As an XD user",", I'd like to click to go the detail page of the job page whether or not the selected entity is singular or part of a composed job.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19337,"int channel id input int object to json transformer input channel input output channel output int channel id output ",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19337,"int channel id input int object to json transformer input channel input output channel output int channel id output ",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
18476,"As a user, I'd like to have an option to have the hdfs sink use Syncable writes to provide better resiliency in the case of sink container failures. I m willing to accept the performance penalty if I choose this option. ","As a user",", I'd like to have an option to have the hdfs sink use Syncable writes to provide better resiliency in the case of sink container failures. I m willing to accept the performance penalty if I choose this option.",NULL,"As a user, I'd like to have an option to have the hdfs sink use Syncable writes to provide better resiliency in the case of sink container failures<span class='highlight-text severity-high'>. I m willing to accept the performance penalty if I choose this option. </span>","minimal","punctuation","high",False
18476,"As a user, I'd like to have an option to have the hdfs sink use Syncable writes to provide better resiliency in the case of sink container failures. I m willing to accept the performance penalty if I choose this option. ","As a user",", I'd like to have an option to have the hdfs sink use Syncable writes to provide better resiliency in the case of sink container failures. I m willing to accept the performance penalty if I choose this option.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18480,"http localhost 9393 admin ui streams definitions test deploy ",NULL,"http localhost 9393 admin ui streams definitions test deploy ",NULL,"Add for who this story is","well_formed","no_role","high",False
18477,"As a developer, I'd like to add support for explicit partition count configuration, so I can use this option to cleverly route the payload to the intended consumer module .","As a developer",", I'd like to add support for explicit partition count configuration,","so I can use this option to cleverly route the payload to the intended consumer module .","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18496,"Currently when modules are composed to a single application context, properties are not inherited. https github.com spring projects spring xd wiki Modules placeholders available to all modules ",NULL,"Currently when modules are composed to a single application context, properties are not inherited. https github.com spring projects spring xd wiki Modules placeholders available to all modules ",NULL,"Add for who this story is","well_formed","no_role","high",False
18496,"Currently when modules are composed to a single application context, properties are not inherited. https github.com spring projects spring xd wiki Modules placeholders available to all modules ",NULL,"Currently when modules are composed to a single application context, properties are not inherited. https github.com spring projects spring xd wiki Modules placeholders available to all modules ",NULL,"Currently when modules are composed to a single application context, properties are not inherited<span class='highlight-text severity-high'>. https github.com spring projects spring xd wiki Modules placeholders available to all modules </span>","minimal","punctuation","high",False
18497,"The current jdbchdfs job does not take advantage of all the features available to write into HDFS provided by Spring Hadoop s DataStoreWriter implementations, such as partitioning. Update the jdbchdfs job to use int hadoop store writer similar to the HDFS Sink inside a new ItemWriter implementation ",NULL,"The current jdbchdfs job does not take advantage of all the features available to write into HDFS provided by Spring Hadoop s DataStoreWriter implementations, such as partitioning. Update the jdbchdfs job to use int hadoop store writer similar to the HDFS Sink inside a new ItemWriter implementation ",NULL,"Add for who this story is","well_formed","no_role","high",False
18497,"The current jdbchdfs job does not take advantage of all the features available to write into HDFS provided by Spring Hadoop s DataStoreWriter implementations, such as partitioning. Update the jdbchdfs job to use int hadoop store writer similar to the HDFS Sink inside a new ItemWriter implementation ",NULL,"The current jdbchdfs job does not take advantage of all the features available to write into HDFS provided by Spring Hadoop s DataStoreWriter implementations, such as partitioning. Update the jdbchdfs job to use int hadoop store writer similar to the HDFS Sink inside a new ItemWriter implementation ",NULL,"The current jdbchdfs job does not take advantage of all the features available to write into HDFS provided by Spring Hadoop s DataStoreWriter implementations, such as partitioning<span class='highlight-text severity-high'>. Update the jdbchdfs job to use int hadoop store writer similar to the HDFS Sink inside a new ItemWriter implementation </span>","minimal","punctuation","high",False
18479,"I had a custom module with a typo base packages base packages com.acme.config The module deploys without error but the stream hangs since the channels, etc. are not found in the stream plugin. Very hard to debug. ",NULL,"I had a custom module with a typo base packages base packages com.acme.config The module deploys without error but the stream hangs since the channels, etc. are not found in the stream plugin. Very hard to debug. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18479,"I had a custom module with a typo base packages base packages com.acme.config The module deploys without error but the stream hangs since the channels, etc. are not found in the stream plugin. Very hard to debug. ",NULL,"I had a custom module with a typo base packages base packages com.acme.config The module deploys without error but the stream hangs since the channels, etc. are not found in the stream plugin. Very hard to debug. ",NULL,"I had a custom module with a typo base packages base packages com<span class='highlight-text severity-high'>.acme.config The module deploys without error but the stream hangs since the channels, etc. are not found in the stream plugin. Very hard to debug. </span>","minimal","punctuation","high",False
18498,"As a developer, I'd like to decouple execution context from job launch lifecycle so that we can avoid CL and serialization errors. This fix needs to be formally applied in Spring Batch itself. XD will upgrade to Batch release in order to inherit this functionality; hence, the current workaround with XD 2486 needs reafctored. ","As a developer",", I'd like to decouple execution context from job launch lifecycle so that we can avoid CL and serialization errors. This fix needs to be formally applied in Spring Batch itself. XD will upgrade to Batch release","in order to inherit this functionality; hence, the current workaround with XD 2486 needs reafctored.","As a developer, I'd like to decouple execution context from job launch lifecycle so that we can avoid CL and serialization errors<span class='highlight-text severity-high'>. This fix needs to be formally applied in Spring Batch itself. XD will upgrade to Batch release in order to inherit this functionality; hence, the current workaround with XD 2486 needs reafctored. </span>","minimal","punctuation","high",False
18498,"As a developer, I'd like to decouple execution context from job launch lifecycle so that we can avoid CL and serialization errors. This fix needs to be formally applied in Spring Batch itself. XD will upgrade to Batch release in order to inherit this functionality; hence, the current workaround with XD 2486 needs reafctored. ","As a developer",", I'd like to decouple execution context from job launch lifecycle so that we can avoid CL and serialization errors. This fix needs to be formally applied in Spring Batch itself. XD will upgrade to Batch release","in order to inherit this functionality; hence, the current workaround with XD 2486 needs reafctored.","As a developer, I'd like to decouple execution context from job launch lifecycle<span class='highlight-text severity-high'> so that </span>we can avoid CL and serialization errors. This fix needs to be formally applied in Spring Batch itself. XD will upgrade to Batch release<span class='highlight-text severity-high'> in order to </span>inherit this functionality; hence, the current workaround with XD 2486 needs reafctored. ","minimal","indicator_repetition","high",False
18498,"As a developer, I'd like to decouple execution context from job launch lifecycle so that we can avoid CL and serialization errors. This fix needs to be formally applied in Spring Batch itself. XD will upgrade to Batch release in order to inherit this functionality; hence, the current workaround with XD 2486 needs reafctored. ","As a developer",", I'd like to decouple execution context from job launch lifecycle so that we can avoid CL and serialization errors. This fix needs to be formally applied in Spring Batch itself. XD will upgrade to Batch release","in order to inherit this functionality; hence, the current workaround with XD 2486 needs reafctored.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18499,". gradlew install fails for spring xd extension batch and spring xd extension reactor. The first case is a simple update to gradle build extensions.gradle. The 2nd causes several compilation errors that are not trivial for a Reactor noob. ",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
18499,". gradlew install fails for spring xd extension batch and spring xd extension reactor. The first case is a simple update to gradle build extensions.gradle. The 2nd causes several compilation errors that are not trivial for a Reactor noob. ",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
18499,". gradlew install fails for spring xd extension batch and spring xd extension reactor. The first case is a simple update to gradle build extensions.gradle. The 2nd causes several compilation errors that are not trivial for a Reactor noob. ",NULL,NULL,NULL,"<span class='highlight-text severity-high'>. gradlew install fails for spring xd extension batch and spring xd extension reactor. The first case is a simple update to gradle build extensions.gradle. The 2nd causes several compilation errors that are not trivial for a Reactor noob. </span>","minimal","punctuation","high",False
18517,"As a developer, I'd like to research and Identify the EC2 infrastructure required so that I can run performance tests on Kafka. ","As a developer",", I'd like to research and Identify the EC2 infrastructure required","so that I can run performance tests on Kafka.","As a developer, I'd like to research<span class='highlight-text severity-high'> and </span>Identify the EC2 infrastructure required so that I can run performance tests on Kafka. ","atomic","conjunctions","high",False
18517,"As a developer, I'd like to research and Identify the EC2 infrastructure required so that I can run performance tests on Kafka. ","As a developer",", I'd like to research and Identify the EC2 infrastructure required","so that I can run performance tests on Kafka.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18243,"As a s c d developer, I'd like to add support to deploy YARN App into HDFS automatically, so I can have the xd admin orchestrate overall deployment by leveraging the manifest to deploy where and with what assets.","As a s c d developer",", I'd like to add support to deploy YARN App into HDFS automatically,","so I can have the xd admin orchestrate overall deployment by leveraging the manifest to deploy where and with what assets.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19148,"See https github.com spring projects spring xd pull 415 issuecomment 29024329 This test should be created so that it reproduces the failure and then turned off, so that when we switch to the new HDFS writing code ",NULL,"See https github.com spring projects spring xd pull 415 issuecomment 29024329 This test should be created","so that it reproduces the failure and then turned off, so that when we switch to the new HDFS writing code","Add for who this story is","well_formed","no_role","high",False
19148,"See https github.com spring projects spring xd pull 415 issuecomment 29024329 This test should be created so that it reproduces the failure and then turned off, so that when we switch to the new HDFS writing code ",NULL,"See https github.com spring projects spring xd pull 415 issuecomment 29024329 This test should be created","so that it reproduces the failure and then turned off, so that when we switch to the new HDFS writing code","See https github.com spring projects spring xd pull 415 issuecomment 29024329 This test should be created<span class='highlight-text severity-high'> so that </span>it reproduces the failure and then turned off,<span class='highlight-text severity-high'> so that </span>when we switch to the new HDFS writing code ","minimal","indicator_repetition","high",False
19148,"See https github.com spring projects spring xd pull 415 issuecomment 29024329 This test should be created so that it reproduces the failure and then turned off, so that when we switch to the new HDFS writing code ",NULL,"See https github.com spring projects spring xd pull 415 issuecomment 29024329 This test should be created","so that it reproduces the failure and then turned off, so that when we switch to the new HDFS writing code","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19492,"Expected usage ATM would be sink channel called foo http transform expression payload.toUppercase foo source channel called foo foo count file",NULL,"Expected usage ATM would be sink channel called foo http transform expression payload.toUppercase foo source channel called foo foo count file",NULL,"Add for who this story is","well_formed","no_role","high",False
18201,"As a s c d developer, I'd like to refactor CC SPI'deployer with CF java client, so I can improve the overall design and performance. ","As a s c d developer",", I'd like to refactor CC SPI'deployer with CF java client,","so I can improve the overall design and performance.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18202,"As a s c d developer, I'd like to pass any overrides via external config file, so I can influence and override the default module configurations. ex module resolution from a different maven coordinate . ","As a s c d developer",", I'd like to pass any overrides via external config file,","so I can influence and override the default module configurations. ex module resolution from a different maven coordinate .","As a s c d developer, I'd like to pass any overrides via external config file, so I can influence and override the default module configurations<span class='highlight-text severity-high'>. ex module resolution from a different maven coordinate . </span>","minimal","punctuation","high",False
18202,"As a s c d developer, I'd like to pass any overrides via external config file, so I can influence and override the default module configurations. ex module resolution from a different maven coordinate . ","As a s c d developer",", I'd like to pass any overrides via external config file,","so I can influence and override the default module configurations. ex module resolution from a different maven coordinate .","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18206,"As a s c d developer, I'd like to move kafka module from XD to s c s repo, so I can use it as source to build streaming pipeline.","As a s c d developer",", I'd like to move kafka module from XD to s c s repo,","so I can use it as source to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18204,"As a s c d developer, I'd like to move rabbit module from XD to s c s repo, so I can use it as sink to build streaming pipeline.","As a s c d developer",", I'd like to move rabbit module from XD to s c s repo,","so I can use it as sink to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18207,"As a s c s m developer, I'd like to move jdbc module from XD to s c s repo, so I can use it as sink to build streaming pipeline. See also XD 2250","As a s c","s m developer, I'd like to move jdbc module from XD to s c s repo,","so I can use it as sink to build streaming pipeline. See also XD 2250","As a s c s m developer, I'd like to move jdbc module from XD to s c s repo, so I can use it as sink to build streaming pipeline<span class='highlight-text severity-high'>. See also XD 2250</span>","minimal","punctuation","high",False
18207,"As a s c s m developer, I'd like to move jdbc module from XD to s c s repo, so I can use it as sink to build streaming pipeline. See also XD 2250","As a s c","s m developer, I'd like to move jdbc module from XD to s c s repo,","so I can use it as sink to build streaming pipeline. See also XD 2250","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18205,"As a s c d developer, I'd like to move kafka module from XD to s c s repo, so I can use it as sink to build streaming pipeline.","As a s c d developer",", I'd like to move kafka module from XD to s c s repo,","so I can use it as sink to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18208,"As a s c s developer, I'd like to support XD like features where modules bind to incoming messages via expressions or other mechanism, so I can bind message properties to every microservice modules. ","As a s c","s developer, I'd like to support XD like features where modules bind to incoming messages via expressions or other mechanism,","so I can bind message properties to every microservice modules.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18212,"As a s c d developer, I'd like to add hdfs sink to module registry, so I can use this module to build streaming pipeline and write to Hadoop.","As a s c d developer",", I'd like to add hdfs sink to module registry,","so I can use this module to build streaming pipeline and write to Hadoop.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18210,"As a XD developer, I'd like to explore repository options for composed jobs , so I have the leverage to read write composed job definitions.","As a XD developer",", I'd like to explore repository options for composed jobs ,","so I have the leverage to read write composed job definitions.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19165,"Restore foo bar as well as foo bar Validation of values should be done as a separate story",NULL,"Restore foo bar as well as foo bar Validation of values should be done as a separate story",NULL,"Add for who this story is","well_formed","no_role","high",False
19163,"Currently, the JobRepoTests use the same batch job repository that the XD runtime uses. Since the batch job repo doesn t delete the job instances, there would be stale data from this test. ",NULL,"Currently, the JobRepoTests use the same batch job repository that the XD runtime uses. Since the batch job repo doesn t delete the job instances, there would be stale data from this test. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19163,"Currently, the JobRepoTests use the same batch job repository that the XD runtime uses. Since the batch job repo doesn t delete the job instances, there would be stale data from this test. ",NULL,"Currently, the JobRepoTests use the same batch job repository that the XD runtime uses. Since the batch job repo doesn t delete the job instances, there would be stale data from this test. ",NULL,"Currently, the JobRepoTests use the same batch job repository that the XD runtime uses<span class='highlight-text severity-high'>. Since the batch job repo doesn t delete the job instances, there would be stale data from this test. </span>","minimal","punctuation","high",False
19166,"Currently, the tests use instantiated DistributedJobService object instead of a simple mock. We can just use mock object for the tests.",NULL,"Currently, the tests use instantiated DistributedJobService object instead of a simple mock. We can just use mock object for the tests.",NULL,"Add for who this story is","well_formed","no_role","high",False
19166,"Currently, the tests use instantiated DistributedJobService object instead of a simple mock. We can just use mock object for the tests.",NULL,"Currently, the tests use instantiated DistributedJobService object instead of a simple mock. We can just use mock object for the tests.",NULL,"Currently, the tests use instantiated DistributedJobService object instead of a simple mock<span class='highlight-text severity-high'>. We can just use mock object for the tests.</span>","minimal","punctuation","high",False
18213,"As a s c d developer, I'd like to document Running on Cloud Foundry https github.com spring cloud spring cloud dataflow running on cloud foundry section in README, so it can be publicly available as deployment guideline.","As a s c d developer",", I'd like to document Running on Cloud Foundry https github.com spring cloud spring cloud dataflow running on cloud foundry section in README,","so it can be publicly available as deployment guideline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18885,"As a consequence of not fixing XD 1289, we should document keys of the form xd.stream.name that are available to users xd. stream job .name and xd.container.??? come to mind, there may be others","As a consequence","of not fixing XD 1289, we should document keys of the form xd.stream.name that are available to users xd. stream job .name and xd.container.??? come to mind, there may be others",NULL,"As a consequence of not fixing XD 1289, we should document keys of the form xd<span class='highlight-text severity-high'>.stream.name that are available to users xd. stream job .name and xd.container.??? come to mind, there may be others</span>","minimal","punctuation","high",False
18885,"As a consequence of not fixing XD 1289, we should document keys of the form xd.stream.name that are available to users xd. stream job .name and xd.container.??? come to mind, there may be others","As a consequence","of not fixing XD 1289, we should document keys of the form xd.stream.name that are available to users xd. stream job .name and xd.container.??? come to mind, there may be others",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18883,"Hitting this issue in Chrome http stackoverflow.com questions 22891611 google font varela round doesnt support font weight in chrome Looks like Chrome has some issues with making text bold if the font does not explicitly support it. ",NULL,"Hitting this issue in Chrome http stackoverflow.com questions 22891611 google font varela round doesnt support font weight in chrome Looks like Chrome has some issues with making text bold if the font does not explicitly support it. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18886,"The TwitterSearch does a case insensitive search. Tests need to do a insensitive check for the keywords in the search result.",NULL,"The TwitterSearch does a case insensitive search. Tests need to do a insensitive check for the keywords in the search result.",NULL,"Add for who this story is","well_formed","no_role","high",False
18886,"The TwitterSearch does a case insensitive search. Tests need to do a insensitive check for the keywords in the search result.",NULL,"The TwitterSearch does a case insensitive search. Tests need to do a insensitive check for the keywords in the search result.",NULL,"The TwitterSearch does a case insensitive search<span class='highlight-text severity-high'>. Tests need to do a insensitive check for the keywords in the search result.</span>","minimal","punctuation","high",False
18244,"As a s c d user, I'd like to have the option to support passing definition parameters into YARN container, so I can effectively use those params within the module running inside the container.","As a s c d user",", I'd like to have the option to support passing definition parameters into YARN container,","so I can effectively use those params within the module running inside the container.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18926,"There are some properties files in the config directory that no longer are needed. We should clean that up and also remove update any documentation references to these files",NULL,"There are some properties files in the config directory that no longer are needed. We should clean that up and also remove update any documentation references to these files",NULL,"Add for who this story is","well_formed","no_role","high",False
18926,"There are some properties files in the config directory that no longer are needed. We should clean that up and also remove update any documentation references to these files",NULL,"There are some properties files in the config directory that no longer are needed. We should clean that up and also remove update any documentation references to these files",NULL,"There are some properties files in the config directory that no longer are needed. We should clean that up<span class='highlight-text severity-high'> and </span>also remove update any documentation references to these files","atomic","conjunctions","high",False
18926,"There are some properties files in the config directory that no longer are needed. We should clean that up and also remove update any documentation references to these files",NULL,"There are some properties files in the config directory that no longer are needed. We should clean that up and also remove update any documentation references to these files",NULL,"There are some properties files in the config directory that no longer are needed<span class='highlight-text severity-high'>. We should clean that up and also remove update any documentation references to these files</span>","minimal","punctuation","high",False
18928,"In this case we will use environment variables to set the JDBC sink settings. Thus we will just remove code.",NULL,"In this case we will use environment variables to set the JDBC sink settings. Thus we will just remove code.",NULL,"Add for who this story is","well_formed","no_role","high",False
18928,"In this case we will use environment variables to set the JDBC sink settings. Thus we will just remove code.",NULL,"In this case we will use environment variables to set the JDBC sink settings. Thus we will just remove code.",NULL,"In this case we will use environment variables to set the JDBC sink settings<span class='highlight-text severity-high'>. Thus we will just remove code.</span>","minimal","punctuation","high",False
19737,"Building XD should not be part of the out first out of the box experience, but we should include some instructions on what targets are available, such as distXD.",NULL,"Building XD should not be part of the out first out of the box experience, but we should include some instructions on what targets are available, such as distXD.",NULL,"Add for who this story is","well_formed","no_role","high",False
18082,"As a developer, I'd like to upgrade to SI 4.2.2.GA release, so I can leverage the latest improvements.","As a developer",", I'd like to upgrade to SI 4.2.2.GA release,","so I can leverage the latest improvements.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18081,"When a problem occurs connecting to admin, we just get Unable to contact Data Flow Admin even if the connection is successful and some problem occurs when interpreting the result. The exception is eaten. Log an error including the exception. Currently investigating an NPE in DataFlowTemplate line 77.",NULL,"When a problem occurs connecting to admin, we just get Unable to contact Data Flow Admin even if the connection is successful and some problem occurs when interpreting the result. The exception is eaten. Log an error including the exception. Currently investigating an NPE in DataFlowTemplate line 77.",NULL,"Add for who this story is","well_formed","no_role","high",False
18081,"When a problem occurs connecting to admin, we just get Unable to contact Data Flow Admin even if the connection is successful and some problem occurs when interpreting the result. The exception is eaten. Log an error including the exception. Currently investigating an NPE in DataFlowTemplate line 77.",NULL,"When a problem occurs connecting to admin, we just get Unable to contact Data Flow Admin even if the connection is successful and some problem occurs when interpreting the result. The exception is eaten. Log an error including the exception. Currently investigating an NPE in DataFlowTemplate line 77.",NULL,"When a problem occurs connecting to admin, we just get Unable to contact Data Flow Admin even if the connection is successful<span class='highlight-text severity-high'> and </span>some problem occurs when interpreting the result. The exception is eaten. Log an error including the exception. Currently investigating an NPE in DataFlowTemplate line 77.","atomic","conjunctions","high",False
18081,"When a problem occurs connecting to admin, we just get Unable to contact Data Flow Admin even if the connection is successful and some problem occurs when interpreting the result. The exception is eaten. Log an error including the exception. Currently investigating an NPE in DataFlowTemplate line 77.",NULL,"When a problem occurs connecting to admin, we just get Unable to contact Data Flow Admin even if the connection is successful and some problem occurs when interpreting the result. The exception is eaten. Log an error including the exception. Currently investigating an NPE in DataFlowTemplate line 77.",NULL,"When a problem occurs connecting to admin, we just get Unable to contact Data Flow Admin even if the connection is successful and some problem occurs when interpreting the result<span class='highlight-text severity-high'>. The exception is eaten. Log an error including the exception. Currently investigating an NPE in DataFlowTemplate line 77.</span>","minimal","punctuation","high",False
18216,"Create Parent pom file for build Create .settings file Migrate Timestamp task from SCSM to SCTM. ",NULL,"Create Parent pom file for build Create .settings file Migrate Timestamp task from SCSM to SCTM. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18219,"Currently we handle only a single page response from CC SPI list requests, but potentially there could be multiple ones.",NULL,"Currently we handle only a single page response from CC SPI list requests, but potentially there could be multiple ones.",NULL,"Add for who this story is","well_formed","no_role","high",False
18217,"As a module author, I would like to apply RxJava processor module with spring cloud stream. ","As a module author",", I would like to apply RxJava processor module with spring cloud stream.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18215,"We currently use fixed paths like spring cloud data yarn spring cloud data yarn appmaster target spring cloud data yarn appmaster 1.0.0.BUILD SNAPSHOT.jar in yml files. Need to make define version during a build and allow to override location of those files.",NULL,"We currently use fixed paths like spring cloud data yarn spring cloud data yarn appmaster target spring cloud data yarn appmaster 1.0.0.BUILD SNAPSHOT.jar in yml files. Need to make define version during a build and allow to override location of those files.",NULL,"Add for who this story is","well_formed","no_role","high",False
18215,"We currently use fixed paths like spring cloud data yarn spring cloud data yarn appmaster target spring cloud data yarn appmaster 1.0.0.BUILD SNAPSHOT.jar in yml files. Need to make define version during a build and allow to override location of those files.",NULL,"We currently use fixed paths like spring cloud data yarn spring cloud data yarn appmaster target spring cloud data yarn appmaster 1.0.0.BUILD SNAPSHOT.jar in yml files. Need to make define version during a build and allow to override location of those files.",NULL,"We currently use fixed paths like spring cloud data yarn spring cloud data yarn appmaster target spring cloud data yarn appmaster 1.0.0.BUILD SNAPSHOT.jar in yml files. Need to make define version during a build<span class='highlight-text severity-high'> and </span>allow to override location of those files.","atomic","conjunctions","high",False
18215,"We currently use fixed paths like spring cloud data yarn spring cloud data yarn appmaster target spring cloud data yarn appmaster 1.0.0.BUILD SNAPSHOT.jar in yml files. Need to make define version during a build and allow to override location of those files.",NULL,"We currently use fixed paths like spring cloud data yarn spring cloud data yarn appmaster target spring cloud data yarn appmaster 1.0.0.BUILD SNAPSHOT.jar in yml files. Need to make define version during a build and allow to override location of those files.",NULL,"We currently use fixed paths like spring cloud data yarn spring cloud data yarn appmaster target spring cloud data yarn appmaster 1<span class='highlight-text severity-high'>.0.0.BUILD SNAPSHOT.jar in yml files. Need to make define version during a build and allow to override location of those files.</span>","minimal","punctuation","high",False
18218,"Upload of module launcher bits is slow because we do not take into account the CC cache. To fix this we need to use an async upload, and to somehow generate the SHA, etc, for the Module Launcher so that CC can pre empt uploading all the bits every time.",NULL,"Upload of module launcher bits is slow because we do not take into account the CC cache. To fix this we need to use an async upload, and to somehow generate the SHA, etc, for the Module Launcher","so that CC can pre empt uploading all the bits every time.","Add for who this story is","well_formed","no_role","high",False
18218,"Upload of module launcher bits is slow because we do not take into account the CC cache. To fix this we need to use an async upload, and to somehow generate the SHA, etc, for the Module Launcher so that CC can pre empt uploading all the bits every time.",NULL,"Upload of module launcher bits is slow because we do not take into account the CC cache. To fix this we need to use an async upload, and to somehow generate the SHA, etc, for the Module Launcher","so that CC can pre empt uploading all the bits every time.","Upload of module launcher bits is slow because we do not take into account the CC cache. To fix this we need to use an async upload,<span class='highlight-text severity-high'> and </span>to somehow generate the SHA, etc, for the Module Launcher so that CC can pre empt uploading all the bits every time.","atomic","conjunctions","high",False
18218,"Upload of module launcher bits is slow because we do not take into account the CC cache. To fix this we need to use an async upload, and to somehow generate the SHA, etc, for the Module Launcher so that CC can pre empt uploading all the bits every time.",NULL,"Upload of module launcher bits is slow because we do not take into account the CC cache. To fix this we need to use an async upload, and to somehow generate the SHA, etc, for the Module Launcher","so that CC can pre empt uploading all the bits every time.","Upload of module launcher bits is slow because we do not take into account the CC cache<span class='highlight-text severity-high'>. To fix this we need to use an async upload, and to somehow generate the SHA, etc, for the Module Launcher so that CC can pre empt uploading all the bits every time.</span>","minimal","punctuation","high",False
18218,"Upload of module launcher bits is slow because we do not take into account the CC cache. To fix this we need to use an async upload, and to somehow generate the SHA, etc, for the Module Launcher so that CC can pre empt uploading all the bits every time.",NULL,"Upload of module launcher bits is slow because we do not take into account the CC cache. To fix this we need to use an async upload, and to somehow generate the SHA, etc, for the Module Launcher","so that CC can pre empt uploading all the bits every time.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18239,"As a s c s developer, I'd like to refactor the current ModuleLauncher contract with Boot s JarLauncher API, so we don t have to maintain duplicate functionality.","As a s c","s developer, I'd like to refactor the current ModuleLauncher contract with Boot s JarLauncher API,","so we don t have to maintain duplicate functionality.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18237,"As a Spring XD developer, I'd like to port SFTP module from XD to s c s repo, so I can use it as source modules to build streaming pipeline. ","As a Spring XD developer",", I'd like to port SFTP module from XD to s c s repo,","so I can use it as source modules to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18240,"As a s c d developer, I'd like to experiment how do we resolve and then add module dependent JAR s to Boot loader, so I have an approach to handle external libraries required by OOTB modules. ","As a s c d developer",", I'd like to experiment how do we re","solve and then add module dependent JAR s to Boot loader, so I have an approach to handle external libraries required by OOTB modules.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18238,"As a s c d developer, I'd like to move the external library to its own project, so we have a clear separation of functionalities in s c d repo.","As a s c d developer",", I'd like to move the external library to its own project,","so we have a clear separation of functionalities in s c d repo.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18241,"As a s c d developer, I'd like to make the deployer work asynchronously, so I can use the shell to return quickly and also queue deploy operations within YARN as tasks.","As a s c d developer",", I'd like to make the deployer work asynchronously,","so I can use the shell to return quickly and also queue deploy operations within YARN as tasks.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18740,"As a user, I'd like to have the option of Cassandra sink, so I can leverage the NoSQL database to write high volumes of variable data segments in high velocity.","As a user",", I'd like to have the option of Cassandra sink,","so I can leverage the NoSQL database to write high volumes of variable data segments in high velocity.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18083,"As a developer, I'd like to upgrade to 2.2.1 GA release, so I can leverage the latest improvements without breaking backwards compatibility. SHDP 2.3.0 uses Boot 1.3 and HDP and CDH versions that drop older Hive support. To avoid breaking changes we should instead use SHDP 2.2.1 that has backported any improvements that we need as well as move Spring and Hadoop versions to more recent ones.","As a developer",", I'd like to upgrade to 2.2.1 GA release,","so I can leverage the latest improvements without breaking backwards compatibility. SHDP 2.3.0 uses Boot 1.3 and HDP and CDH versions that drop older Hive support. To avoid breaking changes we should instead use SHDP 2.2.1 that has backported any improvements that we need as well as move Spring and Hadoop versions to more recent ones.","As a developer, I'd like to upgrade to 2<span class='highlight-text severity-high'>.2.1 GA release, so I can leverage the latest improvements without breaking backwards compatibility. SHDP 2.3.0 uses Boot 1.3 and HDP and CDH versions that drop older Hive support. To avoid breaking changes we should instead use SHDP 2.2.1 that has backported any improvements that we need as well as move Spring and Hadoop versions to more recent ones.</span>","minimal","punctuation","high",False
18083,"As a developer, I'd like to upgrade to 2.2.1 GA release, so I can leverage the latest improvements without breaking backwards compatibility. SHDP 2.3.0 uses Boot 1.3 and HDP and CDH versions that drop older Hive support. To avoid breaking changes we should instead use SHDP 2.2.1 that has backported any improvements that we need as well as move Spring and Hadoop versions to more recent ones.","As a developer",", I'd like to upgrade to 2.2.1 GA release,","so I can leverage the latest improvements without breaking backwards compatibility. SHDP 2.3.0 uses Boot 1.3 and HDP and CDH versions that drop older Hive support. To avoid breaking changes we should instead use SHDP 2.2.1 that has backported any improvements that we need as well as move Spring and Hadoop versions to more recent ones.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18077,"As a developer, I'd like to upgrade Spring XD s ambari plugin to 1.3 release.","As a developer",", I'd like to upgrade Spring XD s ambari plugin to 1.3 release.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18076,"As a developer, I'd like to move k8s SPI to it s own repo.","As a developer",", I'd like to move k8s SPI to it s own repo.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18079,"As a developer, I'd like to upgrade Boot and Spring Cloud Build revisions, so I can leverage the latest updates.","As a developer",", I'd like to upgrade Boot and Spring Cloud Build revisions,","so I can leverage the latest updates.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18078,"As a developer, I'd want to document the limitations of HSQL DB when using composed jobs. ","As a developer",", I'd want to document the limitations of HSQL DB when using composed jobs.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18087,"Users want the ability to use Composed Jobs specifically parallel Jobs without having to update the configurations for the hsqldb and the Isolation Level for spring batch. These should be set by default. ",NULL,"Users want the ability to use Composed Jobs specifically parallel Jobs without having to update the configurations for the hsqldb and the Isolation Level for spring batch. These should be set by default. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18087,"Users want the ability to use Composed Jobs specifically parallel Jobs without having to update the configurations for the hsqldb and the Isolation Level for spring batch. These should be set by default. ",NULL,"Users want the ability to use Composed Jobs specifically parallel Jobs without having to update the configurations for the hsqldb and the Isolation Level for spring batch. These should be set by default. ",NULL,"Users want the ability to use Composed Jobs specifically parallel Jobs without having to update the configurations for the hsqldb and the Isolation Level for spring batch<span class='highlight-text severity-high'>. These should be set by default. </span>","minimal","punctuation","high",False
18097,"As a developer, I'd like to revisit the existing design and identify known limitations and or the gaps. ","As a developer",", I'd like to revisit the existing design and identify known limitations and or the gaps.",NULL,"As a developer, I'd like to revisit the existing design<span class='highlight-text severity-high'> and </span>identify known limitations and<span class='highlight-text severity-high'> or </span>the gaps. ","atomic","conjunctions","high",False
18097,"As a developer, I'd like to revisit the existing design and identify known limitations and or the gaps. ","As a developer",", I'd like to revisit the existing design and identify known limitations and or the gaps.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18091,"As a developer, I'd like to add support for undeployed status consistently across all the deployers, so I can present the correct status instead of the current unknown . This is applicable for existing streams without any deployment context associated with it. ","As a developer",", I'd like to add support for undeployed status consistently across all the deployers,","so I can present the correct status instead of the current unknown . This is applicable for existing streams without any deployment context associated with it.","As a developer, I'd like to add support for undeployed status consistently across all the deployers, so I can present the correct status instead of the current unknown <span class='highlight-text severity-high'>. This is applicable for existing streams without any deployment context associated with it. </span>","minimal","punctuation","high",False
18091,"As a developer, I'd like to add support for undeployed status consistently across all the deployers, so I can present the correct status instead of the current unknown . This is applicable for existing streams without any deployment context associated with it. ","As a developer",", I'd like to add support for undeployed status consistently across all the deployers,","so I can present the correct status instead of the current unknown . This is applicable for existing streams without any deployment context associated with it.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18093,"As a developer, I'd like to add undeployed status for CF SPI, so I can represent the correct status instead of the current unknown state.","As a developer",", I'd like to add undeployed status for CF SPI,","so I can represent the correct status instead of the current unknown state.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18101,"As a developer, I'd like to replace all Job s references with Task s . ","As a developer",", I'd like to replace all Job s references with Task s .",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18099,"As a user, I'd like to see the version and SPI type in the about section, so I can confirm which build of admin ui I m currently using. ","As a user",", I'd like to see the version and SPI type in the about section,","so I can confirm which build of admin ui I m currently using.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18098,"As a user, I'd like Flo Graphs as screenshots while referring to the batch DSL, so it will be easy for me to relate to concepts. ","As a user",", I'd like Flo Graphs as screenshots while referring to the batch DSL,","so it will be easy for me to relate to concepts.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18100,"As a user, I'd like to use the admin ui and flo with consistent look and feel. ","As a user",", I'd like to use the admin ui and flo with consistent look and feel.",NULL,"As a user, I'd like to use the admin ui<span class='highlight-text severity-high'> and </span>flo with consistent look and feel. ","atomic","conjunctions","high",False
18100,"As a user, I'd like to use the admin ui and flo with consistent look and feel. ","As a user",", I'd like to use the admin ui and flo with consistent look and feel.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18102,"As a developer, I'd like to replace all references of Spring XD with Spring Cloud Data Flow. ","As a developer",", I'd like to replace all references of Spring XD with Spring Cloud Data Flow.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18084,"I'don t recall why this commit https github.com garyrussell spring xd commit ba15a1390f7e448dbc723ee76a45c2e239e0994e was not applied to master but having the timestamp for each step in the history will be useful. See this github issue https github.com spring projects spring xd modules issues 24 issuecomment 154436643 . ",NULL,"I'don t recall why this commit https github.com garyrussell spring xd commit ba15a1390f7e448dbc723ee76a45c2e239e0994e was not applied to master but having the timestamp for each step in the history will be useful. See this github issue https github.com spring projects spring xd modules issues 24 issuecomment 154436643 . ",NULL,"Add for who this story is","well_formed","no_role","high",False
18084,"I'don t recall why this commit https github.com garyrussell spring xd commit ba15a1390f7e448dbc723ee76a45c2e239e0994e was not applied to master but having the timestamp for each step in the history will be useful. See this github issue https github.com spring projects spring xd modules issues 24 issuecomment 154436643 . ",NULL,"I'don t recall why this commit https github.com garyrussell spring xd commit ba15a1390f7e448dbc723ee76a45c2e239e0994e was not applied to master but having the timestamp for each step in the history will be useful. See this github issue https github.com spring projects spring xd modules issues 24 issuecomment 154436643 . ",NULL,"I'don t recall why this commit https github<span class='highlight-text severity-high'>.com garyrussell spring xd commit ba15a1390f7e448dbc723ee76a45c2e239e0994e was not applied to master but having the timestamp for each step in the history will be useful. See this github issue https github.com spring projects spring xd modules issues 24 issuecomment 154436643 . </span>","minimal","punctuation","high",False
18089,"As a developer, I'd like to add undeployed status for Mesos SPI, so I can represent the correct status instead of the current unknown state.","As a developer",", I'd like to add undeployed status for Me","sos SPI, so I can represent the correct status instead of the current unknown state.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18095,"As a developer, I'd like to submit a PR for existing work on Mesos SPI. ","As a developer",", I'd like to submit a PR for existing work on Mesos SPI.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18151,"XD Developer does not want the the twitter stream acceptance tests to interfere with other tests.",NULL,"XD Developer does not want the the twitter stream acceptance tests to interfere with other tests.",NULL,"Add for who this story is","well_formed","no_role","high",False
18150,"As an XD module developer I would like to use HDFS for my custom module location even when my namenode is configured for HA. We had an issue filed in the spring xd ambari project It seems like custom module doesn t pickup namenode HA? and still use NameNodeProxies.createNonHAProxy? see https github.com spring projects spring xd ambari issues 14","As an XD module developer","I would like to use HDFS for my custom module location even when my namenode is configured for HA. We had an issue filed in the spring xd ambari project It seems like custom module doesn t pickup namenode HA? and still use NameNodeProxies.createNonHAProxy? see https github.com spring projects spring xd ambari issues 14",NULL,"As an XD module developer I would like to use HDFS for my custom module location even when my namenode is configured for HA. We had an issue filed in the spring xd ambari project It seems like custom module doesn t pickup namenode HA?<span class='highlight-text severity-high'> and </span>still use NameNodeProxies.createNonHAProxy? see https github.com spring projects spring xd ambari issues 14","atomic","conjunctions","high",False
18150,"As an XD module developer I would like to use HDFS for my custom module location even when my namenode is configured for HA. We had an issue filed in the spring xd ambari project It seems like custom module doesn t pickup namenode HA? and still use NameNodeProxies.createNonHAProxy? see https github.com spring projects spring xd ambari issues 14","As an XD module developer","I would like to use HDFS for my custom module location even when my namenode is configured for HA. We had an issue filed in the spring xd ambari project It seems like custom module doesn t pickup namenode HA? and still use NameNodeProxies.createNonHAProxy? see https github.com spring projects spring xd ambari issues 14",NULL,"As an XD module developer I would like to use HDFS for my custom module location even when my namenode is configured for HA<span class='highlight-text severity-high'>. We had an issue filed in the spring xd ambari project It seems like custom module doesn t pickup namenode HA? and still use NameNodeProxies.createNonHAProxy? see https github.com spring projects spring xd ambari issues 14</span>","minimal","punctuation","high",False
18150,"As an XD module developer I would like to use HDFS for my custom module location even when my namenode is configured for HA. We had an issue filed in the spring xd ambari project It seems like custom module doesn t pickup namenode HA? and still use NameNodeProxies.createNonHAProxy? see https github.com spring projects spring xd ambari issues 14","As an XD module developer","I would like to use HDFS for my custom module location even when my namenode is configured for HA. We had an issue filed in the spring xd ambari project It seems like custom module doesn t pickup namenode HA? and still use NameNodeProxies.createNonHAProxy? see https github.com spring projects spring xd ambari issues 14",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18152,"As a developer, I want to be able to connect to multiple external systems for the same binding type, so that I can read data from a system and write it to another.","As a developer,","I want to be able to connect to multiple external systems for the same binding type,","so that I can read data from a system and write it to another.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18153,"As a developer, I want to have a BinderFactory abstraction, so that I can support multiple binder types in the future.","As a developer,","I want to have a BinderFactory abstraction,","so that I can support multiple binder types in the future.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18155,"As a XD user, I'd like to restart the composed job workflow from Shell UI. ","As a XD user",", I'd like to restart the composed job workflow from Shell UI.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18157,"As the system, I would like a way to launch a previously deployed job module from another job module. ","As the system",", I would like a way to launch a previously deployed job module from another job module.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18159,"As an XD developer, I'd like to explore options to represent composed job, so I can create a workflow to orchestrate multiple jobs. ","As an XD developer",", I'd like to explore options to represent composed job,","so I can create a workflow to orchestrate multiple jobs.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18163,"As a Spring XD developer, I'd like to port script module from XD to s c s repo, so I can use it as processor module to build streaming pipeline.","As a Spring XD developer",", I'd like to port script module from XD to s c s repo,","so I can use it as processor module to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18158,"As an XD developer, I'd like to explore options to remove composed job, so I can clean up unused resources and memory footprints. ","As an XD developer",", I'd like to explore options to remove composed job,","so I can clean up unused resources and memory footprints.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18161,"As a Spring XD developer, I'd like to port splitter module from XD to s c s repo, so I can use it as processor module to build streaming pipeline.","As a Spring XD developer",", I'd like to port splitter module from XD to s c s repo,","so I can use it as processor module to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18162,"As a Spring XD developer, I'd like to port shell module from XD to s c s repo, so I can use it as processor module to build streaming pipeline.","As a Spring XD developer",", I'd like to port shell module from XD to s c s repo,","so I can use it as processor module to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18166,"As a Spring XD developer, I'd like to port http client module from XD to s c s repo, so I can use it as processor module to build streaming pipeline.","As a Spring XD developer",", I'd like to port http client module from XD to s c s repo,","so I can use it as processor module to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18164,"As a Spring XD developer, I'd like to port object to json module from XD to s c s repo, so I can use it as processor module to build streaming pipeline.","As a Spring XD developer",", I'd like to port object to j","son module from XD to s c s repo, so I can use it as processor module to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18165,"As a Spring XD developer, I'd like to port json to tuple module from XD to s c s repo, so I can use it as processor module to build streaming pipeline.","As a Spring XD developer",", I'd like to port j","son to tuple module from XD to s c s repo, so I can use it as processor module to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18154,"When a default value is an array, the current behavior using toString not only produces useless results like Ljava.lang.String; 2638011 but also constantly changing results.",NULL,"When a default value is an array, the current behavior using toString not only produces useless results like Ljava.lang.String; 2638011 but also constantly changing results.",NULL,"Add for who this story is","well_formed","no_role","high",False
18154,"When a default value is an array, the current behavior using toString not only produces useless results like Ljava.lang.String; 2638011 but also constantly changing results.",NULL,"When a default value is an array, the current behavior using toString not only produces useless results like Ljava.lang.String; 2638011 but also constantly changing results.",NULL,"When a default value is an array, the current behavior using toString not only produces useless results like Ljava.lang.String<span class='highlight-text severity-high'>; 2638011 but also constantly changing results.</span>","minimal","punctuation","high",False
18167,"As a Spring XD developer, I'd like to port aggregator module from XD to s c s repo, so I can use it as processor module to build streaming pipeline.","As a Spring XD developer",", I'd like to port aggregator module from XD to s c s repo,","so I can use it as processor module to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18168,"As a Spring XD developer, I'd like to move jdbc module from XD to s c s repo, so I can use it as source to build streaming pipeline. ","As a Spring XD developer",", I'd like to move jdbc module from XD to s c s repo,","so I can use it as source to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18160,"For example the generated value for the cassandra sink results in entityBasePackages the base packages to scan for entities annotated with Table annotations String; , default Ljava.lang.String; 2638011 where the default value changes each time the build is run. ",NULL,"For example the generated value for the cassandra sink results in entityBasePackages the base packages to scan for entities annotated with Table annotations String; , default Ljava.lang.String; 2638011 where the default value changes each time the build is run. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18160,"For example the generated value for the cassandra sink results in entityBasePackages the base packages to scan for entities annotated with Table annotations String; , default Ljava.lang.String; 2638011 where the default value changes each time the build is run. ",NULL,"For example the generated value for the cassandra sink results in entityBasePackages the base packages to scan for entities annotated with Table annotations String; , default Ljava.lang.String; 2638011 where the default value changes each time the build is run. ",NULL,"For example the generated value for the cassandra sink results in entityBasePackages the base packages to scan for entities annotated with Table annotations String<span class='highlight-text severity-high'>; , default Ljava.lang.String; 2638011 where the default value changes each time the build is run. </span>","minimal","punctuation","high",False
18156,"Verify that the job launch works as we expect for the composed job. ",NULL,"Verify that the job launch works as we expect for the composed job. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18189,"As an s c d user, I'd like to upload custom modules using shell rest api, so I can contribute modules and create streaming batch pipelines. ","As an s c d user",", I'd like to upload custom modules using shell rest api,","so I can contribute modules and create streaming batch pipelines.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18192,"As an s c d user, I'd like to have documentation on deployment manifest, so I could refer to the relevant bits on partitions . I'd like to understand how streams withe ","As an s c d user",", I'd like to have documentation on deployment manifest,","so I could refer to the relevant bits on partitions . I'd like to understand how streams withe","As an s c d user, I'd like to have documentation on deployment manifest, so I could refer to the relevant bits on partitions <span class='highlight-text severity-high'>. I'd like to understand how streams withe </span>","minimal","punctuation","high",False
18192,"As an s c d user, I'd like to have documentation on deployment manifest, so I could refer to the relevant bits on partitions . I'd like to understand how streams withe ","As an s c d user",", I'd like to have documentation on deployment manifest,","so I could refer to the relevant bits on partitions . I'd like to understand how streams withe","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18190,"As an s c d user, I'd like to tap the primary pipeline, so I can fork the same data and do some ad hoc analysis without impacting the original stream.","As an s c d user",", I'd like to tap the primary pipeline,","so I can fork the same data and do some ad hoc analysis without impacting the original stream.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18197,"As a s c d developer, I'd like to document the use of BOM templates, so the general audience can use it as a reference to include external libraries dynamically.","As a s c d developer",", I'd like to document the use of BOM templates,","so the general audience can use it as a reference to include external libraries dynamically.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18285,"The current implementation makes use of cf java client, which is relatively heavy for our needs. It should be removed in favour of a bespoke RestOperations wrapper. See https github.com Zteve test cc oauth for sample code.",NULL,"The current implementation makes use of cf java client, which is relatively heavy for our needs. It should be removed in favour of a bespoke RestOperations wrapper. See https github.com Zteve test cc oauth for sample code.",NULL,"The current implementation makes use of cf java client, which is relatively heavy for our needs<span class='highlight-text severity-high'>. It should be removed in favour of a bespoke RestOperations wrapper. See https github.com Zteve test cc oauth for sample code.</span>","minimal","punctuation","high",False
18196,"As a user, I m not able to shutdown container from Admin UI with the following stream definition deployed. code stream create swagataTestIssue definition jdbc query select employee id, employee name, employer from EMPLOYEE url jdbc oracle thin localhost 1521 orcl username springxd password xdpwd driverClassName oracle.jdbc.OracleDriver testOnBorrow false hdfs inputType application json deploy code More details here https issuetracker.springsource.com browse VESC 504 .","As a user,","I m not able to shutdown container from Admin UI with the following stream definition deployed. code stream create swagataTestIssue definition jdbc query select employee id, employee name, employer from EMPLOYEE url jdbc oracle thin localhost 1521 orcl username springxd password xdpwd driverClassName oracle.jdbc.OracleDriver testOnBorrow false hdfs inputType application json deploy code More details here https issuetracker.springsource.com browse VESC 504 .",NULL,"As a user, I m not able to shutdown container from Admin UI with the following stream definition deployed<span class='highlight-text severity-high'>. code stream create swagataTestIssue definition jdbc query select employee id, employee name, employer from EMPLOYEE url jdbc oracle thin localhost 1521 orcl username springxd password xdpwd driverClassName oracle.jdbc.OracleDriver testOnBorrow false hdfs inputType application json deploy code More details here https issuetracker.springsource.com browse VESC 504 .</span>","minimal","punctuation","high",False
18196,"As a user, I m not able to shutdown container from Admin UI with the following stream definition deployed. code stream create swagataTestIssue definition jdbc query select employee id, employee name, employer from EMPLOYEE url jdbc oracle thin localhost 1521 orcl username springxd password xdpwd driverClassName oracle.jdbc.OracleDriver testOnBorrow false hdfs inputType application json deploy code More details here https issuetracker.springsource.com browse VESC 504 .","As a user,","I m not able to shutdown container from Admin UI with the following stream definition deployed. code stream create swagataTestIssue definition jdbc query select employee id, employee name, employer from EMPLOYEE url jdbc oracle thin localhost 1521 orcl username springxd password xdpwd driverClassName oracle.jdbc.OracleDriver testOnBorrow false hdfs inputType application json deploy code More details here https issuetracker.springsource.com browse VESC 504 .",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18194,"As a s c s user, I'd like to have the option to use more than one binder connection factory, so I can mix and match where I consume and publish data. More details here https github.com spring cloud spring cloud stream issues 140 .","As a s c","s user, I'd like to have the option to use more than one binder connection factory,","so I can mix and match where I consume and publish data. More details here https github.com spring cloud spring cloud stream issues 140 .","As a s c s user, I'd like to have the option to use more than one binder connection factory, so I can mix and match where I consume and publish data<span class='highlight-text severity-high'>. More details here https github.com spring cloud spring cloud stream issues 140 .</span>","minimal","punctuation","high",False
18194,"As a s c s user, I'd like to have the option to use more than one binder connection factory, so I can mix and match where I consume and publish data. More details here https github.com spring cloud spring cloud stream issues 140 .","As a s c","s user, I'd like to have the option to use more than one binder connection factory,","so I can mix and match where I consume and publish data. More details here https github.com spring cloud spring cloud stream issues 140 .","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18198,"As a XD developer, I'd like to upgrade to SI 4.2, Spring 4.2.1, and AMQP 1.5 dependencies, so I can take advantage of the latest improvements. ","As a XD developer",", I'd like to upgrade to SI 4.2, Spring 4.2.1, and AMQP 1.5 dependencies,","so I can take advantage of the latest improvements.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18193,"As a s c s user, I'd like to have Gemfire message channel binder, so I can use Gemfire as the messaging middleware for low latency use cases. ","As a s c","s user, I'd like to have Gemfire message channel binder,","so I can use Gemfire as the messaging middleware for low latency use cases.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18199,"As a XD developer, I'd like to move header enricher from modules repo to XD proper. ","As a XD developer",", I'd like to move header enricher from modules repo to XD proper.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18200,"As a s c d user, I'd like to have the option to choose Hadoop distribution of choice, so I can load the right Hadoop libraries in the CP. ","As a s c d user",", I'd like to have the option to choose Hadoop distribution of choice,","so I can load the right Hadoop libraries in the CP.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18222,"As a s c d developer, I'd like to produce ref. documentation for s c d architecture, so I could define 1.x and 2.x deployment differences. ","As a s c d developer",", I'd like to produce ref. documentation for s c d architecture,","so I could define 1.x and 2.x deployment differences.","As a s c d developer, I'd like to produce ref<span class='highlight-text severity-high'>. documentation for s c d architecture, so I could define 1.x and 2.x deployment differences. </span>","minimal","punctuation","high",False
18222,"As a s c d developer, I'd like to produce ref. documentation for s c d architecture, so I could define 1.x and 2.x deployment differences. ","As a s c d developer",", I'd like to produce ref. documentation for s c d architecture,","so I could define 1.x and 2.x deployment differences.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18223,"As a s c s developer, I'd like to fix the Kafka binder, so I can create messaging microservices apps and successfully bind them to an operational Kafka broker. ","As a s c","s developer, I'd like to fix the Kafka binder,","so I can create messaging microservices apps and successfully bind them to an operational Kafka broker.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18228,"As a s c d developer, I'd like to provide optional key value pairs as deployment properties, so I could leverage them at the runtime to instruct how the modules will be deployed. The scope of this story is to specifically support count to represent N instances of modules that share the same environment variables. ","As a s c d developer",", I'd like to provide optional key value pairs as deployment properties,","so I could leverage them at the runtime to instruct how the modules will be deployed. The scope of this story is to specifically support count to represent N instances of modules that share the same environment variables.","As a s c d developer, I'd like to provide optional key value pairs as deployment properties, so I could leverage them at the runtime to instruct how the modules will be deployed<span class='highlight-text severity-high'>. The scope of this story is to specifically support count to represent N instances of modules that share the same environment variables. </span>","minimal","punctuation","high",False
18228,"As a s c d developer, I'd like to provide optional key value pairs as deployment properties, so I could leverage them at the runtime to instruct how the modules will be deployed. The scope of this story is to specifically support count to represent N instances of modules that share the same environment variables. ","As a s c d developer",", I'd like to provide optional key value pairs as deployment properties,","so I could leverage them at the runtime to instruct how the modules will be deployed. The scope of this story is to specifically support count to represent N instances of modules that share the same environment variables.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18225,"Create a timestamp job that will be used a a sample for users to create their own spring boot based jobs. ",NULL,"Create a timestamp job that will be used a a sample for users to create their own spring boot based jobs. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18224,"As a s c d developer, I'd like to setup gh pages branch for s c d and s c s m repos, so I can start pushing documentation with PR commits.","As a s c d developer",", I'd like to setup gh pages branch for s c d and s c s m repos,","so I can start pushing documentation with PR commits.","As a s c d developer, I'd like to setup gh pages branch for s c d<span class='highlight-text severity-high'> and </span>s c s m repos, so I can start pushing documentation with PR commits.","atomic","conjunctions","high",False
18224,"As a s c d developer, I'd like to setup gh pages branch for s c d and s c s m repos, so I can start pushing documentation with PR commits.","As a s c d developer",", I'd like to setup gh pages branch for s c d and s c s m repos,","so I can start pushing documentation with PR commits.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18232,"As a s c d developer, I'd like to create ModuleRegistry implementation, so I can use this infrastructure to lookup module coordinates by name.","As a s c d developer",", I'd like to create ModuleRegistry implementation,","so I can use this infrastructure to lookup module coordinates by name.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18227,"Instead of using real moduleDeployer , try using mocks so that the module deployer downloading the maven co ordinates from repo can be avoided for module deployment case . Since module deployer and controllers are tested individually, it would be good to focus on shell functionality only for the shell tests.",NULL,"Instead of using real moduleDeployer , try using mocks","so that the module deployer downloading the maven co ordinates from repo can be avoided for module deployment case . Since module deployer and controllers are tested individually, it would be good to focus on shell functionality only for the shell tests.","Add for who this story is","well_formed","no_role","high",False
18227,"Instead of using real moduleDeployer , try using mocks so that the module deployer downloading the maven co ordinates from repo can be avoided for module deployment case . Since module deployer and controllers are tested individually, it would be good to focus on shell functionality only for the shell tests.",NULL,"Instead of using real moduleDeployer , try using mocks","so that the module deployer downloading the maven co ordinates from repo can be avoided for module deployment case . Since module deployer and controllers are tested individually, it would be good to focus on shell functionality only for the shell tests.","Instead of using real moduleDeployer , try using mocks so that the module deployer downloading the maven co ordinates from repo can be avoided for module deployment case <span class='highlight-text severity-high'>. Since module deployer and controllers are tested individually, it would be good to focus on shell functionality only for the shell tests.</span>","minimal","punctuation","high",False
18227,"Instead of using real moduleDeployer , try using mocks so that the module deployer downloading the maven co ordinates from repo can be avoided for module deployment case . Since module deployer and controllers are tested individually, it would be good to focus on shell functionality only for the shell tests.",NULL,"Instead of using real moduleDeployer , try using mocks","so that the module deployer downloading the maven co ordinates from repo can be avoided for module deployment case . Since module deployer and controllers are tested individually, it would be good to focus on shell functionality only for the shell tests.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18226,"The s c s module launcher document requires update for running it on standalone, docker, lattice. Also, the docker compose yml requires fix so that modules in there are bound together.",NULL,"The s c s module launcher document requires update for running it on standalone, docker, lattice. Also, the docker compose yml requires fix","so that modules in there are bound together.","Add for who this story is","well_formed","no_role","high",False
18226,"The s c s module launcher document requires update for running it on standalone, docker, lattice. Also, the docker compose yml requires fix so that modules in there are bound together.",NULL,"The s c s module launcher document requires update for running it on standalone, docker, lattice. Also, the docker compose yml requires fix","so that modules in there are bound together.","The s c s module launcher document requires update for running it on standalone, docker, lattice<span class='highlight-text severity-high'>. Also, the docker compose yml requires fix so that modules in there are bound together.</span>","minimal","punctuation","high",False
18226,"The s c s module launcher document requires update for running it on standalone, docker, lattice. Also, the docker compose yml requires fix so that modules in there are bound together.",NULL,"The s c s module launcher document requires update for running it on standalone, docker, lattice. Also, the docker compose yml requires fix","so that modules in there are bound together.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18229,"As a s c d developer, I'd like to have module info , module list , module register , and module unregister commands, so I can interact with ModuleRegistry .","As a s c d developer",", I'd like to have module info , module list , module register , and module unregister commands,","so I can interact with ModuleRegistry .","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18230,"As a user, I need to be able to deploy a task boot jar via the CLI. ","As a user",", I need to be able to deploy a task boot jar via the CLI.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18231,"Can take from previous implementation in XD SI Boot. Should have a way to enforce not skipping tests based on an environment variable. Consider moving this coverage to SI commons or equivalent. ",NULL,"Can take from previous implementation in XD SI Boot. Should have a way to enforce not skipping tests based on an environment variable. Consider moving this coverage to SI commons or equivalent. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18231,"Can take from previous implementation in XD SI Boot. Should have a way to enforce not skipping tests based on an environment variable. Consider moving this coverage to SI commons or equivalent. ",NULL,"Can take from previous implementation in XD SI Boot. Should have a way to enforce not skipping tests based on an environment variable. Consider moving this coverage to SI commons or equivalent. ",NULL,"Can take from previous implementation in XD SI Boot<span class='highlight-text severity-high'>. Should have a way to enforce not skipping tests based on an environment variable. Consider moving this coverage to SI commons or equivalent. </span>","minimal","punctuation","high",False
18235,"As a s c d developer, I'd like to create foundation to support processor as OOTB modules, so I can use the processor modules from s c s m repo to build streaming pipeline.","As a s c d developer",", I'd like to create foundation to support proces","sor as OOTB modules, so I can use the processor modules from s c s m repo to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18233,"As a s c s developer, I'd like to enable offline mode for AetherModuleResolver , so I can pull the module artifacts from local instead of remote maven repo.","As a s c","s developer, I'd like to enable offline mode for AetherModuleRe","solver , so I can pull the module artifacts from local instead of remote maven repo.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18234,"Make ChannelBindingAdapter implement SmartLifecycle so that it gets started with the highest precedence and before any other message producing bean.",NULL,"Make ChannelBindingAdapter implement SmartLifecycle","so that it gets started with the highest precedence and before any other message producing bean.","Add for who this story is","well_formed","no_role","high",False
18234,"Make ChannelBindingAdapter implement SmartLifecycle so that it gets started with the highest precedence and before any other message producing bean.",NULL,"Make ChannelBindingAdapter implement SmartLifecycle","so that it gets started with the highest precedence and before any other message producing bean.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18236,"As a s c d developer, I'd like to create a new project to contain all the rules associated RedisRule contract, so it is isolated from core functionalities and reusable by test coverage as needed. Consider moving this coverage to SI commons or equivalent. ","As a s c d developer",", I'd like to create a new project to contain all the rules as","sociated RedisRule contract, so it is isolated from core functionalities and reusable by test coverage as needed. Consider moving this coverage to SI commons or equivalent.","As a s c d developer, I'd like to create a new project to contain all the rules associated RedisRule contract, so it is isolated from core functionalities and reusable by test coverage as needed<span class='highlight-text severity-high'>. Consider moving this coverage to SI commons or equivalent. </span>","minimal","punctuation","high",False
18236,"As a s c d developer, I'd like to create a new project to contain all the rules associated RedisRule contract, so it is isolated from core functionalities and reusable by test coverage as needed. Consider moving this coverage to SI commons or equivalent. ","As a s c d developer",", I'd like to create a new project to contain all the rules as","sociated RedisRule contract, so it is isolated from core functionalities and reusable by test coverage as needed. Consider moving this coverage to SI commons or equivalent.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18220,"Currently only the STARTED application and application instance status is recognised. This issue will look at the other possible states and report them as module instance states. This would be trivial if we knew what the possible states might be and how we should interpret them for module instance state.",NULL,"Currently only the STARTED application and application instance status is recognised. This issue will look at the other possible states and report them as module instance states. This would be trivial if we knew what the possible states might be and how we should interpret them for module instance state.",NULL,"Add for who this story is","well_formed","no_role","high",False
18220,"Currently only the STARTED application and application instance status is recognised. This issue will look at the other possible states and report them as module instance states. This would be trivial if we knew what the possible states might be and how we should interpret them for module instance state.",NULL,"Currently only the STARTED application and application instance status is recognised. This issue will look at the other possible states and report them as module instance states. This would be trivial if we knew what the possible states might be and how we should interpret them for module instance state.",NULL,"Currently only the STARTED application<span class='highlight-text severity-high'> and </span>application instance status is recognised. This issue will look at the other possible states and report them as module instance states. This would be trivial if we knew what the possible states might be and how we should interpret them for module instance state.","atomic","conjunctions","high",False
18220,"Currently only the STARTED application and application instance status is recognised. This issue will look at the other possible states and report them as module instance states. This would be trivial if we knew what the possible states might be and how we should interpret them for module instance state.",NULL,"Currently only the STARTED application and application instance status is recognised. This issue will look at the other possible states and report them as module instance states. This would be trivial if we knew what the possible states might be and how we should interpret them for module instance state.",NULL,"Currently only the STARTED application and application instance status is recognised<span class='highlight-text severity-high'>. This issue will look at the other possible states and report them as module instance states. This would be trivial if we knew what the possible states might be and how we should interpret them for module instance state.</span>","minimal","punctuation","high",False
18252,"As a Spring XD developer, I'd like to move trigger module from XD to s c s repo, so I can use it as source to build streaming pipeline. ","As a Spring XD developer",", I'd like to move trigger module from XD to s c s repo,","so I can use it as source to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18250,"Spring Integration 4.2 changed the default SFTP session factory to not accept keys from unknown hosts by default. This is more secure. You either have to provide a pre populated known hosts file or set allowUnknownKeys to true. If you do both, the keys will be automatically added to the known hosts file. When updating XD to 4.2.0.RC1 I simply set the boolean to true, to retain the previous behavior. Add properties to the SFTP source to allow configuration of these properties at the stream level. ",NULL,"Spring Integration 4.2 changed the default SFTP session factory to not accept keys from unknown hosts by default. This is more secure. You either have to provide a pre populated known hosts file or set allowUnknownKeys to true. If you do both, the keys will be automatically added to the known hosts file. When updating XD to 4.2.0.RC1 I simply set the boolean to true, to retain the previous behavior. Add properties to the SFTP source to allow configuration of these properties at the stream level. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18283,"As a s c d developer, I'd like to add support for dependency resolution, so when two or more modules use different version of jars ex direct binding of two modules that include different versions of spring data , I have the capability to resolve and include the right bits at runtime.","As a s c d developer",", I'd like to add support for dependency re","solution, so when two or more modules use different version of jars ex direct binding of two modules that include different versions of spring data , I have the capability to resolve and include the right bits at runtime.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18290,"Currently there is no ModuleInstanceStatus returned. This issue will fill in the details.",NULL,"Currently there is no ModuleInstanceStatus returned. This issue will fill in the details.",NULL,"Add for who this story is","well_formed","no_role","high",False
19345,"Once spring batch admin 1.3.0.M1 is available, update the build to use it. Likely to be Sept 7 or 9",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
18250,"Spring Integration 4.2 changed the default SFTP session factory to not accept keys from unknown hosts by default. This is more secure. You either have to provide a pre populated known hosts file or set allowUnknownKeys to true. If you do both, the keys will be automatically added to the known hosts file. When updating XD to 4.2.0.RC1 I simply set the boolean to true, to retain the previous behavior. Add properties to the SFTP source to allow configuration of these properties at the stream level. ",NULL,"Spring Integration 4.2 changed the default SFTP session factory to not accept keys from unknown hosts by default. This is more secure. You either have to provide a pre populated known hosts file or set allowUnknownKeys to true. If you do both, the keys will be automatically added to the known hosts file. When updating XD to 4.2.0.RC1 I simply set the boolean to true, to retain the previous behavior. Add properties to the SFTP source to allow configuration of these properties at the stream level. ",NULL,"Spring Integration 4.2 changed the default SFTP session factory to not accept keys from unknown hosts by default. This is more secure. You either have to provide a pre populated known hosts file<span class='highlight-text severity-high'> or </span>set allowUnknownKeys to true. If you do both, the keys will be automatically added to the known hosts file. When updating XD to 4.2.0.RC1 I simply set the boolean to true, to retain the previous behavior. Add properties to the SFTP source to allow configuration of these properties at the stream level. ","atomic","conjunctions","high",False
18250,"Spring Integration 4.2 changed the default SFTP session factory to not accept keys from unknown hosts by default. This is more secure. You either have to provide a pre populated known hosts file or set allowUnknownKeys to true. If you do both, the keys will be automatically added to the known hosts file. When updating XD to 4.2.0.RC1 I simply set the boolean to true, to retain the previous behavior. Add properties to the SFTP source to allow configuration of these properties at the stream level. ",NULL,"Spring Integration 4.2 changed the default SFTP session factory to not accept keys from unknown hosts by default. This is more secure. You either have to provide a pre populated known hosts file or set allowUnknownKeys to true. If you do both, the keys will be automatically added to the known hosts file. When updating XD to 4.2.0.RC1 I simply set the boolean to true, to retain the previous behavior. Add properties to the SFTP source to allow configuration of these properties at the stream level. ",NULL,"Spring Integration 4<span class='highlight-text severity-high'>.2 changed the default SFTP session factory to not accept keys from unknown hosts by default. This is more secure. You either have to provide a pre populated known hosts file or set allowUnknownKeys to true. If you do both, the keys will be automatically added to the known hosts file. When updating XD to 4.2.0.RC1 I simply set the boolean to true, to retain the previous behavior. Add properties to the SFTP source to allow configuration of these properties at the stream level. </span>","minimal","punctuation","high",False
18253,"Hi, Passwords are visibly when using custom modules. Attached is example custom module code and xd shell script to reproduce the problem using stream module of type processor on Spring XD 1.2.1.RELEASE. Compile with Maven mvn clean install and run xd shell script xd shell cmdfile . runme.cmd . ",NULL,"Hi, Passwords are visibly when using custom modules. Attached is example custom module code and xd shell script to reproduce the problem using stream module of type processor on Spring XD 1.2.1.RELEASE. Compile with Maven mvn clean install and run xd shell script xd shell cmdfile . runme.cmd . ",NULL,"Add for who this story is","well_formed","no_role","high",False
18253,"Hi, Passwords are visibly when using custom modules. Attached is example custom module code and xd shell script to reproduce the problem using stream module of type processor on Spring XD 1.2.1.RELEASE. Compile with Maven mvn clean install and run xd shell script xd shell cmdfile . runme.cmd . ",NULL,"Hi, Passwords are visibly when using custom modules. Attached is example custom module code and xd shell script to reproduce the problem using stream module of type processor on Spring XD 1.2.1.RELEASE. Compile with Maven mvn clean install and run xd shell script xd shell cmdfile . runme.cmd . ",NULL,"Hi, Passwords are visibly when using custom modules. Attached is example custom module code<span class='highlight-text severity-high'> and </span>xd shell script to reproduce the problem using stream module of type processor on Spring XD 1.2.1.RELEASE. Compile with Maven mvn clean install and run xd shell script xd shell cmdfile . runme.cmd . ","atomic","conjunctions","high",False
18253,"Hi, Passwords are visibly when using custom modules. Attached is example custom module code and xd shell script to reproduce the problem using stream module of type processor on Spring XD 1.2.1.RELEASE. Compile with Maven mvn clean install and run xd shell script xd shell cmdfile . runme.cmd . ",NULL,"Hi, Passwords are visibly when using custom modules. Attached is example custom module code and xd shell script to reproduce the problem using stream module of type processor on Spring XD 1.2.1.RELEASE. Compile with Maven mvn clean install and run xd shell script xd shell cmdfile . runme.cmd . ",NULL,"Hi, Passwords are visibly when using custom modules<span class='highlight-text severity-high'>. Attached is example custom module code and xd shell script to reproduce the problem using stream module of type processor on Spring XD 1.2.1.RELEASE. Compile with Maven mvn clean install and run xd shell script xd shell cmdfile . runme.cmd . </span>","minimal","punctuation","high",False
18256,"Currently, EnableModule hardcodes references to both the redis and rabbit configuration classes, which trigger or don t trigger based on the presence of another jar which itself has the meat of the configuration . This is typically what boot AutoConfiguration is for. Moreover, adding a new binding eg Kafka or a stub for module testing would require to crack open EnableModule",NULL,"Currently, EnableModule hardcodes references to both the redis and rabbit configuration classes, which trigger or don t trigger based on the presence of another jar which itself has the meat of the configuration . This is typically what boot AutoConfiguration is for. Moreover, adding a new binding eg Kafka or a stub for module testing would require to crack open EnableModule",NULL,"Add for who this story is","well_formed","no_role","high",False
18256,"Currently, EnableModule hardcodes references to both the redis and rabbit configuration classes, which trigger or don t trigger based on the presence of another jar which itself has the meat of the configuration . This is typically what boot AutoConfiguration is for. Moreover, adding a new binding eg Kafka or a stub for module testing would require to crack open EnableModule",NULL,"Currently, EnableModule hardcodes references to both the redis and rabbit configuration classes, which trigger or don t trigger based on the presence of another jar which itself has the meat of the configuration . This is typically what boot AutoConfiguration is for. Moreover, adding a new binding eg Kafka or a stub for module testing would require to crack open EnableModule",NULL,"Currently, EnableModule hardcodes references to both the redis<span class='highlight-text severity-high'> and </span>rabbit configuration classes, which trigger<span class='highlight-text severity-high'> or </span>don t trigger based on the presence of another jar which itself has the meat of the configuration . This is typically what boot AutoConfiguration is for. Moreover, adding a new binding eg Kafka or a stub for module testing would require to crack open EnableModule","atomic","conjunctions","high",False
18256,"Currently, EnableModule hardcodes references to both the redis and rabbit configuration classes, which trigger or don t trigger based on the presence of another jar which itself has the meat of the configuration . This is typically what boot AutoConfiguration is for. Moreover, adding a new binding eg Kafka or a stub for module testing would require to crack open EnableModule",NULL,"Currently, EnableModule hardcodes references to both the redis and rabbit configuration classes, which trigger or don t trigger based on the presence of another jar which itself has the meat of the configuration . This is typically what boot AutoConfiguration is for. Moreover, adding a new binding eg Kafka or a stub for module testing would require to crack open EnableModule",NULL,"Currently, EnableModule hardcodes references to both the redis and rabbit configuration classes, which trigger or don t trigger based on the presence of another jar which itself has the meat of the configuration <span class='highlight-text severity-high'>. This is typically what boot AutoConfiguration is for. Moreover, adding a new binding eg Kafka or a stub for module testing would require to crack open EnableModule</span>","minimal","punctuation","high",False
18251,"As a s c d developer, I'd like to upgrade receptor client to comply with latest Receptor API changes, so I can sync up and take advantage of the recent improvements. ","As a s c d developer",", I'd like to upgrade receptor client to comply with latest Receptor API changes,","so I can sync up and take advantage of the recent improvements.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18254,"As an s c d developer, I'd like to investigate the distributed deployment of s c s modules on YARN, so I can experiment the implementation of YARN SPI and derive the strategy for YARNModuleDeployer .","As an s c d developer",", I'd like to investigate the distributed deployment of s c s modules on YARN,","so I can experiment the implementation of YARN SPI and derive the strategy for YARNModuleDeployer .","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18261,"As a Spring XD developer, I'd like to move redis module from XD to s c s repo, so I can use it as sink to build streaming pipeline.","As a Spring XD developer",", I'd like to move redis module from XD to s c s repo,","so I can use it as sink to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18260,"As a s c d developer, I'd like to move rabbit module from XD to s c s repo, so I can use it as source to build streaming pipeline.","As a s c d developer",", I'd like to move rabbit module from XD to s c s repo,","so I can use it as source to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18267,"As a Spring XD developer, I'd like to move twittersearch module from XD to s c s repo, so I can use it as source modules to build streaming pipeline. ","As a Spring XD developer",", I'd like to move twittersearch module from XD to s c s repo,","so I can use it as source modules to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18262,"As a Spring XD developer, I'd like to port FTP module from XD to s c s repo, so I can use it as sink modules to build streaming pipeline.","As a Spring XD developer",", I'd like to port FTP module from XD to s c s repo,","so I can use it as sink modules to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18264,"As a Spring XD developer, I'd like to port router module from XD to s c s repo, so I can use it as sink module to build streaming pipeline.","As a Spring XD developer",", I'd like to port router module from XD to s c s repo,","so I can use it as sink module to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18263,"As a Spring XD developer, I'd like to port file module from XD to s c s repo, so I can use it as sink module to build streaming pipeline.","As a Spring XD developer",", I'd like to port file module from XD to s c s repo,","so I can use it as sink module to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18268,"As a Spring XD developer, I'd like to move twitterstream module from XD to s c s repo, so I can use it as source modules to build streaming pipeline. ","As a Spring XD developer",", I'd like to move twitterstream module from XD to s c s repo,","so I can use it as source modules to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18274,"This could focus only on the subset Stream operations ",NULL,"This could focus only on the subset Stream operations ",NULL,"Add for who this story is","well_formed","no_role","high",False
18265,"As a Spring XD developer, I'd like to port transform module from XD to s c s repo, so I can use it as processor module to build streaming pipeline.","As a Spring XD developer",", I'd like to port transform module from XD to s c s repo,","so I can use it as processor module to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18266,"As a Spring XD developer, I'd like to port filter module from XD to s c s repo, so I can use it as processor module to build streaming pipeline.","As a Spring XD developer",", I'd like to port filter module from XD to s c s repo,","so I can use it as processor module to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18271,"Enable spring cloud config for all modules Add spring cloud config client to pom dependencies. Add bootstrap.yml to scs project ",NULL,"Enable spring cloud config for all modules Add spring cloud config client to pom dependencies. Add bootstrap.yml to scs project ",NULL,"Add for who this story is","well_formed","no_role","high",False
18271,"Enable spring cloud config for all modules Add spring cloud config client to pom dependencies. Add bootstrap.yml to scs project ",NULL,"Enable spring cloud config for all modules Add spring cloud config client to pom dependencies. Add bootstrap.yml to scs project ",NULL,"Enable spring cloud config for all modules Add spring cloud config client to pom dependencies<span class='highlight-text severity-high'>. Add bootstrap.yml to scs project </span>","minimal","punctuation","high",False
18269,"As a Spring XD developer, I'd like to port tcp module from XD to s c s repo, so I can use it as source module to build streaming pipeline. ","As a Spring XD developer",", I'd like to port tcp module from XD to s c s repo,","so I can use it as source module to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18270,"As a Spring XD developer, I'd like to port http module from XD to s c s repo, so I can use it as source module in streaming pipeline. ","As a Spring XD developer",", I'd like to port http module from XD to s c s repo,","so I can use it as source module in streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18273,"As a s c d developer, I'd like to derive a strategy for module metadata via ConfigurationProperties , so I can implement module info command in shell to list all the module properties. ","As a s c d developer",", I'd like to derive a strategy for module metadata via ConfigurationProperties ,","so I can implement module info command in shell to list all the module properties.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18276,"Create Confguration and ConfigurationProperties. Configuration must support replacing the default Kryo Codec implementation with something else.",NULL,"Create Confguration and ConfigurationProperties. Configuration must support replacing the default Kryo Codec implementation with something else.",NULL,"Add for who this story is","well_formed","no_role","high",False
18276,"Create Confguration and ConfigurationProperties. Configuration must support replacing the default Kryo Codec implementation with something else.",NULL,"Create Confguration and ConfigurationProperties. Configuration must support replacing the default Kryo Codec implementation with something else.",NULL,"Create Confguration and ConfigurationProperties<span class='highlight-text severity-high'>. Configuration must support replacing the default Kryo Codec implementation with something else.</span>","minimal","punctuation","high",False
18275,"As a user I would like to have shell interface to the spring cloud data rest API. The scope for this JIRA could be limited to stream commands.","As a user","I would like to have shell interface to the spring cloud data rest API. The scope for this JIRA could be limited to stream commands.",NULL,"As a user I would like to have shell interface to the spring cloud data rest API<span class='highlight-text severity-high'>. The scope for this JIRA could be limited to stream commands.</span>","minimal","punctuation","high",False
18275,"As a user I would like to have shell interface to the spring cloud data rest API. The scope for this JIRA could be limited to stream commands.","As a user","I would like to have shell interface to the spring cloud data rest API. The scope for this JIRA could be limited to stream commands.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18277,"As a s c d developer, I'd like to add support to expose counter metrics endpoints, so I can consume to feed the dashboards to demonstrate firehose counter pipe.","As a s c d developer",", I'd like to add support to expose counter metrics endpoints,","so I can consume to feed the dashboards to demonstrate firehose counter pipe.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18259,"As a Spring XD developer, I'd like to move gemfire module from XD to s c s repo, so I can use it as sink to build streaming pipeline.","As a Spring XD developer",", I'd like to move gemfire module from XD to s c s repo,","so I can use it as sink to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18258,"Currently the DSL parsing for tasks is a copy and paste of what it is for streams minus the ability to parse multiple modules . This results in a lot of duplication. This should be refactored to remove duplication and remove explicit references to either streams or tasks in common code.",NULL,"Currently the DSL parsing for tasks is a copy and paste of what it is for streams minus the ability to parse multiple modules . This results in a lot of duplication. This should be refactored to remove duplication and remove explicit references to either streams or tasks in common code.",NULL,"Add for who this story is","well_formed","no_role","high",False
18258,"Currently the DSL parsing for tasks is a copy and paste of what it is for streams minus the ability to parse multiple modules . This results in a lot of duplication. This should be refactored to remove duplication and remove explicit references to either streams or tasks in common code.",NULL,"Currently the DSL parsing for tasks is a copy and paste of what it is for streams minus the ability to parse multiple modules . This results in a lot of duplication. This should be refactored to remove duplication and remove explicit references to either streams or tasks in common code.",NULL,"Currently the DSL parsing for tasks is a copy<span class='highlight-text severity-high'> and </span>paste of what it is for streams minus the ability to parse multiple modules . This results in a lot of duplication. This should be refactored to remove duplication and remove explicit references to either streams<span class='highlight-text severity-high'> or </span>tasks in common code.","atomic","conjunctions","high",False
18258,"Currently the DSL parsing for tasks is a copy and paste of what it is for streams minus the ability to parse multiple modules . This results in a lot of duplication. This should be refactored to remove duplication and remove explicit references to either streams or tasks in common code.",NULL,"Currently the DSL parsing for tasks is a copy and paste of what it is for streams minus the ability to parse multiple modules . This results in a lot of duplication. This should be refactored to remove duplication and remove explicit references to either streams or tasks in common code.",NULL,"Currently the DSL parsing for tasks is a copy and paste of what it is for streams minus the ability to parse multiple modules <span class='highlight-text severity-high'>. This results in a lot of duplication. This should be refactored to remove duplication and remove explicit references to either streams or tasks in common code.</span>","minimal","punctuation","high",False
18282,"As a s c d developer, I'd like to publish the s c d image to DockerHub, so I can incrementally push the latest commits to the remote location.","As a s c d developer",", I'd like to publish the s c d image to DockerHub,","so I can incrementally push the latest commits to the remote location.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18289,"Currently undeploy is a no op.",NULL,"Currently undeploy is a no op.",NULL,"Add for who this story is","well_formed","no_role","high",False
18287,"As part of moving to a bespoke RestOperations application we will need credentials to access CloudFoundry. These will need to be supplied from the new XD Admin app at runtime.","As part","of moving to a bespoke RestOperations application we will need credentials to access CloudFoundry. These will need to be supplied from the new XD Admin app at runtime.",NULL,"As part of moving to a bespoke RestOperations application we will need credentials to access CloudFoundry<span class='highlight-text severity-high'>. These will need to be supplied from the new XD Admin app at runtime.</span>","minimal","punctuation","high",False
18287,"As part of moving to a bespoke RestOperations application we will need credentials to access CloudFoundry. These will need to be supplied from the new XD Admin app at runtime.","As part","of moving to a bespoke RestOperations application we will need credentials to access CloudFoundry. These will need to be supplied from the new XD Admin app at runtime.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18286,"This class should not know what the test app is. This means changing the constructors on CloudFoundryApplication.",NULL,"This class should not know what the test app is. This means changing the constructors on CloudFoundryApplication.",NULL,"Add for who this story is","well_formed","no_role","high",False
18286,"This class should not know what the test app is. This means changing the constructors on CloudFoundryApplication.",NULL,"This class should not know what the test app is. This means changing the constructors on CloudFoundryApplication.",NULL,"This class should not know what the test app is<span class='highlight-text severity-high'>. This means changing the constructors on CloudFoundryApplication.</span>","minimal","punctuation","high",False
18291,"Remove all stubs and check all required information is returned accurately.",NULL,"Remove all stubs and check all required information is returned accurately.",NULL,"Add for who this story is","well_formed","no_role","high",False
18291,"Remove all stubs and check all required information is returned accurately.",NULL,"Remove all stubs and check all required information is returned accurately.",NULL,"Remove all stubs<span class='highlight-text severity-high'> and </span>check all required information is returned accurately.","atomic","conjunctions","high",False
18288,"The current ModuleRunner is test app used for validation. This should be replaced by a real app.",NULL,"The current ModuleRunner is test app used for validation. This should be replaced by a real app.",NULL,"Add for who this story is","well_formed","no_role","high",False
18288,"The current ModuleRunner is test app used for validation. This should be replaced by a real app.",NULL,"The current ModuleRunner is test app used for validation. This should be replaced by a real app.",NULL,"The current ModuleRunner is test app used for validation<span class='highlight-text severity-high'>. This should be replaced by a real app.</span>","minimal","punctuation","high",False
18293,"ModuleDefinition contains bindings that need to be passed to the ModuleRunner app. It appears these can be included in the application s environment.",NULL,"ModuleDefinition contains bindings that need to be passed to the ModuleRunner app. It appears these can be included in the application s environment.",NULL,"Add for who this story is","well_formed","no_role","high",False
18293,"ModuleDefinition contains bindings that need to be passed to the ModuleRunner app. It appears these can be included in the application s environment.",NULL,"ModuleDefinition contains bindings that need to be passed to the ModuleRunner app. It appears these can be included in the application s environment.",NULL,"ModuleDefinition contains bindings that need to be passed to the ModuleRunner app<span class='highlight-text severity-high'>. It appears these can be included in the application s environment.</span>","minimal","punctuation","high",False
18292,"A ModuleDefinition contains parameters, which need to be passed to the CloudFoundry application. Currently these are put directly into the application s environment. This issue will verify they are correctly named.",NULL,"A ModuleDefinition contains parameters, which need to be passed to the CloudFoundry application. Currently these are put directly into the application s environment. This issue will verify they are correctly named.",NULL,"Add for who this story is","well_formed","no_role","high",False
18292,"A ModuleDefinition contains parameters, which need to be passed to the CloudFoundry application. Currently these are put directly into the application s environment. This issue will verify they are correctly named.",NULL,"A ModuleDefinition contains parameters, which need to be passed to the CloudFoundry application. Currently these are put directly into the application s environment. This issue will verify they are correctly named.",NULL,"A ModuleDefinition contains parameters, which need to be passed to the CloudFoundry application<span class='highlight-text severity-high'>. Currently these are put directly into the application s environment. This issue will verify they are correctly named.</span>","minimal","punctuation","high",False
18294,"As a s c s developer, I'd like to setup CI infrastructure for spring cloud stream modules s c s m repo, so I can build the project continuously on every commits. ","As a s c","s developer, I'd like to setup CI infrastructure for spring cloud stream modules s c s m repo,","so I can build the project continuously on every commits.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18298,"As a s c d developer, I'd like to resolve and then add module dependent JAR s to Boot loader, so I have an approach to handle external libraries ex database drivers required by OOTB modules. ","As a s c d developer",", I'd like to re","solve and then add module dependent JAR s to Boot loader, so I have an approach to handle external libraries ex database drivers required by OOTB modules.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18296,"As a s c s developer, I'd like to investigate the right approach to port PHD as the provider to support HDFS module from XD, so I can decide better handling of HDFS dependencies, which needs loaded and available in root CP at the runtime. ","As a s c","s developer, I'd like to investigate the right approach to port PHD as the provider to support HDFS module from XD,","so I can decide better handling of HDFS dependencies, which needs loaded and available in root CP at the runtime.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18295,"As a s c d user, I'd like to add REST support for stream commands, so I can maneuver streaming pipeline backed by StreamController.","As a s c d user",", I'd like to add REST support for stream commands,","so I can maneuver streaming pipeline backed by StreamController.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18297,"As a s c s developer, I'd like to bootify ModuleLauncher , so I can use Spring Boot s support for property, setting, as well as adding options and new functionality in the future, such as CP augmentation. ","As a s c","s developer, I'd like to bootify ModuleLauncher ,","so I can use Spring Boot s support for property, setting, as well as adding options and new functionality in the future, such as CP augmentation.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18299,"As a s c d developer, I'd like to setup CI infrastructure for s c d repo https github.com spring cloud spring cloud data , so I can build the project continuously on every commits. ","As a s c d developer",", I'd like to setup CI infrastructure for s c d repo https github.com spring cloud spring cloud data ,","so I can build the project continuously on every commits.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18313,"As a s c s developer, I'd like to setup CI builds for s c s builds, so I can incrementally build and test code commits automatically.","As a s c","s developer, I'd like to setup CI builds for s c s builds,","so I can incrementally build and test code commits automatically.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18320,"As a Spring XD user, I'd like to make SPI implementation profile aware, so I can run java jar admin or cf push admin or ltc create admin and the corresponding implementation gets wired in automatically.","As a Spring XD user",", I'd like to make SPI implementation profile aware,","so I can run java jar admin or cf push admin or ltc create admin and the corresponding implementation gets wired in automatically.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18322,"As a Spring XD developer, I'd like to have a permanent location of SPI implementations, so I could use the common repo every time I contribute or enhance the test coverage. ","As a Spring XD developer",", I'd like to have a permanent location of SPI implementations,","so I could use the common repo every time I contribute or enhance the test coverage.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18314,"As a Spring XD developer, I'd like to self register xd admin server with Eureka , so I could have admin server exposed as discoverable endpoint. ","As a Spring XD developer",", I'd like to self register xd admin server with Eureka ,","so I could have admin server exposed as discoverable endpoint.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18315,"As a Spring XD developer, I'd like to refactor current controller with SPI calls, so I can invoke the respective Admin SPI implementation based on the deployment. Controllers to Refactor ContainersController StreamsController ModulesController JobsController ","As a Spring XD developer",", I'd like to refactor current controller with SPI calls,","so I can invoke the respective Admin SPI implementation based on the deployment. Controllers to Refactor ContainersController StreamsController ModulesController JobsController","As a Spring XD developer, I'd like to refactor current controller with SPI calls, so I can invoke the respective Admin SPI implementation based on the deployment<span class='highlight-text severity-high'>. Controllers to Refactor ContainersController StreamsController ModulesController JobsController </span>","minimal","punctuation","high",False
18315,"As a Spring XD developer, I'd like to refactor current controller with SPI calls, so I can invoke the respective Admin SPI implementation based on the deployment. Controllers to Refactor ContainersController StreamsController ModulesController JobsController ","As a Spring XD developer",", I'd like to refactor current controller with SPI calls,","so I can invoke the respective Admin SPI implementation based on the deployment. Controllers to Refactor ContainersController StreamsController ModulesController JobsController","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18318,"As a s c s user, I'd like to search the modules by it s name aside from the default spring.application.name offered by boot, so I can also fetch modules by it s name.","As a s c","s user, I'd like to search the modules by it s name aside from the default spring.application.name offered by boot,","so I can also fetch modules by it s name.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18323,"As a Spring XD on CF user, I'd like to use Receptor implementation of Admin SPI every time I'deploy Spring XD modules, so I can leverage the SPI to query for module status and health metrics. Possible APIs code ModuleStatus getStatus ModuleDescriptor descriptor ; Collection ModuleDescriptor listModules ; Map ModuleDescriptor.Key, ModuleStatus code ","As a Spring XD on CF u","ser, I'd like to use Receptor implementation of Admin SPI every time I'deploy Spring XD modules,","so I can leverage the SPI to query for module status and health metrics. Possible APIs code ModuleStatus getStatus ModuleDescriptor descriptor ; Collection ModuleDescriptor listModules ; Map ModuleDescriptor.Key, ModuleStatus code","As a Spring XD on CF user, I'd like to use Receptor implementation of Admin SPI every time I'deploy Spring XD modules, so I can leverage the SPI to query for module status and health metrics<span class='highlight-text severity-high'>. Possible APIs code ModuleStatus getStatus ModuleDescriptor descriptor ; Collection ModuleDescriptor listModules ; Map ModuleDescriptor.Key, ModuleStatus code </span>","minimal","punctuation","high",False
18323,"As a Spring XD on CF user, I'd like to use Receptor implementation of Admin SPI every time I'deploy Spring XD modules, so I can leverage the SPI to query for module status and health metrics. Possible APIs code ModuleStatus getStatus ModuleDescriptor descriptor ; Collection ModuleDescriptor listModules ; Map ModuleDescriptor.Key, ModuleStatus code ","As a Spring XD on CF u","ser, I'd like to use Receptor implementation of Admin SPI every time I'deploy Spring XD modules,","so I can leverage the SPI to query for module status and health metrics. Possible APIs code ModuleStatus getStatus ModuleDescriptor descriptor ; Collection ModuleDescriptor listModules ; Map ModuleDescriptor.Key, ModuleStatus code","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18316,"As a s c s user, I'd like to have my modules add update it s current state to Eureka, so I can use the repository to discover the current sate of the module as needed. ","As a s c","s user, I'd like to have my modules add update it s current state to Eureka,","so I can use the repository to discover the current sate of the module as needed.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18317,"As a s c s user, I'd like to store module metadata in Eureka , so I can use the repository to determine the current state.","As a s c","s user, I'd like to store module metadata in Eureka ,","so I can use the repository to determine the current state.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18321,"As a Spring XD developer, I'd like to create initial version of the new module registry abstraction, so we could leverage the foundation to make progress and test the respective SPI receptor or cloudcontroller implementations.","As a Spring XD developer",", I'd like to create initial version of the new module registry abstraction,","so we could leverage the foundation to make progress and test the respective SPI receptor or cloudcontroller implementations.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18311,"As a s c s developer, I'd like to setup a CI workflow to build, bundle and upload the module launcher image to DockerHub, so I'don t have to worry about having a local private docker registry for development testing. It could be nice to have the image uploaded to existing spring cloud https registry.hub.docker.com repos springcloud DockerHub location. ","As a s c","s developer, I'd like to setup a CI workflow to build, bundle and upload the module launcher image to DockerHub,","so I'don t have to worry about having a local private docker registry for development testing. It could be nice to have the image uploaded to existing spring cloud https registry.hub.docker.com repos springcloud DockerHub location.","As a s c s developer, I'd like to setup a CI workflow to build, bundle<span class='highlight-text severity-high'> and </span>upload the module launcher image to DockerHub, so I'don t have to worry about having a local private docker registry for development testing. It could be nice to have the image uploaded to existing spring cloud https registry.hub.docker.com repos springcloud DockerHub location. ","atomic","conjunctions","high",False
18311,"As a s c s developer, I'd like to setup a CI workflow to build, bundle and upload the module launcher image to DockerHub, so I'don t have to worry about having a local private docker registry for development testing. It could be nice to have the image uploaded to existing spring cloud https registry.hub.docker.com repos springcloud DockerHub location. ","As a s c","s developer, I'd like to setup a CI workflow to build, bundle and upload the module launcher image to DockerHub,","so I'don t have to worry about having a local private docker registry for development testing. It could be nice to have the image uploaded to existing spring cloud https registry.hub.docker.com repos springcloud DockerHub location.","As a s c s developer, I'd like to setup a CI workflow to build, bundle and upload the module launcher image to DockerHub, so I'don t have to worry about having a local private docker registry for development testing<span class='highlight-text severity-high'>. It could be nice to have the image uploaded to existing spring cloud https registry.hub.docker.com repos springcloud DockerHub location. </span>","minimal","punctuation","high",False
18311,"As a s c s developer, I'd like to setup a CI workflow to build, bundle and upload the module launcher image to DockerHub, so I'don t have to worry about having a local private docker registry for development testing. It could be nice to have the image uploaded to existing spring cloud https registry.hub.docker.com repos springcloud DockerHub location. ","As a s c","s developer, I'd like to setup a CI workflow to build, bundle and upload the module launcher image to DockerHub,","so I'don t have to worry about having a local private docker registry for development testing. It could be nice to have the image uploaded to existing spring cloud https registry.hub.docker.com repos springcloud DockerHub location.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18312,"As a Spring XD developer, I'd like to port FTP modules from XD to s c s repo, so I can use them as source modules to build streaming pipeline.","As a Spring XD developer",", I'd like to port FTP modules from XD to s c s repo,","so I can use them as source modules to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18319,"As a s c s user, I'd like to have the modules self register itself with Eureka whenever they re installed, so I can also discover the same modules using Spring XD Admin SPI and reuse them to create data pipelines. ","As a s c","s user, I'd like to have the modules self register itself with Eureka whenever they re installed,","so I can also discover the same modules using Spring XD Admin SPI and reuse them to create data pipelines.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18324,"As a Spring XD user, I'd like to use CloudController based implementation of XD Admin SPI based on ModuleLauncher , so I can run data pipeline use cases running on CF. Relevant repos spring cloud data https github.com spring cloud spring cloud data tree master spring cloud data module deployers spring cloud stream https github.com spring cloud spring cloud stream Please refer to XD 3194 or XD 3229 as sample spike deliverables google doc that were completed in the last sprint. ","As a Spring XD user",", I'd like to use CloudController based implementation of XD Admin SPI based on ModuleLauncher ,","so I can run data pipeline use cases running on CF. Relevant repos spring cloud data https github.com spring cloud spring cloud data tree master spring cloud data module deployers spring cloud stream https github.com spring cloud spring cloud stream Please refer to XD 3194 or XD 3229 as sample spike deliverables google doc that were completed in the last sprint.","As a Spring XD user, I'd like to use CloudController based implementation of XD Admin SPI based on ModuleLauncher , so I can run data pipeline use cases running on CF<span class='highlight-text severity-high'>. Relevant repos spring cloud data https github.com spring cloud spring cloud data tree master spring cloud data module deployers spring cloud stream https github.com spring cloud spring cloud stream Please refer to XD 3194 or XD 3229 as sample spike deliverables google doc that were completed in the last sprint. </span>","minimal","punctuation","high",False
18324,"As a Spring XD user, I'd like to use CloudController based implementation of XD Admin SPI based on ModuleLauncher , so I can run data pipeline use cases running on CF. Relevant repos spring cloud data https github.com spring cloud spring cloud data tree master spring cloud data module deployers spring cloud stream https github.com spring cloud spring cloud stream Please refer to XD 3194 or XD 3229 as sample spike deliverables google doc that were completed in the last sprint. ","As a Spring XD user",", I'd like to use CloudController based implementation of XD Admin SPI based on ModuleLauncher ,","so I can run data pipeline use cases running on CF. Relevant repos spring cloud data https github.com spring cloud spring cloud data tree master spring cloud data module deployers spring cloud stream https github.com spring cloud spring cloud stream Please refer to XD 3194 or XD 3229 as sample spike deliverables google doc that were completed in the last sprint.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18328,"replace with xd.messagebus prefix with spring.cloud.stream.binder",NULL,"replace with xd.messagebus prefix with spring.cloud.stream.binder",NULL,"Add for who this story is","well_formed","no_role","high",False
18326,"There is a vulnerability in Groovy that is fixed in 2.4.4 CVE 2015 3253 Remote execution of untrusted code See http groovy lang.org security.html http mail archives.apache.org mod mbox incubator groovy users 201507.mbox 3CCADQzvmmYC7RbZnsQ8O63XN4HCMYh9RGRdMiuWupVt u pjH8 g mail.gmail.com 3E ",NULL,"There is a vulnerability in Groovy that is fixed in 2.4.4 CVE 2015 3253 Remote execution of untrusted code See http groovy lang.org security.html http mail archives.apache.org mod mbox incubator groovy users 201507.mbox 3CCADQzvmmYC7RbZnsQ8O63XN4HCMYh9RGRdMiuWupVt u pjH8 g mail.gmail.com 3E ",NULL,"Add for who this story is","well_formed","no_role","high",False
18335,"As a developer, I'd like to move 1.2.x branch to EC2 infrastructure, so I can reliably run CI test suites.","As a developer",", I'd like to move 1.2.x branch to EC2 infrastructure,","so I can reliably run CI test suites.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18327,"As a developer, I'd like to move serialization codec from Spring XD repo into SI, so I can update Spring XD to inherit the features functionalities via maven dependency.","As a developer",", I'd like to move serialization codec from Spring XD repo into SI,","so I can update Spring XD to inherit the features functionalities via maven dependency.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18332,"This better aligns with boot. Moreover, using Class was a bad design choice one can always get a Class from a String modulo knowing which CL to use , while to converse is not always easy CL not being available ",NULL,"This better aligns with boot. Moreover, using Class was a bad design choice one can always get a Class from a String modulo knowing which CL to use , while to converse is not always easy CL not being available ",NULL,"Add for who this story is","well_formed","no_role","high",False
18332,"This better aligns with boot. Moreover, using Class was a bad design choice one can always get a Class from a String modulo knowing which CL to use , while to converse is not always easy CL not being available ",NULL,"This better aligns with boot. Moreover, using Class was a bad design choice one can always get a Class from a String modulo knowing which CL to use , while to converse is not always easy CL not being available ",NULL,"This better aligns with boot<span class='highlight-text severity-high'>. Moreover, using Class was a bad design choice one can always get a Class from a String modulo knowing which CL to use , while to converse is not always easy CL not being available </span>","minimal","punctuation","high",False
18329,"We need to have some jars as part of the Sqoop job submission to YARN for Avro we need avro 1.7.6.jar avro mapred 1.7.6.jar for Snappy we need snappy java 1.0.5.jar note the 1.1.0.1 version from xd lib doesn t work commons compress 1.4.1.jar We can either have these included using the libjars option or automatically include them. ",NULL,"We need to have some jars as part of the Sqoop job submission to YARN for Avro we need avro 1.7.6.jar avro mapred 1.7.6.jar for Snappy we need snappy java 1.0.5.jar note the 1.1.0.1 version from xd lib doesn t work commons compress 1.4.1.jar We can either have these included using the libjars option or automatically include them. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18329,"We need to have some jars as part of the Sqoop job submission to YARN for Avro we need avro 1.7.6.jar avro mapred 1.7.6.jar for Snappy we need snappy java 1.0.5.jar note the 1.1.0.1 version from xd lib doesn t work commons compress 1.4.1.jar We can either have these included using the libjars option or automatically include them. ",NULL,"We need to have some jars as part of the Sqoop job submission to YARN for Avro we need avro 1.7.6.jar avro mapred 1.7.6.jar for Snappy we need snappy java 1.0.5.jar note the 1.1.0.1 version from xd lib doesn t work commons compress 1.4.1.jar We can either have these included using the libjars option or automatically include them. ",NULL,"We need to have some jars as part of the Sqoop job submission to YARN for Avro we need avro 1.7.6.jar avro mapred 1.7.6.jar for Snappy we need snappy java 1.0.5.jar note the 1.1.0.1 version from xd lib doesn t work commons compress 1.4.1.jar We can either have these included using the libjars option<span class='highlight-text severity-high'> or </span>automatically include them. ","atomic","conjunctions","high",False
18331,"The spring cloud streams samples have module options classes copied over from XD. They should use a pure ConfigurationProperties approach, making sure metadata is generated hand written as appropriate. Mixins are still referenced there but obviously can t work, so provide an equivalent",NULL,"The spring cloud streams samples have module options classes copied over from XD. They should use a pure ConfigurationProperties approach, making sure metadata is generated hand written as appropriate. Mixins are still referenced there but obviously can t work,","so provide an equivalent","Add for who this story is","well_formed","no_role","high",False
18331,"The spring cloud streams samples have module options classes copied over from XD. They should use a pure ConfigurationProperties approach, making sure metadata is generated hand written as appropriate. Mixins are still referenced there but obviously can t work, so provide an equivalent",NULL,"The spring cloud streams samples have module options classes copied over from XD. They should use a pure ConfigurationProperties approach, making sure metadata is generated hand written as appropriate. Mixins are still referenced there but obviously can t work,","so provide an equivalent","The spring cloud streams samples have module options classes copied over from XD<span class='highlight-text severity-high'>. They should use a pure ConfigurationProperties approach, making sure metadata is generated hand written as appropriate. Mixins are still referenced there but obviously can t work, so provide an equivalent</span>","minimal","punctuation","high",False
18331,"The spring cloud streams samples have module options classes copied over from XD. They should use a pure ConfigurationProperties approach, making sure metadata is generated hand written as appropriate. Mixins are still referenced there but obviously can t work, so provide an equivalent",NULL,"The spring cloud streams samples have module options classes copied over from XD. They should use a pure ConfigurationProperties approach, making sure metadata is generated hand written as appropriate. Mixins are still referenced there but obviously can t work,","so provide an equivalent","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18330,"As a developer, I'd like to brainstorm and investigate various techniques around installation of XD modules from a maven repo, so I could define the module artifactId from CLI to have the module downloaded from the repo and installed to a running Spring XD runtime.","As a developer",", I'd like to brainstorm and investigate various techniques around installation of XD modules from a maven repo,","so I could define the module artifactId from CLI to have the module downloaded from the repo and installed to a running Spring XD runtime.","As a developer, I'd like to brainstorm<span class='highlight-text severity-high'> and </span>investigate various techniques around installation of XD modules from a maven repo, so I could define the module artifactId from CLI to have the module downloaded from the repo and installed to a running Spring XD runtime.","atomic","conjunctions","high",False
18330,"As a developer, I'd like to brainstorm and investigate various techniques around installation of XD modules from a maven repo, so I could define the module artifactId from CLI to have the module downloaded from the repo and installed to a running Spring XD runtime.","As a developer",", I'd like to brainstorm and investigate various techniques around installation of XD modules from a maven repo,","so I could define the module artifactId from CLI to have the module downloaded from the repo and installed to a running Spring XD runtime.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18334,"Currently only database connection info can be read from a control file yml format. Should add rest of the missing options to align how native format works.",NULL,"Currently only database connection info can be read from a control file yml format. Should add rest of the missing options to align how native format works.",NULL,"Add for who this story is","well_formed","no_role","high",False
18334,"Currently only database connection info can be read from a control file yml format. Should add rest of the missing options to align how native format works.",NULL,"Currently only database connection info can be read from a control file yml format. Should add rest of the missing options to align how native format works.",NULL,"Currently only database connection info can be read from a control file yml format<span class='highlight-text severity-high'>. Should add rest of the missing options to align how native format works.</span>","minimal","punctuation","high",False
18333,"Currently we can only do plain inserts, should follow same logic from native gpfdist sink and add upserts.",NULL,"Currently we can only do plain inserts, should follow same logic from native gpfdist sink and add upserts.",NULL,"Add for who this story is","well_formed","no_role","high",False
18333,"Currently we can only do plain inserts, should follow same logic from native gpfdist sink and add upserts.",NULL,"Currently we can only do plain inserts, should follow same logic from native gpfdist sink and add upserts.",NULL,"Currently we can only do plain inserts, should follow same logic from native gpfdist sink<span class='highlight-text severity-high'> and </span>add upserts.","atomic","conjunctions","high",False
18336,"As a developer, I'd like to complete the remaining Kryo optimization changes, so I can polish and get the guidelines documented appropriately. ","As a developer",", I'd like to complete the remaining Kryo optimization changes,","so I can polish and get the guidelines documented appropriately.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18337,"The XML REST endpoints are not working correctly interfere with security are not used ",NULL,"The XML REST endpoints are not working correctly interfere with security are not used ",NULL,"Add for who this story is","well_formed","no_role","high",False
18355,"As a developer, I'd like to handle module options via pure boot property source management, so I can leverage Boot s module METADATA http docs.spring.io spring boot docs current SNAPSHOT reference htmlsingle configuration metadata option to inject module options as opposed to maintaining them in core Spring XD runtime CP. ","As a developer",", I'd like to handle module options via pure boot property","source management, so I can leverage Boot s module METADATA http docs.spring.io spring boot docs current SNAPSHOT reference htmlsingle configuration metadata option to inject module options as opposed to maintaining them in core Spring XD runtime CP.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18340,"As a developer, I'd like to update Ambari installed Spring XD cluster to spin up multiple instances of XD Admin servers, so it is setup for HA. ","As a developer",", I'd like to update Ambari installed Spring XD cluster to spin up multiple instances of XD Admin servers,","so it is setup for HA.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18344,"As a developer, I'd like to develop a singlenode in a single JVM implementation of XD Admin SPI based on Module Launcher , so I can run data pipeline use cases locally. ","As a developer",", I'd like to develop a singlenode in a single JVM implementation of XD Admin SPI based on Module Launcher ,","so I can run data pipeline use cases locally.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18342,"As a s c s user, I'd like to investigate the possibility of s c s modules self registering themselves to service discovery, so I could use Spring XD runtime running on CF to discover and orchestrate such modules through streams.","As a s c","s user, I'd like to investigate the possibility of s c s modules self registering themselves to service discovery,","so I could use Spring XD runtime running on CF to discover and orchestrate such modules through streams.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18351,"As a developer, I'd like to investigate channel performance issues in SI 4.2, so I can determine the bottlenecks and take corrective actions to improve overall channel performance. ","As a developer",", I'd like to investigate channel performance issues in SI 4.2,","so I can determine the bottlenecks and take corrective actions to improve overall channel performance.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18345,"As a developer, I'd like to move serialization codec from Spring XD repo into spring bus, so I can update Spring XD to inherit the features functionalities via maven dependency.","As a developer",", I'd like to move serialization codec from Spring XD repo into spring bus,","so I can update Spring XD to inherit the features functionalities via maven dependency.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18349,"As a user, I'd like to upgrade Spring XD from 1.2 RC to 1.2 GA using the Ambari plugin, so I can work on the latest release bits. I'd like to refer to the documentation to do so.","As a user",", I'd like to upgrade Spring XD from 1.2 RC to 1.2 GA using the Ambari plugin,","so I can work on the latest release bits. I'd like to refer to the documentation to do so.","As a user, I'd like to upgrade Spring XD from 1<span class='highlight-text severity-high'>.2 RC to 1.2 GA using the Ambari plugin, so I can work on the latest release bits. I'd like to refer to the documentation to do so.</span>","minimal","punctuation","high",False
18349,"As a user, I'd like to upgrade Spring XD from 1.2 RC to 1.2 GA using the Ambari plugin, so I can work on the latest release bits. I'd like to refer to the documentation to do so.","As a user",", I'd like to upgrade Spring XD from 1.2 RC to 1.2 GA using the Ambari plugin,","so I can work on the latest release bits. I'd like to refer to the documentation to do so.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18339,"Also Spring Framework 4.2.0.RC2, Spring AMQP 1.5.0.M1 Also Batch 3.0.4",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
18339,"Also Spring Framework 4.2.0.RC2, Spring AMQP 1.5.0.M1 Also Batch 3.0.4",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
18341,"As a developer, I'd like to upgrade to Reactor 2.0.4 release, so I could leverage the latest improvements and bug fixes.","As a developer",", I'd like to upgrade to Reactor 2.0.4 release,","so I could leverage the latest improvements and bug fixes.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18350,"As a spring bus lead, I'd like to review the current spring bus architecture and the design specs, so I can address any foundation level gaps.","As a spring bus lead",", I'd like to review the current spring bus architecture and the design specs,","so I can address any foundation level gaps.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18346,"As a developer, I'd like to move input output type conversion from Spring XD repo to spring cloud dataflow, so I can implement a custom module which produces or consumes a custom domain object.","As a developer",", I'd like to move input output type conversion from Spring XD repo to spring cloud dataflow,","so I can implement a custom module which produces or consumes a custom domain object.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18347,"As a developer, I'd like to move message bus from Spring XD repo into spring bus, so I can update Spring XD to inherit the features functionalities via maven dependency. ","As a developer",", I'd like to move message bus from Spring XD repo into spring bus,","so I can update Spring XD to inherit the features functionalities via maven dependency.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18343,"As a Spring XD user, I'd like to use Diego based Receptor implementation of XD Admin SPI based on ModuleLauncher , so I can run data pipeline use cases running on CF Lattice Diego. ","As a Spring XD user",", I'd like to use Diego based Receptor implementation of XD Admin SPI based on ModuleLauncher ,","so I can run data pipeline use cases running on CF Lattice Diego.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18359,"Spring Boot 1.2.4 and earlier does not allow for the retrieval of boolean values from the vcap environment. This means that when XD Admin tries to retrieve the value of for example vcap.services.rabbitmq.credentials.protocols.amqp.ssl it will fail, as that value returns a boolean. Spring Boot 1.2.5 as yet unreleased contains a fix for this issue https github.com spring projects spring boot pull 3237 ",NULL,"Spring Boot 1.2.4 and earlier does not allow for the retrieval of boolean values from the vcap environment. This means that when XD Admin tries to retrieve the value of for example vcap.services.rabbitmq.credentials.protocols.amqp.ssl it will fail, as that value returns a boolean. Spring Boot 1.2.5 as yet unreleased contains a fix for this issue https github.com spring projects spring boot pull 3237 ",NULL,"Add for who this story is","well_formed","no_role","high",False
18359,"Spring Boot 1.2.4 and earlier does not allow for the retrieval of boolean values from the vcap environment. This means that when XD Admin tries to retrieve the value of for example vcap.services.rabbitmq.credentials.protocols.amqp.ssl it will fail, as that value returns a boolean. Spring Boot 1.2.5 as yet unreleased contains a fix for this issue https github.com spring projects spring boot pull 3237 ",NULL,"Spring Boot 1.2.4 and earlier does not allow for the retrieval of boolean values from the vcap environment. This means that when XD Admin tries to retrieve the value of for example vcap.services.rabbitmq.credentials.protocols.amqp.ssl it will fail, as that value returns a boolean. Spring Boot 1.2.5 as yet unreleased contains a fix for this issue https github.com spring projects spring boot pull 3237 ",NULL,"Spring Boot 1<span class='highlight-text severity-high'>.2.4 and earlier does not allow for the retrieval of boolean values from the vcap environment. This means that when XD Admin tries to retrieve the value of for example vcap.services.rabbitmq.credentials.protocols.amqp.ssl it will fail, as that value returns a boolean. Spring Boot 1.2.5 as yet unreleased contains a fix for this issue https github.com spring projects spring boot pull 3237 </span>","minimal","punctuation","high",False
18358,"We need to add the settings needed to run XD on YARN when using Hortonworks HDP 2.2.6.0 which is the version you now get when installing with Ambari.",NULL,"We need to add the settings needed to run XD on YARN when using Hortonworks HDP 2.2.6.0 which is the version you now get when installing with Ambari.",NULL,"Add for who this story is","well_formed","no_role","high",False
18361,"When the bus is used outside of the XD container e.g. spring bus , the inheritance from Spring Boot configuration is broken no application.yml or servers.yml on the cp . Make the bus properties optional Add ",NULL,"When the bus is used outside of the XD container e.g. spring bus , the inheritance from Spring Boot configuration is broken no application.yml or servers.yml on the cp . Make the bus properties optional Add ",NULL,"Add for who this story is","well_formed","no_role","high",False
18361,"When the bus is used outside of the XD container e.g. spring bus , the inheritance from Spring Boot configuration is broken no application.yml or servers.yml on the cp . Make the bus properties optional Add ",NULL,"When the bus is used outside of the XD container e.g. spring bus , the inheritance from Spring Boot configuration is broken no application.yml or servers.yml on the cp . Make the bus properties optional Add ",NULL,"When the bus is used outside of the XD container e<span class='highlight-text severity-high'>.g. spring bus , the inheritance from Spring Boot configuration is broken no application.yml or servers.yml on the cp . Make the bus properties optional Add </span>","minimal","punctuation","high",False
18360,"spring xd Can drag way to configure flow in the form of figure? Similar to configure a workflow graphical interface Looking forward to reply",NULL,"spring xd Can drag way to configure flow in the form of figure? Similar to configure a workflow graphical interface Looking forward to reply",NULL,"Add for who this story is","well_formed","no_role","high",False
18360,"spring xd Can drag way to configure flow in the form of figure? Similar to configure a workflow graphical interface Looking forward to reply",NULL,"spring xd Can drag way to configure flow in the form of figure? Similar to configure a workflow graphical interface Looking forward to reply",NULL,"spring xd Can drag way to configure flow in the form of figure<span class='highlight-text severity-high'>? Similar to configure a workflow graphical interface Looking forward to reply</span>","minimal","punctuation","high",False
18362,"Depends on INT 3727",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
18362,"Depends on INT 3727",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
18364,"As a developer, I'd like a job module to be bootstrapped when the job is launched and shut down once it is complete instead of the current behavior of bootstrapping the context when the module is deployed regardless of if it s being used so that I can achieve better resource utilization.","As a developer",", I'd like a job module to be bootstrapped when the job is launched and shut down once it is complete instead of the current behavior of bootstrapping the context when the module is deployed regardless of if it s being used","so that I can achieve better resource utilization.","As a developer, I'd like a job module to be bootstrapped when the job is launched<span class='highlight-text severity-high'> and </span>shut down once it is complete instead of the current behavior of bootstrapping the context when the module is deployed regardless of if it s being used so that I can achieve better resource utilization.","atomic","conjunctions","high",False
18364,"As a developer, I'd like a job module to be bootstrapped when the job is launched and shut down once it is complete instead of the current behavior of bootstrapping the context when the module is deployed regardless of if it s being used so that I can achieve better resource utilization.","As a developer",", I'd like a job module to be bootstrapped when the job is launched and shut down once it is complete instead of the current behavior of bootstrapping the context when the module is deployed regardless of if it s being used","so that I can achieve better resource utilization.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18365,"As a developer, I'd like to publish performance benchmarks along with the infrastructure specifics, so the users can use it as a reference while setting up Spring XD cluster. ","As a developer",", I'd like to publish performance benchmarks along with the infrastructure specifics,","so the users can use it as a reference while setting up Spring XD cluster.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18371,"As a developer, I'd like to update to Spring Hadoop 2.2.0 GA release, so I can leverage the latest improvements. ","As a developer",", I'd like to update to Spring Hadoop 2.2.0 GA release,","so I can leverage the latest improvements.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18366,"As a PM, I'd like to have XD and XD Ambari RPM scripts into a single public repo, so that users can go to a single location to use the respective build scripts.","As a PM",", I'd like to have XD and XD Ambari RPM scripts into a single public repo,","so that users can go to a single location to use the respective build scripts.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18368,"Need acceptance tests to run on the 1.2.X branch. Needs to be setup as a child of the Publish 1.2.x",NULL,"Need acceptance tests to run on the 1.2.X branch. Needs to be setup as a child of the Publish 1.2.x",NULL,"Add for who this story is","well_formed","no_role","high",False
18369,"Sort alphabetically, nest Available modules section appropriately. Optionally, move to a whole different PART in reference doc",NULL,NULL,NULL,"Sort alphabetically, nest Available modules section appropriately<span class='highlight-text severity-high'>. Optionally, move to a whole different PART in reference doc</span>","minimal","punctuation","high",False
18363,"As a user I need to know the Spark streaming features like adding tap at the spark module output and the examples need to be updated. The documentation also needs some more information on Reliable receiver.","As a user","I need to know the Spark streaming features like adding tap at the spark module output and the examples need to be updated. The documentation also needs some more information on Reliable receiver.",NULL,"As a user I need to know the Spark streaming features like adding tap at the spark module output<span class='highlight-text severity-high'> and </span>the examples need to be updated. The documentation also needs some more information on Reliable receiver.","atomic","conjunctions","high",False
18363,"As a user I need to know the Spark streaming features like adding tap at the spark module output and the examples need to be updated. The documentation also needs some more information on Reliable receiver.","As a user","I need to know the Spark streaming features like adding tap at the spark module output and the examples need to be updated. The documentation also needs some more information on Reliable receiver.",NULL,"As a user I need to know the Spark streaming features like adding tap at the spark module output and the examples need to be updated<span class='highlight-text severity-high'>. The documentation also needs some more information on Reliable receiver.</span>","minimal","punctuation","high",False
18363,"As a user I need to know the Spark streaming features like adding tap at the spark module output and the examples need to be updated. The documentation also needs some more information on Reliable receiver.","As a user","I need to know the Spark streaming features like adding tap at the spark module output and the examples need to be updated. The documentation also needs some more information on Reliable receiver.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18372,"As a developer, I'd like to update to SI Kafka extension 1.2.0, so I can leverage the latest performance improvements.","As a developer",", I'd like to update to SI Kafka extension 1.2.0,","so I can leverage the latest performance improvements.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18357,"User s need ability to wait for user specified time in millis for a file to be created in the XD directory. If file is not created in allotted time then return false else return true. Also check to see if a file exists in the XD directory. ",NULL,"User s need ability to wait for user specified time in millis for a file to be created in the XD directory. If file is not created in allotted time then return false else return true. Also check to see if a file exists in the XD directory. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18357,"User s need ability to wait for user specified time in millis for a file to be created in the XD directory. If file is not created in allotted time then return false else return true. Also check to see if a file exists in the XD directory. ",NULL,"User s need ability to wait for user specified time in millis for a file to be created in the XD directory. If file is not created in allotted time then return false else return true. Also check to see if a file exists in the XD directory. ",NULL,"User s need ability to wait for user specified time in millis for a file to be created in the XD directory<span class='highlight-text severity-high'>. If file is not created in allotted time then return false else return true. Also check to see if a file exists in the XD directory. </span>","minimal","punctuation","high",False
18367,"As a developer, I want to be able to override Kafka bus defaults for module consumers and producers, so that I can finely tune performance and behaviour. Such properties should include autoCommitEnabled,queueSize,maxWait,fetchSize for consumers batchSize,batchTimeout for producers","As a developer,","I want to be able to override Kafka bus defaults for module consumers and producers,","so that I can finely tune performance and behaviour. Such properties should include autoCommitEnabled,queueSize,maxWait,fetchSize for consumers batchSize,batchTimeout for producers","As a developer, I want to be able to override Kafka bus defaults for module consumers and producers, so that I can finely tune performance and behaviour<span class='highlight-text severity-high'>. Such properties should include autoCommitEnabled,queueSize,maxWait,fetchSize for consumers batchSize,batchTimeout for producers</span>","minimal","punctuation","high",False
18367,"As a developer, I want to be able to override Kafka bus defaults for module consumers and producers, so that I can finely tune performance and behaviour. Such properties should include autoCommitEnabled,queueSize,maxWait,fetchSize for consumers batchSize,batchTimeout for producers","As a developer,","I want to be able to override Kafka bus defaults for module consumers and producers,","so that I can finely tune performance and behaviour. Such properties should include autoCommitEnabled,queueSize,maxWait,fetchSize for consumers batchSize,batchTimeout for producers","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18370,"As a developer, I in the new development of component source processor sink , how to get the module id and container id Because components need to generate log, log information must include the unique identifier xd runtime modules ","As a developer",", I in the new development of component source processor sink , how to get the module id and container id Because components need to generate log, log information must include the unique identifier xd runtime modules",NULL,"As a developer, I in the new development of component source processor sink , how to get the module id<span class='highlight-text severity-high'> and </span>container id Because components need to generate log, log information must include the unique identifier xd runtime modules ","atomic","conjunctions","high",False
18370,"As a developer, I in the new development of component source processor sink , how to get the module id and container id Because components need to generate log, log information must include the unique identifier xd runtime modules ","As a developer",", I in the new development of component source processor sink , how to get the module id and container id Because components need to generate log, log information must include the unique identifier xd runtime modules",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18384,"Currently kryo class registration is hard coded in spring xd codec. Users may register their own classes using an extension mechanism, but it is possible to conflict with internal XD class registration, e.g., Tuple. Exposing this using the same extension mechanism will make it more transparent. ",NULL,"Currently kryo class registration is hard coded in spring xd codec. Users may register their own classes using an extension mechanism, but it is possible to conflict with internal XD class registration, e.g., Tuple. Exposing this using the same extension mechanism will make it more transparent. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18384,"Currently kryo class registration is hard coded in spring xd codec. Users may register their own classes using an extension mechanism, but it is possible to conflict with internal XD class registration, e.g., Tuple. Exposing this using the same extension mechanism will make it more transparent. ",NULL,"Currently kryo class registration is hard coded in spring xd codec. Users may register their own classes using an extension mechanism, but it is possible to conflict with internal XD class registration, e.g., Tuple. Exposing this using the same extension mechanism will make it more transparent. ",NULL,"Currently kryo class registration is hard coded in spring xd codec<span class='highlight-text severity-high'>. Users may register their own classes using an extension mechanism, but it is possible to conflict with internal XD class registration, e.g., Tuple. Exposing this using the same extension mechanism will make it more transparent. </span>","minimal","punctuation","high",False
18392,"As a XD build master, I'd like to fix local . gradlew dist and distZip targets the outstanding build issues, so I can evaluate that publish builds works as expected. ","As a XD","build master, I'd like to fix local . gradlew dist and distZip targets the outstanding build issues,","so I can evaluate that publish builds works as expected.","As a XD build master, I'd like to fix local . gradlew dist<span class='highlight-text severity-high'> and </span>distZip targets the outstanding build issues, so I can evaluate that publish builds works as expected. ","atomic","conjunctions","high",False
18392,"As a XD build master, I'd like to fix local . gradlew dist and distZip targets the outstanding build issues, so I can evaluate that publish builds works as expected. ","As a XD","build master, I'd like to fix local . gradlew dist and distZip targets the outstanding build issues,","so I can evaluate that publish builds works as expected.","As a XD build master, I'd like to fix local <span class='highlight-text severity-high'>. gradlew dist and distZip targets the outstanding build issues, so I can evaluate that publish builds works as expected. </span>","minimal","punctuation","high",False
18392,"As a XD build master, I'd like to fix local . gradlew dist and distZip targets the outstanding build issues, so I can evaluate that publish builds works as expected. ","As a XD","build master, I'd like to fix local . gradlew dist and distZip targets the outstanding build issues,","so I can evaluate that publish builds works as expected.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18385,"As a user, I'd like to start multiple instances of xd container s through the RPM scripts, so I can easily spin up instances on the same node vm.","As a user",", I'd like to start multiple instances of xd container s through the RPM scripts,","so I can easily spin up instances on the same node vm.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18387,"RPM scripts will need to change.",NULL,"RPM scripts will need to change.",NULL,"Add for who this story is","well_formed","no_role","high",False
18391,"As a developer, I'd like to clean up compiler and javadoc warnings from the build, so we don t see the warnings in build sysout.","As a developer",", I'd like to clean up compiler and javadoc warnings from the build,","so we don t see the warnings in build sysout.","As a developer, I'd like to clean up compiler<span class='highlight-text severity-high'> and </span>javadoc warnings from the build, so we don t see the warnings in build sysout.","atomic","conjunctions","high",False
18391,"As a developer, I'd like to clean up compiler and javadoc warnings from the build, so we don t see the warnings in build sysout.","As a developer",", I'd like to clean up compiler and javadoc warnings from the build,","so we don t see the warnings in build sysout.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18397,"Also provide better lifecycle shutdown mgmt of handler.",NULL,"Also provide better lifecycle shutdown mgmt of handler.",NULL,"Add for who this story is","well_formed","no_role","high",False
18393,"XD EC2 needs to allow user to set the XD JMX ENABLED flag in the environment prior to admin or container startups. ",NULL,"XD EC2 needs to allow user to set the XD JMX ENABLED flag in the environment prior to admin or container startups. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18395,"As a developer, I'd like to upgrade to 2.0.3 release of Reactor, so I can inherit the latest optimizations to further improve XD performance characteristics. ","As a developer",", I'd like to upgrade to 2.0.3 release of Reactor,","so I can inherit the latest optimizations to further improve XD performance characteristics.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18396,"As a developer, I'd like to rerun baseline , Tuple , and Serialized payloads, so I can compare the differences in performance between 0.8.1 and 0.8.2 Kafka releases. Sinks to be included in test In Memory Transport Hdfs sink Direct Binding Transport Hdfs Sink Kafka Hdfs Sink","As a developer",", I'd like to rerun baseline , Tuple , and Serialized payloads,","so I can compare the differences in performance between 0.8.1 and 0.8.2 Kafka releases. Sinks to be included in test In Memory Transport Hdfs sink Direct Binding Transport Hdfs Sink Kafka Hdfs Sink","As a developer, I'd like to rerun baseline , Tuple ,<span class='highlight-text severity-high'> and </span>Serialized payloads, so I can compare the differences in performance between 0.8.1 and 0.8.2 Kafka releases. Sinks to be included in test In Memory Transport Hdfs sink Direct Binding Transport Hdfs Sink Kafka Hdfs Sink","atomic","conjunctions","high",False
18396,"As a developer, I'd like to rerun baseline , Tuple , and Serialized payloads, so I can compare the differences in performance between 0.8.1 and 0.8.2 Kafka releases. Sinks to be included in test In Memory Transport Hdfs sink Direct Binding Transport Hdfs Sink Kafka Hdfs Sink","As a developer",", I'd like to rerun baseline , Tuple , and Serialized payloads,","so I can compare the differences in performance between 0.8.1 and 0.8.2 Kafka releases. Sinks to be included in test In Memory Transport Hdfs sink Direct Binding Transport Hdfs Sink Kafka Hdfs Sink","As a developer, I'd like to rerun baseline , Tuple , and Serialized payloads, so I can compare the differences in performance between 0<span class='highlight-text severity-high'>.8.1 and 0.8.2 Kafka releases. Sinks to be included in test In Memory Transport Hdfs sink Direct Binding Transport Hdfs Sink Kafka Hdfs Sink</span>","minimal","punctuation","high",False
18396,"As a developer, I'd like to rerun baseline , Tuple , and Serialized payloads, so I can compare the differences in performance between 0.8.1 and 0.8.2 Kafka releases. Sinks to be included in test In Memory Transport Hdfs sink Direct Binding Transport Hdfs Sink Kafka Hdfs Sink","As a developer",", I'd like to rerun baseline , Tuple , and Serialized payloads,","so I can compare the differences in performance between 0.8.1 and 0.8.2 Kafka releases. Sinks to be included in test In Memory Transport Hdfs sink Direct Binding Transport Hdfs Sink Kafka Hdfs Sink","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18383,"This is an enhancement to KryoClassRegistrar or a related mechanism to initialize codecs using custom serializers to improve serialization performance. Currently XD will support POJOs that implement the kryo Serializable interface to gain a 2x performance improvement, however initial benchmarks show that custom serializers are about 10 more performant than Serializable.",NULL,"This is an enhancement to KryoClassRegistrar or a related mechanism to initialize codecs using custom serializers to improve serialization performance. Currently XD will support POJOs that implement the kryo Serializable interface to gain a 2x performance improvement, however initial benchmarks show that custom serializers are about 10 more performant than Serializable.",NULL,"Add for who this story is","well_formed","no_role","high",False
18383,"This is an enhancement to KryoClassRegistrar or a related mechanism to initialize codecs using custom serializers to improve serialization performance. Currently XD will support POJOs that implement the kryo Serializable interface to gain a 2x performance improvement, however initial benchmarks show that custom serializers are about 10 more performant than Serializable.",NULL,"This is an enhancement to KryoClassRegistrar or a related mechanism to initialize codecs using custom serializers to improve serialization performance. Currently XD will support POJOs that implement the kryo Serializable interface to gain a 2x performance improvement, however initial benchmarks show that custom serializers are about 10 more performant than Serializable.",NULL,"This is an enhancement to KryoClassRegistrar<span class='highlight-text severity-high'> or </span>a related mechanism to initialize codecs using custom serializers to improve serialization performance. Currently XD will support POJOs that implement the kryo Serializable interface to gain a 2x performance improvement, however initial benchmarks show that custom serializers are about 10 more performant than Serializable.","atomic","conjunctions","high",False
18383,"This is an enhancement to KryoClassRegistrar or a related mechanism to initialize codecs using custom serializers to improve serialization performance. Currently XD will support POJOs that implement the kryo Serializable interface to gain a 2x performance improvement, however initial benchmarks show that custom serializers are about 10 more performant than Serializable.",NULL,"This is an enhancement to KryoClassRegistrar or a related mechanism to initialize codecs using custom serializers to improve serialization performance. Currently XD will support POJOs that implement the kryo Serializable interface to gain a 2x performance improvement, however initial benchmarks show that custom serializers are about 10 more performant than Serializable.",NULL,"This is an enhancement to KryoClassRegistrar or a related mechanism to initialize codecs using custom serializers to improve serialization performance<span class='highlight-text severity-high'>. Currently XD will support POJOs that implement the kryo Serializable interface to gain a 2x performance improvement, however initial benchmarks show that custom serializers are about 10 more performant than Serializable.</span>","minimal","punctuation","high",False
18394,"As a developer, I'd like to have JMX turned off by default, so I can take advantage of the performance throughput benefits. ","As a developer",", I'd like to have JMX turned off by default,","so I can take advantage of the performance throughput benefits.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18388,"Provide unit tests",NULL,"Provide unit tests",NULL,"Add for who this story is","well_formed","no_role","high",False
18389,"https sonar.spring.io drilldown issues org.springframework.xd spring xd?severity CRITICAL",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
18389,"https sonar.spring.io drilldown issues org.springframework.xd spring xd?severity CRITICAL",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
18390,"This type is used in password field in the jdbc sink module provided by Spring XD defined in org.springframework.xd.jdbc.JdbcConnectionMixin class . It seems that Spring XD Admin UI is always displaying the password in plain text please see attached screen shot. Is there a way to somehow hide the passwords used as module properties in streams from being displayed in Spring XD Admin UI? This is similar issue as below and fixed in batch jobs but not in streams. https github.com spring projects spring xd pull 1325",NULL,"This type is used in password field in the jdbc sink module provided by Spring XD defined in org.springframework.xd.jdbc.JdbcConnectionMixin class . It seems that Spring XD Admin UI is always displaying the password in plain text please see attached screen shot. Is there a way to somehow hide the passwords used as module properties in streams from being displayed in Spring XD Admin UI? This is similar issue as below and fixed in batch jobs but not in streams. https github.com spring projects spring xd pull 1325",NULL,"Add for who this story is","well_formed","no_role","high",False
18443,"As a developer, I'd like revisit the design to determine the necessity for ID and TimeStamp attributes in Tuple , so I can refactor in order to improve performance throughput.","As a developer, I'd like revisit the design to determine the necessity for ID and TimeStamp attributes in Tuple , so","I can refactor","in order to improve performance throughput.","As a developer, I'd like revisit the design to determine the necessity for ID<span class='highlight-text severity-high'> and </span>TimeStamp attributes in Tuple , so I can refactor in order to improve performance throughput.","atomic","conjunctions","high",False
19345,"Once spring batch admin 1.3.0.M1 is available, update the build to use it. Likely to be Sept 7 or 9",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
18390,"This type is used in password field in the jdbc sink module provided by Spring XD defined in org.springframework.xd.jdbc.JdbcConnectionMixin class . It seems that Spring XD Admin UI is always displaying the password in plain text please see attached screen shot. Is there a way to somehow hide the passwords used as module properties in streams from being displayed in Spring XD Admin UI? This is similar issue as below and fixed in batch jobs but not in streams. https github.com spring projects spring xd pull 1325",NULL,"This type is used in password field in the jdbc sink module provided by Spring XD defined in org.springframework.xd.jdbc.JdbcConnectionMixin class . It seems that Spring XD Admin UI is always displaying the password in plain text please see attached screen shot. Is there a way to somehow hide the passwords used as module properties in streams from being displayed in Spring XD Admin UI? This is similar issue as below and fixed in batch jobs but not in streams. https github.com spring projects spring xd pull 1325",NULL,"This type is used in password field in the jdbc sink module provided by Spring XD defined in org.springframework.xd.jdbc.JdbcConnectionMixin class . It seems that Spring XD Admin UI is always displaying the password in plain text please see attached screen shot. Is there a way to somehow hide the passwords used as module properties in streams from being displayed in Spring XD Admin UI? This is similar issue as below<span class='highlight-text severity-high'> and </span>fixed in batch jobs but not in streams. https github.com spring projects spring xd pull 1325","atomic","conjunctions","high",False
18390,"This type is used in password field in the jdbc sink module provided by Spring XD defined in org.springframework.xd.jdbc.JdbcConnectionMixin class . It seems that Spring XD Admin UI is always displaying the password in plain text please see attached screen shot. Is there a way to somehow hide the passwords used as module properties in streams from being displayed in Spring XD Admin UI? This is similar issue as below and fixed in batch jobs but not in streams. https github.com spring projects spring xd pull 1325",NULL,"This type is used in password field in the jdbc sink module provided by Spring XD defined in org.springframework.xd.jdbc.JdbcConnectionMixin class . It seems that Spring XD Admin UI is always displaying the password in plain text please see attached screen shot. Is there a way to somehow hide the passwords used as module properties in streams from being displayed in Spring XD Admin UI? This is similar issue as below and fixed in batch jobs but not in streams. https github.com spring projects spring xd pull 1325",NULL,"This type is used in password field in the jdbc sink module provided by Spring XD defined in org<span class='highlight-text severity-high'>.springframework.xd.jdbc.JdbcConnectionMixin class . It seems that Spring XD Admin UI is always displaying the password in plain text please see attached screen shot. Is there a way to somehow hide the passwords used as module properties in streams from being displayed in Spring XD Admin UI? This is similar issue as below and fixed in batch jobs but not in streams. https github.com spring projects spring xd pull 1325</span>","minimal","punctuation","high",False
18399,"The incremental load introduced with XD 2309 should be added to the batch docs",NULL,"The incremental load introduced with XD 2309 should be added to the batch docs",NULL,"Add for who this story is","well_formed","no_role","high",False
18402,"As a Flo developer, I'd like to add improvements to existing Flo parser endpoints, so I can streamline the error reporting strategy.","As a Flo developer",", I'd like to add improvements to existing Flo parser endpoints,","so I can streamline the error reporting strategy.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18400,"As a developer, I'd like to default to HDFS as distributed remote location for custom module registry, so I can use xd shell or the REST API'directly to upload the custom module bits. I would also like to remove custom modules.zip artifact from YARN distribution.","As a developer",", I'd like to default to HDFS as distributed remote location for custom module registry,","so I can use xd shell or the REST API'directly to upload the custom module bits. I would also like to remove custom modules.zip artifact from YARN distribution.","As a developer, I'd like to default to HDFS as distributed remote location for custom module registry, so I can use xd shell or the REST API'directly to upload the custom module bits<span class='highlight-text severity-high'>. I would also like to remove custom modules.zip artifact from YARN distribution.</span>","minimal","punctuation","high",False
18400,"As a developer, I'd like to default to HDFS as distributed remote location for custom module registry, so I can use xd shell or the REST API'directly to upload the custom module bits. I would also like to remove custom modules.zip artifact from YARN distribution.","As a developer",", I'd like to default to HDFS as distributed remote location for custom module registry,","so I can use xd shell or the REST API'directly to upload the custom module bits. I would also like to remove custom modules.zip artifact from YARN distribution.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18401,"As a user, I'd like to use the Mail source to connect to secured IMAP and or SMTP mail servers. Mail source config file requires a util properties bean with ssl tls properties , provided to the adapter via the java mail properties attribute. Ref. Example http docs.spring.io spring integration docs latest ga reference html mail.html . code xml beans beans profile default util properties id javaMailProperties beans prop key mail.imap.socketFactory.class javax.net.ssl.SSLSocketFactory beans prop beans prop key mail.imap.socketFactory.fallback false beans prop beans prop key mail.store.protocol imaps beans prop beans prop key mail.debug false beans prop util properties beans beans code List of all java mail properties https javamail.java.net nonav docs api com sun mail smtp package summary.html ","As a user",", I'd like to use the Mail source to connect to secured IMAP and or SMTP mail servers. Mail source config file requires a util properties bean with ssl tls properties , provided to the adapter via the java mail properties attribute. Ref. Example http docs.spring.io spring integration docs latest ga reference html mail.html . code xml beans beans profile default util properties id javaMailProperties beans prop key mail.imap.socketFactory.class javax.net.ssl.SSLSocketFactory beans prop beans prop key mail.imap.socketFactory.fallback false beans prop beans prop key mail.store.protocol imaps beans prop beans prop key mail.debug false beans prop util properties beans beans code List of all java mail properties https javamail.java.net nonav docs api com sun mail smtp package summary.html",NULL,"As a user, I'd like to use the Mail source to connect to secured IMAP<span class='highlight-text severity-high'> and </span>span class='highlight-text severity-high'> or </span>SMTP mail servers. Mail source config file requires a util properties bean with ssl tls properties , provided to the adapter via the java mail properties attribute. Ref. Example http docs.spring.io spring integration docs latest ga reference html mail.html . code xml beans beans profile default util properties id javaMailProperties beans prop key mail.imap.socketFactory.class javax.net.ssl.SSLSocketFactory beans prop beans prop key mail.imap.socketFactory.fallback false beans prop beans prop key mail.store.protocol imaps beans prop beans prop key mail.debug false beans prop util properties beans beans code List of all java mail properties https javamail.java.net nonav docs api com sun mail smtp package summary.html ","atomic","conjunctions","high",False
18401,"As a user, I'd like to use the Mail source to connect to secured IMAP and or SMTP mail servers. Mail source config file requires a util properties bean with ssl tls properties , provided to the adapter via the java mail properties attribute. Ref. Example http docs.spring.io spring integration docs latest ga reference html mail.html . code xml beans beans profile default util properties id javaMailProperties beans prop key mail.imap.socketFactory.class javax.net.ssl.SSLSocketFactory beans prop beans prop key mail.imap.socketFactory.fallback false beans prop beans prop key mail.store.protocol imaps beans prop beans prop key mail.debug false beans prop util properties beans beans code List of all java mail properties https javamail.java.net nonav docs api com sun mail smtp package summary.html ","As a user",", I'd like to use the Mail source to connect to secured IMAP and or SMTP mail servers. Mail source config file requires a util properties bean with ssl tls properties , provided to the adapter via the java mail properties attribute. Ref. Example http docs.spring.io spring integration docs latest ga reference html mail.html . code xml beans beans profile default util properties id javaMailProperties beans prop key mail.imap.socketFactory.class javax.net.ssl.SSLSocketFactory beans prop beans prop key mail.imap.socketFactory.fallback false beans prop beans prop key mail.store.protocol imaps beans prop beans prop key mail.debug false beans prop util properties beans beans code List of all java mail properties https javamail.java.net nonav docs api com sun mail smtp package summary.html",NULL,"As a user, I'd like to use the Mail source to connect to secured IMAP and or SMTP mail servers<span class='highlight-text severity-high'>. Mail source config file requires a util properties bean with ssl tls properties , provided to the adapter via the java mail properties attribute. Ref. Example http docs.spring.io spring integration docs latest ga reference html mail.html . code xml beans beans profile default util properties id javaMailProperties beans prop key mail.imap.socketFactory.class javax.net.ssl.SSLSocketFactory beans prop beans prop key mail.imap.socketFactory.fallback false beans prop beans prop key mail.store.protocol imaps beans prop beans prop key mail.debug false beans prop util properties beans beans code List of all java mail properties https javamail.java.net nonav docs api com sun mail smtp package summary.html </span>","minimal","punctuation","high",False
18401,"As a user, I'd like to use the Mail source to connect to secured IMAP and or SMTP mail servers. Mail source config file requires a util properties bean with ssl tls properties , provided to the adapter via the java mail properties attribute. Ref. Example http docs.spring.io spring integration docs latest ga reference html mail.html . code xml beans beans profile default util properties id javaMailProperties beans prop key mail.imap.socketFactory.class javax.net.ssl.SSLSocketFactory beans prop beans prop key mail.imap.socketFactory.fallback false beans prop beans prop key mail.store.protocol imaps beans prop beans prop key mail.debug false beans prop util properties beans beans code List of all java mail properties https javamail.java.net nonav docs api com sun mail smtp package summary.html ","As a user",", I'd like to use the Mail source to connect to secured IMAP and or SMTP mail servers. Mail source config file requires a util properties bean with ssl tls properties , provided to the adapter via the java mail properties attribute. Ref. Example http docs.spring.io spring integration docs latest ga reference html mail.html . code xml beans beans profile default util properties id javaMailProperties beans prop key mail.imap.socketFactory.class javax.net.ssl.SSLSocketFactory beans prop beans prop key mail.imap.socketFactory.fallback false beans prop beans prop key mail.store.protocol imaps beans prop beans prop key mail.debug false beans prop util properties beans beans code List of all java mail properties https javamail.java.net nonav docs api com sun mail smtp package summary.html",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18404,"The POC for XD on Lattice uses the following interface for module deployment https github.com markfisher xolpoc admin blob master src main java xolpoc spi ModuleDeployer.java code public interface ModuleDeployer void deploy ModuleDescriptor descriptor ; void undeploy ModuleDescriptor descriptor ; ModuleStatus getStatus ModuleDescriptor descriptor ; code This spike is to introduce this interface and the Lattice implementation in the XD admin. The goals are to Demo a POC showing simple stream deployment with the existing shell admin to Lattice Learn from the experience to help guide the re architecture splitting of stream job repositories especially in regard to AbstractDeployer and related classes . Note that this work will not necessarily be merged into XD itself, although some of the concepts may be included in a future PR.",NULL,"The POC for XD on Lattice uses the following interface for module deployment https github.com markfisher xolpoc admin blob master src main java xolpoc spi ModuleDeployer.java code public interface ModuleDeployer void deploy ModuleDescriptor descriptor ; void undeploy ModuleDescriptor descriptor ; ModuleStatus getStatus ModuleDescriptor descriptor ; code This spike is to introduce this interface and the Lattice implementation in the XD admin. The goals are to Demo a POC showing simple stream deployment with the existing shell admin to Lattice Learn from the experience to help guide the re architecture splitting of stream job repositories especially in regard to AbstractDeployer and related classes . Note that this work will not necessarily be merged into XD itself, although some of the concepts may be included in a future PR.",NULL,"Add for who this story is","well_formed","no_role","high",False
18404,"The POC for XD on Lattice uses the following interface for module deployment https github.com markfisher xolpoc admin blob master src main java xolpoc spi ModuleDeployer.java code public interface ModuleDeployer void deploy ModuleDescriptor descriptor ; void undeploy ModuleDescriptor descriptor ; ModuleStatus getStatus ModuleDescriptor descriptor ; code This spike is to introduce this interface and the Lattice implementation in the XD admin. The goals are to Demo a POC showing simple stream deployment with the existing shell admin to Lattice Learn from the experience to help guide the re architecture splitting of stream job repositories especially in regard to AbstractDeployer and related classes . Note that this work will not necessarily be merged into XD itself, although some of the concepts may be included in a future PR.",NULL,"The POC for XD on Lattice uses the following interface for module deployment https github.com markfisher xolpoc admin blob master src main java xolpoc spi ModuleDeployer.java code public interface ModuleDeployer void deploy ModuleDescriptor descriptor ; void undeploy ModuleDescriptor descriptor ; ModuleStatus getStatus ModuleDescriptor descriptor ; code This spike is to introduce this interface and the Lattice implementation in the XD admin. The goals are to Demo a POC showing simple stream deployment with the existing shell admin to Lattice Learn from the experience to help guide the re architecture splitting of stream job repositories especially in regard to AbstractDeployer and related classes . Note that this work will not necessarily be merged into XD itself, although some of the concepts may be included in a future PR.",NULL,"The POC for XD on Lattice uses the following interface for module deployment https github<span class='highlight-text severity-high'>.com markfisher xolpoc admin blob master src main java xolpoc spi ModuleDeployer.java code public interface ModuleDeployer void deploy ModuleDescriptor descriptor ; void undeploy ModuleDescriptor descriptor ; ModuleStatus getStatus ModuleDescriptor descriptor ; code This spike is to introduce this interface and the Lattice implementation in the XD admin. The goals are to Demo a POC showing simple stream deployment with the existing shell admin to Lattice Learn from the experience to help guide the re architecture splitting of stream job repositories especially in regard to AbstractDeployer and related classes . Note that this work will not necessarily be merged into XD itself, although some of the concepts may be included in a future PR.</span>","minimal","punctuation","high",False
18406,"Polled message sources return only one message per poll by default. When polling, say, a file directory with many files, files will be emitted once per fixedDelay . As a user I need to configure a limit for the number of messages that will be emitted per poll.",NULL,"Polled message sources return only one message per poll by default. When polling, say, a file directory with many files, files will be emitted once per fixedDelay . As a user I need to configure a limit for the number of messages that will be emitted per poll.",NULL,"Add for who this story is","well_formed","no_role","high",False
18406,"Polled message sources return only one message per poll by default. When polling, say, a file directory with many files, files will be emitted once per fixedDelay . As a user I need to configure a limit for the number of messages that will be emitted per poll.",NULL,"Polled message sources return only one message per poll by default. When polling, say, a file directory with many files, files will be emitted once per fixedDelay . As a user I need to configure a limit for the number of messages that will be emitted per poll.",NULL,"Polled message sources return only one message per poll by default<span class='highlight-text severity-high'>. When polling, say, a file directory with many files, files will be emitted once per fixedDelay . As a user I need to configure a limit for the number of messages that will be emitted per poll.</span>","minimal","punctuation","high",False
18398,"There are a range of issues such as XD 3083, XD 2671 that are caused by asynchronous deployments issued by the REST API. The flow of events is deploy undeploy request received by REST API controller queues up request to be processed by supervisor controller returns HTTP 2xx This proposal is to have the thread executing the deploy undeploy request block until the request has been processed by the supervisor. This will have the side effect of deploys appearing to take longer, but when the HTTP request completes, the deployment undeployment will have been fulfilled. ",NULL,"There are a range of issues such as XD 3083, XD 2671 that are caused by asynchronous deployments issued by the REST API. The flow of events is deploy undeploy request received by REST API controller queues up request to be processed by supervisor controller returns HTTP 2xx This proposal is to have the thread executing the deploy undeploy request block until the request has been processed by the supervisor. This will have the side effect of deploys appearing to take longer, but when the HTTP request completes, the deployment undeployment will have been fulfilled. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18398,"There are a range of issues such as XD 3083, XD 2671 that are caused by asynchronous deployments issued by the REST API. The flow of events is deploy undeploy request received by REST API controller queues up request to be processed by supervisor controller returns HTTP 2xx This proposal is to have the thread executing the deploy undeploy request block until the request has been processed by the supervisor. This will have the side effect of deploys appearing to take longer, but when the HTTP request completes, the deployment undeployment will have been fulfilled. ",NULL,"There are a range of issues such as XD 3083, XD 2671 that are caused by asynchronous deployments issued by the REST API. The flow of events is deploy undeploy request received by REST API controller queues up request to be processed by supervisor controller returns HTTP 2xx This proposal is to have the thread executing the deploy undeploy request block until the request has been processed by the supervisor. This will have the side effect of deploys appearing to take longer, but when the HTTP request completes, the deployment undeployment will have been fulfilled. ",NULL,"There are a range of issues such as XD 3083, XD 2671 that are caused by asynchronous deployments issued by the REST API<span class='highlight-text severity-high'>. The flow of events is deploy undeploy request received by REST API controller queues up request to be processed by supervisor controller returns HTTP 2xx This proposal is to have the thread executing the deploy undeploy request block until the request has been processed by the supervisor. This will have the side effect of deploys appearing to take longer, but when the HTTP request completes, the deployment undeployment will have been fulfilled. </span>","minimal","punctuation","high",False
18405,"If you have a an option mode textLine , presently the enum MUST be named textLine . I think it would improve the user experience if we allowed users to pass in values such as mode textLine mode text line mode TEXT LINE ",NULL,"If you have a an option mode textLine , presently the enum MUST be named textLine . I think it would improve the user experience if we allowed users to pass in values such as mode textLine mode text line mode TEXT LINE ",NULL,"Add for who this story is","well_formed","no_role","high",False
18405,"If you have a an option mode textLine , presently the enum MUST be named textLine . I think it would improve the user experience if we allowed users to pass in values such as mode textLine mode text line mode TEXT LINE ",NULL,"If you have a an option mode textLine , presently the enum MUST be named textLine . I think it would improve the user experience if we allowed users to pass in values such as mode textLine mode text line mode TEXT LINE ",NULL,"If you have a an option mode textLine , presently the enum MUST be named textLine <span class='highlight-text severity-high'>. I think it would improve the user experience if we allowed users to pass in values such as mode textLine mode text line mode TEXT LINE </span>","minimal","punctuation","high",False
18414,"See http docs.spring.io spring xd docs current SNAPSHOT reference html introduction 26 There should be chapter section title before this.",NULL,"See http docs.spring.io spring xd docs current SNAPSHOT reference html introduction 26 There should be chapter section title before this.",NULL,"Add for who this story is","well_formed","no_role","high",False
18415,"Spring Data Gemfire is version 8.0.0 in Folwer, which is the same as in BDS Should check minor version number in BDS . ATM we are using gemfire 7.0.x",NULL,"Spring Data Gemfire is version 8.0.0 in Folwer, which is the same as in BDS Should check minor version number in BDS . ATM we are using gemfire 7.0.x",NULL,"Add for who this story is","well_formed","no_role","high",False
18415,"Spring Data Gemfire is version 8.0.0 in Folwer, which is the same as in BDS Should check minor version number in BDS . ATM we are using gemfire 7.0.x",NULL,"Spring Data Gemfire is version 8.0.0 in Folwer, which is the same as in BDS Should check minor version number in BDS . ATM we are using gemfire 7.0.x",NULL,"Spring Data Gemfire is version 8<span class='highlight-text severity-high'>.0.0 in Folwer, which is the same as in BDS Should check minor version number in BDS . ATM we are using gemfire 7.0.x</span>","minimal","punctuation","high",False
18417,"As a user, I'd like to have a REST API to get all the counters , gauges , and rich gauges in a single request, so I'don t have to issue multiple request to fetch each one of the metrics by name id for custom dashboards. Example code metrics counters all fetches all available counters metrics gauges all fetches all available gauges metrics rich gauges all fetches all available rich gauges code ","As a user",", I'd like to have a REST API to get all the counters , gauges , and rich gauges in a single request,","so I'don t have to issue multiple request to fetch each one of the metrics by name id for custom dashboards. Example code metrics counters all fetches all available counters metrics gauges all fetches all available gauges metrics rich gauges all fetches all available rich gauges code","As a user, I'd like to have a REST API to get all the counters , gauges ,<span class='highlight-text severity-high'> and </span>rich gauges in a single request, so I'don t have to issue multiple request to fetch each one of the metrics by name id for custom dashboards. Example code metrics counters all fetches all available counters metrics gauges all fetches all available gauges metrics rich gauges all fetches all available rich gauges code ","atomic","conjunctions","high",False
18417,"As a user, I'd like to have a REST API to get all the counters , gauges , and rich gauges in a single request, so I'don t have to issue multiple request to fetch each one of the metrics by name id for custom dashboards. Example code metrics counters all fetches all available counters metrics gauges all fetches all available gauges metrics rich gauges all fetches all available rich gauges code ","As a user",", I'd like to have a REST API to get all the counters , gauges , and rich gauges in a single request,","so I'don t have to issue multiple request to fetch each one of the metrics by name id for custom dashboards. Example code metrics counters all fetches all available counters metrics gauges all fetches all available gauges metrics rich gauges all fetches all available rich gauges code","As a user, I'd like to have a REST API to get all the counters , gauges , and rich gauges in a single request, so I'don t have to issue multiple request to fetch each one of the metrics by name id for custom dashboards<span class='highlight-text severity-high'>. Example code metrics counters all fetches all available counters metrics gauges all fetches all available gauges metrics rich gauges all fetches all available rich gauges code </span>","minimal","punctuation","high",False
18417,"As a user, I'd like to have a REST API to get all the counters , gauges , and rich gauges in a single request, so I'don t have to issue multiple request to fetch each one of the metrics by name id for custom dashboards. Example code metrics counters all fetches all available counters metrics gauges all fetches all available gauges metrics rich gauges all fetches all available rich gauges code ","As a user",", I'd like to have a REST API to get all the counters , gauges , and rich gauges in a single request,","so I'don t have to issue multiple request to fetch each one of the metrics by name id for custom dashboards. Example code metrics counters all fetches all available counters metrics gauges all fetches all available gauges metrics rich gauges all fetches all available rich gauges code","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18416,"4.1.4 and 1.4.5 respectively.",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
18416,"4.1.4 and 1.4.5 respectively.",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
18418,"As a user, I'd like to refer to the documentation, so I can configure HDFS backed module registry XD 2287 as recommended. ","As a user",", I'd like to refer to the documentation,","so I can configure HDFS backed module registry XD 2287 as recommended.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18419,"We should update to use spring data hadoop 2.2.0.M1in order to use the fixes available for the HDFS writing there syncable writes, timeout . A few things to keep in mind this updates Cloudera CDH to 5.3.3 Kite version is now 1.0 need to test the hdfs dataset sink ",NULL,"We should update to use spring data hadoop 2.2.0.M1in order to use the fixes available for the HDFS writing there syncable writes, timeout . A few things to keep in mind this updates Cloudera CDH to 5.3.3 Kite version is now 1.0 need to test the hdfs dataset sink ",NULL,"Add for who this story is","well_formed","no_role","high",False
18419,"We should update to use spring data hadoop 2.2.0.M1in order to use the fixes available for the HDFS writing there syncable writes, timeout . A few things to keep in mind this updates Cloudera CDH to 5.3.3 Kite version is now 1.0 need to test the hdfs dataset sink ",NULL,"We should update to use spring data hadoop 2.2.0.M1in order to use the fixes available for the HDFS writing there syncable writes, timeout . A few things to keep in mind this updates Cloudera CDH to 5.3.3 Kite version is now 1.0 need to test the hdfs dataset sink ",NULL,"We should update to use spring data hadoop 2<span class='highlight-text severity-high'>.2.0.M1in order to use the fixes available for the HDFS writing there syncable writes, timeout . A few things to keep in mind this updates Cloudera CDH to 5.3.3 Kite version is now 1.0 need to test the hdfs dataset sink </span>","minimal","punctuation","high",False
18420,"See https sonar43.spring.io drilldown measures 7173?metric package tangle index",NULL,"See https sonar43.spring.io drilldown measures 7173?metric package tangle index",NULL,"Add for who this story is","well_formed","no_role","high",False
18424,"As a user, I'd like to have the option to change the default Sqoop metastore , so I can implement a DB of my choice and not tied to default specifications. Refer to this thread http stackoverflow.com questions 24078668 how to change sqoop metastore for more details. ","As a user",", I'd like to have the option to change the default Sqoop metastore ,","so I can implement a DB of my choice and not tied to default specifications. Refer to this thread http stackoverflow.com questions 24078668 how to change sqoop metastore for more details.","As a user, I'd like to have the option to change the default Sqoop metastore , so I can implement a DB of my choice and not tied to default specifications<span class='highlight-text severity-high'>. Refer to this thread http stackoverflow.com questions 24078668 how to change sqoop metastore for more details. </span>","minimal","punctuation","high",False
18424,"As a user, I'd like to have the option to change the default Sqoop metastore , so I can implement a DB of my choice and not tied to default specifications. Refer to this thread http stackoverflow.com questions 24078668 how to change sqoop metastore for more details. ","As a user",", I'd like to have the option to change the default Sqoop metastore ,","so I can implement a DB of my choice and not tied to default specifications. Refer to this thread http stackoverflow.com questions 24078668 how to change sqoop metastore for more details.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18425,"Profile TupleCodec and implement performance optimizations",NULL,"Profile TupleCodec and implement performance optimizations",NULL,"Add for who this story is","well_formed","no_role","high",False
18425,"Profile TupleCodec and implement performance optimizations",NULL,"Profile TupleCodec and implement performance optimizations",NULL,"Profile TupleCodec<span class='highlight-text severity-high'> and </span>implement performance optimizations","atomic","conjunctions","high",False
18428,"This tests should prove that Kafka can handle a module count greater than one with the proper concurrency settings. load generator should be used as the foundation for this test with the following settings module.load generator.count 10,module.throughput.consumer.concurrency 10 An environment should be provisioned to support the containers, Zookeeper and Kafka. ",NULL,"This tests should prove that Kafka can handle a module count greater than one with the proper concurrency settings. load generator should be used as the foundation for this test with the following settings module.load generator.count 10,module.throughput.consumer.concurrency 10 An environment should be provisioned to support the containers, Zookeeper and Kafka. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18428,"This tests should prove that Kafka can handle a module count greater than one with the proper concurrency settings. load generator should be used as the foundation for this test with the following settings module.load generator.count 10,module.throughput.consumer.concurrency 10 An environment should be provisioned to support the containers, Zookeeper and Kafka. ",NULL,"This tests should prove that Kafka can handle a module count greater than one with the proper concurrency settings. load generator should be used as the foundation for this test with the following settings module.load generator.count 10,module.throughput.consumer.concurrency 10 An environment should be provisioned to support the containers, Zookeeper and Kafka. ",NULL,"This tests should prove that Kafka can handle a module count greater than one with the proper concurrency settings<span class='highlight-text severity-high'>. load generator should be used as the foundation for this test with the following settings module.load generator.count 10,module.throughput.consumer.concurrency 10 An environment should be provisioned to support the containers, Zookeeper and Kafka. </span>","minimal","punctuation","high",False
18426,"As a developer, I'd like to benchmark a stream with and without JMX enabled, so I can test in isolation, and document the differences in performance.","As a developer",", I'd like to benchmark a stream with and without JMX enabled,","so I can test in isolation, and document the differences in performance.","As a developer, I'd like to benchmark a stream with<span class='highlight-text severity-high'> and </span>without JMX enabled, so I can test in isolation, and document the differences in performance.","atomic","conjunctions","high",False
18426,"As a developer, I'd like to benchmark a stream with and without JMX enabled, so I can test in isolation, and document the differences in performance.","As a developer",", I'd like to benchmark a stream with and without JMX enabled,","so I can test in isolation, and document the differences in performance.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18427,"As a developer, I'd like to handle the non default ConfigurableConversionService tuples in an uniform manner, so they re not reset after deserialization. ","As a developer",", I'd like to handle the non default ConfigurableConversionService tuples in an uniform manner,","so they re not reset after deserialization.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18413,"As a user, I'd like to use Boot based ModuleRunner for use in container managed environments, so I can run XD without xd containers . Scope ","As a user",", I'd like to use Boot based ModuleRunner for use in container managed environments,","so I can run XD without xd containers . Scope","As a user, I'd like to use Boot based ModuleRunner for use in container managed environments, so I can run XD without xd containers <span class='highlight-text severity-high'>. Scope </span>","minimal","punctuation","high",False
18413,"As a user, I'd like to use Boot based ModuleRunner for use in container managed environments, so I can run XD without xd containers . Scope ","As a user",", I'd like to use Boot based ModuleRunner for use in container managed environments,","so I can run XD without xd containers . Scope","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18412,"As a user, I'd like to use the GF source along with native GF authentication enabled, so I can consume data from GF in a secured way. I'd like to refer to documentation on where the GF specific native and security properties needs configured. See this SC post https gopivotal com.socialcast.com messages 24377202 for more details.","As a user",", I'd like to use the GF","source along with native GF authentication enabled, so I can consume data from GF in a secured way. I'd like to refer to documentation on where the GF specific native and security properties needs configured. See this SC post https gopivotal com.socialcast.com messages 24377202 for more details.","As a user, I'd like to use the GF source along with native GF authentication enabled, so I can consume data from GF in a secured way<span class='highlight-text severity-high'>. I'd like to refer to documentation on where the GF specific native and security properties needs configured. See this SC post https gopivotal com.socialcast.com messages 24377202 for more details.</span>","minimal","punctuation","high",False
18412,"As a user, I'd like to use the GF source along with native GF authentication enabled, so I can consume data from GF in a secured way. I'd like to refer to documentation on where the GF specific native and security properties needs configured. See this SC post https gopivotal com.socialcast.com messages 24377202 for more details.","As a user",", I'd like to use the GF","source along with native GF authentication enabled, so I can consume data from GF in a secured way. I'd like to refer to documentation on where the GF specific native and security properties needs configured. See this SC post https gopivotal com.socialcast.com messages 24377202 for more details.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18423,"The call to modules?detailed true that was introduced for Flo proves to be a performance hog, most certainly because of all the metadata resolution that has to occur there and no caching takes place ",NULL,"The call to modules?detailed true that was introduced for Flo proves to be a performance hog, most certainly because of all the metadata resolution that has to occur there and no caching takes place ",NULL,"Add for who this story is","well_formed","no_role","high",False
18423,"The call to modules?detailed true that was introduced for Flo proves to be a performance hog, most certainly because of all the metadata resolution that has to occur there and no caching takes place ",NULL,"The call to modules?detailed true that was introduced for Flo proves to be a performance hog, most certainly because of all the metadata resolution that has to occur there and no caching takes place ",NULL,"The call to modules?detailed true that was introduced for Flo proves to be a performance hog, most certainly because of all the metadata resolution that has to occur there<span class='highlight-text severity-high'> and </span>no caching takes place ","atomic","conjunctions","high",False
18421,"For example, how to specify the partition count for topics that are created by the message bus.",NULL,"For example, how to specify the partition count for topics that are created by the message bus.",NULL,"Add for who this story is","well_formed","no_role","high",False
18441,"As a developer, I'd like to upgrade to Kafka 0.8.2, so I can leverage the latest features in order to test the performance characteristics.","As a developer, I'd like to upgrade to Kafka 0.8.2, so","I can leverage the latest features","in order to test the performance characteristics.","As a developer, I'd like to upgrade to Kafka 0.8.2,<span class='highlight-text severity-high'> so </span>I can leverage the latest features<span class='highlight-text severity-high'> in order to </span>test the performance characteristics.","minimal","indicator_repetition","high",False
18441,"As a developer, I'd like to upgrade to Kafka 0.8.2, so I can leverage the latest features in order to test the performance characteristics.","As a developer, I'd like to upgrade to Kafka 0.8.2, so","I can leverage the latest features","in order to test the performance characteristics.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18430,"As a user, I'd like to consume multiple topic partitions, so I can have the option to consume from multiple data endpoints and still be able to serve the data via single queue.","As a user",", I'd like to consume multiple topic partitions,","so I can have the option to consume from multiple data endpoints and still be able to serve the data via single queue.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18432,"As a follow up to XD 2877, experiment with the removal of the list of modules from BaseDefinition and reparse as needed. Branch is here https github.com pperalta spring xd tree deploy refactor 2",NULL,"As a follow up to XD 2877, experiment with the removal of the list of modules from BaseDefinition and reparse as needed. Branch is here https github.com pperalta spring xd tree deploy refactor 2",NULL,"Add for who this story is","well_formed","no_role","high",False
18432,"As a follow up to XD 2877, experiment with the removal of the list of modules from BaseDefinition and reparse as needed. Branch is here https github.com pperalta spring xd tree deploy refactor 2",NULL,"As a follow up to XD 2877, experiment with the removal of the list of modules from BaseDefinition and reparse as needed. Branch is here https github.com pperalta spring xd tree deploy refactor 2",NULL,"As a follow up to XD 2877, experiment with the removal of the list of modules from BaseDefinition<span class='highlight-text severity-high'> and </span>reparse as needed. Branch is here https github.com pperalta spring xd tree deploy refactor 2","atomic","conjunctions","high",False
18432,"As a follow up to XD 2877, experiment with the removal of the list of modules from BaseDefinition and reparse as needed. Branch is here https github.com pperalta spring xd tree deploy refactor 2",NULL,"As a follow up to XD 2877, experiment with the removal of the list of modules from BaseDefinition and reparse as needed. Branch is here https github.com pperalta spring xd tree deploy refactor 2",NULL,"As a follow up to XD 2877, experiment with the removal of the list of modules from BaseDefinition and reparse as needed<span class='highlight-text severity-high'>. Branch is here https github.com pperalta spring xd tree deploy refactor 2</span>","minimal","punctuation","high",False
18431,"the update to the JMX was introduced in XD 2941. Also noticed that we should have been checking source and not sink . This was also resolved.",NULL,"the update to the JMX was introduced in XD 2941. Also noticed that we should have been checking source and not sink . This was also resolved.",NULL,"Add for who this story is","well_formed","no_role","high",False
18431,"the update to the JMX was introduced in XD 2941. Also noticed that we should have been checking source and not sink . This was also resolved.",NULL,"the update to the JMX was introduced in XD 2941. Also noticed that we should have been checking source and not sink . This was also resolved.",NULL,"the update to the JMX was introduced in XD 2941. Also noticed that we should have been checking source<span class='highlight-text severity-high'> and </span>not sink . This was also resolved.","atomic","conjunctions","high",False
18431,"the update to the JMX was introduced in XD 2941. Also noticed that we should have been checking source and not sink . This was also resolved.",NULL,"the update to the JMX was introduced in XD 2941. Also noticed that we should have been checking source and not sink . This was also resolved.",NULL,"the update to the JMX was introduced in XD 2941<span class='highlight-text severity-high'>. Also noticed that we should have been checking source and not sink . This was also resolved.</span>","minimal","punctuation","high",False
18435,"As a user, I'd like to run the sqoop jobs against secured hdfs cluster, so I can restrict access to only authorized users. ","As a user",", I'd like to run the sqoop jobs against secured hdfs cluster,","so I can restrict access to only authorized users.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18433,"As a user, I'd like to use Boot based ModuleRunner for use in container managed environments, so I can run XD without xd containers . Scope Complete the remaining deployment properties work","As a user",", I'd like to use Boot based ModuleRunner for use in container managed environments,","so I can run XD without xd containers . Scope Complete the remaining deployment properties work","As a user, I'd like to use Boot based ModuleRunner for use in container managed environments, so I can run XD without xd containers <span class='highlight-text severity-high'>. Scope Complete the remaining deployment properties work</span>","minimal","punctuation","high",False
18433,"As a user, I'd like to use Boot based ModuleRunner for use in container managed environments, so I can run XD without xd containers . Scope Complete the remaining deployment properties work","As a user",", I'd like to use Boot based ModuleRunner for use in container managed environments,","so I can run XD without xd containers . Scope Complete the remaining deployment properties work","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18439,"As a developer, I'd like to document performance benchmark results along with the infrastructure specifics, so I can publish the blog for customers users to use it as a reference while setting up Spring XD cluster.","As a developer",", I'd like to document performance benchmark results along with the infrastructure specifics,","so I can publish the blog for customers users to use it as a reference while setting up Spring XD cluster.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18436,"As a user, I'd like to refer to the documentation to configure the properties file, so I can use it as recommended to represent the deployment manifest.","As a user",", I'd like to refer to the documentation to configure the properties file,","so I can use it as recommended to represent the deployment manifest.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18443,"As a developer, I'd like revisit the design to determine the necessity for ID and TimeStamp attributes in Tuple , so I can refactor in order to improve performance throughput.","As a developer, I'd like revisit the design to determine the necessity for ID and TimeStamp attributes in Tuple , so","I can refactor","in order to improve performance throughput.","As a developer, I'd like revisit the design to determine the necessity for ID and TimeStamp attributes in Tuple ,<span class='highlight-text severity-high'> so </span>I can refactor<span class='highlight-text severity-high'> in order to </span>improve performance throughput.","minimal","indicator_repetition","high",False
18443,"As a developer, I'd like revisit the design to determine the necessity for ID and TimeStamp attributes in Tuple , so I can refactor in order to improve performance throughput.","As a developer, I'd like revisit the design to determine the necessity for ID and TimeStamp attributes in Tuple , so","I can refactor","in order to improve performance throughput.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18442,"As a developer, I'd like to document the Kryo optimization guidelines, so the end users can refer to it while tuning to improve performance.","As a developer",", I'd like to document the Kryo optimization guidelines,","so the end users can refer to it while tuning to improve performance.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18440,"As a developer, I'd like to rerun baseline , Tuple , and Serialized payloads, so I can compare the differences in performance between 0.8.1 and 0.8.2 Kafka releases. Note 1.1.1 Benched against 0.8.1 1.2 Benched against 0.8.2 ","As a developer",", I'd like to rerun baseline , Tuple , and Serialized payloads,","so I can compare the differences in performance between 0.8.1 and 0.8.2 Kafka releases. Note 1.1.1 Benched against 0.8.1 1.2 Benched against 0.8.2","As a developer, I'd like to rerun baseline , Tuple ,<span class='highlight-text severity-high'> and </span>Serialized payloads, so I can compare the differences in performance between 0.8.1 and 0.8.2 Kafka releases. Note 1.1.1 Benched against 0.8.1 1.2 Benched against 0.8.2 ","atomic","conjunctions","high",False
18440,"As a developer, I'd like to rerun baseline , Tuple , and Serialized payloads, so I can compare the differences in performance between 0.8.1 and 0.8.2 Kafka releases. Note 1.1.1 Benched against 0.8.1 1.2 Benched against 0.8.2 ","As a developer",", I'd like to rerun baseline , Tuple , and Serialized payloads,","so I can compare the differences in performance between 0.8.1 and 0.8.2 Kafka releases. Note 1.1.1 Benched against 0.8.1 1.2 Benched against 0.8.2","As a developer, I'd like to rerun baseline , Tuple , and Serialized payloads, so I can compare the differences in performance between 0<span class='highlight-text severity-high'>.8.1 and 0.8.2 Kafka releases. Note 1.1.1 Benched against 0.8.1 1.2 Benched against 0.8.2 </span>","minimal","punctuation","high",False
18440,"As a developer, I'd like to rerun baseline , Tuple , and Serialized payloads, so I can compare the differences in performance between 0.8.1 and 0.8.2 Kafka releases. Note 1.1.1 Benched against 0.8.1 1.2 Benched against 0.8.2 ","As a developer",", I'd like to rerun baseline , Tuple , and Serialized payloads,","so I can compare the differences in performance between 0.8.1 and 0.8.2 Kafka releases. Note 1.1.1 Benched against 0.8.1 1.2 Benched against 0.8.2","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18434,"As a user, I'd like to use the Java receptor client, so I can interact with Diego runtime using the Java receptor REST APIs.","As a user",", I'd like to use the Java receptor client,","so I can interact with Diego runtime using the Java receptor REST APIs.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18438,"As a developer, I'd like to study the state management requirements, so I can brainstorm and identify the design to natively add stateful stream processing support in XD. ","As a developer",", I'd like to study the state management requirements,","so I can brainstorm and identify the design to natively add stateful stream processing support in XD.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18455,"As a developer, I'd like to create persistent repository for streams, so I could leverage the persisted metadata and reestablish the streaming pipe under failure conditions.","As a developer",", I'd like to create persistent repository for streams,","so I could leverage the persisted metadata and reestablish the streaming pipe under failure conditions.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18458,"As a developer, I'd like to build isolated Boot based ModuleRunner for use in container managed environments, so I can run XD without the hard requirement for running xd containers .","As a developer",", I'd like to build i","solated Boot based ModuleRunner for use in container managed environments, so I can run XD without the hard requirement for running xd containers .","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18454,"As a developer, I'd like to add documentation on escape quotes, so when someone using Sqoop job can double escape N instead of sending quotes N to successfully submit the job.","As a developer",", I'd like to add documentation on escape quotes,","so when someone using Sqoop job can double escape N instead of sending quotes N to successfully submit the job.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18459,"As a developer, I'd like to migrate module deployment from the repository abstraction used for stream job definitions , so I can create it as a pluggable runtime SPI.","As a developer",", I'd like to migrate module deployment from the repository abstraction used for stream job definitions ,","so I can create it as a pluggable runtime SPI.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18457,"As a developer, I'd like to create a java client https github.com markfisher receptor client for Receptor, so I can interact with Diego runtime via Receptor API calls from XD. ","As a developer",", I'd like to create a java client https github.com markfisher receptor client for Receptor,","so I can interact with Diego runtime via Receptor API calls from XD.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18456,"As a developer, I'd like to define pluggable runtime SPI, so I have the option to choose the implementation based on deployment targets such as CF, on prem, Mesos etc.","As a developer",", I'd like to define pluggable runtime SPI,","so I have the option to choose the implementation based on deployment targets such as CF, on prem, Mesos etc.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18463,"As a developer, I'd like to add a new CI build to include install target, so I can verify the target expectations, as it is often time consuming to verify it in the development environment.","As a developer",", I'd like to add a new CI build to include install target,","so I can verify the target expectations, as it is often time consuming to verify it in the development environment.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18461,"As a developer, I'd like to revisit performance benchmarks with new improvements, so I can verify the optimizations around jdbchdfs .","As a developer",", I'd like to revisit performance benchmarks with new improvements,","so I can verify the optimizations around jdbchdfs .","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18464,"As a developer, I'd like to complete the remaining work with DEBS challenge, so I can submit by the deadline.","As a developer",", I'd like to complete the remaining work with DEBS challenge,","so I can submit by the deadline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18460,"As a developer, I'd like to bench test cases around TupleBuilder , so I can identify the bottlenecks and tune for performance optimizations. ","As a developer",", I'd like to bench test cases around TupleBuilder ,","so I can identify the bottlenecks and tune for performance optimizations.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18465,"As a user, I'd like to upgrade to Spring Boot 1.2.3 release, do I can leverage the latest improvements and bug fixes. We should also sync up the following dependency updates to synchronize with Boot https github.com spring projects spring boot blob master spring boot dependencies pom.xml code logback.version 1.1.3 logback.version jackson.version 2.5.1 jackson.version gemfire.version 8.0.0 gemfire.version ","As a user, I'd like to upgrade to Spring Boot 1.2.3 release, do","I can leverage the latest improvements and bug fixes. We should also sync up the following dependency updates to synchronize with Boot https github.com spring projects spring boot blob master spring boot dependencies pom.xml code logback.version 1.1.3 logback.version jackson.version 2.5.1 jackson.version gemfire.version 8.0.0 gemfire.version",NULL,"As a user, I'd like to upgrade to Spring Boot 1<span class='highlight-text severity-high'>.2.3 release, do I can leverage the latest improvements and bug fixes. We should also sync up the following dependency updates to synchronize with Boot https github.com spring projects spring boot blob master spring boot dependencies pom.xml code logback.version 1.1.3 logback.version jackson.version 2.5.1 jackson.version gemfire.version 8.0.0 gemfire.version </span>","minimal","punctuation","high",False
18465,"As a user, I'd like to upgrade to Spring Boot 1.2.3 release, do I can leverage the latest improvements and bug fixes. We should also sync up the following dependency updates to synchronize with Boot https github.com spring projects spring boot blob master spring boot dependencies pom.xml code logback.version 1.1.3 logback.version jackson.version 2.5.1 jackson.version gemfire.version 8.0.0 gemfire.version ","As a user, I'd like to upgrade to Spring Boot 1.2.3 release, do","I can leverage the latest improvements and bug fixes. We should also sync up the following dependency updates to synchronize with Boot https github.com spring projects spring boot blob master spring boot dependencies pom.xml code logback.version 1.1.3 logback.version jackson.version 2.5.1 jackson.version gemfire.version 8.0.0 gemfire.version",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18466,"As a developer, I'd like to upgrade to SI Kafka release, so I can synchronize with latest improvements and bug fixes. ","As a developer",", I'd like to upgrade to SI Kafka release,","so I can synchronize with latest improvements and bug fixes.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18462,"After the Introduction to XD 2861 the acquisition of JobResources takes more time. We have to introduce a pause to wait for getJobDefinitionResource to be populated. ",NULL,"After the Introduction to XD 2861 the acquisition of JobResources takes more time. We have to introduce a pause to wait for getJobDefinitionResource to be populated. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18462,"After the Introduction to XD 2861 the acquisition of JobResources takes more time. We have to introduce a pause to wait for getJobDefinitionResource to be populated. ",NULL,"After the Introduction to XD 2861 the acquisition of JobResources takes more time. We have to introduce a pause to wait for getJobDefinitionResource to be populated. ",NULL,"After the Introduction to XD 2861 the acquisition of JobResources takes more time<span class='highlight-text severity-high'>. We have to introduce a pause to wait for getJobDefinitionResource to be populated. </span>","minimal","punctuation","high",False
18453,"As a Spring XD user, I'd like to create streaming pipelines, so I can take advantage of latest specs from both XD and Spark Spark Streaming.","As a Spring XD user",", I'd like to create streaming pipelines,","so I can take advantage of latest specs from both XD and Spark Spark Streaming.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18484,"As a developer, I'd like to setup UI infrastructure, so I can integrate admin ui and Flo.","As a developer",", I'd like to setup UI infrastructure,","so I can integrate admin ui and Flo.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18482,"As a developer, I'd like to create a gpload tasklet, so I can ingest data from various sources into GPDB in an efficient manner.","As a developer",", I'd like to create a gpload tasklet,","so I can ingest data from various sources into GPDB in an efficient manner.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18488,"As a developer, I'd like to continue XD on Lattice Diego PoC, and will be focused on the design of a pluggable SPI, so it is more generally applicable than Lattice, with the Receptor API being just one implementation option. ","As a developer",", I'd like to continue XD on Lattice Diego PoC, and will be focused on the design of a pluggable SPI,","so it is more generally applicable than Lattice, with the Receptor API being just one implementation option.","As a developer, I'd like to continue XD on Lattice Diego PoC,<span class='highlight-text severity-high'> and </span>will be focused on the design of a pluggable SPI, so it is more generally applicable than Lattice, with the Receptor API being just one implementation option. ","atomic","conjunctions","high",False
18488,"As a developer, I'd like to continue XD on Lattice Diego PoC, and will be focused on the design of a pluggable SPI, so it is more generally applicable than Lattice, with the Receptor API being just one implementation option. ","As a developer",", I'd like to continue XD on Lattice Diego PoC, and will be focused on the design of a pluggable SPI,","so it is more generally applicable than Lattice, with the Receptor API being just one implementation option.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18483,"As a developer, I'd like to use an efficient approach to read files, so I'don t have to read line by line and keep it in memory in order to consume write the file content. Would the tasklet approach be better as opposed to transmitting data via message bus as streams ? ","As a developer",", I'd like to use an efficient approach to read files, so I'don t have to read line by line and keep it in memory","in order to consume write the file content. Would the tasklet approach be better as opposed to transmitting data via message bus as streams ?","As a developer, I'd like to use an efficient approach to read files, so I'don t have to read line by line<span class='highlight-text severity-high'> and </span>keep it in memory in order to consume write the file content. Would the tasklet approach be better as opposed to transmitting data via message bus as streams ? ","atomic","conjunctions","high",False
18483,"As a developer, I'd like to use an efficient approach to read files, so I'don t have to read line by line and keep it in memory in order to consume write the file content. Would the tasklet approach be better as opposed to transmitting data via message bus as streams ? ","As a developer",", I'd like to use an efficient approach to read files, so I'don t have to read line by line and keep it in memory","in order to consume write the file content. Would the tasklet approach be better as opposed to transmitting data via message bus as streams ?","As a developer, I'd like to use an efficient approach to read files, so I'don t have to read line by line and keep it in memory in order to consume write the file content<span class='highlight-text severity-high'>. Would the tasklet approach be better as opposed to transmitting data via message bus as streams ? </span>","minimal","punctuation","high",False
18483,"As a developer, I'd like to use an efficient approach to read files, so I'don t have to read line by line and keep it in memory in order to consume write the file content. Would the tasklet approach be better as opposed to transmitting data via message bus as streams ? ","As a developer",", I'd like to use an efficient approach to read files, so I'don t have to read line by line and keep it in memory","in order to consume write the file content. Would the tasklet approach be better as opposed to transmitting data via message bus as streams ?","As a developer, I'd like to use an efficient approach to read files,<span class='highlight-text severity-high'> so </span>I'don t have to read line by line and keep it in memory<span class='highlight-text severity-high'> in order to </span>consume write the file content. Would the tasklet approach be better as opposed to transmitting data via message bus as streams ? ","minimal","indicator_repetition","high",False
18483,"As a developer, I'd like to use an efficient approach to read files, so I'don t have to read line by line and keep it in memory in order to consume write the file content. Would the tasklet approach be better as opposed to transmitting data via message bus as streams ? ","As a developer",", I'd like to use an efficient approach to read files, so I'don t have to read line by line and keep it in memory","in order to consume write the file content. Would the tasklet approach be better as opposed to transmitting data via message bus as streams ?","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18485,"As a user, I'd like to have the OOTB gpfdist sink module, so I can use this module to do ultra fast data movement from various sources into GPDB HAWQ.","As a user",", I'd like to have the OOTB gpfdist sink module,","so I can use this module to do ultra fast data movement from various sources into GPDB HAWQ.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18489,"Following the recent move of the doco to the main repo, it makes sense to have the doc generation be part of the main build, at an early stage, as an incentive for developers to push doc changes as soon as they change the code. ",NULL,"Following the recent move of the doco to the main repo, it makes sense to have the doc generation be part of the main build, at an early stage, as an incentive for developers to push doc changes as soon as they change the code. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18486,"As a developer, I'd like to host read Python script file from HDFS, so I can use the shell processor in XD on CF to delegate data science functionality to Py runtime and receive the feedback back in XD.","As a developer",", I'd like to host read Python script file from HDFS,","so I can use the shell processor in XD on CF to delegate data science functionality to Py runtime and receive the feedback back in XD.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18492,"Thanks to Gary I found this little gem of documentation to be able to use xpath expression in XD. Only hiccup is that I had to also add the spring xml.jar to the classpath otherwise it is missing XPathException class . http stackoverflow.com questions 29110757 spring xd work with xml payload",NULL,"Thanks to Gary I found this little gem of documentation to be able to use xpath expression in XD. Only hiccup is that I had to also add the spring xml.jar to the classpath otherwise it is missing XPathException class . http stackoverflow.com questions 29110757 spring xd work with xml payload",NULL,"Add for who this story is","well_formed","no_role","high",False
18492,"Thanks to Gary I found this little gem of documentation to be able to use xpath expression in XD. Only hiccup is that I had to also add the spring xml.jar to the classpath otherwise it is missing XPathException class . http stackoverflow.com questions 29110757 spring xd work with xml payload",NULL,"Thanks to Gary I found this little gem of documentation to be able to use xpath expression in XD. Only hiccup is that I had to also add the spring xml.jar to the classpath otherwise it is missing XPathException class . http stackoverflow.com questions 29110757 spring xd work with xml payload",NULL,"Thanks to Gary I found this little gem of documentation to be able to use xpath expression in XD<span class='highlight-text severity-high'>. Only hiccup is that I had to also add the spring xml.jar to the classpath otherwise it is missing XPathException class . http stackoverflow.com questions 29110757 spring xd work with xml payload</span>","minimal","punctuation","high",False
18487,"As a developer, I'd like to update all the module docs to also include shortDescription so that it s available for users to learn more about the module.","As a developer",", I'd like to update all the module docs to also include shortDescription","so that it s available for users to learn more about the module.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18493,"A placeholder to investigate what can be done with Spring configuration in Module Options Metadata classes to simplify enhance property configuration. With Configuration modules, these may now be beans in the module context. ",NULL,"A placeholder to investigate what can be done with Spring configuration in Module Options Metadata classes to simplify enhance property configuration. With Configuration modules, these may now be beans in the module context. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18493,"A placeholder to investigate what can be done with Spring configuration in Module Options Metadata classes to simplify enhance property configuration. With Configuration modules, these may now be beans in the module context. ",NULL,"A placeholder to investigate what can be done with Spring configuration in Module Options Metadata classes to simplify enhance property configuration. With Configuration modules, these may now be beans in the module context. ",NULL,"A placeholder to investigate what can be done with Spring configuration in Module Options Metadata classes to simplify enhance property configuration<span class='highlight-text severity-high'>. With Configuration modules, these may now be beans in the module context. </span>","minimal","punctuation","high",False
18952,"INFO main zookeeper.ZooKeeper 100 Client environment java.class.path produces a huge amount of output and is rather distracting, if that specific entry could be at a log level of debug it would make the startup of the server look cleaner.",NULL,"INFO main zookeeper.ZooKeeper 100 Client environment java.class.path produces a huge amount of output and is rather distracting, if that specific entry could be at a log level of debug it would make the startup of the server look cleaner.",NULL,"INFO main zookeeper.ZooKeeper 100 Client environment java.class.path produces a huge amount of output<span class='highlight-text severity-high'> and </span>is rather distracting, if that specific entry could be at a log level of debug it would make the startup of the server look cleaner.","atomic","conjunctions","high",False
18494,"Not going to integrate with Reactor for stream processing.",NULL,"Not going to integrate with Reactor for stream processing.",NULL,"Add for who this story is","well_formed","no_role","high",False
18490,"As a developer, I'd like to create a custom job module using Java Config so that I'don t have to deal with XML configurations. While deploying launching the following job, I get the error attached below. code xml job create name CDK Global definition customBatchJob deploy module upload type job name customBatchJob file Users mminella Documents IntelliJWorkspace CustomBatchModule build libs CustomBatchModule 1.1.0.RELEASE.jar job launch name CDK Global code Error I m getting an exception that the job doesn t exist asking if it s deployed","As a developer",", I'd like to create a custom job module using Java Config","so that I'don t have to deal with XML configurations. While deploying launching the following job, I get the error attached below. code xml job create name CDK Global definition customBatchJob deploy module upload type job name customBatchJob file Users mminella Documents IntelliJWorkspace CustomBatchModule build libs CustomBatchModule 1.1.0.RELEASE.jar job launch name CDK Global code Error I m getting an exception that the job doesn t exist asking if it s deployed","As a developer, I'd like to create a custom job module using Java Config so that I'don t have to deal with XML configurations<span class='highlight-text severity-high'>. While deploying launching the following job, I get the error attached below. code xml job create name CDK Global definition customBatchJob deploy module upload type job name customBatchJob file Users mminella Documents IntelliJWorkspace CustomBatchModule build libs CustomBatchModule 1.1.0.RELEASE.jar job launch name CDK Global code Error I m getting an exception that the job doesn t exist asking if it s deployed</span>","minimal","punctuation","high",False
18490,"As a developer, I'd like to create a custom job module using Java Config so that I'don t have to deal with XML configurations. While deploying launching the following job, I get the error attached below. code xml job create name CDK Global definition customBatchJob deploy module upload type job name customBatchJob file Users mminella Documents IntelliJWorkspace CustomBatchModule build libs CustomBatchModule 1.1.0.RELEASE.jar job launch name CDK Global code Error I m getting an exception that the job doesn t exist asking if it s deployed","As a developer",", I'd like to create a custom job module using Java Config","so that I'don t have to deal with XML configurations. While deploying launching the following job, I get the error attached below. code xml job create name CDK Global definition customBatchJob deploy module upload type job name customBatchJob file Users mminella Documents IntelliJWorkspace CustomBatchModule build libs CustomBatchModule 1.1.0.RELEASE.jar job launch name CDK Global code Error I m getting an exception that the job doesn t exist asking if it s deployed","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18491,"This keeps coming up as an issue that prevents us from publishing to maven central.",NULL,"This keeps coming up as an issue that prevents us from publishing to maven central.",NULL,"Add for who this story is","well_formed","no_role","high",False
18502,"If automatic binding of dead letter is enabled for rabbit mq and taps are deployed, anytime the tap is undeployed, the dead letter for that tap still remains. The tap uses a unique name and the queue for that is automatically deleted, but the dead letter queue for it is not. This problem becomes worse when containers are running in yarn and may not live for long periods of time. Many dead letter queues for taps can become overwhelming.",NULL,"If automatic binding of dead letter is enabled for rabbit mq and taps are deployed, anytime the tap is undeployed, the dead letter for that tap still remains. The tap uses a unique name and the queue for that is automatically deleted, but the dead letter queue for it is not. This problem becomes worse when containers are running in yarn and may not live for long periods of time. Many dead letter queues for taps can become overwhelming.",NULL,"Add for who this story is","well_formed","no_role","high",False
18502,"If automatic binding of dead letter is enabled for rabbit mq and taps are deployed, anytime the tap is undeployed, the dead letter for that tap still remains. The tap uses a unique name and the queue for that is automatically deleted, but the dead letter queue for it is not. This problem becomes worse when containers are running in yarn and may not live for long periods of time. Many dead letter queues for taps can become overwhelming.",NULL,"If automatic binding of dead letter is enabled for rabbit mq and taps are deployed, anytime the tap is undeployed, the dead letter for that tap still remains. The tap uses a unique name and the queue for that is automatically deleted, but the dead letter queue for it is not. This problem becomes worse when containers are running in yarn and may not live for long periods of time. Many dead letter queues for taps can become overwhelming.",NULL,"If automatic binding of dead letter is enabled for rabbit mq<span class='highlight-text severity-high'> and </span>taps are deployed, anytime the tap is undeployed, the dead letter for that tap still remains. The tap uses a unique name and the queue for that is automatically deleted, but the dead letter queue for it is not. This problem becomes worse when containers are running in yarn and may not live for long periods of time. Many dead letter queues for taps can become overwhelming.","atomic","conjunctions","high",False
18502,"If automatic binding of dead letter is enabled for rabbit mq and taps are deployed, anytime the tap is undeployed, the dead letter for that tap still remains. The tap uses a unique name and the queue for that is automatically deleted, but the dead letter queue for it is not. This problem becomes worse when containers are running in yarn and may not live for long periods of time. Many dead letter queues for taps can become overwhelming.",NULL,"If automatic binding of dead letter is enabled for rabbit mq and taps are deployed, anytime the tap is undeployed, the dead letter for that tap still remains. The tap uses a unique name and the queue for that is automatically deleted, but the dead letter queue for it is not. This problem becomes worse when containers are running in yarn and may not live for long periods of time. Many dead letter queues for taps can become overwhelming.",NULL,"If automatic binding of dead letter is enabled for rabbit mq and taps are deployed, anytime the tap is undeployed, the dead letter for that tap still remains<span class='highlight-text severity-high'>. The tap uses a unique name and the queue for that is automatically deleted, but the dead letter queue for it is not. This problem becomes worse when containers are running in yarn and may not live for long periods of time. Many dead letter queues for taps can become overwhelming.</span>","minimal","punctuation","high",False
18503,"Similar to Sqoop where we move data from RDBMS to HDFS we should look at integrating with Camus to load data from Kafka to HDFS.",NULL,"Similar to Sqoop where we move data from RDBMS to HDFS we should look at integrating with Camus to load data from Kafka to HDFS.",NULL,"Add for who this story is","well_formed","no_role","high",False
18504,"The build has some inconsistencies that should be taken care of. Amongst the one I know The UI project is always getting cleaned, for no apparent reason there might have been one before , thus triggering a rebuild of everything downstream, most notably DIRT The exec task is not used anymore Lots of projects are getting the boot plugin applied to them. I m not sure 100 what that plugin does, but we don t need the repackage bit for example.",NULL,"The build has some inconsistencies that should be taken care of. Amongst the one I know The UI project is always getting cleaned, for no apparent reason there might have been one before , thus triggering a rebuild of everything downstream, most notably DIRT The exec task is not used anymore Lots of projects are getting the boot plugin applied to them. I m not sure 100 what that plugin does, but we don t need the repackage bit for example.",NULL,"Add for who this story is","well_formed","no_role","high",False
18504,"The build has some inconsistencies that should be taken care of. Amongst the one I know The UI project is always getting cleaned, for no apparent reason there might have been one before , thus triggering a rebuild of everything downstream, most notably DIRT The exec task is not used anymore Lots of projects are getting the boot plugin applied to them. I m not sure 100 what that plugin does, but we don t need the repackage bit for example.",NULL,"The build has some inconsistencies that should be taken care of. Amongst the one I know The UI project is always getting cleaned, for no apparent reason there might have been one before , thus triggering a rebuild of everything downstream, most notably DIRT The exec task is not used anymore Lots of projects are getting the boot plugin applied to them. I m not sure 100 what that plugin does, but we don t need the repackage bit for example.",NULL,"The build has some inconsistencies that should be taken care of<span class='highlight-text severity-high'>. Amongst the one I know The UI project is always getting cleaned, for no apparent reason there might have been one before , thus triggering a rebuild of everything downstream, most notably DIRT The exec task is not used anymore Lots of projects are getting the boot plugin applied to them. I m not sure 100 what that plugin does, but we don t need the repackage bit for example.</span>","minimal","punctuation","high",False
18506,"As a developer, I'd like to migrate the wiki to project repo so that it can be tagged with the code and versioned etc. ","As a developer",", I'd like to migrate the wiki to project repo","so that it can be tagged with the code and versioned etc.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18505,"As a user, I'd like to add the Hadoop namenode specifics in a config file so that I'don t have to incur the hassle of pointing to the namenode location every time I open a new DSL session, but it is automatically configured. ","As a user",", I'd like to add the Hadoop namenode specifics in a config file","so that I'don t have to incur the hassle of pointing to the namenode location every time I open a new DSL session, but it is automatically configured.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18508,"As a developer, I'd like to continue Lattice Diego POC so that I can identify the scope, risks, and the overall design for a pluggable SPI in XD runtime.","As a developer",", I'd like to continue Lattice Diego POC","so that I can identify the scope, risks, and the overall design for a pluggable SPI in XD runtime.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18509,"As a developer, I'd like to measure performance numbers for a simple stream so that I can characterize the overall throughput. ","As a developer",", I'd like to measure performance numbers for a simple stream","so that I can characterize the overall throughput.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18510,"As a developer, I'd like to fix the offset management with Kafka source module so that I can efficiently perform fetch operation from the given offsets.","As a developer",", I'd like to fix the offset management with Kafka source module","so that I can efficiently perform fetch operation from the given offsets.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18511,"As a build manager, I'd like to schedule CI builds for windows so that I can verify XD runtime features functionality. The scope is to isolate the remaining test failures; perhaps, experiment with new AMI images until we have a solid infrastructure to fix the failing tests.",NULL,"As a build manager, I'd like to schedule CI builds for windows","so that I can verify XD runtime features functionality. The scope is to isolate the remaining test failures; perhaps, experiment with new AMI images until we have a solid infrastructure to fix the failing tests.","Add for who this story is","well_formed","no_role","high",False
18511,"As a build manager, I'd like to schedule CI builds for windows so that I can verify XD runtime features functionality. The scope is to isolate the remaining test failures; perhaps, experiment with new AMI images until we have a solid infrastructure to fix the failing tests.",NULL,"As a build manager, I'd like to schedule CI builds for windows","so that I can verify XD runtime features functionality. The scope is to isolate the remaining test failures; perhaps, experiment with new AMI images until we have a solid infrastructure to fix the failing tests.","As a build manager, I'd like to schedule CI builds for windows so that I can verify XD runtime features functionality<span class='highlight-text severity-high'>. The scope is to isolate the remaining test failures; perhaps, experiment with new AMI images until we have a solid infrastructure to fix the failing tests.</span>","minimal","punctuation","high",False
18511,"As a build manager, I'd like to schedule CI builds for windows so that I can verify XD runtime features functionality. The scope is to isolate the remaining test failures; perhaps, experiment with new AMI images until we have a solid infrastructure to fix the failing tests.",NULL,"As a build manager, I'd like to schedule CI builds for windows","so that I can verify XD runtime features functionality. The scope is to isolate the remaining test failures; perhaps, experiment with new AMI images until we have a solid infrastructure to fix the failing tests.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18512,"Currently it is necessary to specify mappedRequestHeaders on the rabbit sink, otherwise no headers are mapped to AMQP. This should be the default behavior.",NULL,"Currently it is necessary to specify mappedRequestHeaders on the rabbit sink, otherwise no headers are mapped to AMQP. This should be the default behavior.",NULL,"Add for who this story is","well_formed","no_role","high",False
18512,"Currently it is necessary to specify mappedRequestHeaders on the rabbit sink, otherwise no headers are mapped to AMQP. This should be the default behavior.",NULL,"Currently it is necessary to specify mappedRequestHeaders on the rabbit sink, otherwise no headers are mapped to AMQP. This should be the default behavior.",NULL,"Currently it is necessary to specify mappedRequestHeaders on the rabbit sink, otherwise no headers are mapped to AMQP<span class='highlight-text severity-high'>. This should be the default behavior.</span>","minimal","punctuation","high",False
18515,"As a developer, I'd like to create EC2 AMI with the necessary packages so that I can run the Kafka Perf tests.","As a developer",", I'd like to create EC2 AMI with the necessary packages","so that I can run the Kafka Perf tests.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18514,"As a developer, I'd like to add load generator source module so that I could use it for performance testing use cases. ","As a developer",", I'd like to add load generator source module","so that I could use it for performance testing use cases.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18513,"As a developer, I'd like to add load receiving sink module so that I can measure received throughput","As a developer",", I'd like to add load receiving sink module","so that I can measure received throughput","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18516,"As a developer, I'd like to identify the Kafka configurations so that I could setup infrastructure to perform performance testing. ","As a developer",", I'd like to identify the Kafka configurations","so that I could setup infrastructure to perform performance testing.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18501,"As a user, I'd like to use a jdbchdfs batch job as a passthrough without chunk processing so that I'don t have to incur the batch read write overhead.","As a user",", I'd like to use a jdbchdfs batch job as a passthrough without chunk processing","so that I'don t have to incur the batch read write overhead.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18507,"Mask out all properties for XD EC2",NULL,"Mask out all properties for XD EC2",NULL,"Add for who this story is","well_formed","no_role","high",False
18519,"As a developer, I'd like to bench Kafka as message bus using in built perf testing producer consumer utilities so that I can use that as a foundation to build XD use cases and measure performance. I'd like to reproduce baseline performance metrics as identified by the Kafka engineering team https engineering.linkedin.com kafka benchmarking apache kafka 2 million writes second three cheap machines .","As a developer",", I'd like to bench Kafka as message bus using in built perf testing producer consumer utilities","so that I can use that as a foundation to build XD use cases and measure performance. I'd like to reproduce baseline performance metrics as identified by the Kafka engineering team https engineering.linkedin.com kafka benchmarking apache kafka 2 million writes second three cheap machines .","As a developer, I'd like to bench Kafka as message bus using in built perf testing producer consumer utilities so that I can use that as a foundation to build XD use cases and measure performance<span class='highlight-text severity-high'>. I'd like to reproduce baseline performance metrics as identified by the Kafka engineering team https engineering.linkedin.com kafka benchmarking apache kafka 2 million writes second three cheap machines .</span>","minimal","punctuation","high",False
18519,"As a developer, I'd like to bench Kafka as message bus using in built perf testing producer consumer utilities so that I can use that as a foundation to build XD use cases and measure performance. I'd like to reproduce baseline performance metrics as identified by the Kafka engineering team https engineering.linkedin.com kafka benchmarking apache kafka 2 million writes second three cheap machines .","As a developer",", I'd like to bench Kafka as message bus using in built perf testing producer consumer utilities","so that I can use that as a foundation to build XD use cases and measure performance. I'd like to reproduce baseline performance metrics as identified by the Kafka engineering team https engineering.linkedin.com kafka benchmarking apache kafka 2 million writes second three cheap machines .","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18520,"Some modules inherit application.yml servers.yml via a properties file in config modules ; others have the values defined in the ...OptionsMetadata classes. Switch all modules to use the latter technique for consistency.",NULL,"Some modules inherit application.yml servers.yml via a properties file in config modules ; others have the values defined in the ...OptionsMetadata classes. Switch all modules to use the latter technique for consistency.",NULL,"Add for who this story is","well_formed","no_role","high",False
18520,"Some modules inherit application.yml servers.yml via a properties file in config modules ; others have the values defined in the ...OptionsMetadata classes. Switch all modules to use the latter technique for consistency.",NULL,"Some modules inherit application.yml servers.yml via a properties file in config modules ; others have the values defined in the ...OptionsMetadata classes. Switch all modules to use the latter technique for consistency.",NULL,"Some modules inherit application<span class='highlight-text severity-high'>.yml servers.yml via a properties file in config modules ; others have the values defined in the ...OptionsMetadata classes. Switch all modules to use the latter technique for consistency.</span>","minimal","punctuation","high",False
18522,"The current implementation of partitioned job management is entirely based on message exchange over the message bus, in a request reply scenario. This creates challenges when it comes to using certain types of transports, as well as acknowledging crashes. To that effect, the option of using a different partitioned job coordination strategy, that relies on a distributed computing coordination mechanism such as ZooKeeper should be investigated. ",NULL,"The current implementation of partitioned job management is entirely based on message exchange over the message bus, in a request reply scenario. This creates challenges when it comes to using certain types of transports, as well as acknowledging crashes. To that effect, the option of using a different partitioned job coordination strategy, that relies on a distributed computing coordination mechanism such as ZooKeeper should be investigated. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18522,"The current implementation of partitioned job management is entirely based on message exchange over the message bus, in a request reply scenario. This creates challenges when it comes to using certain types of transports, as well as acknowledging crashes. To that effect, the option of using a different partitioned job coordination strategy, that relies on a distributed computing coordination mechanism such as ZooKeeper should be investigated. ",NULL,"The current implementation of partitioned job management is entirely based on message exchange over the message bus, in a request reply scenario. This creates challenges when it comes to using certain types of transports, as well as acknowledging crashes. To that effect, the option of using a different partitioned job coordination strategy, that relies on a distributed computing coordination mechanism such as ZooKeeper should be investigated. ",NULL,"The current implementation of partitioned job management is entirely based on message exchange over the message bus, in a request reply scenario<span class='highlight-text severity-high'>. This creates challenges when it comes to using certain types of transports, as well as acknowledging crashes. To that effect, the option of using a different partitioned job coordination strategy, that relies on a distributed computing coordination mechanism such as ZooKeeper should be investigated. </span>","minimal","punctuation","high",False
18521,"As a user, I'd like to have the description for each of the modules so that I can use it to understand the module purpose and it s capabilities presumably what is captured in javadoc for the module definition .","As a user",", I'd like to have the description for each of the modules","so that I can use it to understand the module purpose and it s capabilities presumably what is captured in javadoc for the module definition .","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18959,"the algorithm and approach in https docs.google.com a gopivotal.com document d 12Cboa7nyVVKVxDIsHLJ 68m5f78ayXn14EJLrclJYVg edit needs to be added to the section on application configuration",NULL,"the algorithm and approach in https docs.google.com a gopivotal.com document d 12Cboa7nyVVKVxDIsHLJ 68m5f78ayXn14EJLrclJYVg edit needs to be added to the section on application configuration",NULL,"Add for who this story is","well_formed","no_role","high",False
18954,"See ContainerListener.loadStream and StreamListener.onChildAdded . Both require the stream definition as well as stream deployment manifest.",NULL,"See ContainerListener.loadStream and StreamListener.onChildAdded . Both require the stream definition as well as stream deployment manifest.",NULL,"Add for who this story is","well_formed","no_role","high",False
18954,"See ContainerListener.loadStream and StreamListener.onChildAdded . Both require the stream definition as well as stream deployment manifest.",NULL,"See ContainerListener.loadStream and StreamListener.onChildAdded . Both require the stream definition as well as stream deployment manifest.",NULL,"See ContainerListener.loadStream<span class='highlight-text severity-high'> and </span>StreamListener.onChildAdded . Both require the stream definition as well as stream deployment manifest.","atomic","conjunctions","high",False
18954,"See ContainerListener.loadStream and StreamListener.onChildAdded . Both require the stream definition as well as stream deployment manifest.",NULL,"See ContainerListener.loadStream and StreamListener.onChildAdded . Both require the stream definition as well as stream deployment manifest.",NULL,"See ContainerListener<span class='highlight-text severity-high'>.loadStream and StreamListener.onChildAdded . Both require the stream definition as well as stream deployment manifest.</span>","minimal","punctuation","high",False
18956,"Update code to use the management jolokia endpoint.",NULL,"Update code to use the management jolokia endpoint.",NULL,"Add for who this story is","well_formed","no_role","high",False
18958,"Allow users to configure arbitrary key value pairs for a container instance that may be referenced in deployment descriptors to target specific container instances. ",NULL,"Allow users to configure arbitrary key value pairs for a container instance that may be referenced in deployment descriptors to target specific container instances. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18960,"For PR https github.com spring projects spring xd pull 682 see if one can have a test case such that a test module would have a PATH property that overlaps with the environment variable. It should never resolve to the real unix windows path.",NULL,"For PR https github.com spring projects spring xd pull 682 see if one can have a test case such that a test module would have a PATH property that overlaps with the environment variable. It should never resolve to the real unix windows path.",NULL,"Add for who this story is","well_formed","no_role","high",False
18960,"For PR https github.com spring projects spring xd pull 682 see if one can have a test case such that a test module would have a PATH property that overlaps with the environment variable. It should never resolve to the real unix windows path.",NULL,"For PR https github.com spring projects spring xd pull 682 see if one can have a test case such that a test module would have a PATH property that overlaps with the environment variable. It should never resolve to the real unix windows path.",NULL,"For PR https github<span class='highlight-text severity-high'>.com spring projects spring xd pull 682 see if one can have a test case such that a test module would have a PATH property that overlaps with the environment variable. It should never resolve to the real unix windows path.</span>","minimal","punctuation","high",False
18961,"Currently it watches xd deployments containerid , but due to the reuse of that top level node for XD 1483 and XD 1484, we should instead use xd deployments modules containerid ",NULL,"Currently it watches xd deployments containerid , but due to the reuse of that top level node for XD 1483 and XD 1484, we should instead use xd deployments modules containerid ",NULL,"Add for who this story is","well_formed","no_role","high",False
18961,"Currently it watches xd deployments containerid , but due to the reuse of that top level node for XD 1483 and XD 1484, we should instead use xd deployments modules containerid ",NULL,"Currently it watches xd deployments containerid , but due to the reuse of that top level node for XD 1483 and XD 1484, we should instead use xd deployments modules containerid ",NULL,"Currently it watches xd deployments containerid , but due to the reuse of that top level node for XD 1483<span class='highlight-text severity-high'> and </span>XD 1484, we should instead use xd deployments modules containerid ","atomic","conjunctions","high",False
18953,"See the various module.xyz directories here https repo.spring.io libs snapshot org springframework xd ",NULL,"See the various module.xyz directories here https repo.spring.io libs snapshot org springframework xd ",NULL,"Add for who this story is","well_formed","no_role","high",False
18955,"Describe the algorithm as defined in https docs.google.com a gopivotal.com document d 12Cboa7nyVVKVxDIsHLJ 68m5f78ayXn14EJLrclJYVg edit?usp sharing ",NULL,"Describe the algorithm as defined in https docs.google.com a gopivotal.com document d 12Cboa7nyVVKVxDIsHLJ 68m5f78ayXn14EJLrclJYVg edit?usp sharing ",NULL,"Add for who this story is","well_formed","no_role","high",False
18957,"A container instance may be designated as belonging to a group. The end user may define this attribute using yaml config as xd.container.groups comma delimited string or an environment variable or groups command line argument. ",NULL,"A container instance may be designated as belonging to a group. The end user may define this attribute using yaml config as xd.container.groups comma delimited string or an environment variable or groups command line argument. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18957,"A container instance may be designated as belonging to a group. The end user may define this attribute using yaml config as xd.container.groups comma delimited string or an environment variable or groups command line argument. ",NULL,"A container instance may be designated as belonging to a group. The end user may define this attribute using yaml config as xd.container.groups comma delimited string or an environment variable or groups command line argument. ",NULL,"A container instance may be designated as belonging to a group. The end user may define this attribute using yaml config as xd.container.groups comma delimited string<span class='highlight-text severity-high'> or </span>an environment variable or groups command line argument. ","atomic","conjunctions","high",False
18957,"A container instance may be designated as belonging to a group. The end user may define this attribute using yaml config as xd.container.groups comma delimited string or an environment variable or groups command line argument. ",NULL,"A container instance may be designated as belonging to a group. The end user may define this attribute using yaml config as xd.container.groups comma delimited string or an environment variable or groups command line argument. ",NULL,"A container instance may be designated as belonging to a group<span class='highlight-text severity-high'>. The end user may define this attribute using yaml config as xd.container.groups comma delimited string or an environment variable or groups command line argument. </span>","minimal","punctuation","high",False
18524,"Currently PojoCodec calls kryo.register Class ? type on every ser deser invocation. This fails with 1.1 because instances are pooled and a different instance may be used to serialize and deserialize. See https github.com EsotericSoftware kryo registration. The fix is to not register classes on the fly. Classes serialized by PojoCodec will not be registered by default. This will work but is less efficient. XD should provide an easy way to register types known to be serialized on the MessageBus passed between modules ",NULL,"Currently PojoCodec calls kryo.register Class ? type on every ser deser invocation. This fails with 1.1 because instances are pooled and a different instance may be used to serialize and deserialize. See https github.com EsotericSoftware kryo registration. The fix is to not register classes on the fly. Classes serialized by PojoCodec will not be registered by default. This will work but is less efficient. XD should provide an easy way to register types known to be serialized on the MessageBus passed between modules ",NULL,"Add for who this story is","well_formed","no_role","high",False
18524,"Currently PojoCodec calls kryo.register Class ? type on every ser deser invocation. This fails with 1.1 because instances are pooled and a different instance may be used to serialize and deserialize. See https github.com EsotericSoftware kryo registration. The fix is to not register classes on the fly. Classes serialized by PojoCodec will not be registered by default. This will work but is less efficient. XD should provide an easy way to register types known to be serialized on the MessageBus passed between modules ",NULL,"Currently PojoCodec calls kryo.register Class ? type on every ser deser invocation. This fails with 1.1 because instances are pooled and a different instance may be used to serialize and deserialize. See https github.com EsotericSoftware kryo registration. The fix is to not register classes on the fly. Classes serialized by PojoCodec will not be registered by default. This will work but is less efficient. XD should provide an easy way to register types known to be serialized on the MessageBus passed between modules ",NULL,"Currently PojoCodec calls kryo.register Class ? type on every ser deser invocation. This fails with 1.1 because instances are pooled<span class='highlight-text severity-high'> and </span>a different instance may be used to serialize and deserialize. See https github.com EsotericSoftware kryo registration. The fix is to not register classes on the fly. Classes serialized by PojoCodec will not be registered by default. This will work but is less efficient. XD should provide an easy way to register types known to be serialized on the MessageBus passed between modules ","atomic","conjunctions","high",False
18524,"Currently PojoCodec calls kryo.register Class ? type on every ser deser invocation. This fails with 1.1 because instances are pooled and a different instance may be used to serialize and deserialize. See https github.com EsotericSoftware kryo registration. The fix is to not register classes on the fly. Classes serialized by PojoCodec will not be registered by default. This will work but is less efficient. XD should provide an easy way to register types known to be serialized on the MessageBus passed between modules ",NULL,"Currently PojoCodec calls kryo.register Class ? type on every ser deser invocation. This fails with 1.1 because instances are pooled and a different instance may be used to serialize and deserialize. See https github.com EsotericSoftware kryo registration. The fix is to not register classes on the fly. Classes serialized by PojoCodec will not be registered by default. This will work but is less efficient. XD should provide an easy way to register types known to be serialized on the MessageBus passed between modules ",NULL,"Currently PojoCodec calls kryo<span class='highlight-text severity-high'>.register Class ? type on every ser deser invocation. This fails with 1.1 because instances are pooled and a different instance may be used to serialize and deserialize. See https github.com EsotericSoftware kryo registration. The fix is to not register classes on the fly. Classes serialized by PojoCodec will not be registered by default. This will work but is less efficient. XD should provide an easy way to register types known to be serialized on the MessageBus passed between modules </span>","minimal","punctuation","high",False
18526,"The spark streaming message bus receiver isn t reliable yet. The receiver needs to handle data loss in case of worker node that has it running. We currently handle the driver failure automatically by re deploying spark streaming module. But, this is about the data loss when the worker node dies. Please see the documents here https databricks.com blog 2015 01 15 improved driver fault tolerance and zero data loss in spark streaming.html http spark.apache.org docs latest streaming custom receivers.html",NULL,"The spark streaming message bus receiver isn t reliable yet. The receiver needs to handle data loss in case of worker node that has it running. We currently handle the driver failure automatically by re deploying spark streaming module. But, this is about the data loss when the worker node dies. Please see the documents here https databricks.com blog 2015 01 15 improved driver fault tolerance and zero data loss in spark streaming.html http spark.apache.org docs latest streaming custom receivers.html",NULL,"Add for who this story is","well_formed","no_role","high",False
18526,"The spark streaming message bus receiver isn t reliable yet. The receiver needs to handle data loss in case of worker node that has it running. We currently handle the driver failure automatically by re deploying spark streaming module. But, this is about the data loss when the worker node dies. Please see the documents here https databricks.com blog 2015 01 15 improved driver fault tolerance and zero data loss in spark streaming.html http spark.apache.org docs latest streaming custom receivers.html",NULL,"The spark streaming message bus receiver isn t reliable yet. The receiver needs to handle data loss in case of worker node that has it running. We currently handle the driver failure automatically by re deploying spark streaming module. But, this is about the data loss when the worker node dies. Please see the documents here https databricks.com blog 2015 01 15 improved driver fault tolerance and zero data loss in spark streaming.html http spark.apache.org docs latest streaming custom receivers.html",NULL,"The spark streaming message bus receiver isn t reliable yet. The receiver needs to handle data loss in case of worker node that has it running. We currently handle the driver failure automatically by re deploying spark streaming module. But, this is about the data loss when the worker node dies. Please see the documents here https databricks.com blog 2015 01 15 improved driver fault tolerance<span class='highlight-text severity-high'> and </span>zero data loss in spark streaming.html http spark.apache.org docs latest streaming custom receivers.html","atomic","conjunctions","high",False
18526,"The spark streaming message bus receiver isn t reliable yet. The receiver needs to handle data loss in case of worker node that has it running. We currently handle the driver failure automatically by re deploying spark streaming module. But, this is about the data loss when the worker node dies. Please see the documents here https databricks.com blog 2015 01 15 improved driver fault tolerance and zero data loss in spark streaming.html http spark.apache.org docs latest streaming custom receivers.html",NULL,"The spark streaming message bus receiver isn t reliable yet. The receiver needs to handle data loss in case of worker node that has it running. We currently handle the driver failure automatically by re deploying spark streaming module. But, this is about the data loss when the worker node dies. Please see the documents here https databricks.com blog 2015 01 15 improved driver fault tolerance and zero data loss in spark streaming.html http spark.apache.org docs latest streaming custom receivers.html",NULL,"The spark streaming message bus receiver isn t reliable yet<span class='highlight-text severity-high'>. The receiver needs to handle data loss in case of worker node that has it running. We currently handle the driver failure automatically by re deploying spark streaming module. But, this is about the data loss when the worker node dies. Please see the documents here https databricks.com blog 2015 01 15 improved driver fault tolerance and zero data loss in spark streaming.html http spark.apache.org docs latest streaming custom receivers.html</span>","minimal","punctuation","high",False
18525,"As a developer, I'd like to have the high level description for each of the modules so that I can use the description presumably what is captured in javadoc for the module definition to understand the purpose of the module itself. ","As a developer",", I'd like to have the high level description for each of the modules","so that I can use the description presumably what is captured in javadoc for the module definition to understand the purpose of the module itself.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18528,"The scope is to address the sub tasks linked with this story. ",NULL,"The scope is to address the sub tasks linked with this story. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18527,"As a developer, I'd like to benchmark Rabbit performance so that I can use the results as reference to setup XD cluster.","As a developer",", I'd like to benchmark Rabbit performance","so that I can use the results as reference to setup XD cluster.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18529,"As a developer, I'd like to study the taxi trips based on a stream of trip reports from New York City so that I can evaluate event based systems in the context of real time analytics using Spring XD. Challenge Details http www.debs2015.org call grand challenge.html ","As a developer",", I'd like to study the taxi trips based on a stream of trip reports from New York City","so that I can evaluate event based systems in the context of real time analytics using Spring XD. Challenge Details http www.debs2015.org call grand challenge.html","As a developer, I'd like to study the taxi trips based on a stream of trip reports from New York City so that I can evaluate event based systems in the context of real time analytics using Spring XD<span class='highlight-text severity-high'>. Challenge Details http www.debs2015.org call grand challenge.html </span>","minimal","punctuation","high",False
18529,"As a developer, I'd like to study the taxi trips based on a stream of trip reports from New York City so that I can evaluate event based systems in the context of real time analytics using Spring XD. Challenge Details http www.debs2015.org call grand challenge.html ","As a developer",", I'd like to study the taxi trips based on a stream of trip reports from New York City","so that I can evaluate event based systems in the context of real time analytics using Spring XD. Challenge Details http www.debs2015.org call grand challenge.html","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18547,"code logger.error Failed to deliver message; retries exhausted; message sent to queue ERRORS name , context.getLastThrowable ; code Should be code logger.error Failed to deliver message; retries exhausted; message sent to queue ERRORS name , context.getLastThrowable ; code So the actual name is logged. ",NULL,"code logger.error Failed to deliver message; retries exhausted; message sent to queue ERRORS name , context.getLastThrowable ; code Should be code logger.error Failed to deliver message; retries exhausted; message sent to queue ERRORS name , context.getLastThrowable ; code","So the actual name is logged.","Add for who this story is","well_formed","no_role","high",False
18547,"code logger.error Failed to deliver message; retries exhausted; message sent to queue ERRORS name , context.getLastThrowable ; code Should be code logger.error Failed to deliver message; retries exhausted; message sent to queue ERRORS name , context.getLastThrowable ; code So the actual name is logged. ",NULL,"code logger.error Failed to deliver message; retries exhausted; message sent to queue ERRORS name , context.getLastThrowable ; code Should be code logger.error Failed to deliver message; retries exhausted; message sent to queue ERRORS name , context.getLastThrowable ; code","So the actual name is logged.","code logger.error Failed to deliver message<span class='highlight-text severity-high'>; retries exhausted; message sent to queue ERRORS name , context.getLastThrowable ; code Should be code logger.error Failed to deliver message; retries exhausted; message sent to queue ERRORS name , context.getLastThrowable ; code So the actual name is logged. </span>","minimal","punctuation","high",False
18547,"code logger.error Failed to deliver message; retries exhausted; message sent to queue ERRORS name , context.getLastThrowable ; code Should be code logger.error Failed to deliver message; retries exhausted; message sent to queue ERRORS name , context.getLastThrowable ; code So the actual name is logged. ",NULL,"code logger.error Failed to deliver message; retries exhausted; message sent to queue ERRORS name , context.getLastThrowable ; code Should be code logger.error Failed to deliver message; retries exhausted; message sent to queue ERRORS name , context.getLastThrowable ; code","So the actual name is logged.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18530,"The configuration for the redis sink must be provided for explicitly instead of falling back to values defined in servers.yml. The default behavior configuration should be address in a manner consistent with the default behavior of module config for rabbit source sink, jdbc sink....",NULL,"The configuration for the redis sink must be provided for explicitly instead of falling back to values defined in servers.yml. The default behavior configuration should be address in a manner consistent with the default behavior of module config for rabbit source sink, jdbc sink....",NULL,"Add for who this story is","well_formed","no_role","high",False
18530,"The configuration for the redis sink must be provided for explicitly instead of falling back to values defined in servers.yml. The default behavior configuration should be address in a manner consistent with the default behavior of module config for rabbit source sink, jdbc sink....",NULL,"The configuration for the redis sink must be provided for explicitly instead of falling back to values defined in servers.yml. The default behavior configuration should be address in a manner consistent with the default behavior of module config for rabbit source sink, jdbc sink....",NULL,"The configuration for the redis sink must be provided for explicitly instead of falling back to values defined in servers<span class='highlight-text severity-high'>.yml. The default behavior configuration should be address in a manner consistent with the default behavior of module config for rabbit source sink, jdbc sink....</span>","minimal","punctuation","high",False
18533,"As a field engineer, I'd like to have reference architectures built on Spring XD so that I can use that as reference building POCs. The scope is to get the raw domain specific ideas captured as first step. ","As a field engineer",", I'd like to have reference architectures built on Spring XD","so that I can use that as reference building POCs. The scope is to get the raw domain specific ideas captured as first step.","As a field engineer, I'd like to have reference architectures built on Spring XD so that I can use that as reference building POCs<span class='highlight-text severity-high'>. The scope is to get the raw domain specific ideas captured as first step. </span>","minimal","punctuation","high",False
18533,"As a field engineer, I'd like to have reference architectures built on Spring XD so that I can use that as reference building POCs. The scope is to get the raw domain specific ideas captured as first step. ","As a field engineer",", I'd like to have reference architectures built on Spring XD","so that I can use that as reference building POCs. The scope is to get the raw domain specific ideas captured as first step.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18534,"Since the refactoring of the module registry that does not look inside a module, it can t know that the scripts directory is not a module. Everything that is a direct child of source, processor, sink, job should be a module archive. Everything else supporting that should be moved out, e.g. in modules common",NULL,"Since the refactoring of the module registry that does not look inside a module, it can t know that the scripts directory is not a module. Everything that is a direct child of source, processor, sink, job should be a module archive. Everything else supporting that should be moved out, e.g. in modules common",NULL,"Since the refactoring of the module registry that does not look inside a module, it can t know that the scripts directory is not a module<span class='highlight-text severity-high'>. Everything that is a direct child of source, processor, sink, job should be a module archive. Everything else supporting that should be moved out, e.g. in modules common</span>","minimal","punctuation","high",False
18538,"As a user, I'd like to have the option of editing the deployed undeployed stream so that I'don t have to destroy to just change any deployment property.","As a user",", I'd like to have the option of editing the deployed undeployed stream","so that I'don t have to destroy to just change any deployment property.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18536,"Upload payload conversion demo such that a user can use the module upload feature against the sample.",NULL,"Upload payload conversion demo such that a user can use the module upload feature against the sample.",NULL,"Add for who this story is","well_formed","no_role","high",False
18541,"As a user, I'd like to clean up message bus resources associated with the stream so that when the stream is destroyed so does the coupled queues topics.","As a user",", I'd like to clean up message bus resources associated with the stream","so that when the stream is destroyed so does the coupled queues topics.","As a user, I'd like to clean up message bus resources associated with the stream<span class='highlight-text severity-high'> so that </span>when the stream is destroyed<span class='highlight-text severity-high'> so </span>does the coupled queues topics.","minimal","indicator_repetition","high",False
18541,"As a user, I'd like to clean up message bus resources associated with the stream so that when the stream is destroyed so does the coupled queues topics.","As a user",", I'd like to clean up message bus resources associated with the stream","so that when the stream is destroyed so does the coupled queues topics.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18604,"Create a load generator source module that will generate messages and dispatch messages to a XD stream. ",NULL,"Create a load generator source module that will generate messages and dispatch messages to a XD stream. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18604,"Create a load generator source module that will generate messages and dispatch messages to a XD stream. ",NULL,"Create a load generator source module that will generate messages and dispatch messages to a XD stream. ",NULL,"Create a load generator source module that will generate messages<span class='highlight-text severity-high'> and </span>dispatch messages to a XD stream. ","atomic","conjunctions","high",False
18545,"As a field engineer, I'd like to have a comparison of Storm examples in Spring XD so that it is easy to relate from implementation standpoint. ","As a field engineer",", I'd like to have a comparison of Storm examples in Spring XD","so that it is easy to relate from implementation standpoint.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18540,"As a user, I'd like to clean up stale queues topics associated with the stream so when the stream gets destroyed I can clean up resources. ","As a user",", I'd like to clean up stale queues topics as","sociated with the stream so when the stream gets destroyed I can clean up resources.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18607,"As a user, I'd like to have the option to extend compression support so that I can override the defaults and customize as needed. Follow up from this PR https github.com spring projects spring xd pull 1346","As a user",", I'd like to have the option to extend compression support","so that I can override the defaults and customize as needed. Follow up from this PR https github.com spring projects spring xd pull 1346","As a user, I'd like to have the option to extend compression support so that I can override the defaults and customize as needed<span class='highlight-text severity-high'>. Follow up from this PR https github.com spring projects spring xd pull 1346</span>","minimal","punctuation","high",False
18607,"As a user, I'd like to have the option to extend compression support so that I can override the defaults and customize as needed. Follow up from this PR https github.com spring projects spring xd pull 1346","As a user",", I'd like to have the option to extend compression support","so that I can override the defaults and customize as needed. Follow up from this PR https github.com spring projects spring xd pull 1346","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18542,"As a stream definer, when defining a stream ending with a file sink, I would like to have more flexibility for naming the file. Add an alternative nameExpression option, allowing complete control over the finename generator expression attribute. See http stackoverflow.com questions 28466477 issue with file sink and filename expression 28467069 28467069","As a stream definer",", when defining a stream ending with a file sink, I would like to have more flexibility for naming the file. Add an alternative nameExpression option, allowing complete control over the finename generator expression attribute. See http stackoverflow.com questions 28466477 issue with file sink and filename expression 28467069 28467069",NULL,"As a stream definer, when defining a stream ending with a file sink, I would like to have more flexibility for naming the file<span class='highlight-text severity-high'>. Add an alternative nameExpression option, allowing complete control over the finename generator expression attribute. See http stackoverflow.com questions 28466477 issue with file sink and filename expression 28467069 28467069</span>","minimal","punctuation","high",False
18542,"As a stream definer, when defining a stream ending with a file sink, I would like to have more flexibility for naming the file. Add an alternative nameExpression option, allowing complete control over the finename generator expression attribute. See http stackoverflow.com questions 28466477 issue with file sink and filename expression 28467069 28467069","As a stream definer",", when defining a stream ending with a file sink, I would like to have more flexibility for naming the file. Add an alternative nameExpression option, allowing complete control over the finename generator expression attribute. See http stackoverflow.com questions 28466477 issue with file sink and filename expression 28467069 28467069",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18544,"As a PM, I'd like to have the Smart Grid demo from s1 2014 ported into Spring XD samples repo.","As a PM",", I'd like to have the Smart Grid demo from s1 2014 ported into Spring XD samples repo.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18606,"As a user, I'd like to implement the core interface contract so that I can create a processor module that uses RxJava API.","As a user",", I'd like to implement the core interface contract","so that I can create a processor module that uses RxJava API.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18605,"As a user, I'd like to have a flexible RxJava module so that it can as a processor. ","As a user",", I'd like to have a flexible RxJava module","so that it can as a processor.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18535,"As a user, I'd like to include the deployment manifest from the file so that I'don t have spend time typing as inline properties .","As a user",", I'd like to include the deployment manifest from the file","so that I'don t have spend time typing as inline properties .","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18539,"The partitionResultsTimeout is set to 300000 as default 5min . This is way to short for long running steps. We should increase this default.",NULL,"The partitionResultsTimeout is set to 300000 as default 5min . This is way to short for long running steps. We should increase this default.",NULL,"Add for who this story is","well_formed","no_role","high",False
18539,"The partitionResultsTimeout is set to 300000 as default 5min . This is way to short for long running steps. We should increase this default.",NULL,"The partitionResultsTimeout is set to 300000 as default 5min . This is way to short for long running steps. We should increase this default.",NULL,"The partitionResultsTimeout is set to 300000 as default 5min <span class='highlight-text severity-high'>. This is way to short for long running steps. We should increase this default.</span>","minimal","punctuation","high",False
18543,"As a developer, I'd like to create a example to demonstrate JDBC to HDFS data movement.","As a developer",", I'd like to create a example to demonstrate JDBC to HDFS data movement.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18555,"Can revert part of the commit that went into upgrading to reactor 2.0 https github.com spring projects spring xd pull 1342 files ",NULL,"Can revert part of the commit that went into upgrading to reactor 2.0 https github.com spring projects spring xd pull 1342 files ",NULL,"Add for who this story is","well_formed","no_role","high",False
18549,"As a consequence, change gradle script regarding generation of documentation remove pushGeneratedDocs task, etc remove link rewriting that is no longer needed ","As a consequence, change gradle scrip","t regarding generation of documentation remove pushGeneratedDocs task, etc remove link rewriting that is no longer needed",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18550,"As a developer, I'd like to build batch sample using Sqoop so that we can demonstrate some of the capabilities. Use cases to consider JDBC to HDFS HDFS to JDBC ","As a developer",", I'd like to build batch sample using Sqoop","so that we can demonstrate some of the capabilities. Use cases to consider JDBC to HDFS HDFS to JDBC","As a developer, I'd like to build batch sample using Sqoop so that we can demonstrate some of the capabilities<span class='highlight-text severity-high'>. Use cases to consider JDBC to HDFS HDFS to JDBC </span>","minimal","punctuation","high",False
18550,"As a developer, I'd like to build batch sample using Sqoop so that we can demonstrate some of the capabilities. Use cases to consider JDBC to HDFS HDFS to JDBC ","As a developer",", I'd like to build batch sample using Sqoop","so that we can demonstrate some of the capabilities. Use cases to consider JDBC to HDFS HDFS to JDBC","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18561,"Min JDK version for XD 1.1 is 7. Change the sourceCompatibility to 1.7 but leave targetCompatibility at 1.6. Changes are also needed in the mdule parent pom and gradle plugins.",NULL,"Min JDK version for XD 1.1 is 7. Change the sourceCompatibility to 1.7 but leave targetCompatibility at 1.6. Changes are also needed in the mdule parent pom and gradle plugins.",NULL,"Add for who this story is","well_formed","no_role","high",False
18561,"Min JDK version for XD 1.1 is 7. Change the sourceCompatibility to 1.7 but leave targetCompatibility at 1.6. Changes are also needed in the mdule parent pom and gradle plugins.",NULL,"Min JDK version for XD 1.1 is 7. Change the sourceCompatibility to 1.7 but leave targetCompatibility at 1.6. Changes are also needed in the mdule parent pom and gradle plugins.",NULL,"Min JDK version for XD 1<span class='highlight-text severity-high'>.1 is 7. Change the sourceCompatibility to 1.7 but leave targetCompatibility at 1.6. Changes are also needed in the mdule parent pom and gradle plugins.</span>","minimal","punctuation","high",False
18551,"As a developer, I'd like to build data pipeline using Kafka as as message bus in XD so that we can demonstrate some of the capabilities. Use case to consider Log aggregation and analysis Lambda architecture how to avoid code duplication how to eliminate tight coupling of business logic how Kafka can be used for reliable reprocessing ","As a developer",", I'd like to build data pipeline using Kafka as as message bus in XD","so that we can demonstrate some of the capabilities. Use case to consider Log aggregation and analysis Lambda architecture how to avoid code duplication how to eliminate tight coupling of business logic how Kafka can be used for reliable reprocessing","As a developer, I'd like to build data pipeline using Kafka as as message bus in XD so that we can demonstrate some of the capabilities<span class='highlight-text severity-high'>. Use case to consider Log aggregation and analysis Lambda architecture how to avoid code duplication how to eliminate tight coupling of business logic how Kafka can be used for reliable reprocessing </span>","minimal","punctuation","high",False
18551,"As a developer, I'd like to build data pipeline using Kafka as as message bus in XD so that we can demonstrate some of the capabilities. Use case to consider Log aggregation and analysis Lambda architecture how to avoid code duplication how to eliminate tight coupling of business logic how Kafka can be used for reliable reprocessing ","As a developer",", I'd like to build data pipeline using Kafka as as message bus in XD","so that we can demonstrate some of the capabilities. Use case to consider Log aggregation and analysis Lambda architecture how to avoid code duplication how to eliminate tight coupling of business logic how Kafka can be used for reliable reprocessing","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18556,"As a developer, I'd like the publish maven.gradle script to use values for dependencies e.g. Spring Boot and hadoop common from our central dependency list in this case dependencies.properties so that I'don t have to update them manually anymore.","As a developer",", I'd like the publish maven.gradle script to use values for dependencies e.g. Spring Boot and hadoop common from our central dependency list in this case dependencies.properties","so that I'don t have to update them manually anymore.","As a developer, I'd like the publish maven<span class='highlight-text severity-high'>.gradle script to use values for dependencies e.g. Spring Boot and hadoop common from our central dependency list in this case dependencies.properties so that I'don t have to update them manually anymore.</span>","minimal","punctuation","high",False
18556,"As a developer, I'd like the publish maven.gradle script to use values for dependencies e.g. Spring Boot and hadoop common from our central dependency list in this case dependencies.properties so that I'don t have to update them manually anymore.","As a developer",", I'd like the publish maven.gradle script to use values for dependencies e.g. Spring Boot and hadoop common from our central dependency list in this case dependencies.properties","so that I'don t have to update them manually anymore.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18557,"As a developer, I'd like to run Kafka tests with Kafka Server as a separate running process so that I can improve build experience. ","As a developer",", I'd like to run Kafka tests with Kafka Server as a separate running process","so that I can improve build experience.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18558,"Gradle 2.x is required for the latest Sonar version sonar.spring.io We may need to wait for a fix in Groovy itself 2.4.1 Please see the following links for details http forums.gradle.org gradle topics after upgrade gradle to 2 0 version the maven pom not support build property http jira.codehaus.org browse GROOVY 7023 ",NULL,"Gradle 2.x is required for the latest Sonar version sonar.spring.io We may need to wait for a fix in Groovy itself 2.4.1 Please see the following links for details http forums.gradle.org gradle topics after upgrade gradle to 2 0 version the maven pom not support build property http jira.codehaus.org browse GROOVY 7023 ",NULL,"Add for who this story is","well_formed","no_role","high",False
18962,"The REST API for deploy should accept parameters, which provide manifest key value pairs e.g. ?http.instances 5 . Ultimately we will want to support passing a named manifest which had been stored previously. The stream deploy shell command should support passing these as options. Initially we should support modulename .instances and modulename .group.",NULL,"The REST API for deploy should accept parameters, which provide manifest key value pairs e.g. ?http.instances 5 . Ultimately we will want to support passing a named manifest which had been stored previously. The stream deploy shell command should support passing these as options. Initially we should support modulename .instances and modulename .group.",NULL,"Add for who this story is","well_formed","no_role","high",False
18962,"The REST API for deploy should accept parameters, which provide manifest key value pairs e.g. ?http.instances 5 . Ultimately we will want to support passing a named manifest which had been stored previously. The stream deploy shell command should support passing these as options. Initially we should support modulename .instances and modulename .group.",NULL,"The REST API for deploy should accept parameters, which provide manifest key value pairs e.g. ?http.instances 5 . Ultimately we will want to support passing a named manifest which had been stored previously. The stream deploy shell command should support passing these as options. Initially we should support modulename .instances and modulename .group.",NULL,"The REST API for deploy should accept parameters, which provide manifest key value pairs e<span class='highlight-text severity-high'>.g. ?http.instances 5 . Ultimately we will want to support passing a named manifest which had been stored previously. The stream deploy shell command should support passing these as options. Initially we should support modulename .instances and modulename .group.</span>","minimal","punctuation","high",False
18560,"As a user, I'd like to see the date in logs so that I can troubleshoot issues that had occurred on a specific day and time. Property that needs adjusted https github.com spring projects spring xd blob master config xd container logger.properties L11","As a user",", I'd like to see the date in logs","so that I can troubleshoot issues that had occurred on a specific day and time. Property that needs adjusted https github.com spring projects spring xd blob master config xd container logger.properties L11","As a user, I'd like to see the date in logs so that I can troubleshoot issues that had occurred on a specific day and time<span class='highlight-text severity-high'>. Property that needs adjusted https github.com spring projects spring xd blob master config xd container logger.properties L11</span>","minimal","punctuation","high",False
18560,"As a user, I'd like to see the date in logs so that I can troubleshoot issues that had occurred on a specific day and time. Property that needs adjusted https github.com spring projects spring xd blob master config xd container logger.properties L11","As a user",", I'd like to see the date in logs","so that I can troubleshoot issues that had occurred on a specific day and time. Property that needs adjusted https github.com spring projects spring xd blob master config xd container logger.properties L11","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18554,"As a developer, I want to have to run Kafka tests on an external broker, so that I reduce the footprint of the build process. ","As a developer,","I want to have to run Kafka tests on an external broker,","so that I reduce the footprint of the build process.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18552,"remove spark and hadoop requirements from spring xd module parent and gradle module plugin",NULL,"remove spark and hadoop requirements from spring xd module parent and gradle module plugin",NULL,"Add for who this story is","well_formed","no_role","high",False
18559,"see https github.com spring projects spring boot issues 2454",NULL,"see https github.com spring projects spring boot issues 2454",NULL,"Add for who this story is","well_formed","no_role","high",False
18553,"As a developer, I'd like to review the current sonar violations so that I can fix the relevant and update the irrelevant ones as invalid.","As a developer",", I'd like to review the current sonar violations","so that I can fix the relevant and update the irrelevant ones as invalid.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18570,"As a user, I'd like to build XD in Windows machine so that I can develop, test, and contributed to OSS.","As a user",", I'd like to build XD in Windows machine","so that I can develop, test, and contributed to OSS.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18567,"As a developer, I'd like to upgrade to Kafka s SI GA release so that I can sync up with the latest bits. The scope is to backport Kafka XD changes to SI Kafka and then upgrade to the GA release.","As a developer",", I'd like to upgrade to Kafka s SI GA release","so that I can sync up with the latest bits. The scope is to backport Kafka XD changes to SI Kafka and then upgrade to the GA release.","As a developer, I'd like to upgrade to Kafka s SI GA release so that I can sync up with the latest bits<span class='highlight-text severity-high'>. The scope is to backport Kafka XD changes to SI Kafka and then upgrade to the GA release.</span>","minimal","punctuation","high",False
18567,"As a developer, I'd like to upgrade to Kafka s SI GA release so that I can sync up with the latest bits. The scope is to backport Kafka XD changes to SI Kafka and then upgrade to the GA release.","As a developer",", I'd like to upgrade to Kafka s SI GA release","so that I can sync up with the latest bits. The scope is to backport Kafka XD changes to SI Kafka and then upgrade to the GA release.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18571,"As a user, I'd like to refer to a Pig script job sample so that I can use that as a reference to integrate Pig jobs in XD.","As a user",", I'd like to refer to a Pig script job sample","so that I can use that as a reference to integrate Pig jobs in XD.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18569,"As a developer, I'd like to build Spark Streaming as data processors in XD so that we can demonstrate some of the capabilities. Implement using Java Java Lambdas Scala","As a developer",", I'd like to build Spark Streaming as data processors in XD","so that we can demonstrate some of the capabilities. Implement using Java Java Lambdas Scala","As a developer, I'd like to build Spark Streaming as data processors in XD so that we can demonstrate some of the capabilities<span class='highlight-text severity-high'>. Implement using Java Java Lambdas Scala</span>","minimal","punctuation","high",False
18569,"As a developer, I'd like to build Spark Streaming as data processors in XD so that we can demonstrate some of the capabilities. Implement using Java Java Lambdas Scala","As a developer",", I'd like to build Spark Streaming as data processors in XD","so that we can demonstrate some of the capabilities. Implement using Java Java Lambdas Scala","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18576,"Hey Guys, Im trying to run WordCount example in kerberized cluster with the attached job configuration. The job upload the test file to the HDFS without problems but it fails in wordcount step. I am running the example on singlenode and I configured the config hadoop.properties file for shell and container with kerberos setting such as https github.com spring projects spring xd wiki Hadoop Kerberos. The error log is attached. Thanks in advance for the help. Regards",NULL,"Hey Guys, Im trying to run WordCount example in kerberized cluster with the attached job configuration. The job upload the test file to the HDFS without problems but it fails in wordcount step. I am running the example on singlenode and I configured the config hadoop.properties file for shell and container with kerberos setting such as https github.com spring projects spring xd wiki Hadoop Kerberos. The error log is attached. Thanks in advance for the help. Regards",NULL,"Add for who this story is","well_formed","no_role","high",False
18576,"Hey Guys, Im trying to run WordCount example in kerberized cluster with the attached job configuration. The job upload the test file to the HDFS without problems but it fails in wordcount step. I am running the example on singlenode and I configured the config hadoop.properties file for shell and container with kerberos setting such as https github.com spring projects spring xd wiki Hadoop Kerberos. The error log is attached. Thanks in advance for the help. Regards",NULL,"Hey Guys, Im trying to run WordCount example in kerberized cluster with the attached job configuration. The job upload the test file to the HDFS without problems but it fails in wordcount step. I am running the example on singlenode and I configured the config hadoop.properties file for shell and container with kerberos setting such as https github.com spring projects spring xd wiki Hadoop Kerberos. The error log is attached. Thanks in advance for the help. Regards",NULL,"Hey Guys, Im trying to run WordCount example in kerberized cluster with the attached job configuration. The job upload the test file to the HDFS without problems but it fails in wordcount step. I am running the example on singlenode<span class='highlight-text severity-high'> and </span>I configured the config hadoop.properties file for shell and container with kerberos setting such as https github.com spring projects spring xd wiki Hadoop Kerberos. The error log is attached. Thanks in advance for the help. Regards","atomic","conjunctions","high",False
18576,"Hey Guys, Im trying to run WordCount example in kerberized cluster with the attached job configuration. The job upload the test file to the HDFS without problems but it fails in wordcount step. I am running the example on singlenode and I configured the config hadoop.properties file for shell and container with kerberos setting such as https github.com spring projects spring xd wiki Hadoop Kerberos. The error log is attached. Thanks in advance for the help. Regards",NULL,"Hey Guys, Im trying to run WordCount example in kerberized cluster with the attached job configuration. The job upload the test file to the HDFS without problems but it fails in wordcount step. I am running the example on singlenode and I configured the config hadoop.properties file for shell and container with kerberos setting such as https github.com spring projects spring xd wiki Hadoop Kerberos. The error log is attached. Thanks in advance for the help. Regards",NULL,"Hey Guys, Im trying to run WordCount example in kerberized cluster with the attached job configuration<span class='highlight-text severity-high'>. The job upload the test file to the HDFS without problems but it fails in wordcount step. I am running the example on singlenode and I configured the config hadoop.properties file for shell and container with kerberos setting such as https github.com spring projects spring xd wiki Hadoop Kerberos. The error log is attached. Thanks in advance for the help. Regards</span>","minimal","punctuation","high",False
18572,"Use latest version, might need to exclude version from other dependencies, e.g. SI, in build common.gradle.",NULL,"Use latest version, might need to exclude version from other dependencies, e.g. SI, in build common.gradle.",NULL,"Add for who this story is","well_formed","no_role","high",False
18572,"Use latest version, might need to exclude version from other dependencies, e.g. SI, in build common.gradle.",NULL,"Use latest version, might need to exclude version from other dependencies, e.g. SI, in build common.gradle.",NULL,"Use latest version, might need to exclude version from other dependencies, e<span class='highlight-text severity-high'>.g. SI, in build common.gradle.</span>","minimal","punctuation","high",False
18573,"As a user, I'd like to migrate from 1.0 to 1.1 and be able to port my custom modules so that I can operationalize existing data pipelines and also take advantage of latest XD features.","As a user",", I'd like to migrate from 1.0 to 1.1 and be able to port my custom modules","so that I can operationalize existing data pipelines and also take advantage of latest XD features.","As a user, I'd like to migrate from 1.0 to 1.1<span class='highlight-text severity-high'> and </span>be able to port my custom modules so that I can operationalize existing data pipelines and also take advantage of latest XD features.","atomic","conjunctions","high",False
18573,"As a user, I'd like to migrate from 1.0 to 1.1 and be able to port my custom modules so that I can operationalize existing data pipelines and also take advantage of latest XD features.","As a user",", I'd like to migrate from 1.0 to 1.1 and be able to port my custom modules","so that I can operationalize existing data pipelines and also take advantage of latest XD features.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18620,"The code base is changing a bit, so using 2.0 M1 for development is stable up until all major JIRA issues have bee completed. Then we should track snapshots in preparation to move to 2.0. M2 when it gets released.",NULL,"The code base is changing a bit,","so using 2.0 M1 for development is stable up until all major JIRA issues have bee completed. Then we should track snapshots in preparation to move to 2.0. M2 when it gets released.","Add for who this story is","well_formed","no_role","high",False
18620,"The code base is changing a bit, so using 2.0 M1 for development is stable up until all major JIRA issues have bee completed. Then we should track snapshots in preparation to move to 2.0. M2 when it gets released.",NULL,"The code base is changing a bit,","so using 2.0 M1 for development is stable up until all major JIRA issues have bee completed. Then we should track snapshots in preparation to move to 2.0. M2 when it gets released.","The code base is changing a bit, so using 2<span class='highlight-text severity-high'>.0 M1 for development is stable up until all major JIRA issues have bee completed. Then we should track snapshots in preparation to move to 2.0. M2 when it gets released.</span>","minimal","punctuation","high",False
18620,"The code base is changing a bit, so using 2.0 M1 for development is stable up until all major JIRA issues have bee completed. Then we should track snapshots in preparation to move to 2.0. M2 when it gets released.",NULL,"The code base is changing a bit,","so using 2.0 M1 for development is stable up until all major JIRA issues have bee completed. Then we should track snapshots in preparation to move to 2.0. M2 when it gets released.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18574,"The scope is to research the available options to provide request reply support for Kafka. Document findings POCs Previous Desc The bindRequestor and bindReplier methods of the message bus need to be implemented.",NULL,"The scope is to research the available options to provide request reply support for Kafka. Document findings POCs Previous Desc The bindRequestor and bindReplier methods of the message bus need to be implemented.",NULL,"Add for who this story is","well_formed","no_role","high",False
18574,"The scope is to research the available options to provide request reply support for Kafka. Document findings POCs Previous Desc The bindRequestor and bindReplier methods of the message bus need to be implemented.",NULL,"The scope is to research the available options to provide request reply support for Kafka. Document findings POCs Previous Desc The bindRequestor and bindReplier methods of the message bus need to be implemented.",NULL,"The scope is to research the available options to provide request reply support for Kafka. Document findings POCs Previous Desc The bindRequestor<span class='highlight-text severity-high'> and </span>bindReplier methods of the message bus need to be implemented.","atomic","conjunctions","high",False
18574,"The scope is to research the available options to provide request reply support for Kafka. Document findings POCs Previous Desc The bindRequestor and bindReplier methods of the message bus need to be implemented.",NULL,"The scope is to research the available options to provide request reply support for Kafka. Document findings POCs Previous Desc The bindRequestor and bindReplier methods of the message bus need to be implemented.",NULL,"The scope is to research the available options to provide request reply support for Kafka<span class='highlight-text severity-high'>. Document findings POCs Previous Desc The bindRequestor and bindReplier methods of the message bus need to be implemented.</span>","minimal","punctuation","high",False
18577,"Multiple threads invoke the shell processor result in I O errors and or data corruption. send and receive should be synchronized. ",NULL,"Multiple threads invoke the shell processor result in I O errors and or data corruption. send and receive should be synchronized. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18577,"Multiple threads invoke the shell processor result in I O errors and or data corruption. send and receive should be synchronized. ",NULL,"Multiple threads invoke the shell processor result in I O errors and or data corruption. send and receive should be synchronized. ",NULL,"Multiple threads invoke the shell processor result in I O errors<span class='highlight-text severity-high'> and </span>span class='highlight-text severity-high'> or </span>data corruption. send and receive should be synchronized. ","atomic","conjunctions","high",False
18577,"Multiple threads invoke the shell processor result in I O errors and or data corruption. send and receive should be synchronized. ",NULL,"Multiple threads invoke the shell processor result in I O errors and or data corruption. send and receive should be synchronized. ",NULL,"Multiple threads invoke the shell processor result in I O errors and or data corruption<span class='highlight-text severity-high'>. send and receive should be synchronized. </span>","minimal","punctuation","high",False
18563,"There are several issues making it hard to impossible to create batch jobs that use Pig, Hive, HBase or other technologies supported by Spring for Apache Hadoop project. We need to make the corresponding dependencies available on the Hadoop classpath.",NULL,"There are several issues making it hard to impossible to create batch jobs that use Pig, Hive, HBase or other technologies supported by Spring for Apache Hadoop project. We need to make the corresponding dependencies available on the Hadoop classpath.",NULL,"Add for who this story is","well_formed","no_role","high",False
18563,"There are several issues making it hard to impossible to create batch jobs that use Pig, Hive, HBase or other technologies supported by Spring for Apache Hadoop project. We need to make the corresponding dependencies available on the Hadoop classpath.",NULL,"There are several issues making it hard to impossible to create batch jobs that use Pig, Hive, HBase or other technologies supported by Spring for Apache Hadoop project. We need to make the corresponding dependencies available on the Hadoop classpath.",NULL,"There are several issues making it hard to impossible to create batch jobs that use Pig, Hive, HBase<span class='highlight-text severity-high'> or </span>other technologies supported by Spring for Apache Hadoop project. We need to make the corresponding dependencies available on the Hadoop classpath.","atomic","conjunctions","high",False
18563,"There are several issues making it hard to impossible to create batch jobs that use Pig, Hive, HBase or other technologies supported by Spring for Apache Hadoop project. We need to make the corresponding dependencies available on the Hadoop classpath.",NULL,"There are several issues making it hard to impossible to create batch jobs that use Pig, Hive, HBase or other technologies supported by Spring for Apache Hadoop project. We need to make the corresponding dependencies available on the Hadoop classpath.",NULL,"There are several issues making it hard to impossible to create batch jobs that use Pig, Hive, HBase or other technologies supported by Spring for Apache Hadoop project<span class='highlight-text severity-high'>. We need to make the corresponding dependencies available on the Hadoop classpath.</span>","minimal","punctuation","high",False
18568,"As a PM, I'd like to have the copyright message in the reference guide PDF updated to include 2015 instead of 2014. ","As a PM",", I'd like to have the copyright message in the reference guide PDF updated to include 2015 instead of 2014.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18565,"When available",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
18565,"When available",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
18619,"Some cleanup to make the tests a bit easer to read.",NULL,"Some cleanup to make the tests a bit easer to read.",NULL,"Add for who this story is","well_formed","no_role","high",False
18579,"As a developer, I'd like to use Ambari plugin so that I can provision, manage, and monitor Spring XD cluster using the same tool I use for Hadoop clusters.","As a developer",", I'd like to use Ambari plugin","so that I can provision, manage, and monitor Spring XD cluster using the same tool I use for Hadoop clusters.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18588,"When deploying XD using Java 7 the user must be able to set the permsize to a value larger than the default. The reason this is required is that if we deploy a gemfire component more than 2 times or a kafka source sink more than 2 times, stream deployment begins to fail. The only exception that was captured was the following noformat Exception in thread ec2Test3 ip 10 146 213 31 1421176704238 e1786039 watcher executor Exception java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread ec2Test3 ip 10 146 213 31 1421176704238 e1786039 watcher executor noformat Logs are not available at this time. ",NULL,"When deploying XD using Java 7 the user must be able to set the permsize to a value larger than the default. The reason this is required is that if we deploy a gemfire component more than 2 times or a kafka source sink more than 2 times, stream deployment begins to fail. The only exception that was captured was the following noformat Exception in thread ec2Test3 ip 10 146 213 31 1421176704238 e1786039 watcher executor Exception java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread ec2Test3 ip 10 146 213 31 1421176704238 e1786039 watcher executor noformat Logs are not available at this time. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18588,"When deploying XD using Java 7 the user must be able to set the permsize to a value larger than the default. The reason this is required is that if we deploy a gemfire component more than 2 times or a kafka source sink more than 2 times, stream deployment begins to fail. The only exception that was captured was the following noformat Exception in thread ec2Test3 ip 10 146 213 31 1421176704238 e1786039 watcher executor Exception java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread ec2Test3 ip 10 146 213 31 1421176704238 e1786039 watcher executor noformat Logs are not available at this time. ",NULL,"When deploying XD using Java 7 the user must be able to set the permsize to a value larger than the default. The reason this is required is that if we deploy a gemfire component more than 2 times or a kafka source sink more than 2 times, stream deployment begins to fail. The only exception that was captured was the following noformat Exception in thread ec2Test3 ip 10 146 213 31 1421176704238 e1786039 watcher executor Exception java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread ec2Test3 ip 10 146 213 31 1421176704238 e1786039 watcher executor noformat Logs are not available at this time. ",NULL,"When deploying XD using Java 7 the user must be able to set the permsize to a value larger than the default. The reason this is required is that if we deploy a gemfire component more than 2 times<span class='highlight-text severity-high'> or </span>a kafka source sink more than 2 times, stream deployment begins to fail. The only exception that was captured was the following noformat Exception in thread ec2Test3 ip 10 146 213 31 1421176704238 e1786039 watcher executor Exception java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread ec2Test3 ip 10 146 213 31 1421176704238 e1786039 watcher executor noformat Logs are not available at this time. ","atomic","conjunctions","high",False
18602,"As a performance tester, I'd like to investigate why there s high CPU startup time for both admin and container servers. Perhaps profiling would assist isolating the bottlenecks. Scope Identify the bottlenecks Document reasons List pros cons","As a performance tester",", I'd like to investigate why there s high CPU startup time for both admin and container servers. Perhaps profiling would assist isolating the bottlenecks. Scope Identify the bottlenecks Document reasons List pros cons",NULL,"As a performance tester, I'd like to investigate why there s high CPU startup time for both admin and container servers<span class='highlight-text severity-high'>. Perhaps profiling would assist isolating the bottlenecks. Scope Identify the bottlenecks Document reasons List pros cons</span>","minimal","punctuation","high",False
18602,"As a performance tester, I'd like to investigate why there s high CPU startup time for both admin and container servers. Perhaps profiling would assist isolating the bottlenecks. Scope Identify the bottlenecks Document reasons List pros cons","As a performance tester",", I'd like to investigate why there s high CPU startup time for both admin and container servers. Perhaps profiling would assist isolating the bottlenecks. Scope Identify the bottlenecks Document reasons List pros cons",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18588,"When deploying XD using Java 7 the user must be able to set the permsize to a value larger than the default. The reason this is required is that if we deploy a gemfire component more than 2 times or a kafka source sink more than 2 times, stream deployment begins to fail. The only exception that was captured was the following noformat Exception in thread ec2Test3 ip 10 146 213 31 1421176704238 e1786039 watcher executor Exception java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread ec2Test3 ip 10 146 213 31 1421176704238 e1786039 watcher executor noformat Logs are not available at this time. ",NULL,"When deploying XD using Java 7 the user must be able to set the permsize to a value larger than the default. The reason this is required is that if we deploy a gemfire component more than 2 times or a kafka source sink more than 2 times, stream deployment begins to fail. The only exception that was captured was the following noformat Exception in thread ec2Test3 ip 10 146 213 31 1421176704238 e1786039 watcher executor Exception java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread ec2Test3 ip 10 146 213 31 1421176704238 e1786039 watcher executor noformat Logs are not available at this time. ",NULL,"When deploying XD using Java 7 the user must be able to set the permsize to a value larger than the default<span class='highlight-text severity-high'>. The reason this is required is that if we deploy a gemfire component more than 2 times or a kafka source sink more than 2 times, stream deployment begins to fail. The only exception that was captured was the following noformat Exception in thread ec2Test3 ip 10 146 213 31 1421176704238 e1786039 watcher executor Exception java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread ec2Test3 ip 10 146 213 31 1421176704238 e1786039 watcher executor noformat Logs are not available at this time. </span>","minimal","punctuation","high",False
18584,"This is a parallel implementation to the RxJava https github.com spring projects spring xd blob master spring xd rxjava src main java org springframework xd rxjava SubjectMessageHandler.java That will allow multiple threads to broadcast an event but allow processing to occur one at a time on any thread.",NULL,"This is a parallel implementation to the RxJava https github.com spring projects spring xd blob master spring xd rxjava src main java org springframework xd rxjava SubjectMessageHandler.java That will allow multiple threads to broadcast an event but allow processing to occur one at a time on any thread.",NULL,"Add for who this story is","well_formed","no_role","high",False
18585,"As a user deploying XD on YARN I need a convenient way to get info like the admin port for my current deployment. Best way, for now, would be to add an info command to the xd yarn script. With the latest changes the admin server runs on a random port when we deploy to YARN. In order for the user to connect they would have to query Zookeeper. This is inconvenient.","As a user","deploying XD on YARN I need a convenient way to get info like the admin port for my current deployment. Best way, for now, would be to add an info command to the xd yarn script. With the latest changes the admin server runs on a random port when we deploy to YARN. In order for the user to connect they would have to query Zookeeper. This is inconvenient.",NULL,"As a user deploying XD on YARN I need a convenient way to get info like the admin port for my current deployment<span class='highlight-text severity-high'>. Best way, for now, would be to add an info command to the xd yarn script. With the latest changes the admin server runs on a random port when we deploy to YARN. In order for the user to connect they would have to query Zookeeper. This is inconvenient.</span>","minimal","punctuation","high",False
18585,"As a user deploying XD on YARN I need a convenient way to get info like the admin port for my current deployment. Best way, for now, would be to add an info command to the xd yarn script. With the latest changes the admin server runs on a random port when we deploy to YARN. In order for the user to connect they would have to query Zookeeper. This is inconvenient.","As a user","deploying XD on YARN I need a convenient way to get info like the admin port for my current deployment. Best way, for now, would be to add an info command to the xd yarn script. With the latest changes the admin server runs on a random port when we deploy to YARN. In order for the user to connect they would have to query Zookeeper. This is inconvenient.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18582,"As a user, I'd like to have an option to track history so that I get the visibility of stream name, module name etc. added as part of the message header.","As a user",", I'd like to have an option to track history","so that I get the visibility of stream name, module name etc. added as part of the message header.","As a user, I'd like to have an option to track history so that I get the visibility of stream name, module name etc<span class='highlight-text severity-high'>. added as part of the message header.</span>","minimal","punctuation","high",False
18582,"As a user, I'd like to have an option to track history so that I get the visibility of stream name, module name etc. added as part of the message header.","As a user",", I'd like to have an option to track history","so that I get the visibility of stream name, module name etc. added as part of the message header.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18583,"This fix for RichGauge should go into the 1.0.x line.",NULL,"This fix for RichGauge should go into the 1.0.x line.",NULL,"Add for who this story is","well_formed","no_role","high",False
18586,"Test basic functionality hdfs sink, jdbchdfs job on hadoop26, hdp22, cdh5, phd21 Test XD on YARN on hadoop26, hdp22, cdh5 and phd21 ",NULL,"Test basic functionality hdfs sink, jdbchdfs job on hadoop26, hdp22, cdh5, phd21 Test XD on YARN on hadoop26, hdp22, cdh5 and phd21 ",NULL,"Add for who this story is","well_formed","no_role","high",False
18587,"Update spring data hadoop version to 2.1.0.RC1. This also includes updating the following adding hadoop26 Apache Hadoop 2.6.0 as distro adding hdp22 Hortonworks HDP 2.2 as distro set default distro to hadoop26 update cdh5 to version 5.3.0 remove older distros hadoop24, hdp21 ",NULL,"Update spring data hadoop version to 2.1.0.RC1. This also includes updating the following adding hadoop26 Apache Hadoop 2.6.0 as distro adding hdp22 Hortonworks HDP 2.2 as distro set default distro to hadoop26 update cdh5 to version 5.3.0 remove older distros hadoop24, hdp21 ",NULL,"Add for who this story is","well_formed","no_role","high",False
18587,"Update spring data hadoop version to 2.1.0.RC1. This also includes updating the following adding hadoop26 Apache Hadoop 2.6.0 as distro adding hdp22 Hortonworks HDP 2.2 as distro set default distro to hadoop26 update cdh5 to version 5.3.0 remove older distros hadoop24, hdp21 ",NULL,"Update spring data hadoop version to 2.1.0.RC1. This also includes updating the following adding hadoop26 Apache Hadoop 2.6.0 as distro adding hdp22 Hortonworks HDP 2.2 as distro set default distro to hadoop26 update cdh5 to version 5.3.0 remove older distros hadoop24, hdp21 ",NULL,"Update spring data hadoop version to 2<span class='highlight-text severity-high'>.1.0.RC1. This also includes updating the following adding hadoop26 Apache Hadoop 2.6.0 as distro adding hdp22 Hortonworks HDP 2.2 as distro set default distro to hadoop26 update cdh5 to version 5.3.0 remove older distros hadoop24, hdp21 </span>","minimal","punctuation","high",False
18621,"A sample, perhaps taken from Pivotal Labs use case in Denver, that would calculate some time window averages for a many individual senor values .",NULL,"A sample, perhaps taken from Pivotal Labs use case in Denver, that would calculate some time window averages for a many individual senor values .",NULL,"Add for who this story is","well_formed","no_role","high",False
18622,"The module should be flexible to act as a sink as well as a processor. ErrorHandling will be considered as part of another JIRA",NULL,"The module should be flexible to act as a sink as well as a processor. ErrorHandling will be considered as part of another JIRA",NULL,"Add for who this story is","well_formed","no_role","high",False
18622,"The module should be flexible to act as a sink as well as a processor. ErrorHandling will be considered as part of another JIRA",NULL,"The module should be flexible to act as a sink as well as a processor. ErrorHandling will be considered as part of another JIRA",NULL,"The module should be flexible to act as a sink as well as a processor<span class='highlight-text severity-high'>. ErrorHandling will be considered as part of another JIRA</span>","minimal","punctuation","high",False
18581,"https github.com EsotericSoftware kryo pooling kryo instances",NULL,"https github.com EsotericSoftware kryo pooling kryo instances",NULL,"Add for who this story is","well_formed","no_role","high",False
18580,"Configure Redis Cluster with Sentinal v 2.8.19. Verify fail over, experiment with settings. Useful reference https code.flickr.net 2014 07 31 redis sentinel at flickr All analytics test cases should be run as well as test that deploy streams that make use of redis analytics. There might be some minor code changes required as mentioned in the flickr article.",NULL,"Configure Redis Cluster with Sentinal v 2.8.19. Verify fail over, experiment with settings. Useful reference https code.flickr.net 2014 07 31 redis sentinel at flickr All analytics test cases should be run as well as test that deploy streams that make use of redis analytics. There might be some minor code changes required as mentioned in the flickr article.",NULL,"Add for who this story is","well_formed","no_role","high",False
18580,"Configure Redis Cluster with Sentinal v 2.8.19. Verify fail over, experiment with settings. Useful reference https code.flickr.net 2014 07 31 redis sentinel at flickr All analytics test cases should be run as well as test that deploy streams that make use of redis analytics. There might be some minor code changes required as mentioned in the flickr article.",NULL,"Configure Redis Cluster with Sentinal v 2.8.19. Verify fail over, experiment with settings. Useful reference https code.flickr.net 2014 07 31 redis sentinel at flickr All analytics test cases should be run as well as test that deploy streams that make use of redis analytics. There might be some minor code changes required as mentioned in the flickr article.",NULL,"Configure Redis Cluster with Sentinal v 2<span class='highlight-text severity-high'>.8.19. Verify fail over, experiment with settings. Useful reference https code.flickr.net 2014 07 31 redis sentinel at flickr All analytics test cases should be run as well as test that deploy streams that make use of redis analytics. There might be some minor code changes required as mentioned in the flickr article.</span>","minimal","punctuation","high",False
18963,"AGPL license issues",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
18963,"AGPL license issues",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
18592,"Upgrade in 1.0.x branch what was done in this commit on master. https github.com spring projects spring xd commit 16062d771e23187a1d9e8d549abc646ff44e435b ",NULL,"Upgrade in 1.0.x branch what was done in this commit on master. https github.com spring projects spring xd commit 16062d771e23187a1d9e8d549abc646ff44e435b ",NULL,"Add for who this story is","well_formed","no_role","high",False
18592,"Upgrade in 1.0.x branch what was done in this commit on master. https github.com spring projects spring xd commit 16062d771e23187a1d9e8d549abc646ff44e435b ",NULL,"Upgrade in 1.0.x branch what was done in this commit on master. https github.com spring projects spring xd commit 16062d771e23187a1d9e8d549abc646ff44e435b ",NULL,"Upgrade in 1<span class='highlight-text severity-high'>.0.x branch what was done in this commit on master. https github.com spring projects spring xd commit 16062d771e23187a1d9e8d549abc646ff44e435b </span>","minimal","punctuation","high",False
18593,"As a user, I'd like to use RxJava based processor module so that I can leverage RxJava APIs for data computations. ","As a user",", I'd like to use RxJava based processor module","so that I can leverage RxJava APIs for data computations.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18596,"Create an 2.6 Yarn Environment on EC2 for which XD can be deployed for acceptance tests.",NULL,"Create an 2.6 Yarn Environment on EC2 for which XD can be deployed for acceptance tests.",NULL,"Add for who this story is","well_formed","no_role","high",False
18603,"Expecting module name in module configuration is brittle, especially in conjunction with module upload command which permits the module to be registered under a different name. The convention should be dropped in favor of any file name. This requires at most one foo.xml, foo.groovy, and or foo.properties in the top level config folder. It is an exception if multiples are found. Accepting any file name provides the most flexibility without sacrificing backward compatibility except in rare cases in which a module developer may have violated the multiple xml or properties files condition . An alternate approach requiring a well known file name such as spring module were rejected over concerns that it would break any existing custom module implementations.",NULL,"Expecting module name in module configuration is brittle, especially in conjunction with module upload command which permits the module to be registered under a different name. The convention should be dropped in favor of any file name. This requires at most one foo.xml, foo.groovy, and or foo.properties in the top level config folder. It is an exception if multiples are found. Accepting any file name provides the most flexibility without sacrificing backward compatibility except in rare cases in which a module developer may have violated the multiple xml or properties files condition . An alternate approach requiring a well known file name such as spring module were rejected over concerns that it would break any existing custom module implementations.",NULL,"Add for who this story is","well_formed","no_role","high",False
18603,"Expecting module name in module configuration is brittle, especially in conjunction with module upload command which permits the module to be registered under a different name. The convention should be dropped in favor of any file name. This requires at most one foo.xml, foo.groovy, and or foo.properties in the top level config folder. It is an exception if multiples are found. Accepting any file name provides the most flexibility without sacrificing backward compatibility except in rare cases in which a module developer may have violated the multiple xml or properties files condition . An alternate approach requiring a well known file name such as spring module were rejected over concerns that it would break any existing custom module implementations.",NULL,"Expecting module name in module configuration is brittle, especially in conjunction with module upload command which permits the module to be registered under a different name. The convention should be dropped in favor of any file name. This requires at most one foo.xml, foo.groovy, and or foo.properties in the top level config folder. It is an exception if multiples are found. Accepting any file name provides the most flexibility without sacrificing backward compatibility except in rare cases in which a module developer may have violated the multiple xml or properties files condition . An alternate approach requiring a well known file name such as spring module were rejected over concerns that it would break any existing custom module implementations.",NULL,"Expecting module name in module configuration is brittle, especially in conjunction with module upload command which permits the module to be registered under a different name. The convention should be dropped in favor of any file name. This requires at most one foo.xml, foo.groovy,<span class='highlight-text severity-high'> and </span>span class='highlight-text severity-high'> or </span>foo.properties in the top level config folder. It is an exception if multiples are found. Accepting any file name provides the most flexibility without sacrificing backward compatibility except in rare cases in which a module developer may have violated the multiple xml or properties files condition . An alternate approach requiring a well known file name such as spring module were rejected over concerns that it would break any existing custom module implementations.","atomic","conjunctions","high",False
18603,"Expecting module name in module configuration is brittle, especially in conjunction with module upload command which permits the module to be registered under a different name. The convention should be dropped in favor of any file name. This requires at most one foo.xml, foo.groovy, and or foo.properties in the top level config folder. It is an exception if multiples are found. Accepting any file name provides the most flexibility without sacrificing backward compatibility except in rare cases in which a module developer may have violated the multiple xml or properties files condition . An alternate approach requiring a well known file name such as spring module were rejected over concerns that it would break any existing custom module implementations.",NULL,"Expecting module name in module configuration is brittle, especially in conjunction with module upload command which permits the module to be registered under a different name. The convention should be dropped in favor of any file name. This requires at most one foo.xml, foo.groovy, and or foo.properties in the top level config folder. It is an exception if multiples are found. Accepting any file name provides the most flexibility without sacrificing backward compatibility except in rare cases in which a module developer may have violated the multiple xml or properties files condition . An alternate approach requiring a well known file name such as spring module were rejected over concerns that it would break any existing custom module implementations.",NULL,"Expecting module name in module configuration is brittle, especially in conjunction with module upload command which permits the module to be registered under a different name<span class='highlight-text severity-high'>. The convention should be dropped in favor of any file name. This requires at most one foo.xml, foo.groovy, and or foo.properties in the top level config folder. It is an exception if multiples are found. Accepting any file name provides the most flexibility without sacrificing backward compatibility except in rare cases in which a module developer may have violated the multiple xml or properties files condition . An alternate approach requiring a well known file name such as spring module were rejected over concerns that it would break any existing custom module implementations.</span>","minimal","punctuation","high",False
18595,"The data that is entering a broadcast stream can only occur from one thread at a time to prevent race conditions inside the stream implementation. The current handler shares a single broadcast stream. Change to create a new one per thread usage.",NULL,"The data that is entering a broadcast stream can only occur from one thread at a time to prevent race conditions inside the stream implementation. The current handler shares a single broadcast stream. Change to create a new one per thread usage.",NULL,"Add for who this story is","well_formed","no_role","high",False
18595,"The data that is entering a broadcast stream can only occur from one thread at a time to prevent race conditions inside the stream implementation. The current handler shares a single broadcast stream. Change to create a new one per thread usage.",NULL,"The data that is entering a broadcast stream can only occur from one thread at a time to prevent race conditions inside the stream implementation. The current handler shares a single broadcast stream. Change to create a new one per thread usage.",NULL,"The data that is entering a broadcast stream can only occur from one thread at a time to prevent race conditions inside the stream implementation<span class='highlight-text severity-high'>. The current handler shares a single broadcast stream. Change to create a new one per thread usage.</span>","minimal","punctuation","high",False
18598,"MongoDb driver is present on DIRT s classpath, while it should not should be present on mongo related modules though . This is blocked by the shortcoming described here https github.com spring projects spring xd pull 1116",NULL,"MongoDb driver is present on DIRT s classpath, while it should not should be present on mongo related modules though . This is blocked by the shortcoming described here https github.com spring projects spring xd pull 1116",NULL,"Add for who this story is","well_formed","no_role","high",False
18598,"MongoDb driver is present on DIRT s classpath, while it should not should be present on mongo related modules though . This is blocked by the shortcoming described here https github.com spring projects spring xd pull 1116",NULL,"MongoDb driver is present on DIRT s classpath, while it should not should be present on mongo related modules though . This is blocked by the shortcoming described here https github.com spring projects spring xd pull 1116",NULL,"MongoDb driver is present on DIRT s classpath, while it should not should be present on mongo related modules though <span class='highlight-text severity-high'>. This is blocked by the shortcoming described here https github.com spring projects spring xd pull 1116</span>","minimal","punctuation","high",False
18597,"As a developer, I'd like to have acceptance test coverage for XD YARN on EC2 so that I can verify simple XD features running on YARN on every build cycle.","As a developer",", I'd like to have acceptance test coverage for XD YARN on EC2","so that I can verify simple XD features running on YARN on every build cycle.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18600,"As a developer, I'd like to upgrade to Reactor 2.0 RC1 release so that we can synchronize with stable dependencies.","As a developer",", I'd like to upgrade to Reactor 2.0 RC1 release","so that we can synchronize with stable dependencies.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18599,"As a developer, I'd like to isolate the Hadoop tests in a different project so that the DIRT project doesn t have to depend upon, thus eliminating the incorrect CP file generation in eclipse. ","As a developer",", I'd like to isolate the Hadoop tests in a different project","so that the DIRT project doesn t have to depend upon, thus eliminating the incorrect CP file generation in eclipse.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18589,"As a user, I'd like to have the option to extend the default message handling behavior for HTTP source module so that I can override the settings via servers.yml to control the default message size. Notes The adapter currently has that hard coded 1MB limit in the HttpChunkAggregator. We will have to expose this property for overrides. Related PR https github.com spring projects spring xd pull 1367 .","As a user",", I'd like to have the option to extend the default message handling behavior for HTTP source module","so that I can override the settings via servers.yml to control the default message size. Notes The adapter currently has that hard coded 1MB limit in the HttpChunkAggregator. We will have to expose this property for overrides. Related PR https github.com spring projects spring xd pull 1367 .","As a user, I'd like to have the option to extend the default message handling behavior for HTTP source module so that I can override the settings via servers<span class='highlight-text severity-high'>.yml to control the default message size. Notes The adapter currently has that hard coded 1MB limit in the HttpChunkAggregator. We will have to expose this property for overrides. Related PR https github.com spring projects spring xd pull 1367 .</span>","minimal","punctuation","high",False
18589,"As a user, I'd like to have the option to extend the default message handling behavior for HTTP source module so that I can override the settings via servers.yml to control the default message size. Notes The adapter currently has that hard coded 1MB limit in the HttpChunkAggregator. We will have to expose this property for overrides. Related PR https github.com spring projects spring xd pull 1367 .","As a user",", I'd like to have the option to extend the default message handling behavior for HTTP source module","so that I can override the settings via servers.yml to control the default message size. Notes The adapter currently has that hard coded 1MB limit in the HttpChunkAggregator. We will have to expose this property for overrides. Related PR https github.com spring projects spring xd pull 1367 .","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18594,"Ensure build works in Windows environments",NULL,"Ensure build works in Windows environments",NULL,"Add for who this story is","well_formed","no_role","high",False
18609,"https jira.spring.io browse AMQP 453 Added support for compression with RabbitMQ. XD should expose configuration options to enable and configure compression on the message bus. Note, some options may be specific for brokers or require additional functionality in XD. This issue should not address adding additional functionality to make the feature set as common as possible across msg bus implementations, but expose what makes sense with the current code base for rabbitmq As an example, Kafka supports compressed.topics which lets you pick a subset of topics to be compressed. ",NULL,"https jira.spring.io browse AMQP 453 Added support for compression with RabbitMQ. XD should expose configuration options to enable and configure compression on the message bus. Note, some options may be specific for brokers or require additional functionality in XD. This issue should not address adding additional functionality to make the feature set as common as possible across msg bus implementations, but expose what makes sense with the current code base for rabbitmq As an example, Kafka supports compressed.topics which lets you pick a subset of topics to be compressed. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18609,"https jira.spring.io browse AMQP 453 Added support for compression with RabbitMQ. XD should expose configuration options to enable and configure compression on the message bus. Note, some options may be specific for brokers or require additional functionality in XD. This issue should not address adding additional functionality to make the feature set as common as possible across msg bus implementations, but expose what makes sense with the current code base for rabbitmq As an example, Kafka supports compressed.topics which lets you pick a subset of topics to be compressed. ",NULL,"https jira.spring.io browse AMQP 453 Added support for compression with RabbitMQ. XD should expose configuration options to enable and configure compression on the message bus. Note, some options may be specific for brokers or require additional functionality in XD. This issue should not address adding additional functionality to make the feature set as common as possible across msg bus implementations, but expose what makes sense with the current code base for rabbitmq As an example, Kafka supports compressed.topics which lets you pick a subset of topics to be compressed. ",NULL,"https jira.spring.io browse AMQP 453 Added support for compression with RabbitMQ. XD should expose configuration options to enable<span class='highlight-text severity-high'> and </span>configure compression on the message bus. Note, some options may be specific for brokers<span class='highlight-text severity-high'> or </span>require additional functionality in XD. This issue should not address adding additional functionality to make the feature set as common as possible across msg bus implementations, but expose what makes sense with the current code base for rabbitmq As an example, Kafka supports compressed.topics which lets you pick a subset of topics to be compressed. ","atomic","conjunctions","high",False
18609,"https jira.spring.io browse AMQP 453 Added support for compression with RabbitMQ. XD should expose configuration options to enable and configure compression on the message bus. Note, some options may be specific for brokers or require additional functionality in XD. This issue should not address adding additional functionality to make the feature set as common as possible across msg bus implementations, but expose what makes sense with the current code base for rabbitmq As an example, Kafka supports compressed.topics which lets you pick a subset of topics to be compressed. ",NULL,"https jira.spring.io browse AMQP 453 Added support for compression with RabbitMQ. XD should expose configuration options to enable and configure compression on the message bus. Note, some options may be specific for brokers or require additional functionality in XD. This issue should not address adding additional functionality to make the feature set as common as possible across msg bus implementations, but expose what makes sense with the current code base for rabbitmq As an example, Kafka supports compressed.topics which lets you pick a subset of topics to be compressed. ",NULL,"https jira<span class='highlight-text severity-high'>.spring.io browse AMQP 453 Added support for compression with RabbitMQ. XD should expose configuration options to enable and configure compression on the message bus. Note, some options may be specific for brokers or require additional functionality in XD. This issue should not address adding additional functionality to make the feature set as common as possible across msg bus implementations, but expose what makes sense with the current code base for rabbitmq As an example, Kafka supports compressed.topics which lets you pick a subset of topics to be compressed. </span>","minimal","punctuation","high",False
18610,"We should move the org.springframework.xd.batch.jdbc.ColumnRangePartitioner and org.springframework.xd.batch.item.jdbc.FieldSetSqlParameterSourceProvider to the spring xd extension batch project",NULL,"We should move the org.springframework.xd.batch.jdbc.ColumnRangePartitioner and org.springframework.xd.batch.item.jdbc.FieldSetSqlParameterSourceProvider to the spring xd extension batch project",NULL,"Add for who this story is","well_formed","no_role","high",False
18613,"The EXAMPLE in the documentation and the paragraph preceding the example for the script processor uses both location and properties location options, but these are in actuality script and locationProperties according to module info processor script and the text of the documentation. See http docs.spring.io spring xd docs 1.0.2.RELEASE reference html script quote To use the module, pass the location of a Groovy script using the location attribute. If you want to pass variable values to your script, you can optionally pass the path to a properties file using the properties location attribute. All properties in the file will be made available to the script as variables. code xd stream create name groovyprocessortest definition http port 9006 script location custom processor.groovy properties location custom processor.properties log deploy code quote ",NULL,"The EXAMPLE in the documentation and the paragraph preceding the example for the script processor uses both location and properties location options, but these are in actuality script and locationProperties according to module info processor script and the text of the documentation. See http docs.spring.io spring xd docs 1.0.2.RELEASE reference html script quote To use the module, pass the location of a Groovy script using the location attribute. If you want to pass variable values to your script, you can optionally pass the path to a properties file using the properties location attribute. All properties in the file will be made available to the script as variables. code xd stream create name groovyprocessortest definition http port 9006 script location custom processor.groovy properties location custom processor.properties log deploy code quote ",NULL,"Add for who this story is","well_formed","no_role","high",False
18613,"The EXAMPLE in the documentation and the paragraph preceding the example for the script processor uses both location and properties location options, but these are in actuality script and locationProperties according to module info processor script and the text of the documentation. See http docs.spring.io spring xd docs 1.0.2.RELEASE reference html script quote To use the module, pass the location of a Groovy script using the location attribute. If you want to pass variable values to your script, you can optionally pass the path to a properties file using the properties location attribute. All properties in the file will be made available to the script as variables. code xd stream create name groovyprocessortest definition http port 9006 script location custom processor.groovy properties location custom processor.properties log deploy code quote ",NULL,"The EXAMPLE in the documentation and the paragraph preceding the example for the script processor uses both location and properties location options, but these are in actuality script and locationProperties according to module info processor script and the text of the documentation. See http docs.spring.io spring xd docs 1.0.2.RELEASE reference html script quote To use the module, pass the location of a Groovy script using the location attribute. If you want to pass variable values to your script, you can optionally pass the path to a properties file using the properties location attribute. All properties in the file will be made available to the script as variables. code xd stream create name groovyprocessortest definition http port 9006 script location custom processor.groovy properties location custom processor.properties log deploy code quote ",NULL,"The EXAMPLE in the documentation<span class='highlight-text severity-high'> and </span>the paragraph preceding the example for the script processor uses both location and properties location options, but these are in actuality script and locationProperties according to module info processor script and the text of the documentation. See http docs.spring.io spring xd docs 1.0.2.RELEASE reference html script quote To use the module, pass the location of a Groovy script using the location attribute. If you want to pass variable values to your script, you can optionally pass the path to a properties file using the properties location attribute. All properties in the file will be made available to the script as variables. code xd stream create name groovyprocessortest definition http port 9006 script location custom processor.groovy properties location custom processor.properties log deploy code quote ","atomic","conjunctions","high",False
18613,"The EXAMPLE in the documentation and the paragraph preceding the example for the script processor uses both location and properties location options, but these are in actuality script and locationProperties according to module info processor script and the text of the documentation. See http docs.spring.io spring xd docs 1.0.2.RELEASE reference html script quote To use the module, pass the location of a Groovy script using the location attribute. If you want to pass variable values to your script, you can optionally pass the path to a properties file using the properties location attribute. All properties in the file will be made available to the script as variables. code xd stream create name groovyprocessortest definition http port 9006 script location custom processor.groovy properties location custom processor.properties log deploy code quote ",NULL,"The EXAMPLE in the documentation and the paragraph preceding the example for the script processor uses both location and properties location options, but these are in actuality script and locationProperties according to module info processor script and the text of the documentation. See http docs.spring.io spring xd docs 1.0.2.RELEASE reference html script quote To use the module, pass the location of a Groovy script using the location attribute. If you want to pass variable values to your script, you can optionally pass the path to a properties file using the properties location attribute. All properties in the file will be made available to the script as variables. code xd stream create name groovyprocessortest definition http port 9006 script location custom processor.groovy properties location custom processor.properties log deploy code quote ",NULL,"The EXAMPLE in the documentation and the paragraph preceding the example for the script processor uses both location and properties location options, but these are in actuality script and locationProperties according to module info processor script and the text of the documentation<span class='highlight-text severity-high'>. See http docs.spring.io spring xd docs 1.0.2.RELEASE reference html script quote To use the module, pass the location of a Groovy script using the location attribute. If you want to pass variable values to your script, you can optionally pass the path to a properties file using the properties location attribute. All properties in the file will be made available to the script as variables. code xd stream create name groovyprocessortest definition http port 9006 script location custom processor.groovy properties location custom processor.properties log deploy code quote </span>","minimal","punctuation","high",False
18611,"As a user, I'd like to have the option to compress messages so that I can influence the performance throughput. It d be beneficial to have support for gzip, zip compression, and decompression.","As a user",", I'd like to have the option to compress messages","so that I can influence the performance throughput. It d be beneficial to have support for gzip, zip compression, and decompression.","As a user, I'd like to have the option to compress messages so that I can influence the performance throughput<span class='highlight-text severity-high'>. It d be beneficial to have support for gzip, zip compression, and decompression.</span>","minimal","punctuation","high",False
18611,"As a user, I'd like to have the option to compress messages so that I can influence the performance throughput. It d be beneficial to have support for gzip, zip compression, and decompression.","As a user",", I'd like to have the option to compress messages","so that I can influence the performance throughput. It d be beneficial to have support for gzip, zip compression, and decompression.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18614,"Replace the current paravirtual AMI used for CI tests needed to be replaced with a HVM based AMI Paravirtual is being phased out by Amazon. Also so we can utilize VPC and placement groups in the future. ",NULL,"Replace the current paravirtual AMI used for CI tests needed to be replaced with a HVM based AMI Paravirtual is being phased out by Amazon. Al","so so we can utilize VPC and placement groups in the future.","Add for who this story is","well_formed","no_role","high",False
18614,"Replace the current paravirtual AMI used for CI tests needed to be replaced with a HVM based AMI Paravirtual is being phased out by Amazon. Also so we can utilize VPC and placement groups in the future. ",NULL,"Replace the current paravirtual AMI used for CI tests needed to be replaced with a HVM based AMI Paravirtual is being phased out by Amazon. Al","so so we can utilize VPC and placement groups in the future.","Replace the current paravirtual AMI used for CI tests needed to be replaced with a HVM based AMI Paravirtual is being phased out by Amazon<span class='highlight-text severity-high'>. Also so we can utilize VPC and placement groups in the future. </span>","minimal","punctuation","high",False
18614,"Replace the current paravirtual AMI used for CI tests needed to be replaced with a HVM based AMI Paravirtual is being phased out by Amazon. Also so we can utilize VPC and placement groups in the future. ",NULL,"Replace the current paravirtual AMI used for CI tests needed to be replaced with a HVM based AMI Paravirtual is being phased out by Amazon. Al","so so we can utilize VPC and placement groups in the future.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18618,"When building on a branch, the docs should be defaulting to build from origin master, but that doesn t seem to be happening. Instead an explicit Pwikibranch origin master is required to be specified on the command line. ",NULL,"When building on a branch, the docs should be defaulting to build from origin master, but that doesn t seem to be happening. Instead an explicit Pwikibranch origin master is required to be specified on the command line. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18618,"When building on a branch, the docs should be defaulting to build from origin master, but that doesn t seem to be happening. Instead an explicit Pwikibranch origin master is required to be specified on the command line. ",NULL,"When building on a branch, the docs should be defaulting to build from origin master, but that doesn t seem to be happening. Instead an explicit Pwikibranch origin master is required to be specified on the command line. ",NULL,"When building on a branch, the docs should be defaulting to build from origin master, but that doesn t seem to be happening<span class='highlight-text severity-high'>. Instead an explicit Pwikibranch origin master is required to be specified on the command line. </span>","minimal","punctuation","high",False
18617,"As a user, I'd like to use partitionResultsTimeout attribute for jobs that inherit singlestep partitioning strategy but it is not exposed as a metadata attribute in the wiki. Note The property should be available for all the jobs that import; 3 OOTB jobs have it imported ref. attachment ","As a user",", I'd like to use partitionResultsTimeout attribute for jobs that inherit singlestep partitioning strategy but it is not exposed as a metadata attribute in the wiki. Note The property should be available for all the jobs that import; 3 OOTB jobs have it imported ref. attachment",NULL,"As a user, I'd like to use partitionResultsTimeout attribute for jobs that inherit singlestep partitioning strategy but it is not exposed as a metadata attribute in the wiki<span class='highlight-text severity-high'>. Note The property should be available for all the jobs that import; 3 OOTB jobs have it imported ref. attachment </span>","minimal","punctuation","high",False
18617,"As a user, I'd like to use partitionResultsTimeout attribute for jobs that inherit singlestep partitioning strategy but it is not exposed as a metadata attribute in the wiki. Note The property should be available for all the jobs that import; 3 OOTB jobs have it imported ref. attachment ","As a user",", I'd like to use partitionResultsTimeout attribute for jobs that inherit singlestep partitioning strategy but it is not exposed as a metadata attribute in the wiki. Note The property should be available for all the jobs that import; 3 OOTB jobs have it imported ref. attachment",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18615,"Test is failing since Kafka isn t installed on the CI server. Using an embedded server will make the testing more robust vs. needing an external server.",NULL,"Test is failing since Kafka isn t installed on the CI server. Using an embedded server will make the testing more robust vs. needing an external server.",NULL,"Add for who this story is","well_formed","no_role","high",False
18615,"Test is failing since Kafka isn t installed on the CI server. Using an embedded server will make the testing more robust vs. needing an external server.",NULL,"Test is failing since Kafka isn t installed on the CI server. Using an embedded server will make the testing more robust vs. needing an external server.",NULL,"Test is failing since Kafka isn t installed on the CI server<span class='highlight-text severity-high'>. Using an embedded server will make the testing more robust vs. needing an external server.</span>","minimal","punctuation","high",False
18612,"HA Configuration, async sends. http docs.spring.io spring integration reference html whats new.html 4.1 mqtt",NULL,"HA Configuration, async sends. http docs.spring.io spring integration reference html whats new.html 4.1 mqtt",NULL,"Add for who this story is","well_formed","no_role","high",False
18612,"HA Configuration, async sends. http docs.spring.io spring integration reference html whats new.html 4.1 mqtt",NULL,"HA Configuration, async sends. http docs.spring.io spring integration reference html whats new.html 4.1 mqtt",NULL,"HA Configuration, async sends<span class='highlight-text severity-high'>. http docs.spring.io spring integration reference html whats new.html 4.1 mqtt</span>","minimal","punctuation","high",False
18629,"As a user, I'd like to have the option to implement bindRequestor and bindReplier so that I can bind a producer that expects async replies and bind a consumer that handles requests from a requestor and asynchronously sends replies respectively. ","As a user",", I'd like to have the option to implement bindRequestor and bindReplier","so that I can bind a producer that expects async replies and bind a consumer that handles requests from a requestor and asynchronously sends replies respectively.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18624,"What is the core interface contract users will be exposed to when creating a processor module that uses Reactor s Stream API. Some consideration for error handling should be considered as it maybe outside normal exception throwing signatures.",NULL,"What is the core interface contract users will be exposed to when creating a processor module that uses Reactor s Stream API. Some consideration for error handling should be considered as it maybe outside normal exception throwing signatures.",NULL,"Add for who this story is","well_formed","no_role","high",False
18624,"What is the core interface contract users will be exposed to when creating a processor module that uses Reactor s Stream API. Some consideration for error handling should be considered as it maybe outside normal exception throwing signatures.",NULL,"What is the core interface contract users will be exposed to when creating a processor module that uses Reactor s Stream API. Some consideration for error handling should be considered as it maybe outside normal exception throwing signatures.",NULL,"What is the core interface contract users will be exposed to when creating a processor module that uses Reactor s Stream API<span class='highlight-text severity-high'>. Some consideration for error handling should be considered as it maybe outside normal exception throwing signatures.</span>","minimal","punctuation","high",False
18630,"As a user, I'd like to have the option to ACK messages so that I can guarantee that the message request sent is successful. ","As a user",", I'd like to have the option to ACK messages","so that I can guarantee that the message request sent is successful.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18631,"As a user, I'd like to have concurrency and compression support for Kafka so that I can increase performance throughput and or increase responsiveness Things to consider make global configuration options be defaults and allow per deployment overrides add options for concurrency compression support","As a user",", I'd like to have concurrency and compression support for Kafka","so that I can increase performance throughput and or increase responsiveness Things to consider make global configuration options be defaults and allow per deployment overrides add options for concurrency compression support","As a user, I'd like to have concurrency<span class='highlight-text severity-high'> and </span>compression support for Kafka so that I can increase performance throughput and<span class='highlight-text severity-high'> or </span>increase responsiveness Things to consider make global configuration options be defaults and allow per deployment overrides add options for concurrency compression support","atomic","conjunctions","high",False
18631,"As a user, I'd like to have concurrency and compression support for Kafka so that I can increase performance throughput and or increase responsiveness Things to consider make global configuration options be defaults and allow per deployment overrides add options for concurrency compression support","As a user",", I'd like to have concurrency and compression support for Kafka","so that I can increase performance throughput and or increase responsiveness Things to consider make global configuration options be defaults and allow per deployment overrides add options for concurrency compression support","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18625,"As a QA, I'd like to benchmark Sqoop vs. jdbchdfs batch job so that I can compare and contrast performance stats. ","As a QA",", I'd like to benchmark Sqoop vs. jdbchdfs batch job","so that I can compare and contrast performance stats.","As a QA, I'd like to benchmark Sqoop vs<span class='highlight-text severity-high'>. jdbchdfs batch job so that I can compare and contrast performance stats. </span>","minimal","punctuation","high",False
18625,"As a QA, I'd like to benchmark Sqoop vs. jdbchdfs batch job so that I can compare and contrast performance stats. ","As a QA",", I'd like to benchmark Sqoop vs. jdbchdfs batch job","so that I can compare and contrast performance stats.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18626,"As a user, I'd like to access Sqoop logs so that I can troubleshoot or evaluate the errors or current state respectively. We will have to identify how to capture the Sqoop logs and stream them to our logging mechanism.","As a user",", I'd like to access Sqoop logs","so that I can troubleshoot or evaluate the errors or current state respectively. We will have to identify how to capture the Sqoop logs and stream them to our logging mechanism.","As a user, I'd like to access Sqoop logs so that I can troubleshoot or evaluate the errors or current state respectively<span class='highlight-text severity-high'>. We will have to identify how to capture the Sqoop logs and stream them to our logging mechanism.</span>","minimal","punctuation","high",False
18626,"As a user, I'd like to access Sqoop logs so that I can troubleshoot or evaluate the errors or current state respectively. We will have to identify how to capture the Sqoop logs and stream them to our logging mechanism.","As a user",", I'd like to access Sqoop logs","so that I can troubleshoot or evaluate the errors or current state respectively. We will have to identify how to capture the Sqoop logs and stream them to our logging mechanism.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18627,"As a user, I'd like to have the option to stop an existing Sqoop job so that I can clean up resources at the time of completion.","As a user",", I'd like to have the option to stop an existing Sqoop job","so that I can clean up resources at the time of completion.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18628,"As a user, I'd like to have the option to setup batching so that I can ingest data in batches as opposed to payload at a time.","As a user",", I'd like to have the option to setup batching","so that I can ingest data in batches as opposed to payload at a time.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18635,"The workaround explicitly updates spring core latest boot needs it merges all application.yml documents that are not profile specific under on spring key the latest boot requires it, at least for now. Boot may go back, see spring projects spring boot 2022",NULL,"The workaround explicitly updates spring core latest boot needs it merges all application.yml documents that are not profile specific under on spring key the latest boot requires it, at least for now. Boot may go back, see spring projects spring boot 2022",NULL,"Add for who this story is","well_formed","no_role","high",False
18635,"The workaround explicitly updates spring core latest boot needs it merges all application.yml documents that are not profile specific under on spring key the latest boot requires it, at least for now. Boot may go back, see spring projects spring boot 2022",NULL,"The workaround explicitly updates spring core latest boot needs it merges all application.yml documents that are not profile specific under on spring key the latest boot requires it, at least for now. Boot may go back, see spring projects spring boot 2022",NULL,"The workaround explicitly updates spring core latest boot needs it merges all application<span class='highlight-text severity-high'>.yml documents that are not profile specific under on spring key the latest boot requires it, at least for now. Boot may go back, see spring projects spring boot 2022</span>","minimal","punctuation","high",False
18639,"Travis CI recently introduced docker based builds. This prevents root access which we don t need , but allows caching which we could not use before and seems to come with beefier machine specs",NULL,"Travis CI recently introduced docker based builds. This prevents root access which we don t need , but allows caching which we could not use before and seems to come with beefier machine specs",NULL,"Add for who this story is","well_formed","no_role","high",False
18639,"Travis CI recently introduced docker based builds. This prevents root access which we don t need , but allows caching which we could not use before and seems to come with beefier machine specs",NULL,"Travis CI recently introduced docker based builds. This prevents root access which we don t need , but allows caching which we could not use before and seems to come with beefier machine specs",NULL,"Travis CI recently introduced docker based builds. This prevents root access which we don t need , but allows caching which we could not use before<span class='highlight-text severity-high'> and </span>seems to come with beefier machine specs","atomic","conjunctions","high",False
18639,"Travis CI recently introduced docker based builds. This prevents root access which we don t need , but allows caching which we could not use before and seems to come with beefier machine specs",NULL,"Travis CI recently introduced docker based builds. This prevents root access which we don t need , but allows caching which we could not use before and seems to come with beefier machine specs",NULL,"Travis CI recently introduced docker based builds<span class='highlight-text severity-high'>. This prevents root access which we don t need , but allows caching which we could not use before and seems to come with beefier machine specs</span>","minimal","punctuation","high",False
18634,"As a QA, I'd like to include acceptance test coverage for spark app batch job so that I can validate the functionality as part of every CI build. ","As a QA",", I'd like to include acceptance test coverage for spark app batch job","so that I can validate the functionality as part of every CI build.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18636,"Based on the POC from XD 2124 we should create the actual implementation. Things to consider to store in step context capture Log output MapReduce job counters capture last value from incremental imports ",NULL,"Based on the POC from XD 2124 we should create the actual implementation. Things to consider to store in step context capture Log output MapReduce job counters capture last value from incremental imports ",NULL,"Add for who this story is","well_formed","no_role","high",False
18636,"Based on the POC from XD 2124 we should create the actual implementation. Things to consider to store in step context capture Log output MapReduce job counters capture last value from incremental imports ",NULL,"Based on the POC from XD 2124 we should create the actual implementation. Things to consider to store in step context capture Log output MapReduce job counters capture last value from incremental imports ",NULL,"Based on the POC from XD 2124 we should create the actual implementation<span class='highlight-text severity-high'>. Things to consider to store in step context capture Log output MapReduce job counters capture last value from incremental imports </span>","minimal","punctuation","high",False
18637,"The reference states the following The JDBC driver jars for the HSQLDB, MySql, and Postgres are already on the XD classpath It looks like this is true for Postgres and HSQLDB, but I can t see a driver for MySQL shipped with the distribution.",NULL,"The reference states the following The JDBC driver jars for the HSQLDB, MySql, and Postgres are already on the XD classpath It looks like this is true for Postgres and HSQLDB, but I can t see a driver for MySQL shipped with the distribution.",NULL,"Add for who this story is","well_formed","no_role","high",False
18637,"The reference states the following The JDBC driver jars for the HSQLDB, MySql, and Postgres are already on the XD classpath It looks like this is true for Postgres and HSQLDB, but I can t see a driver for MySQL shipped with the distribution.",NULL,"The reference states the following The JDBC driver jars for the HSQLDB, MySql, and Postgres are already on the XD classpath It looks like this is true for Postgres and HSQLDB, but I can t see a driver for MySQL shipped with the distribution.",NULL,"The reference states the following The JDBC driver jars for the HSQLDB, MySql,<span class='highlight-text severity-high'> and </span>Postgres are already on the XD classpath It looks like this is true for Postgres and HSQLDB, but I can t see a driver for MySQL shipped with the distribution.","atomic","conjunctions","high",False
18637,"The reference states the following The JDBC driver jars for the HSQLDB, MySql, and Postgres are already on the XD classpath It looks like this is true for Postgres and HSQLDB, but I can t see a driver for MySQL shipped with the distribution.",NULL,"The reference states the following The JDBC driver jars for the HSQLDB, MySql, and Postgres are already on the XD classpath It looks like this is true for Postgres and HSQLDB, but I can t see a driver for MySQL shipped with the distribution.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18638,"In order to improve the build reliability, we should be using the NPM repo provided by repo.spring.io See spring xd ui README.md for further details.",NULL,"In order to improve the build reliability, we should be using the NPM repo provided by repo.spring.io See spring xd ui README.md for further details.",NULL,"Add for who this story is","well_formed","no_role","high",False
18640,"See discussion at https github.com spring projects spring xd pull 1311 1 there seems to be unused SimpleDateFormat in TupleBuilder which hurst perf 2 More generally, should take some time to profile micro benchmark TupleBuilder",NULL,"See discussion at https github.com spring projects spring xd pull 1311 1 there seems to be unused SimpleDateFormat in TupleBuilder which hurst perf 2 More generally, should take some time to profile micro benchmark TupleBuilder",NULL,"Add for who this story is","well_formed","no_role","high",False
18642,"That method is actually currently never called, but The case where a mapping already exists is not covered outstanding TODO comment the semantics of the method should just be to save and override ",NULL,"That method is actually currently never called, but The case where a mapping already exists is not covered outstanding TODO comment the semantics of the method should just be to save and override ",NULL,"Add for who this story is","well_formed","no_role","high",False
18643,"The current implementation makes individual reads from redis and then writes back the average, so in a cluster environment the reads and writes are not serialized, client reads and writes for specific keys can interfere with each other. Investigate options, such as use of redis transactions or use of lua scripting to solve this problem.",NULL,"The current implementation makes individual reads from redis and then writes back the average,","so in a cluster environment the reads and writes are not serialized, client reads and writes for specific keys can interfere with each other. Investigate options, such as use of redis transactions or use of lua scripting to solve this problem.","Add for who this story is","well_formed","no_role","high",False
18643,"The current implementation makes individual reads from redis and then writes back the average, so in a cluster environment the reads and writes are not serialized, client reads and writes for specific keys can interfere with each other. Investigate options, such as use of redis transactions or use of lua scripting to solve this problem.",NULL,"The current implementation makes individual reads from redis and then writes back the average,","so in a cluster environment the reads and writes are not serialized, client reads and writes for specific keys can interfere with each other. Investigate options, such as use of redis transactions or use of lua scripting to solve this problem.","The current implementation makes individual reads from redis<span class='highlight-text severity-high'> and </span>then writes back the average, so in a cluster environment the reads and writes are not serialized, client reads and writes for specific keys can interfere with each other. Investigate options, such as use of redis transactions<span class='highlight-text severity-high'> or </span>use of lua scripting to solve this problem.","atomic","conjunctions","high",False
18643,"The current implementation makes individual reads from redis and then writes back the average, so in a cluster environment the reads and writes are not serialized, client reads and writes for specific keys can interfere with each other. Investigate options, such as use of redis transactions or use of lua scripting to solve this problem.",NULL,"The current implementation makes individual reads from redis and then writes back the average,","so in a cluster environment the reads and writes are not serialized, client reads and writes for specific keys can interfere with each other. Investigate options, such as use of redis transactions or use of lua scripting to solve this problem.","The current implementation makes individual reads from redis and then writes back the average, so in a cluster environment the reads and writes are not serialized, client reads and writes for specific keys can interfere with each other<span class='highlight-text severity-high'>. Investigate options, such as use of redis transactions or use of lua scripting to solve this problem.</span>","minimal","punctuation","high",False
18643,"The current implementation makes individual reads from redis and then writes back the average, so in a cluster environment the reads and writes are not serialized, client reads and writes for specific keys can interfere with each other. Investigate options, such as use of redis transactions or use of lua scripting to solve this problem.",NULL,"The current implementation makes individual reads from redis and then writes back the average,","so in a cluster environment the reads and writes are not serialized, client reads and writes for specific keys can interfere with each other. Investigate options, such as use of redis transactions or use of lua scripting to solve this problem.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18641,"The kafka sink supports properties for an async producer e.g. queue.buffering.max.ms but you cannot enable such a producer only sync . Async producers batch messages at the risk of message loss . Add a new property async default false and add the corresponding attribute to the int kafka producer configuration element async async ",NULL,"The kafka sink supports properties for an async producer e.g. queue.buffering.max.ms but you cannot enable such a producer only sync . Async producers batch messages at the risk of message loss . Add a new property async default false and add the corresponding attribute to the int kafka producer configuration element async async ",NULL,"Add for who this story is","well_formed","no_role","high",False
18641,"The kafka sink supports properties for an async producer e.g. queue.buffering.max.ms but you cannot enable such a producer only sync . Async producers batch messages at the risk of message loss . Add a new property async default false and add the corresponding attribute to the int kafka producer configuration element async async ",NULL,"The kafka sink supports properties for an async producer e.g. queue.buffering.max.ms but you cannot enable such a producer only sync . Async producers batch messages at the risk of message loss . Add a new property async default false and add the corresponding attribute to the int kafka producer configuration element async async ",NULL,"The kafka sink supports properties for an async producer e.g. queue.buffering.max.ms but you cannot enable such a producer only sync . Async producers batch messages at the risk of message loss . Add a new property async default false<span class='highlight-text severity-high'> and </span>add the corresponding attribute to the int kafka producer configuration element async async ","atomic","conjunctions","high",False
18641,"The kafka sink supports properties for an async producer e.g. queue.buffering.max.ms but you cannot enable such a producer only sync . Async producers batch messages at the risk of message loss . Add a new property async default false and add the corresponding attribute to the int kafka producer configuration element async async ",NULL,"The kafka sink supports properties for an async producer e.g. queue.buffering.max.ms but you cannot enable such a producer only sync . Async producers batch messages at the risk of message loss . Add a new property async default false and add the corresponding attribute to the int kafka producer configuration element async async ",NULL,"The kafka sink supports properties for an async producer e<span class='highlight-text severity-high'>.g. queue.buffering.max.ms but you cannot enable such a producer only sync . Async producers batch messages at the risk of message loss . Add a new property async default false and add the corresponding attribute to the int kafka producer configuration element async async </span>","minimal","punctuation","high",False
18632,"This should include lifecycle management, so that when the module s stream is undeployed, the Spark Streaming application should be stopped, etc. Deploying a number of module instances should result in multiple receiver tasks, and those should bind to the bus using the consumer side partitioning metadata. ",NULL,"This should include lifecycle management,","so that when the module s stream is undeployed, the Spark Streaming application should be stopped, etc. Deploying a number of module instances should result in multiple receiver tasks, and those should bind to the bus using the consumer side partitioning metadata.","Add for who this story is","well_formed","no_role","high",False
18632,"This should include lifecycle management, so that when the module s stream is undeployed, the Spark Streaming application should be stopped, etc. Deploying a number of module instances should result in multiple receiver tasks, and those should bind to the bus using the consumer side partitioning metadata. ",NULL,"This should include lifecycle management,","so that when the module s stream is undeployed, the Spark Streaming application should be stopped, etc. Deploying a number of module instances should result in multiple receiver tasks, and those should bind to the bus using the consumer side partitioning metadata.","This should include lifecycle management, so that when the module s stream is undeployed, the Spark Streaming application should be stopped, etc<span class='highlight-text severity-high'>. Deploying a number of module instances should result in multiple receiver tasks, and those should bind to the bus using the consumer side partitioning metadata. </span>","minimal","punctuation","high",False
18632,"This should include lifecycle management, so that when the module s stream is undeployed, the Spark Streaming application should be stopped, etc. Deploying a number of module instances should result in multiple receiver tasks, and those should bind to the bus using the consumer side partitioning metadata. ",NULL,"This should include lifecycle management,","so that when the module s stream is undeployed, the Spark Streaming application should be stopped, etc. Deploying a number of module instances should result in multiple receiver tasks, and those should bind to the bus using the consumer side partitioning metadata.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18633,"As a QA, I'd like to include acceptance test coverage for Kafka as a message bus so that I can validate the functionality as part of every CI build.","As a QA",", I'd like to include acceptance test coverage for Kafka as a message bus","so that I can validate the functionality as part of every CI build.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18648,"As a developer, I'd like to include the following improvements as part of the EC2 CI infrastructure, so that we can reliably run the CI builds and also assert over feature functionalities. Scope Enable distributed jvm test Change from using artifactory gradle task to a command task that calls . gradlew Test w embedded hadoop off Turn on maxParallelForks ","As a developer",", I'd like to include the following improvements as part of the EC2 CI infrastructure,","so that we can reliably run the CI builds and also assert over feature functionalities. Scope Enable distributed jvm test Change from using artifactory gradle task to a command task that calls . gradlew Test w embedded hadoop off Turn on maxParallelForks","As a developer, I'd like to include the following improvements as part of the EC2 CI infrastructure, so that we can reliably run the CI builds and also assert over feature functionalities<span class='highlight-text severity-high'>. Scope Enable distributed jvm test Change from using artifactory gradle task to a command task that calls . gradlew Test w embedded hadoop off Turn on maxParallelForks </span>","minimal","punctuation","high",False
18649,"As a build master, I'd like to research CI options so that I can improve CI build stability and reliability. Potential Option npm cache https github.com swarajban npm cache Caching previously installed dependencies, npm cache doesn t require downloading the internet each time we build.",NULL,"As a build master, I'd like to research CI options","so that I can improve CI build stability and reliability. Potential Option npm cache https github.com swarajban npm cache Caching previously installed dependencies, npm cache doesn t require downloading the internet each time we build.","As a build master, I'd like to research CI options so that I can improve CI build stability and reliability<span class='highlight-text severity-high'>. Potential Option npm cache https github.com swarajban npm cache Caching previously installed dependencies, npm cache doesn t require downloading the internet each time we build.</span>","minimal","punctuation","high",False
18649,"As a build master, I'd like to research CI options so that I can improve CI build stability and reliability. Potential Option npm cache https github.com swarajban npm cache Caching previously installed dependencies, npm cache doesn t require downloading the internet each time we build.",NULL,"As a build master, I'd like to research CI options","so that I can improve CI build stability and reliability. Potential Option npm cache https github.com swarajban npm cache Caching previously installed dependencies, npm cache doesn t require downloading the internet each time we build.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18645,"Create one or more Sample module projects in the Spring XD Examples repo to serve as templates for Spring XD module projects. Similar to https github.com dturanski siDslModule, these should include unit and single node integration tests, and demonstrate the use of Spring XD build and packaging tools, and other module development support. This may be split out into separate tasks, but should include a sample for source, processor, sink, and job, using Configuration or XML configuration either as separate samples or using build profiles . ",NULL,"Create one or more Sample module projects in the Spring XD Examples repo to serve as templates for Spring XD module projects. Similar to https github.com dturanski siDslModule, these should include unit and single node integration tests, and demonstrate the use of Spring XD build and packaging tools, and other module development support. This may be split out into separate tasks, but should include a sample for source, processor, sink, and job, using Configuration or XML configuration either as separate samples or using build profiles . ",NULL,"Add for who this story is","well_formed","no_role","high",False
18645,"Create one or more Sample module projects in the Spring XD Examples repo to serve as templates for Spring XD module projects. Similar to https github.com dturanski siDslModule, these should include unit and single node integration tests, and demonstrate the use of Spring XD build and packaging tools, and other module development support. This may be split out into separate tasks, but should include a sample for source, processor, sink, and job, using Configuration or XML configuration either as separate samples or using build profiles . ",NULL,"Create one or more Sample module projects in the Spring XD Examples repo to serve as templates for Spring XD module projects. Similar to https github.com dturanski siDslModule, these should include unit and single node integration tests, and demonstrate the use of Spring XD build and packaging tools, and other module development support. This may be split out into separate tasks, but should include a sample for source, processor, sink, and job, using Configuration or XML configuration either as separate samples or using build profiles . ",NULL,"Create one<span class='highlight-text severity-high'> or </span>more Sample module projects in the Spring XD Examples repo to serve as templates for Spring XD module projects. Similar to https github.com dturanski siDslModule, these should include unit<span class='highlight-text severity-high'> and </span>single node integration tests, and demonstrate the use of Spring XD build and packaging tools, and other module development support. This may be split out into separate tasks, but should include a sample for source, processor, sink, and job, using Configuration or XML configuration either as separate samples or using build profiles . ","atomic","conjunctions","high",False
18645,"Create one or more Sample module projects in the Spring XD Examples repo to serve as templates for Spring XD module projects. Similar to https github.com dturanski siDslModule, these should include unit and single node integration tests, and demonstrate the use of Spring XD build and packaging tools, and other module development support. This may be split out into separate tasks, but should include a sample for source, processor, sink, and job, using Configuration or XML configuration either as separate samples or using build profiles . ",NULL,"Create one or more Sample module projects in the Spring XD Examples repo to serve as templates for Spring XD module projects. Similar to https github.com dturanski siDslModule, these should include unit and single node integration tests, and demonstrate the use of Spring XD build and packaging tools, and other module development support. This may be split out into separate tasks, but should include a sample for source, processor, sink, and job, using Configuration or XML configuration either as separate samples or using build profiles . ",NULL,"Create one or more Sample module projects in the Spring XD Examples repo to serve as templates for Spring XD module projects<span class='highlight-text severity-high'>. Similar to https github.com dturanski siDslModule, these should include unit and single node integration tests, and demonstrate the use of Spring XD build and packaging tools, and other module development support. This may be split out into separate tasks, but should include a sample for source, processor, sink, and job, using Configuration or XML configuration either as separate samples or using build profiles . </span>","minimal","punctuation","high",False
18646,"Provide A maven pom to support module projects that will declare the Spring XD dependencies as provided, configure the boot plugin for MODULE layout and other boilerplate build configuration. This should include a similar feature for gradle.",NULL,"Provide A maven pom to support module projects that will declare the Spring XD dependencies as provided, configure the boot plugin for MODULE layout and other boilerplate build configuration. This should include a similar feature for gradle.",NULL,"Add for who this story is","well_formed","no_role","high",False
18646,"Provide A maven pom to support module projects that will declare the Spring XD dependencies as provided, configure the boot plugin for MODULE layout and other boilerplate build configuration. This should include a similar feature for gradle.",NULL,"Provide A maven pom to support module projects that will declare the Spring XD dependencies as provided, configure the boot plugin for MODULE layout and other boilerplate build configuration. This should include a similar feature for gradle.",NULL,"Provide A maven pom to support module projects that will declare the Spring XD dependencies as provided, configure the boot plugin for MODULE layout<span class='highlight-text severity-high'> and </span>other boilerplate build configuration. This should include a similar feature for gradle.","atomic","conjunctions","high",False
18646,"Provide A maven pom to support module projects that will declare the Spring XD dependencies as provided, configure the boot plugin for MODULE layout and other boilerplate build configuration. This should include a similar feature for gradle.",NULL,"Provide A maven pom to support module projects that will declare the Spring XD dependencies as provided, configure the boot plugin for MODULE layout and other boilerplate build configuration. This should include a similar feature for gradle.",NULL,"Provide A maven pom to support module projects that will declare the Spring XD dependencies as provided, configure the boot plugin for MODULE layout and other boilerplate build configuration<span class='highlight-text severity-high'>. This should include a similar feature for gradle.</span>","minimal","punctuation","high",False
18647,"As a build manager, I'd like to setup CI infrastructure so that I can run integration tests in Windows OS automatically as we commit trigger new builds. Scope Use the environment where Bamboo is running Gain access to powershell Setup services redis, rabbit, etc. Kick off CI task",NULL,"As a build manager, I'd like to setup CI infrastructure","so that I can run integration tests in Windows OS automatically as we commit trigger new builds. Scope Use the environment where Bamboo is running Gain access to powershell Setup services redis, rabbit, etc. Kick off CI task","Add for who this story is","well_formed","no_role","high",False
18647,"As a build manager, I'd like to setup CI infrastructure so that I can run integration tests in Windows OS automatically as we commit trigger new builds. Scope Use the environment where Bamboo is running Gain access to powershell Setup services redis, rabbit, etc. Kick off CI task",NULL,"As a build manager, I'd like to setup CI infrastructure","so that I can run integration tests in Windows OS automatically as we commit trigger new builds. Scope Use the environment where Bamboo is running Gain access to powershell Setup services redis, rabbit, etc. Kick off CI task","As a build manager, I'd like to setup CI infrastructure so that I can run integration tests in Windows OS automatically as we commit trigger new builds<span class='highlight-text severity-high'>. Scope Use the environment where Bamboo is running Gain access to powershell Setup services redis, rabbit, etc. Kick off CI task</span>","minimal","punctuation","high",False
18647,"As a build manager, I'd like to setup CI infrastructure so that I can run integration tests in Windows OS automatically as we commit trigger new builds. Scope Use the environment where Bamboo is running Gain access to powershell Setup services redis, rabbit, etc. Kick off CI task",NULL,"As a build manager, I'd like to setup CI infrastructure","so that I can run integration tests in Windows OS automatically as we commit trigger new builds. Scope Use the environment where Bamboo is running Gain access to powershell Setup services redis, rabbit, etc. Kick off CI task","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18651,"As a user, I'd like to have the custom module built as uber jar hosted in HDFS so that I can deploy the module to newly arriving containers. ","As a user",", I'd like to have the custom module built as uber jar hosted in HDFS","so that I can deploy the module to newly arriving containers.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18656,"As a user, I'd like to have microbatching capability so that I can ingest based on batch intervals for enhanced performance throughput. Example http batchInterval 10 log ","As a user",", I'd like to have microbatching capability","so that I can ingest based on batch intervals for enhanced performance throughput. Example http batchInterval 10 log","As a user, I'd like to have microbatching capability so that I can ingest based on batch intervals for enhanced performance throughput<span class='highlight-text severity-high'>. Example http batchInterval 10 log </span>","minimal","punctuation","high",False
18656,"As a user, I'd like to have microbatching capability so that I can ingest based on batch intervals for enhanced performance throughput. Example http batchInterval 10 log ","As a user",", I'd like to have microbatching capability","so that I can ingest based on batch intervals for enhanced performance throughput. Example http batchInterval 10 log","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18662,"Create the infrastructure Mongo, Hadoop, ActiveMQ, Gemfire, Mysql, etc in EC2 for the 1.0.2 acceptance tests Retrofit the 1.0.2 to use the new infrastructure Create a 1.0.2 branch for XD EC2",NULL,"Create the infrastructure Mongo, Hadoop, ActiveMQ, Gemfire, Mysql, etc in EC2 for the 1.0.2 acceptance tests Retrofit the 1.0.2 to use the new infrastructure Create a 1.0.2 branch for XD EC2",NULL,"Add for who this story is","well_formed","no_role","high",False
18658,"The acceptance tests cover the entire suite of script tests. Thus they are no longer needed. The only test that was remaining was posting 10 messages to a http source and writing to a long and making sure we didn t get an error. This test httpbash was never called from the scripts CI build.",NULL,"The acceptance tests cover the entire suite of script tests. Thus they are no longer needed. The only test that was remaining was posting 10 messages to a http source and writing to a long and making sure we didn t get an error. This test httpbash was never called from the scripts CI build.",NULL,"Add for who this story is","well_formed","no_role","high",False
18658,"The acceptance tests cover the entire suite of script tests. Thus they are no longer needed. The only test that was remaining was posting 10 messages to a http source and writing to a long and making sure we didn t get an error. This test httpbash was never called from the scripts CI build.",NULL,"The acceptance tests cover the entire suite of script tests. Thus they are no longer needed. The only test that was remaining was posting 10 messages to a http source and writing to a long and making sure we didn t get an error. This test httpbash was never called from the scripts CI build.",NULL,"The acceptance tests cover the entire suite of script tests. Thus they are no longer needed. The only test that was remaining was posting 10 messages to a http source<span class='highlight-text severity-high'> and </span>writing to a long and making sure we didn t get an error. This test httpbash was never called from the scripts CI build.","atomic","conjunctions","high",False
18658,"The acceptance tests cover the entire suite of script tests. Thus they are no longer needed. The only test that was remaining was posting 10 messages to a http source and writing to a long and making sure we didn t get an error. This test httpbash was never called from the scripts CI build.",NULL,"The acceptance tests cover the entire suite of script tests. Thus they are no longer needed. The only test that was remaining was posting 10 messages to a http source and writing to a long and making sure we didn t get an error. This test httpbash was never called from the scripts CI build.",NULL,"The acceptance tests cover the entire suite of script tests<span class='highlight-text severity-high'>. Thus they are no longer needed. The only test that was remaining was posting 10 messages to a http source and writing to a long and making sure we didn t get an error. This test httpbash was never called from the scripts CI build.</span>","minimal","punctuation","high",False
18653,"As a developer, I'd like to setup a performance testing infrastructure rackspace , so I can start benching Kafka baselines and continue with XD use cases.","As a developer",", I'd like to setup a performance testing infrastructure rackspace ,","so I can start benching Kafka baselines and continue with XD use cases.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18654,"Refactoring scope spring xd dirt Message bus dependencies The goal is to decouple them from startup phase to further enhance initialization time. ",NULL,"Refactoring scope spring xd dirt Message bus dependencies The goal is to decouple them from startup phase to further enhance initialization time. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18655,"While there is a server endpoint to logout, we don t have that ability yet from the UI. As indicated by XD 2122 we will also need a meta data REST endpoint so we can interrogate whether security is enabled, whether the user is logged etc. So we can fulfill the requirements Show a logout button only if a security is enabled and b user is logged in Show the username and or full name of the user being logged in ",NULL,"While there is a server endpoint to logout, we don t have that ability yet from the UI. As indicated by XD 2122 we will al","so need a meta data REST endpoint so we can interrogate whether security is enabled, whether the user is logged etc. So we can fulfill the requirements Show a logout button only if a security is enabled and b user is logged in Show the username and or full name of the user being logged in","Add for who this story is","well_formed","no_role","high",False
18655,"While there is a server endpoint to logout, we don t have that ability yet from the UI. As indicated by XD 2122 we will also need a meta data REST endpoint so we can interrogate whether security is enabled, whether the user is logged etc. So we can fulfill the requirements Show a logout button only if a security is enabled and b user is logged in Show the username and or full name of the user being logged in ",NULL,"While there is a server endpoint to logout, we don t have that ability yet from the UI. As indicated by XD 2122 we will al","so need a meta data REST endpoint so we can interrogate whether security is enabled, whether the user is logged etc. So we can fulfill the requirements Show a logout button only if a security is enabled and b user is logged in Show the username and or full name of the user being logged in","While there is a server endpoint to logout, we don t have that ability yet from the UI<span class='highlight-text severity-high'>. As indicated by XD 2122 we will also need a meta data REST endpoint so we can interrogate whether security is enabled, whether the user is logged etc. So we can fulfill the requirements Show a logout button only if a security is enabled and b user is logged in Show the username and or full name of the user being logged in </span>","minimal","punctuation","high",False
18655,"While there is a server endpoint to logout, we don t have that ability yet from the UI. As indicated by XD 2122 we will also need a meta data REST endpoint so we can interrogate whether security is enabled, whether the user is logged etc. So we can fulfill the requirements Show a logout button only if a security is enabled and b user is logged in Show the username and or full name of the user being logged in ",NULL,"While there is a server endpoint to logout, we don t have that ability yet from the UI. As indicated by XD 2122 we will al","so need a meta data REST endpoint so we can interrogate whether security is enabled, whether the user is logged etc. So we can fulfill the requirements Show a logout button only if a security is enabled and b user is logged in Show the username and or full name of the user being logged in","While there is a server endpoint to logout, we don t have that ability yet from the UI. As indicated by XD 2122 we will also need a meta data REST endpoint<span class='highlight-text severity-high'> so </span>we can interrogate whether security is enabled, whether the user is logged etc.<span class='highlight-text severity-high'> So </span>we can fulfill the requirements Show a logout button only if a security is enabled and b user is logged in Show the username and or full name of the user being logged in ","minimal","indicator_repetition","high",False
18655,"While there is a server endpoint to logout, we don t have that ability yet from the UI. As indicated by XD 2122 we will also need a meta data REST endpoint so we can interrogate whether security is enabled, whether the user is logged etc. So we can fulfill the requirements Show a logout button only if a security is enabled and b user is logged in Show the username and or full name of the user being logged in ",NULL,"While there is a server endpoint to logout, we don t have that ability yet from the UI. As indicated by XD 2122 we will al","so need a meta data REST endpoint so we can interrogate whether security is enabled, whether the user is logged etc. So we can fulfill the requirements Show a logout button only if a security is enabled and b user is logged in Show the username and or full name of the user being logged in","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18664,"As a user, I want Spring XD to pre allocate a set of partitions between the Kafka source modules when a stream is deployed, so that deployment is simpler, and rebalancing doesn t take place. ","As a user,","I want Spring XD to pre allocate a set of partitions between the Kafka source modules when a stream is deployed,","so that deployment is simpler, and rebalancing doesn t take place.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18660,"TypeConvertingStreamTests.testBasicTypeConversionWithTap is failing intermittently. Why?",NULL,"TypeConvertingStreamTests.testBasicTypeConversionWithTap is failing intermittently. Why?",NULL,"Add for who this story is","well_formed","no_role","high",False
18660,"TypeConvertingStreamTests.testBasicTypeConversionWithTap is failing intermittently. Why?",NULL,"TypeConvertingStreamTests.testBasicTypeConversionWithTap is failing intermittently. Why?",NULL,"TypeConvertingStreamTests<span class='highlight-text severity-high'>.testBasicTypeConversionWithTap is failing intermittently. Why?</span>","minimal","punctuation","high",False
18659,"As a continuation, we would like to further investigate Spark, develop POC and identify the best appropriate design and implementation for XD.","As a continuation",", we would like to further investigate Spark, develop POC and identify the best appropriate design and implementation for XD.",NULL,"As a continuation, we would like to further investigate Spark, develop POC<span class='highlight-text severity-high'> and </span>identify the best appropriate design and implementation for XD.","atomic","conjunctions","high",False
18659,"As a continuation, we would like to further investigate Spark, develop POC and identify the best appropriate design and implementation for XD.","As a continuation",", we would like to further investigate Spark, develop POC and identify the best appropriate design and implementation for XD.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18652,"As a user, I'd like to refer to documentation so that I can build the custom module based on recommended standards and patterns.","As a user",", I'd like to refer to documentation","so that I can build the custom module based on recommended standards and patterns.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18661,"This doesn t follow the conventions we have with other modules and it also means it isn t easy to override via environment variables etc. This is in HDFS and some others.",NULL,"This doesn t follow the conventions we have with other modules and it also means it isn t easy to override via environment variables etc. This is in HDFS and some others.",NULL,"Add for who this story is","well_formed","no_role","high",False
19473,"http static.springsource.org spring xd docs 1.0.0.BUILD SNAPSHOT reference html sources should have jms added to the list and also the corresponding section that shows some basic usage.",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
18661,"This doesn t follow the conventions we have with other modules and it also means it isn t easy to override via environment variables etc. This is in HDFS and some others.",NULL,"This doesn t follow the conventions we have with other modules and it also means it isn t easy to override via environment variables etc. This is in HDFS and some others.",NULL,"This doesn t follow the conventions we have with other modules<span class='highlight-text severity-high'> and </span>it also means it isn t easy to override via environment variables etc. This is in HDFS and some others.","atomic","conjunctions","high",False
18661,"This doesn t follow the conventions we have with other modules and it also means it isn t easy to override via environment variables etc. This is in HDFS and some others.",NULL,"This doesn t follow the conventions we have with other modules and it also means it isn t easy to override via environment variables etc. This is in HDFS and some others.",NULL,"This doesn t follow the conventions we have with other modules and it also means it isn t easy to override via environment variables etc<span class='highlight-text severity-high'>. This is in HDFS and some others.</span>","minimal","punctuation","high",False
18663,"As a user, I want Spring XD s message bus to be able to pre allocate partitions between nodes when a stream is deployed, so that rebalancing doesn t happen when a container crashes and or it s redeployed.","As a user,","I want Spring XD s message bus to be able to pre allocate partitions between nodes when a stream is deployed,","so that rebalancing doesn t happen when a container crashes and or it s redeployed.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18668,"We are referencing Spring.IO deps when we shouldn t since we moved to a different version of boot than in in the platform .",NULL,"We are referencing Spring.IO deps when we shouldn t since we moved to a different version of boot than in in the platform .",NULL,"Add for who this story is","well_formed","no_role","high",False
18665,"As a user, I want to be able to control the starting offset of the Kafka source when a stream is deployed, so that I can replay a topic if necessary. Note starting offset is only considered when the stream is deployed progress made by modules must survive their crash for a running stream undeploying and redeploying a stream with a specific start offset will cause the stream to read again from the start TBD what happens when streams are undeployed redeployed where do they resume from?","As a user,","I want to be able to control the starting offset of the Kafka source when a stream is deployed,","so that I can replay a topic if necessary. Note starting offset is only considered when the stream is deployed progress made by modules must survive their crash for a running stream undeploying and redeploying a stream with a specific start offset will cause the stream to read again from the start TBD what happens when streams are undeployed redeployed where do they resume from?","As a user, I want to be able to control the starting offset of the Kafka source when a stream is deployed, so that I can replay a topic if necessary<span class='highlight-text severity-high'>. Note starting offset is only considered when the stream is deployed progress made by modules must survive their crash for a running stream undeploying and redeploying a stream with a specific start offset will cause the stream to read again from the start TBD what happens when streams are undeployed redeployed where do they resume from?</span>","minimal","punctuation","high",False
18665,"As a user, I want to be able to control the starting offset of the Kafka source when a stream is deployed, so that I can replay a topic if necessary. Note starting offset is only considered when the stream is deployed progress made by modules must survive their crash for a running stream undeploying and redeploying a stream with a specific start offset will cause the stream to read again from the start TBD what happens when streams are undeployed redeployed where do they resume from?","As a user,","I want to be able to control the starting offset of the Kafka source when a stream is deployed,","so that I can replay a topic if necessary. Note starting offset is only considered when the stream is deployed progress made by modules must survive their crash for a running stream undeploying and redeploying a stream with a specific start offset will cause the stream to read again from the start TBD what happens when streams are undeployed redeployed where do they resume from?","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18669,"As a user, I'd like to refer to documentation in wiki so that I can configure the new sources, sinks and processor modules and as well as any new features. ","As a user",", I'd like to refer to documentation in wiki","so that I can configure the new sources, sinks and processor modules and as well as any new features.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18666,"Many of the tests fail with code java.lang.IllegalStateException Cannot find template location class path resource templates please add some templates, check your Groovy configuration, or set spring.groovy.template.check template location false code Somehow we need to disable this check, using the property suggested.",NULL,"Many of the tests fail with code java.lang.IllegalStateException Cannot find template location class path resource templates please add some templates, check your Groovy configuration, or set spring.groovy.template.check template location false code Somehow we need to disable this check, using the property suggested.",NULL,"Add for who this story is","well_formed","no_role","high",False
18666,"Many of the tests fail with code java.lang.IllegalStateException Cannot find template location class path resource templates please add some templates, check your Groovy configuration, or set spring.groovy.template.check template location false code Somehow we need to disable this check, using the property suggested.",NULL,"Many of the tests fail with code java.lang.IllegalStateException Cannot find template location class path resource templates please add some templates, check your Groovy configuration, or set spring.groovy.template.check template location false code Somehow we need to disable this check, using the property suggested.",NULL,"Many of the tests fail with code java.lang.IllegalStateException Cannot find template location class path resource templates please add some templates, check your Groovy configuration,<span class='highlight-text severity-high'> or </span>set spring.groovy.template.check template location false code Somehow we need to disable this check, using the property suggested.","atomic","conjunctions","high",False
18683,"Create various boot starter projects for module developers. This should include templates for source, processor, sink, and job and ideally different options for each. For example, a processor configured with XML, SI Java DSL, or SI Java DSL with lambdas. ",NULL,"Create various boot starter projects for module developers. This should include templates for source, processor, sink, and job and ideally different options for each. For example, a processor configured with XML, SI Java DSL, or SI Java DSL with lambdas. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18683,"Create various boot starter projects for module developers. This should include templates for source, processor, sink, and job and ideally different options for each. For example, a processor configured with XML, SI Java DSL, or SI Java DSL with lambdas. ",NULL,"Create various boot starter projects for module developers. This should include templates for source, processor, sink, and job and ideally different options for each. For example, a processor configured with XML, SI Java DSL, or SI Java DSL with lambdas. ",NULL,"Create various boot starter projects for module developers. This should include templates for source, processor, sink,<span class='highlight-text severity-high'> and </span>job and ideally different options for each. For example, a processor configured with XML, SI Java DSL,<span class='highlight-text severity-high'> or </span>SI Java DSL with lambdas. ","atomic","conjunctions","high",False
18683,"Create various boot starter projects for module developers. This should include templates for source, processor, sink, and job and ideally different options for each. For example, a processor configured with XML, SI Java DSL, or SI Java DSL with lambdas. ",NULL,"Create various boot starter projects for module developers. This should include templates for source, processor, sink, and job and ideally different options for each. For example, a processor configured with XML, SI Java DSL, or SI Java DSL with lambdas. ",NULL,"Create various boot starter projects for module developers<span class='highlight-text severity-high'>. This should include templates for source, processor, sink, and job and ideally different options for each. For example, a processor configured with XML, SI Java DSL, or SI Java DSL with lambdas. </span>","minimal","punctuation","high",False
18672,"There are some modules that use external config properties kafka producer consumer, hadoop properties etc., . We need to avoid using such properties and have them configured inside module so that module and its properties are self contained. ",NULL,"There are some modules that use external config properties kafka producer consumer, hadoop properties etc., . We need to avoid using such properties and have them configured inside module","so that module and its properties are self contained.","Add for who this story is","well_formed","no_role","high",False
18672,"There are some modules that use external config properties kafka producer consumer, hadoop properties etc., . We need to avoid using such properties and have them configured inside module so that module and its properties are self contained. ",NULL,"There are some modules that use external config properties kafka producer consumer, hadoop properties etc., . We need to avoid using such properties and have them configured inside module","so that module and its properties are self contained.","There are some modules that use external config properties kafka producer consumer, hadoop properties etc., . We need to avoid using such properties<span class='highlight-text severity-high'> and </span>have them configured inside module so that module and its properties are self contained. ","atomic","conjunctions","high",False
18672,"There are some modules that use external config properties kafka producer consumer, hadoop properties etc., . We need to avoid using such properties and have them configured inside module so that module and its properties are self contained. ",NULL,"There are some modules that use external config properties kafka producer consumer, hadoop properties etc., . We need to avoid using such properties and have them configured inside module","so that module and its properties are self contained.","There are some modules that use external config properties kafka producer consumer, hadoop properties etc<span class='highlight-text severity-high'>., . We need to avoid using such properties and have them configured inside module so that module and its properties are self contained. </span>","minimal","punctuation","high",False
18672,"There are some modules that use external config properties kafka producer consumer, hadoop properties etc., . We need to avoid using such properties and have them configured inside module so that module and its properties are self contained. ",NULL,"There are some modules that use external config properties kafka producer consumer, hadoop properties etc., . We need to avoid using such properties and have them configured inside module","so that module and its properties are self contained.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18675,"Since Kafka and Rabbit have different strategies on how a message system is implemented, we will need to update the tests used on rabbit to work with Kafka. While they will not be exactly the same as before, they should exercise the same principles. This story covers Create the consumer and producer execution configurations for kafka producer perf test.sh and kafka consumer perf test.sh. Record the tests a spreadsheet much like the Rabbit Base test spreadsheet ",NULL,"Since Kafka and Rabbit have different strategies on how a message system is implemented, we will need to update the tests used on rabbit to work with Kafka. While they will not be exactly the same as before, they should exercise the same principles. This story covers Create the consumer and producer execution configurations for kafka producer perf test.sh and kafka consumer perf test.sh. Record the tests a spreadsheet much like the Rabbit Base test spreadsheet ",NULL,"Add for who this story is","well_formed","no_role","high",False
18675,"Since Kafka and Rabbit have different strategies on how a message system is implemented, we will need to update the tests used on rabbit to work with Kafka. While they will not be exactly the same as before, they should exercise the same principles. This story covers Create the consumer and producer execution configurations for kafka producer perf test.sh and kafka consumer perf test.sh. Record the tests a spreadsheet much like the Rabbit Base test spreadsheet ",NULL,"Since Kafka and Rabbit have different strategies on how a message system is implemented, we will need to update the tests used on rabbit to work with Kafka. While they will not be exactly the same as before, they should exercise the same principles. This story covers Create the consumer and producer execution configurations for kafka producer perf test.sh and kafka consumer perf test.sh. Record the tests a spreadsheet much like the Rabbit Base test spreadsheet ",NULL,"Since Kafka and Rabbit have different strategies on how a message system is implemented, we will need to update the tests used on rabbit to work with Kafka<span class='highlight-text severity-high'>. While they will not be exactly the same as before, they should exercise the same principles. This story covers Create the consumer and producer execution configurations for kafka producer perf test.sh and kafka consumer perf test.sh. Record the tests a spreadsheet much like the Rabbit Base test spreadsheet </span>","minimal","punctuation","high",False
18673,"Looks like upgrade to Gradle 2.2 is not a simple version change, e.g. I see code FAILURE Build failed with an exception. Where Build file Users hillert dev git spring xd build.gradle line 219 What went wrong A problem occurred evaluating root project spring xd . Could not find method forceDependencyVersions for arguments project documentation toolchain on root project spring xd . code ",NULL,"Looks like upgrade to Gradle 2.2 is not a simple version change, e.g. I see code FAILURE Build failed with an exception. Where Build file Users hillert dev git spring xd build.gradle line 219 What went wrong A problem occurred evaluating root project spring xd . Could not find method forceDependencyVersions for arguments project documentation toolchain on root project spring xd . code ",NULL,"Add for who this story is","well_formed","no_role","high",False
18673,"Looks like upgrade to Gradle 2.2 is not a simple version change, e.g. I see code FAILURE Build failed with an exception. Where Build file Users hillert dev git spring xd build.gradle line 219 What went wrong A problem occurred evaluating root project spring xd . Could not find method forceDependencyVersions for arguments project documentation toolchain on root project spring xd . code ",NULL,"Looks like upgrade to Gradle 2.2 is not a simple version change, e.g. I see code FAILURE Build failed with an exception. Where Build file Users hillert dev git spring xd build.gradle line 219 What went wrong A problem occurred evaluating root project spring xd . Could not find method forceDependencyVersions for arguments project documentation toolchain on root project spring xd . code ",NULL,"Looks like upgrade to Gradle 2<span class='highlight-text severity-high'>.2 is not a simple version change, e.g. I see code FAILURE Build failed with an exception. Where Build file Users hillert dev git spring xd build.gradle line 219 What went wrong A problem occurred evaluating root project spring xd . Could not find method forceDependencyVersions for arguments project documentation toolchain on root project spring xd . code </span>","minimal","punctuation","high",False
18681,"Create a dedicated Scheduling Page for Jobs. Currently we create a form underneath the deployments table. That is a bit unwieldy when many deployed jobs are shown in the table. Similar to XD 2320",NULL,"Create a dedicated Scheduling Page for Jobs. Currently we create a form underneath the deployments table. That is a bit unwieldy when many deployed jobs are shown in the table. Similar to XD 2320",NULL,"Add for who this story is","well_formed","no_role","high",False
18681,"Create a dedicated Scheduling Page for Jobs. Currently we create a form underneath the deployments table. That is a bit unwieldy when many deployed jobs are shown in the table. Similar to XD 2320",NULL,"Create a dedicated Scheduling Page for Jobs. Currently we create a form underneath the deployments table. That is a bit unwieldy when many deployed jobs are shown in the table. Similar to XD 2320",NULL,"Create a dedicated Scheduling Page for Jobs<span class='highlight-text severity-high'>. Currently we create a form underneath the deployments table. That is a bit unwieldy when many deployed jobs are shown in the table. Similar to XD 2320</span>","minimal","punctuation","high",False
18676,"As a PM, I'd like to have test coverage for both Kafka source and sink modules so that we can assert its functionality as part of the CI builds. ","As a PM",", I'd like to have test coverage for both Kafka source and sink modules","so that we can assert its functionality as part of the CI builds.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18680,"We have to explicitly set it to false, in order to avoid an early start of the poller and the associated DistpatcherHasNoSubscribersException.",NULL,"We have to explicitly set it to false,","in order to avoid an early start of the poller and the associated DistpatcherHasNoSubscribersException.","Add for who this story is","well_formed","no_role","high",False
18680,"We have to explicitly set it to false, in order to avoid an early start of the poller and the associated DistpatcherHasNoSubscribersException.",NULL,"We have to explicitly set it to false,","in order to avoid an early start of the poller and the associated DistpatcherHasNoSubscribersException.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18677,"It is easy to get a cron expression wrong. Provide validation of the cron expression on the Schedule Job page using async validation. Submit the cron expression to the server side and validate that the expression is valid. Send a success message back we may even send back some meta data e.g. when is the next execution going to take place ",NULL,"It is easy to get a cron expression wrong. Provide validation of the cron expression on the Schedule Job page using async validation. Submit the cron expression to the server side and validate that the expression is valid. Send a success message back we may even send back some meta data e.g. when is the next execution going to take place ",NULL,"Add for who this story is","well_formed","no_role","high",False
18677,"It is easy to get a cron expression wrong. Provide validation of the cron expression on the Schedule Job page using async validation. Submit the cron expression to the server side and validate that the expression is valid. Send a success message back we may even send back some meta data e.g. when is the next execution going to take place ",NULL,"It is easy to get a cron expression wrong. Provide validation of the cron expression on the Schedule Job page using async validation. Submit the cron expression to the server side and validate that the expression is valid. Send a success message back we may even send back some meta data e.g. when is the next execution going to take place ",NULL,"It is easy to get a cron expression wrong. Provide validation of the cron expression on the Schedule Job page using async validation. Submit the cron expression to the server side<span class='highlight-text severity-high'> and </span>validate that the expression is valid. Send a success message back we may even send back some meta data e.g. when is the next execution going to take place ","atomic","conjunctions","high",False
18677,"It is easy to get a cron expression wrong. Provide validation of the cron expression on the Schedule Job page using async validation. Submit the cron expression to the server side and validate that the expression is valid. Send a success message back we may even send back some meta data e.g. when is the next execution going to take place ",NULL,"It is easy to get a cron expression wrong. Provide validation of the cron expression on the Schedule Job page using async validation. Submit the cron expression to the server side and validate that the expression is valid. Send a success message back we may even send back some meta data e.g. when is the next execution going to take place ",NULL,"It is easy to get a cron expression wrong<span class='highlight-text severity-high'>. Provide validation of the cron expression on the Schedule Job page using async validation. Submit the cron expression to the server side and validate that the expression is valid. Send a success message back we may even send back some meta data e.g. when is the next execution going to take place </span>","minimal","punctuation","high",False
18678,"XD, gemfire, directories in the zip file are missing.",NULL,"XD, gemfire, directories in the zip file are missing.",NULL,"Add for who this story is","well_formed","no_role","high",False
18674,"Create an AMI that will contain the Kafka Executable as well as the Kafka performance test tools.",NULL,"Create an AMI that will contain the Kafka Executable as well as the Kafka performance test tools.",NULL,"Add for who this story is","well_formed","no_role","high",False
18670,"Currently, module composition always guesses the correct type because we don t have a module with a given name N that is both a source and a processor, or a processor and a sink we only have the case source and sink, as in jdbc jdbc or file file . If it were the case, then the heuristics for guessing the resulting type of a composition would break. This issue is about adding the option for the user to explicitly specify the expected type of the composition, if needed .",NULL,"Currently, module composition always guesses the correct type because we don t have a module with a given name N that is both a source and a processor, or a processor and a sink we only have the case source and sink, as in jdbc jdbc or file file . If it were the case, then the heuristics for guessing the resulting type of a composition would break. This issue is about adding the option for the user to explicitly specify the expected type of the composition, if needed .",NULL,"Add for who this story is","well_formed","no_role","high",False
18728,"XD 1.0.2 Release PHD 2.1 Upgrade Action Items Update to SHDP 2.0.3 Add Hadoop 2.5 hadoop25 Change PHD 2.x from phd20 to phd21 Test PHD 2.0 with phd21 Document that both PHD 2.1 and PHD 2.0 is supported with phd21 ",NULL,"XD 1.0.2 Release PHD 2.1 Upgrade Action Items Update to SHDP 2.0.3 Add Hadoop 2.5 hadoop25 Change PHD 2.x from phd20 to phd21 Test PHD 2.0 with phd21 Document that both PHD 2.1 and PHD 2.0 is supported with phd21 ",NULL,"XD 1.0.2 Release PHD 2.1 Upgrade Action Items Update to SHDP 2.0.3 Add Hadoop 2.5 hadoop25 Change PHD 2.x from phd20 to phd21 Test PHD 2.0 with phd21 Document that both PHD 2.1<span class='highlight-text severity-high'> and </span>PHD 2.0 is supported with phd21 ","atomic","conjunctions","high",False
18670,"Currently, module composition always guesses the correct type because we don t have a module with a given name N that is both a source and a processor, or a processor and a sink we only have the case source and sink, as in jdbc jdbc or file file . If it were the case, then the heuristics for guessing the resulting type of a composition would break. This issue is about adding the option for the user to explicitly specify the expected type of the composition, if needed .",NULL,"Currently, module composition always guesses the correct type because we don t have a module with a given name N that is both a source and a processor, or a processor and a sink we only have the case source and sink, as in jdbc jdbc or file file . If it were the case, then the heuristics for guessing the resulting type of a composition would break. This issue is about adding the option for the user to explicitly specify the expected type of the composition, if needed .",NULL,"Currently, module composition always guesses the correct type because we don t have a module with a given name N that is both a source<span class='highlight-text severity-high'> and </span>a processor,<span class='highlight-text severity-high'> or </span>a processor and a sink we only have the case source and sink, as in jdbc jdbc or file file . If it were the case, then the heuristics for guessing the resulting type of a composition would break. This issue is about adding the option for the user to explicitly specify the expected type of the composition, if needed .","atomic","conjunctions","high",False
18670,"Currently, module composition always guesses the correct type because we don t have a module with a given name N that is both a source and a processor, or a processor and a sink we only have the case source and sink, as in jdbc jdbc or file file . If it were the case, then the heuristics for guessing the resulting type of a composition would break. This issue is about adding the option for the user to explicitly specify the expected type of the composition, if needed .",NULL,"Currently, module composition always guesses the correct type because we don t have a module with a given name N that is both a source and a processor, or a processor and a sink we only have the case source and sink, as in jdbc jdbc or file file . If it were the case, then the heuristics for guessing the resulting type of a composition would break. This issue is about adding the option for the user to explicitly specify the expected type of the composition, if needed .",NULL,"Currently, module composition always guesses the correct type because we don t have a module with a given name N that is both a source and a processor, or a processor and a sink we only have the case source and sink, as in jdbc jdbc or file file <span class='highlight-text severity-high'>. If it were the case, then the heuristics for guessing the resulting type of a composition would break. This issue is about adding the option for the user to explicitly specify the expected type of the composition, if needed .</span>","minimal","punctuation","high",False
18679,"Placeholder to update to Apache 2.5.1 on CI machines. Next steps It looks like the dependencies.properties still file has 2.2.0; update it to 2.5.1 I guess we just bumped the spring data hadoop dependency; check for relevant other dependencies ",NULL,"Placeholder to update to Apache 2.5.1 on CI machines. Next steps It looks like the dependencies.properties still file has 2.2.0; update it to 2.5.1 I guess we just bumped the spring data hadoop dependency; check for relevant other dependencies ",NULL,"Add for who this story is","well_formed","no_role","high",False
18679,"Placeholder to update to Apache 2.5.1 on CI machines. Next steps It looks like the dependencies.properties still file has 2.2.0; update it to 2.5.1 I guess we just bumped the spring data hadoop dependency; check for relevant other dependencies ",NULL,"Placeholder to update to Apache 2.5.1 on CI machines. Next steps It looks like the dependencies.properties still file has 2.2.0; update it to 2.5.1 I guess we just bumped the spring data hadoop dependency; check for relevant other dependencies ",NULL,"Placeholder to update to Apache 2<span class='highlight-text severity-high'>.5.1 on CI machines. Next steps It looks like the dependencies.properties still file has 2.2.0; update it to 2.5.1 I guess we just bumped the spring data hadoop dependency; check for relevant other dependencies </span>","minimal","punctuation","high",False
18684,"User should have the option of Greenplum DB sink so they can write data directly to Greenplum DB via the pgfdist gploader Greenplum bulk loader . The existing JDBC sinks are not suitable for high volume loads. The JDBC approach utilizes the master segment of Greenplum for loading datasets instead of the bulk loader utility.",NULL,"User should have the option of Greenplum DB sink","so they can write data directly to Greenplum DB via the pgfdist gploader Greenplum bulk loader . The existing JDBC sinks are not suitable for high volume loads. The JDBC approach utilizes the master segment of Greenplum for loading datasets instead of the bulk loader utility.","Add for who this story is","well_formed","no_role","high",False
18684,"User should have the option of Greenplum DB sink so they can write data directly to Greenplum DB via the pgfdist gploader Greenplum bulk loader . The existing JDBC sinks are not suitable for high volume loads. The JDBC approach utilizes the master segment of Greenplum for loading datasets instead of the bulk loader utility.",NULL,"User should have the option of Greenplum DB sink","so they can write data directly to Greenplum DB via the pgfdist gploader Greenplum bulk loader . The existing JDBC sinks are not suitable for high volume loads. The JDBC approach utilizes the master segment of Greenplum for loading datasets instead of the bulk loader utility.","User should have the option of Greenplum DB sink so they can write data directly to Greenplum DB via the pgfdist gploader Greenplum bulk loader <span class='highlight-text severity-high'>. The existing JDBC sinks are not suitable for high volume loads. The JDBC approach utilizes the master segment of Greenplum for loading datasets instead of the bulk loader utility.</span>","minimal","punctuation","high",False
18684,"User should have the option of Greenplum DB sink so they can write data directly to Greenplum DB via the pgfdist gploader Greenplum bulk loader . The existing JDBC sinks are not suitable for high volume loads. The JDBC approach utilizes the master segment of Greenplum for loading datasets instead of the bulk loader utility.",NULL,"User should have the option of Greenplum DB sink","so they can write data directly to Greenplum DB via the pgfdist gploader Greenplum bulk loader . The existing JDBC sinks are not suitable for high volume loads. The JDBC approach utilizes the master segment of Greenplum for loading datasets instead of the bulk loader utility.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18686,"As a user, I'd like to have a sample app GitHub project so that I can use it as a reference while provisioning Spring XD cluster with Kafka. Consider Kafka as message bus Kafka as source ","As a user",", I'd like to have a sample app GitHub project","so that I can use it as a reference while provisioning Spring XD cluster with Kafka. Consider Kafka as message bus Kafka as source","As a user, I'd like to have a sample app GitHub project so that I can use it as a reference while provisioning Spring XD cluster with Kafka<span class='highlight-text severity-high'>. Consider Kafka as message bus Kafka as source </span>","minimal","punctuation","high",False
18686,"As a user, I'd like to have a sample app GitHub project so that I can use it as a reference while provisioning Spring XD cluster with Kafka. Consider Kafka as message bus Kafka as source ","As a user",", I'd like to have a sample app GitHub project","so that I can use it as a reference while provisioning Spring XD cluster with Kafka. Consider Kafka as message bus Kafka as source","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18690,"https docs.angularjs.org guide migration migrating from 1 2 to 1 3 http angularjs.blogspot.com 2014 10 ng europe angular 13 and beyond.html http angularjs.blogspot.com 2014 10 angularjs 130 superluminal nudge.html ",NULL,"https docs.angularjs.org guide migration migrating from 1 2 to 1 3 http angularjs.blogspot.com 2014 10 ng europe angular 13 and beyond.html http angularjs.blogspot.com 2014 10 angularjs 130 superluminal nudge.html ",NULL,"Add for who this story is","well_formed","no_role","high",False
18687,"XD 1.1 M1 Release PHD 2.1 Upgrade Action Items Update to SHDP 2.1.M2, Add Hadoop 2.5 hadoop25 Remove hadoop22 Remove PHD 1.0 phd1 Change PHD 2.x from phd20 to phd21 Test PHD 2.0 with phd21",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
18687,"XD 1.1 M1 Release PHD 2.1 Upgrade Action Items Update to SHDP 2.1.M2, Add Hadoop 2.5 hadoop25 Remove hadoop22 Remove PHD 1.0 phd1 Change PHD 2.x from phd20 to phd21 Test PHD 2.0 with phd21",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
18689,"As a user, I'd like to use Kafka source through simple consumer API as opposed to high level so that I can gain full control to offsets and partition assignment deterministically. Spike scope Study simple consumer API functionality Document findings, approach and next steps","As a user",", I'd like to use Kafka source through simple consumer API as opposed to high level","so that I can gain full control to offsets and partition assignment deterministically. Spike scope Study simple consumer API functionality Document findings, approach and next steps","As a user, I'd like to use Kafka source through simple consumer API as opposed to high level so that I can gain full control to offsets and partition assignment deterministically<span class='highlight-text severity-high'>. Spike scope Study simple consumer API functionality Document findings, approach and next steps</span>","minimal","punctuation","high",False
18689,"As a user, I'd like to use Kafka source through simple consumer API as opposed to high level so that I can gain full control to offsets and partition assignment deterministically. Spike scope Study simple consumer API functionality Document findings, approach and next steps","As a user",", I'd like to use Kafka source through simple consumer API as opposed to high level","so that I can gain full control to offsets and partition assignment deterministically. Spike scope Study simple consumer API functionality Document findings, approach and next steps","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18711,"As a user, I'd like to have the flexibility to change the namespace so that I can isolate ZK metadata based on each tenant profile. ","As a user",", I'd like to have the flexibility to change the namespace","so that I can isolate ZK metadata based on each tenant profile.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18691,"Spike scope Brainstorm Identify options Document ",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
18691,"Spike scope Brainstorm Identify options Document ",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
18695,"Rerun test XD 2278 on a EC2 32 core machine and see when we max out.",NULL,"Rerun test XD 2278 on a EC2 32 core machine and see when we max out.",NULL,"Add for who this story is","well_formed","no_role","high",False
18712,"Implement pagination for http localhost 9393 jobs executions",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
18712,"Implement pagination for http localhost 9393 jobs executions",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
18710,"As a user, I'd like to type the username and password to gain access to Admin server so that I'don t have to add it in some file; hence I'don t have to worry about having the password getting logged somewhere.","As a user",", I'd like to type the username and password to gain access to Admin server","so that I'don t have to add it in some file; hence I'don t have to worry about having the password getting logged somewhere.","As a user, I'd like to type the username<span class='highlight-text severity-high'> and </span>password to gain access to Admin server so that I'don t have to add it in some file; hence I'don t have to worry about having the password getting logged somewhere.","atomic","conjunctions","high",False
18710,"As a user, I'd like to type the username and password to gain access to Admin server so that I'don t have to add it in some file; hence I'don t have to worry about having the password getting logged somewhere.","As a user",", I'd like to type the username and password to gain access to Admin server","so that I'don t have to add it in some file; hence I'don t have to worry about having the password getting logged somewhere.","As a user, I'd like to type the username and password to gain access to Admin server so that I'don t have to add it in some file<span class='highlight-text severity-high'>; hence I'don t have to worry about having the password getting logged somewhere.</span>","minimal","punctuation","high",False
18710,"As a user, I'd like to type the username and password to gain access to Admin server so that I'don t have to add it in some file; hence I'don t have to worry about having the password getting logged somewhere.","As a user",", I'd like to type the username and password to gain access to Admin server","so that I'don t have to add it in some file; hence I'don t have to worry about having the password getting logged somewhere.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18713,"Add pagination for http localhost 9393 jobs configurations Related to XD 1864",NULL,"Add pagination for http localhost 9393 jobs configurations Related to XD 1864",NULL,"Add for who this story is","well_formed","no_role","high",False
18694,"Similar to time log , we should ship a simple batch job that appends a timestamp to a file. This will make it much easier to validate job functionality, especially in automated tests.",NULL,"Similar to time log , we should ship a simple batch job that appends a timestamp to a file. This will make it much easier to validate job functionality, especially in automated tests.",NULL,"Add for who this story is","well_formed","no_role","high",False
18694,"Similar to time log , we should ship a simple batch job that appends a timestamp to a file. This will make it much easier to validate job functionality, especially in automated tests.",NULL,"Similar to time log , we should ship a simple batch job that appends a timestamp to a file. This will make it much easier to validate job functionality, especially in automated tests.",NULL,"Similar to time log , we should ship a simple batch job that appends a timestamp to a file<span class='highlight-text severity-high'>. This will make it much easier to validate job functionality, especially in automated tests.</span>","minimal","punctuation","high",False
18693,"As a user, I'd like to have a config parameter preferably in servers.yml file so that I can enable disable message rates in the cluster view. ","As a user",", I'd like to have a config parameter preferably in servers.yml file","so that I can enable disable message rates in the cluster view.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18688,"As a user, I'd like to push the custom module built as uber jar via a REST API so that I can install the custom module in cluster. ","As a user",", I'd like to push the custom module built as uber jar via a REST API","so that I can install the custom module in cluster.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18692,"As a user, I'd like to mass ingest data from databases and others into HDFS HAWQ GPDB so that I'don t have to write custom code and as well as be able to ingest in an efficient way.","As a user",", I'd like to mass ingest data from databases and others into HDFS HAWQ GPDB","so that I'don t have to write custom code and as well as be able to ingest in an efficient way.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18697,"Use the number of consumers gave a maximum throughput in the previous test say 10 consumers , message size 100 bytes, Prefetch 100. Send 1M messages Vary the number of producers. Measure the msg sec rate and calculate the data transfer rate in MB sec. Number of producers 2 4 6 10 50 During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",NULL,"Use the number of consumers gave a maximum throughput in the previous test say 10 consumers , message size 100 bytes, Prefetch 100. Send 1M messages Vary the number of producers. Measure the msg sec rate and calculate the data transfer rate in MB sec. Number of producers 2 4 6 10 50 During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",NULL,"Add for who this story is","well_formed","no_role","high",False
18697,"Use the number of consumers gave a maximum throughput in the previous test say 10 consumers , message size 100 bytes, Prefetch 100. Send 1M messages Vary the number of producers. Measure the msg sec rate and calculate the data transfer rate in MB sec. Number of producers 2 4 6 10 50 During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",NULL,"Use the number of consumers gave a maximum throughput in the previous test say 10 consumers , message size 100 bytes, Prefetch 100. Send 1M messages Vary the number of producers. Measure the msg sec rate and calculate the data transfer rate in MB sec. Number of producers 2 4 6 10 50 During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",NULL,"Use the number of consumers gave a maximum throughput in the previous test say 10 consumers , message size 100 bytes, Prefetch 100. Send 1M messages Vary the number of producers. Measure the msg sec rate<span class='highlight-text severity-high'> and </span>calculate the data transfer rate in MB sec. Number of producers 2 4 6 10 50 During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.","atomic","conjunctions","high",False
18697,"Use the number of consumers gave a maximum throughput in the previous test say 10 consumers , message size 100 bytes, Prefetch 100. Send 1M messages Vary the number of producers. Measure the msg sec rate and calculate the data transfer rate in MB sec. Number of producers 2 4 6 10 50 During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",NULL,"Use the number of consumers gave a maximum throughput in the previous test say 10 consumers , message size 100 bytes, Prefetch 100. Send 1M messages Vary the number of producers. Measure the msg sec rate and calculate the data transfer rate in MB sec. Number of producers 2 4 6 10 50 During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",NULL,"Use the number of consumers gave a maximum throughput in the previous test say 10 consumers , message size 100 bytes, Prefetch 100<span class='highlight-text severity-high'>. Send 1M messages Vary the number of producers. Measure the msg sec rate and calculate the data transfer rate in MB sec. Number of producers 2 4 6 10 50 During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.</span>","minimal","punctuation","high",False
18696,"The results from EC2 testing show that once prefetch and message size are set, varying the number of producers or consumers independently does not impact the message rate. The in house testing numbers need another plan to try an understand some discrepancies. Using 500 prefetch, 1000 byte message size run two instances of PerfTest at the same time with each instance referencing a different queue. Vary the number of consumer and publishers. Test 1 one producer one consumer a u q1 p x 1 y 1 s 1000 z 60 q 500 a u q2 p x 1 y 1 s 1000 z 60 q 500 Test 2 one producer two consumers a u q1 p x 1 y 2 s 1000 z 60 q 500 a u q2 p x 1 y 2 s 1000 z 60 q 500 Test 3 two producers one consumer a u q1 p x 2 y 1 s 1000 z 120 q 500 a u q2 p x 2 y 1 s 1000 z 120 q 500 Test 4 two producers two consumers a u q1 p x 2 y 2 s 1000 z 60 q 500 a u q2 p x 2 y 2 s 1000 z 60 q 500 ",NULL,"The results from EC2 testing show that once prefetch and message size are set, varying the number of producers or consumers independently does not impact the message rate. The in house testing numbers need another plan to try an understand some discrepancies. Using 500 prefetch, 1000 byte message size run two instances of PerfTest at the same time with each instance referencing a different queue. Vary the number of consumer and publishers. Test 1 one producer one consumer a u q1 p x 1 y 1 s 1000 z 60 q 500 a u q2 p x 1 y 1 s 1000 z 60 q 500 Test 2 one producer two consumers a u q1 p x 1 y 2 s 1000 z 60 q 500 a u q2 p x 1 y 2 s 1000 z 60 q 500 Test 3 two producers one consumer a u q1 p x 2 y 1 s 1000 z 120 q 500 a u q2 p x 2 y 1 s 1000 z 120 q 500 Test 4 two producers two consumers a u q1 p x 2 y 2 s 1000 z 60 q 500 a u q2 p x 2 y 2 s 1000 z 60 q 500 ",NULL,"Add for who this story is","well_formed","no_role","high",False
18725,"As a user, I want to know my configuration options are for enabling SSL HTTPS and Basic authentication for administration endpoints, so that I can secure my application.","As a user,","I want to know my configuration options are for enabling SSL HTTPS and Basic authentication for administration endpoints,","so that I can secure my application.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18726,"As a user, I'd like to override the default commit interval so that I can configure commit interval depending on data volume. Note This would apply for all OOTB jobs that has partition support. The property could be part of servers.yml file.","As a user",", I'd like to override the default commit interval","so that I can configure commit interval depending on data volume. Note This would apply for all OOTB jobs that has partition support. The property could be part of servers.yml file.","As a user, I'd like to override the default commit interval so that I can configure commit interval depending on data volume<span class='highlight-text severity-high'>. Note This would apply for all OOTB jobs that has partition support. The property could be part of servers.yml file.</span>","minimal","punctuation","high",False
18726,"As a user, I'd like to override the default commit interval so that I can configure commit interval depending on data volume. Note This would apply for all OOTB jobs that has partition support. The property could be part of servers.yml file.","As a user",", I'd like to override the default commit interval","so that I can configure commit interval depending on data volume. Note This would apply for all OOTB jobs that has partition support. The property could be part of servers.yml file.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18696,"The results from EC2 testing show that once prefetch and message size are set, varying the number of producers or consumers independently does not impact the message rate. The in house testing numbers need another plan to try an understand some discrepancies. Using 500 prefetch, 1000 byte message size run two instances of PerfTest at the same time with each instance referencing a different queue. Vary the number of consumer and publishers. Test 1 one producer one consumer a u q1 p x 1 y 1 s 1000 z 60 q 500 a u q2 p x 1 y 1 s 1000 z 60 q 500 Test 2 one producer two consumers a u q1 p x 1 y 2 s 1000 z 60 q 500 a u q2 p x 1 y 2 s 1000 z 60 q 500 Test 3 two producers one consumer a u q1 p x 2 y 1 s 1000 z 120 q 500 a u q2 p x 2 y 1 s 1000 z 120 q 500 Test 4 two producers two consumers a u q1 p x 2 y 2 s 1000 z 60 q 500 a u q2 p x 2 y 2 s 1000 z 60 q 500 ",NULL,"The results from EC2 testing show that once prefetch and message size are set, varying the number of producers or consumers independently does not impact the message rate. The in house testing numbers need another plan to try an understand some discrepancies. Using 500 prefetch, 1000 byte message size run two instances of PerfTest at the same time with each instance referencing a different queue. Vary the number of consumer and publishers. Test 1 one producer one consumer a u q1 p x 1 y 1 s 1000 z 60 q 500 a u q2 p x 1 y 1 s 1000 z 60 q 500 Test 2 one producer two consumers a u q1 p x 1 y 2 s 1000 z 60 q 500 a u q2 p x 1 y 2 s 1000 z 60 q 500 Test 3 two producers one consumer a u q1 p x 2 y 1 s 1000 z 120 q 500 a u q2 p x 2 y 1 s 1000 z 120 q 500 Test 4 two producers two consumers a u q1 p x 2 y 2 s 1000 z 60 q 500 a u q2 p x 2 y 2 s 1000 z 60 q 500 ",NULL,"The results from EC2 testing show that once prefetch<span class='highlight-text severity-high'> and </span>message size are set, varying the number of producers<span class='highlight-text severity-high'> or </span>consumers independently does not impact the message rate. The in house testing numbers need another plan to try an understand some discrepancies. Using 500 prefetch, 1000 byte message size run two instances of PerfTest at the same time with each instance referencing a different queue. Vary the number of consumer and publishers. Test 1 one producer one consumer a u q1 p x 1 y 1 s 1000 z 60 q 500 a u q2 p x 1 y 1 s 1000 z 60 q 500 Test 2 one producer two consumers a u q1 p x 1 y 2 s 1000 z 60 q 500 a u q2 p x 1 y 2 s 1000 z 60 q 500 Test 3 two producers one consumer a u q1 p x 2 y 1 s 1000 z 120 q 500 a u q2 p x 2 y 1 s 1000 z 120 q 500 Test 4 two producers two consumers a u q1 p x 2 y 2 s 1000 z 60 q 500 a u q2 p x 2 y 2 s 1000 z 60 q 500 ","atomic","conjunctions","high",False
18696,"The results from EC2 testing show that once prefetch and message size are set, varying the number of producers or consumers independently does not impact the message rate. The in house testing numbers need another plan to try an understand some discrepancies. Using 500 prefetch, 1000 byte message size run two instances of PerfTest at the same time with each instance referencing a different queue. Vary the number of consumer and publishers. Test 1 one producer one consumer a u q1 p x 1 y 1 s 1000 z 60 q 500 a u q2 p x 1 y 1 s 1000 z 60 q 500 Test 2 one producer two consumers a u q1 p x 1 y 2 s 1000 z 60 q 500 a u q2 p x 1 y 2 s 1000 z 60 q 500 Test 3 two producers one consumer a u q1 p x 2 y 1 s 1000 z 120 q 500 a u q2 p x 2 y 1 s 1000 z 120 q 500 Test 4 two producers two consumers a u q1 p x 2 y 2 s 1000 z 60 q 500 a u q2 p x 2 y 2 s 1000 z 60 q 500 ",NULL,"The results from EC2 testing show that once prefetch and message size are set, varying the number of producers or consumers independently does not impact the message rate. The in house testing numbers need another plan to try an understand some discrepancies. Using 500 prefetch, 1000 byte message size run two instances of PerfTest at the same time with each instance referencing a different queue. Vary the number of consumer and publishers. Test 1 one producer one consumer a u q1 p x 1 y 1 s 1000 z 60 q 500 a u q2 p x 1 y 1 s 1000 z 60 q 500 Test 2 one producer two consumers a u q1 p x 1 y 2 s 1000 z 60 q 500 a u q2 p x 1 y 2 s 1000 z 60 q 500 Test 3 two producers one consumer a u q1 p x 2 y 1 s 1000 z 120 q 500 a u q2 p x 2 y 1 s 1000 z 120 q 500 Test 4 two producers two consumers a u q1 p x 2 y 2 s 1000 z 60 q 500 a u q2 p x 2 y 2 s 1000 z 60 q 500 ",NULL,"The results from EC2 testing show that once prefetch and message size are set, varying the number of producers or consumers independently does not impact the message rate<span class='highlight-text severity-high'>. The in house testing numbers need another plan to try an understand some discrepancies. Using 500 prefetch, 1000 byte message size run two instances of PerfTest at the same time with each instance referencing a different queue. Vary the number of consumer and publishers. Test 1 one producer one consumer a u q1 p x 1 y 1 s 1000 z 60 q 500 a u q2 p x 1 y 1 s 1000 z 60 q 500 Test 2 one producer two consumers a u q1 p x 1 y 2 s 1000 z 60 q 500 a u q2 p x 1 y 2 s 1000 z 60 q 500 Test 3 two producers one consumer a u q1 p x 2 y 1 s 1000 z 120 q 500 a u q2 p x 2 y 1 s 1000 z 120 q 500 Test 4 two producers two consumers a u q1 p x 2 y 2 s 1000 z 60 q 500 a u q2 p x 2 y 2 s 1000 z 60 q 500 </span>","minimal","punctuation","high",False
18700,"Use a single producer, single consumer, prefetch size 50. Send 1M messages and increase or decrease so that a given test iteration takes about 2 minutes. Vary the message size and measure the msg sec rate and calculate data transfer rate in MB sec. Message Sizes 100 bytes 1000 10,000 100,000 During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",NULL,"Use a single producer, single consumer, prefetch size 50. Send 1M messages and increase or decrease","so that a given test iteration takes about 2 minutes. Vary the message size and measure the msg sec rate and calculate data transfer rate in MB sec. Message Sizes 100 bytes 1000 10,000 100,000 During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.","Add for who this story is","well_formed","no_role","high",False
18700,"Use a single producer, single consumer, prefetch size 50. Send 1M messages and increase or decrease so that a given test iteration takes about 2 minutes. Vary the message size and measure the msg sec rate and calculate data transfer rate in MB sec. Message Sizes 100 bytes 1000 10,000 100,000 During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",NULL,"Use a single producer, single consumer, prefetch size 50. Send 1M messages and increase or decrease","so that a given test iteration takes about 2 minutes. Vary the message size and measure the msg sec rate and calculate data transfer rate in MB sec. Message Sizes 100 bytes 1000 10,000 100,000 During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.","Use a single producer, single consumer, prefetch size 50. Send 1M messages<span class='highlight-text severity-high'> and </span>increase<span class='highlight-text severity-high'> or </span>decrease so that a given test iteration takes about 2 minutes. Vary the message size and measure the msg sec rate and calculate data transfer rate in MB sec. Message Sizes 100 bytes 1000 10,000 100,000 During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.","atomic","conjunctions","high",False
18700,"Use a single producer, single consumer, prefetch size 50. Send 1M messages and increase or decrease so that a given test iteration takes about 2 minutes. Vary the message size and measure the msg sec rate and calculate data transfer rate in MB sec. Message Sizes 100 bytes 1000 10,000 100,000 During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",NULL,"Use a single producer, single consumer, prefetch size 50. Send 1M messages and increase or decrease","so that a given test iteration takes about 2 minutes. Vary the message size and measure the msg sec rate and calculate data transfer rate in MB sec. Message Sizes 100 bytes 1000 10,000 100,000 During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.","Use a single producer, single consumer, prefetch size 50<span class='highlight-text severity-high'>. Send 1M messages and increase or decrease so that a given test iteration takes about 2 minutes. Vary the message size and measure the msg sec rate and calculate data transfer rate in MB sec. Message Sizes 100 bytes 1000 10,000 100,000 During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.</span>","minimal","punctuation","high",False
18700,"Use a single producer, single consumer, prefetch size 50. Send 1M messages and increase or decrease so that a given test iteration takes about 2 minutes. Vary the message size and measure the msg sec rate and calculate data transfer rate in MB sec. Message Sizes 100 bytes 1000 10,000 100,000 During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",NULL,"Use a single producer, single consumer, prefetch size 50. Send 1M messages and increase or decrease","so that a given test iteration takes about 2 minutes. Vary the message size and measure the msg sec rate and calculate data transfer rate in MB sec. Message Sizes 100 bytes 1000 10,000 100,000 During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18701,"As to prepare for 1.1 release, we would like to upgrade to Spring Integration 4.1.0 RC so that we can leverage the new features, enhancement and bug fixes. ",NULL,"As to prepare for 1.1 release, we would like to upgrade to Spring Integration 4.1.0 RC","so that we can leverage the new features, enhancement and bug fixes.","Add for who this story is","well_formed","no_role","high",False
18701,"As to prepare for 1.1 release, we would like to upgrade to Spring Integration 4.1.0 RC so that we can leverage the new features, enhancement and bug fixes. ",NULL,"As to prepare for 1.1 release, we would like to upgrade to Spring Integration 4.1.0 RC","so that we can leverage the new features, enhancement and bug fixes.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18699,"Use a single producer, single consumer, message size of 1000 bytes. Send 1M messages and increase or decrease so that a given test iteration takes about 2 minutes. Vary the prefetch size. Measure the msg sec rate and calculate the data transfer rate in MB sec. Prefetch Sizes 1 10 50 100 10000 During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",NULL,"Use a single producer, single consumer, message size of 1000 bytes. Send 1M messages and increase or decrease","so that a given test iteration takes about 2 minutes. Vary the prefetch size. Measure the msg sec rate and calculate the data transfer rate in MB sec. Prefetch Sizes 1 10 50 100 10000 During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.","Add for who this story is","well_formed","no_role","high",False
18699,"Use a single producer, single consumer, message size of 1000 bytes. Send 1M messages and increase or decrease so that a given test iteration takes about 2 minutes. Vary the prefetch size. Measure the msg sec rate and calculate the data transfer rate in MB sec. Prefetch Sizes 1 10 50 100 10000 During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",NULL,"Use a single producer, single consumer, message size of 1000 bytes. Send 1M messages and increase or decrease","so that a given test iteration takes about 2 minutes. Vary the prefetch size. Measure the msg sec rate and calculate the data transfer rate in MB sec. Prefetch Sizes 1 10 50 100 10000 During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.","Use a single producer, single consumer, message size of 1000 bytes. Send 1M messages<span class='highlight-text severity-high'> and </span>increase<span class='highlight-text severity-high'> or </span>decrease so that a given test iteration takes about 2 minutes. Vary the prefetch size. Measure the msg sec rate and calculate the data transfer rate in MB sec. Prefetch Sizes 1 10 50 100 10000 During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.","atomic","conjunctions","high",False
18699,"Use a single producer, single consumer, message size of 1000 bytes. Send 1M messages and increase or decrease so that a given test iteration takes about 2 minutes. Vary the prefetch size. Measure the msg sec rate and calculate the data transfer rate in MB sec. Prefetch Sizes 1 10 50 100 10000 During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",NULL,"Use a single producer, single consumer, message size of 1000 bytes. Send 1M messages and increase or decrease","so that a given test iteration takes about 2 minutes. Vary the prefetch size. Measure the msg sec rate and calculate the data transfer rate in MB sec. Prefetch Sizes 1 10 50 100 10000 During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.","Use a single producer, single consumer, message size of 1000 bytes<span class='highlight-text severity-high'>. Send 1M messages and increase or decrease so that a given test iteration takes about 2 minutes. Vary the prefetch size. Measure the msg sec rate and calculate the data transfer rate in MB sec. Prefetch Sizes 1 10 50 100 10000 During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.</span>","minimal","punctuation","high",False
18699,"Use a single producer, single consumer, message size of 1000 bytes. Send 1M messages and increase or decrease so that a given test iteration takes about 2 minutes. Vary the prefetch size. Measure the msg sec rate and calculate the data transfer rate in MB sec. Prefetch Sizes 1 10 50 100 10000 During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",NULL,"Use a single producer, single consumer, message size of 1000 bytes. Send 1M messages and increase or decrease","so that a given test iteration takes about 2 minutes. Vary the prefetch size. Measure the msg sec rate and calculate the data transfer rate in MB sec. Prefetch Sizes 1 10 50 100 10000 During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18702,"As to prepare for 1.1 release, we would like to upgrade to Spring Boot 1.2.0 RC1 depends on Spring 4.1.2 so that we can leverage the new features, enhancement and bug fixes. Spring Boot Milestones https github.com spring projects spring boot milestones ",NULL,"As to prepare for 1.1 release, we would like to upgrade to Spring Boot 1.2.0 RC1 depends on Spring 4.1.2","so that we can leverage the new features, enhancement and bug fixes. Spring Boot Milestones https github.com spring projects spring boot milestones","Add for who this story is","well_formed","no_role","high",False
18702,"As to prepare for 1.1 release, we would like to upgrade to Spring Boot 1.2.0 RC1 depends on Spring 4.1.2 so that we can leverage the new features, enhancement and bug fixes. Spring Boot Milestones https github.com spring projects spring boot milestones ",NULL,"As to prepare for 1.1 release, we would like to upgrade to Spring Boot 1.2.0 RC1 depends on Spring 4.1.2","so that we can leverage the new features, enhancement and bug fixes. Spring Boot Milestones https github.com spring projects spring boot milestones","As to prepare for 1<span class='highlight-text severity-high'>.1 release, we would like to upgrade to Spring Boot 1.2.0 RC1 depends on Spring 4.1.2 so that we can leverage the new features, enhancement and bug fixes. Spring Boot Milestones https github.com spring projects spring boot milestones </span>","minimal","punctuation","high",False
18702,"As to prepare for 1.1 release, we would like to upgrade to Spring Boot 1.2.0 RC1 depends on Spring 4.1.2 so that we can leverage the new features, enhancement and bug fixes. Spring Boot Milestones https github.com spring projects spring boot milestones ",NULL,"As to prepare for 1.1 release, we would like to upgrade to Spring Boot 1.2.0 RC1 depends on Spring 4.1.2","so that we can leverage the new features, enhancement and bug fixes. Spring Boot Milestones https github.com spring projects spring boot milestones","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18724,"Once the container s management server is secured, the admin server needs to know which REST template to use to get the message rates from the deployed modules inside the containers.",NULL,"Once the container s management server is secured, the admin server needs to know which REST template to use to get the message rates from the deployed modules inside the containers.",NULL,"Add for who this story is","well_formed","no_role","high",False
18715,"As a user, I'd like to have Spring Core upgraded to 4.1.1 milestone so that I can benefit from performance improvements associated with compiled SpEL and other enhancements. ","As a user",", I'd like to have Spring Core upgraded to 4.1.1 milestone","so that I can benefit from performance improvements associated with compiled SpEL and other enhancements.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18716,"To enrich acceptance test coverage, I'd like to have stress test scenario that includes ingesting data from HTTP and then writing it to Log sink.",NULL,"To enrich acceptance test coverage, I'd like to have stress test scenario that includes ingesting data from HTTP and then writing it to Log sink.",NULL,"Add for who this story is","well_formed","no_role","high",False
18716,"To enrich acceptance test coverage, I'd like to have stress test scenario that includes ingesting data from HTTP and then writing it to Log sink.",NULL,"To enrich acceptance test coverage, I'd like to have stress test scenario that includes ingesting data from HTTP and then writing it to Log sink.",NULL,"To enrich acceptance test coverage, I'd like to have stress test scenario that includes ingesting data from HTTP<span class='highlight-text severity-high'> and </span>then writing it to Log sink.","atomic","conjunctions","high",False
18723,"See https build.spring.io browse XD SCRIPTS 723",NULL,"See https build.spring.io browse XD SCRIPTS 723",NULL,"Add for who this story is","well_formed","no_role","high",False
18722,"As a user, I'd like to have a Python processor so that I can efficiently perform data computations and statistical analysis. Investigate the right approach native or via stdin stdout that fits Spring XD model. Integrate Java and Python https wiki.python.org moin IntegratingPythonWithOtherLanguages Java ","As a user",", I'd like to have a Python processor","so that I can efficiently perform data computations and statistical analysis. Investigate the right approach native or via stdin stdout that fits Spring XD model. Integrate Java and Python https wiki.python.org moin IntegratingPythonWithOtherLanguages Java","As a user, I'd like to have a Python processor so that I can efficiently perform data computations and statistical analysis<span class='highlight-text severity-high'>. Investigate the right approach native or via stdin stdout that fits Spring XD model. Integrate Java and Python https wiki.python.org moin IntegratingPythonWithOtherLanguages Java </span>","minimal","punctuation","high",False
18722,"As a user, I'd like to have a Python processor so that I can efficiently perform data computations and statistical analysis. Investigate the right approach native or via stdin stdout that fits Spring XD model. Integrate Java and Python https wiki.python.org moin IntegratingPythonWithOtherLanguages Java ","As a user",", I'd like to have a Python processor","so that I can efficiently perform data computations and statistical analysis. Investigate the right approach native or via stdin stdout that fits Spring XD model. Integrate Java and Python https wiki.python.org moin IntegratingPythonWithOtherLanguages Java","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18720,"To enrich acceptance test, I'd like to have basic coverage to evaluate Gemfire use cases. An example would be to ingest data from HTTP source and write it to Gemfire server.",NULL,"To enrich acceptance test, I'd like to have basic coverage to evaluate Gemfire use cases. An example would be to ingest data from HTTP source and write it to Gemfire server.",NULL,"Add for who this story is","well_formed","no_role","high",False
18720,"To enrich acceptance test, I'd like to have basic coverage to evaluate Gemfire use cases. An example would be to ingest data from HTTP source and write it to Gemfire server.",NULL,"To enrich acceptance test, I'd like to have basic coverage to evaluate Gemfire use cases. An example would be to ingest data from HTTP source and write it to Gemfire server.",NULL,"To enrich acceptance test, I'd like to have basic coverage to evaluate Gemfire use cases. An example would be to ingest data from HTTP source<span class='highlight-text severity-high'> and </span>write it to Gemfire server.","atomic","conjunctions","high",False
18720,"To enrich acceptance test, I'd like to have basic coverage to evaluate Gemfire use cases. An example would be to ingest data from HTTP source and write it to Gemfire server.",NULL,"To enrich acceptance test, I'd like to have basic coverage to evaluate Gemfire use cases. An example would be to ingest data from HTTP source and write it to Gemfire server.",NULL,"To enrich acceptance test, I'd like to have basic coverage to evaluate Gemfire use cases<span class='highlight-text severity-high'>. An example would be to ingest data from HTTP source and write it to Gemfire server.</span>","minimal","punctuation","high",False
18721,"To enrich the acceptance test, I'd like to evaluate JSON object to extract Good and Bad instead of just relying on a basic filter test to assert the payload content. ",NULL,"To enrich the acceptance test, I'd like to evaluate JSON object to extract Good and Bad instead of just relying on a basic filter test to assert the payload content. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18721,"To enrich the acceptance test, I'd like to evaluate JSON object to extract Good and Bad instead of just relying on a basic filter test to assert the payload content. ",NULL,"To enrich the acceptance test, I'd like to evaluate JSON object to extract Good and Bad instead of just relying on a basic filter test to assert the payload content. ",NULL,"To enrich the acceptance test, I'd like to evaluate JSON object to extract Good<span class='highlight-text severity-high'> and </span>Bad instead of just relying on a basic filter test to assert the payload content. ","atomic","conjunctions","high",False
18718,"To enrich acceptance test, I'd like to lazy initialize DB for JDBC Sink acceptance tests. Check to see if it is already initialized and decide. ",NULL,"To enrich acceptance test, I'd like to lazy initialize DB for JDBC Sink acceptance tests. Check to see if it is already initialized and decide. ",NULL,"To enrich acceptance test, I'd like to lazy initialize DB for JDBC Sink acceptance tests. Check to see if it is already initialized<span class='highlight-text severity-high'> and </span>decide. ","atomic","conjunctions","high",False
18718,"To enrich acceptance test, I'd like to lazy initialize DB for JDBC Sink acceptance tests. Check to see if it is already initialized and decide. ",NULL,"To enrich acceptance test, I'd like to lazy initialize DB for JDBC Sink acceptance tests. Check to see if it is already initialized and decide. ",NULL,"To enrich acceptance test, I'd like to lazy initialize DB for JDBC Sink acceptance tests<span class='highlight-text severity-high'>. Check to see if it is already initialized and decide. </span>","minimal","punctuation","high",False
18731,"As a user, I need a sandbox Docker Image so that I can get started to experiment XD deployment with the following setup Ubuntu OS Full XD Jar Java 7.x Redis RabbitMQ","As a user",", I need a sandbox Docker Image","so that I can get started to experiment XD deployment with the following setup Ubuntu OS Full XD Jar Java 7.x Redis RabbitMQ","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18719,"To enrich acceptance test, I'd like to add coverage to JDBC sink by including driverclass and url options.",NULL,"To enrich acceptance test, I'd like to add coverage to JDBC sink by including driverclass and url options.",NULL,"Add for who this story is","well_formed","no_role","high",False
18717,"To enrich acceptance tests, I'd like to have test coverage to evaluate FieldValueCounts and AggregateCounts for a given scenario.",NULL,"To enrich acceptance tests, I'd like to have test coverage to evaluate FieldValueCounts and AggregateCounts for a given scenario.",NULL,"Add for who this story is","well_formed","no_role","high",False
18717,"To enrich acceptance tests, I'd like to have test coverage to evaluate FieldValueCounts and AggregateCounts for a given scenario.",NULL,"To enrich acceptance tests, I'd like to have test coverage to evaluate FieldValueCounts and AggregateCounts for a given scenario.",NULL,"To enrich acceptance tests, I'd like to have test coverage to evaluate FieldValueCounts<span class='highlight-text severity-high'> and </span>AggregateCounts for a given scenario.","atomic","conjunctions","high",False
18737,"As a user, I'd like to have the option to provide single user security configurations so that I can override them as needed. Reference Spring Boot Security http docs.spring.io spring boot docs 1.1.x SNAPSHOT reference html boot features security.html Scope Configurations can be provided through servers.yml file.","As a user",", I'd like to have the option to provide single user security configurations","so that I can override them as needed. Reference Spring Boot Security http docs.spring.io spring boot docs 1.1.x SNAPSHOT reference html boot features security.html Scope Configurations can be provided through servers.yml file.","As a user, I'd like to have the option to provide single user security configurations so that I can override them as needed<span class='highlight-text severity-high'>. Reference Spring Boot Security http docs.spring.io spring boot docs 1.1.x SNAPSHOT reference html boot features security.html Scope Configurations can be provided through servers.yml file.</span>","minimal","punctuation","high",False
18737,"As a user, I'd like to have the option to provide single user security configurations so that I can override them as needed. Reference Spring Boot Security http docs.spring.io spring boot docs 1.1.x SNAPSHOT reference html boot features security.html Scope Configurations can be provided through servers.yml file.","As a user",", I'd like to have the option to provide single user security configurations","so that I can override them as needed. Reference Spring Boot Security http docs.spring.io spring boot docs 1.1.x SNAPSHOT reference html boot features security.html Scope Configurations can be provided through servers.yml file.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18738,"As a user, I'd like to have the option of Basic Auth so that I m challenged to provide user name and password when making a request. Technical Implementation This functionality is provided in Spring Boot 1.1.x, it should be a matter of adding the spring boot security starter dependency to the spring xd dirt project. It will be controlled using the spring boot property server.basic.enabled true false. Our default in application.yml for this property should be false. ","As a user",", I'd like to have the option of Basic Auth","so that I m challenged to provide user name and password when making a request. Technical Implementation This functionality is provided in Spring Boot 1.1.x, it should be a matter of adding the spring boot security starter dependency to the spring xd dirt project. It will be controlled using the spring boot property server.basic.enabled true false. Our default in application.yml for this property should be false.","As a user, I'd like to have the option of Basic Auth so that I m challenged to provide user name and password when making a request<span class='highlight-text severity-high'>. Technical Implementation This functionality is provided in Spring Boot 1.1.x, it should be a matter of adding the spring boot security starter dependency to the spring xd dirt project. It will be controlled using the spring boot property server.basic.enabled true false. Our default in application.yml for this property should be false. </span>","minimal","punctuation","high",False
18738,"As a user, I'd like to have the option of Basic Auth so that I m challenged to provide user name and password when making a request. Technical Implementation This functionality is provided in Spring Boot 1.1.x, it should be a matter of adding the spring boot security starter dependency to the spring xd dirt project. It will be controlled using the spring boot property server.basic.enabled true false. Our default in application.yml for this property should be false. ","As a user",", I'd like to have the option of Basic Auth","so that I m challenged to provide user name and password when making a request. Technical Implementation This functionality is provided in Spring Boot 1.1.x, it should be a matter of adding the spring boot security starter dependency to the spring xd dirt project. It will be controlled using the spring boot property server.basic.enabled true false. Our default in application.yml for this property should be false.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18733,"As a user, I'd like to have a REST API to point and push an archive that includes custom module definitions and configurations so that I'don t have to manually move and set it up. Scope of this spike Assess customer requirement, brainstorm, and document options Socialize with the team to collect feedback Identify phases Create new stories","As a user",", I'd like to have a REST API to point and push an archive that includes custom module definitions and configurations","so that I'don t have to manually move and set it up. Scope of this spike Assess customer requirement, brainstorm, and document options Socialize with the team to collect feedback Identify phases Create new stories","As a user, I'd like to have a REST API to point<span class='highlight-text severity-high'> and </span>push an archive that includes custom module definitions and configurations so that I'don t have to manually move and set it up. Scope of this spike Assess customer requirement, brainstorm, and document options Socialize with the team to collect feedback Identify phases Create new stories","atomic","conjunctions","high",False
18733,"As a user, I'd like to have a REST API to point and push an archive that includes custom module definitions and configurations so that I'don t have to manually move and set it up. Scope of this spike Assess customer requirement, brainstorm, and document options Socialize with the team to collect feedback Identify phases Create new stories","As a user",", I'd like to have a REST API to point and push an archive that includes custom module definitions and configurations","so that I'don t have to manually move and set it up. Scope of this spike Assess customer requirement, brainstorm, and document options Socialize with the team to collect feedback Identify phases Create new stories","As a user, I'd like to have a REST API to point and push an archive that includes custom module definitions and configurations so that I'don t have to manually move and set it up<span class='highlight-text severity-high'>. Scope of this spike Assess customer requirement, brainstorm, and document options Socialize with the team to collect feedback Identify phases Create new stories</span>","minimal","punctuation","high",False
18733,"As a user, I'd like to have a REST API to point and push an archive that includes custom module definitions and configurations so that I'don t have to manually move and set it up. Scope of this spike Assess customer requirement, brainstorm, and document options Socialize with the team to collect feedback Identify phases Create new stories","As a user",", I'd like to have a REST API to point and push an archive that includes custom module definitions and configurations","so that I'don t have to manually move and set it up. Scope of this spike Assess customer requirement, brainstorm, and document options Socialize with the team to collect feedback Identify phases Create new stories","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18734,"As a user, I'd like to have latest Spring Boot RELEASE pulled as a dependency so that I can inherit and implement the OOTB security features.","As a user",", I'd like to have latest Spring Boot RELEASE pulled as a dependency","so that I can inherit and implement the OOTB security features.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18735,"As a user, I'd like to have latest Spring Boot snapshot pulled as a dependency so that I can inherit and implement the OOTB security features.","As a user",", I'd like to have latest Spring Boot snapshot pulled as a dependency","so that I can inherit and implement the OOTB security features.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18736,"As a user, I want to be able to provide security credentials to the XD Shell so that I can interact with an xd admin server that is secured via basic auth Technical implementation Add password and username to the admin config command. ","As a user,","I want to be able to provide security credentials to the XD Shell","so that I can interact with an xd admin server that is secured via basic auth Technical implementation Add password and username to the admin config command.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18746,"As a user, I'd like to have the option to enable HTTPS so that I can access XD s Admin server endpoints https github.com spring projects spring xd wiki REST API xd resources over secured communication. Technical Implementation This functionality is available in Spring Boot 1.2.0 M1 and has been backported into the 1.1.x branch to be released under Spring 1.1.7. We can test against 1.1.7 SNAPSHOT. Working through the way to update the build file to pick up a new version of boot is a bit tricky ","As a user",", I'd like to have the option to enable HTTPS","so that I can access XD s Admin server endpoints https github.com spring projects spring xd wiki REST API xd resources over secured communication. Technical Implementation This functionality is available in Spring Boot 1.2.0 M1 and has been backported into the 1.1.x branch to be released under Spring 1.1.7. We can test against 1.1.7 SNAPSHOT. Working through the way to update the build file to pick up a new version of boot is a bit tricky","As a user, I'd like to have the option to enable HTTPS so that I can access XD s Admin server endpoints https github<span class='highlight-text severity-high'>.com spring projects spring xd wiki REST API xd resources over secured communication. Technical Implementation This functionality is available in Spring Boot 1.2.0 M1 and has been backported into the 1.1.x branch to be released under Spring 1.1.7. We can test against 1.1.7 SNAPSHOT. Working through the way to update the build file to pick up a new version of boot is a bit tricky </span>","minimal","punctuation","high",False
18746,"As a user, I'd like to have the option to enable HTTPS so that I can access XD s Admin server endpoints https github.com spring projects spring xd wiki REST API xd resources over secured communication. Technical Implementation This functionality is available in Spring Boot 1.2.0 M1 and has been backported into the 1.1.x branch to be released under Spring 1.1.7. We can test against 1.1.7 SNAPSHOT. Working through the way to update the build file to pick up a new version of boot is a bit tricky ","As a user",", I'd like to have the option to enable HTTPS","so that I can access XD s Admin server endpoints https github.com spring projects spring xd wiki REST API xd resources over secured communication. Technical Implementation This functionality is available in Spring Boot 1.2.0 M1 and has been backported into the 1.1.x branch to be released under Spring 1.1.7. We can test against 1.1.7 SNAPSHOT. Working through the way to update the build file to pick up a new version of boot is a bit tricky","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18747,"Create processor and sink modules that can execute a shell command using stdin and stdout to stream data.",NULL,"Create processor and sink modules that can execute a shell command using stdin and stdout to stream data.",NULL,"Add for who this story is","well_formed","no_role","high",False
18747,"Create processor and sink modules that can execute a shell command using stdin and stdout to stream data.",NULL,"Create processor and sink modules that can execute a shell command using stdin and stdout to stream data.",NULL,"Create processor<span class='highlight-text severity-high'> and </span>sink modules that can execute a shell command using stdin and stdout to stream data.","atomic","conjunctions","high",False
18750,"Currently, the aggregate counter only adds 1 to the individual values, even though support is there to add any increment. This ticket is about surfacing a SpEL expression on the message to choose the increment",NULL,"Currently, the aggregate counter only adds 1 to the individual values, even though support is there to add any increment. This ticket is about surfacing a SpEL expression on the message to choose the increment",NULL,"Add for who this story is","well_formed","no_role","high",False
18750,"Currently, the aggregate counter only adds 1 to the individual values, even though support is there to add any increment. This ticket is about surfacing a SpEL expression on the message to choose the increment",NULL,"Currently, the aggregate counter only adds 1 to the individual values, even though support is there to add any increment. This ticket is about surfacing a SpEL expression on the message to choose the increment",NULL,"Currently, the aggregate counter only adds 1 to the individual values, even though support is there to add any increment<span class='highlight-text severity-high'>. This ticket is about surfacing a SpEL expression on the message to choose the increment</span>","minimal","punctuation","high",False
18751,"Currently, custom SI property accessors are registered by a plugin org.springframework.xd.dirt.plugins.SpelPropertyAccessorPlugin and are not visible by the bus. I believe they should be. This may just be a matter of moving them around.",NULL,"Currently, custom SI property accessors are registered by a plugin org.springframework.xd.dirt.plugins.SpelPropertyAccessorPlugin and are not visible by the bus. I believe they should be. This may just be a matter of moving them around.",NULL,"Add for who this story is","well_formed","no_role","high",False
18751,"Currently, custom SI property accessors are registered by a plugin org.springframework.xd.dirt.plugins.SpelPropertyAccessorPlugin and are not visible by the bus. I believe they should be. This may just be a matter of moving them around.",NULL,"Currently, custom SI property accessors are registered by a plugin org.springframework.xd.dirt.plugins.SpelPropertyAccessorPlugin and are not visible by the bus. I believe they should be. This may just be a matter of moving them around.",NULL,"Currently, custom SI property accessors are registered by a plugin org.springframework.xd.dirt.plugins.SpelPropertyAccessorPlugin<span class='highlight-text severity-high'> and </span>are not visible by the bus. I believe they should be. This may just be a matter of moving them around.","atomic","conjunctions","high",False
18751,"Currently, custom SI property accessors are registered by a plugin org.springframework.xd.dirt.plugins.SpelPropertyAccessorPlugin and are not visible by the bus. I believe they should be. This may just be a matter of moving them around.",NULL,"Currently, custom SI property accessors are registered by a plugin org.springframework.xd.dirt.plugins.SpelPropertyAccessorPlugin and are not visible by the bus. I believe they should be. This may just be a matter of moving them around.",NULL,"Currently, custom SI property accessors are registered by a plugin org<span class='highlight-text severity-high'>.springframework.xd.dirt.plugins.SpelPropertyAccessorPlugin and are not visible by the bus. I believe they should be. This may just be a matter of moving them around.</span>","minimal","punctuation","high",False
18749,"Current Behavior Currently XD EC2 downloads an XD zip file from the location specified by the xd dist url after verifying that the file is accessible.. ",NULL,"Current Behavior Currently XD EC2 downloads an XD zip file from the location specified by the xd dist url after verifying that the file is accessible.. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18754,"Kafka lends itself well as a message bus, and kafka partitioning maps to MB partitioning. Implement KafkaMessageBus and supporting classes and UT IT.",NULL,"Kafka lends itself well as a message bus, and kafka partitioning maps to MB partitioning. Implement KafkaMessageBus and supporting classes and UT IT.",NULL,"Add for who this story is","well_formed","no_role","high",False
18754,"Kafka lends itself well as a message bus, and kafka partitioning maps to MB partitioning. Implement KafkaMessageBus and supporting classes and UT IT.",NULL,"Kafka lends itself well as a message bus, and kafka partitioning maps to MB partitioning. Implement KafkaMessageBus and supporting classes and UT IT.",NULL,"Kafka lends itself well as a message bus,<span class='highlight-text severity-high'> and </span>kafka partitioning maps to MB partitioning. Implement KafkaMessageBus and supporting classes and UT IT.","atomic","conjunctions","high",False
18754,"Kafka lends itself well as a message bus, and kafka partitioning maps to MB partitioning. Implement KafkaMessageBus and supporting classes and UT IT.",NULL,"Kafka lends itself well as a message bus, and kafka partitioning maps to MB partitioning. Implement KafkaMessageBus and supporting classes and UT IT.",NULL,"Kafka lends itself well as a message bus, and kafka partitioning maps to MB partitioning<span class='highlight-text severity-high'>. Implement KafkaMessageBus and supporting classes and UT IT.</span>","minimal","punctuation","high",False
18753,"XD Module configured for component scanning classes or methods? annotated with Source, consider Processor, and Sink as well and simply provide the POJOs and dependent jars in the module lib directory. Custom processor is fairly straightforward currently, but still requires an XML module definition to wire up the POJO as a service activator or transformer to the input and output channel. A service activator works for a POJO backed sink. Writing a source that is not backed by an existing inbound channel adapter is a bit more involved and requires more than basic familiarity with SI. It should be possible for XD to automatically create a polling source by wiring a Java method to an inbound adapter configured with a poller. Ideally, we would require no XML, even to enable component scanning this will require some changes to the module registry module initializer.",NULL,"XD Module configured for component scanning classes or methods? annotated with Source, consider Processor, and Sink as well and simply provide the POJOs and dependent jars in the module lib directory. Custom processor is fairly straightforward currently, but still requires an XML module definition to wire up the POJO as a service activator or transformer to the input and output channel. A service activator works for a POJO backed sink. Writing a source that is not backed by an existing inbound channel adapter is a bit more involved and requires more than basic familiarity with SI. It should be possible for XD to automatically create a polling source by wiring a Java method to an inbound adapter configured with a poller. Ideally, we would require no XML, even to enable component scanning this will require some changes to the module registry module initializer.",NULL,"Add for who this story is","well_formed","no_role","high",False
18779,"If the XD admin server lets tomcat chooses random http port by setting PORT or server.port to 0 , the XD config logging and admin server context id still points to port zero as these are set before tomcat assigns the available random port. We also need to persist the admin servers ports into ZK so that this repo can be accessed by the client.",NULL,"If the XD admin server lets tomcat chooses random http port by setting PORT or server.port to 0 , the XD config logging and admin server context id still points to port zero as these are set before tomcat assigns the available random port. We also need to persist the admin servers ports into ZK","so that this repo can be accessed by the client.","If the XD admin server lets tomcat chooses random http port by setting PORT<span class='highlight-text severity-high'> or </span>server.port to 0 , the XD config logging<span class='highlight-text severity-high'> and </span>admin server context id still points to port zero as these are set before tomcat assigns the available random port. We also need to persist the admin servers ports into ZK so that this repo can be accessed by the client.","atomic","conjunctions","high",False
19345,"Once spring batch admin 1.3.0.M1 is available, update the build to use it. Likely to be Sept 7 or 9",NULL,NULL,NULL,"Once spring batch admin 1<span class='highlight-text severity-high'>.3.0.M1 is available, update the build to use it. Likely to be Sept 7 or 9</span>","minimal","punctuation","high",False
18753,"XD Module configured for component scanning classes or methods? annotated with Source, consider Processor, and Sink as well and simply provide the POJOs and dependent jars in the module lib directory. Custom processor is fairly straightforward currently, but still requires an XML module definition to wire up the POJO as a service activator or transformer to the input and output channel. A service activator works for a POJO backed sink. Writing a source that is not backed by an existing inbound channel adapter is a bit more involved and requires more than basic familiarity with SI. It should be possible for XD to automatically create a polling source by wiring a Java method to an inbound adapter configured with a poller. Ideally, we would require no XML, even to enable component scanning this will require some changes to the module registry module initializer.",NULL,"XD Module configured for component scanning classes or methods? annotated with Source, consider Processor, and Sink as well and simply provide the POJOs and dependent jars in the module lib directory. Custom processor is fairly straightforward currently, but still requires an XML module definition to wire up the POJO as a service activator or transformer to the input and output channel. A service activator works for a POJO backed sink. Writing a source that is not backed by an existing inbound channel adapter is a bit more involved and requires more than basic familiarity with SI. It should be possible for XD to automatically create a polling source by wiring a Java method to an inbound adapter configured with a poller. Ideally, we would require no XML, even to enable component scanning this will require some changes to the module registry module initializer.",NULL,"XD Module configured for component scanning classes<span class='highlight-text severity-high'> or </span>methods? annotated with Source, consider Processor,<span class='highlight-text severity-high'> and </span>Sink as well and simply provide the POJOs and dependent jars in the module lib directory. Custom processor is fairly straightforward currently, but still requires an XML module definition to wire up the POJO as a service activator or transformer to the input and output channel. A service activator works for a POJO backed sink. Writing a source that is not backed by an existing inbound channel adapter is a bit more involved and requires more than basic familiarity with SI. It should be possible for XD to automatically create a polling source by wiring a Java method to an inbound adapter configured with a poller. Ideally, we would require no XML, even to enable component scanning this will require some changes to the module registry module initializer.","atomic","conjunctions","high",False
18753,"XD Module configured for component scanning classes or methods? annotated with Source, consider Processor, and Sink as well and simply provide the POJOs and dependent jars in the module lib directory. Custom processor is fairly straightforward currently, but still requires an XML module definition to wire up the POJO as a service activator or transformer to the input and output channel. A service activator works for a POJO backed sink. Writing a source that is not backed by an existing inbound channel adapter is a bit more involved and requires more than basic familiarity with SI. It should be possible for XD to automatically create a polling source by wiring a Java method to an inbound adapter configured with a poller. Ideally, we would require no XML, even to enable component scanning this will require some changes to the module registry module initializer.",NULL,"XD Module configured for component scanning classes or methods? annotated with Source, consider Processor, and Sink as well and simply provide the POJOs and dependent jars in the module lib directory. Custom processor is fairly straightforward currently, but still requires an XML module definition to wire up the POJO as a service activator or transformer to the input and output channel. A service activator works for a POJO backed sink. Writing a source that is not backed by an existing inbound channel adapter is a bit more involved and requires more than basic familiarity with SI. It should be possible for XD to automatically create a polling source by wiring a Java method to an inbound adapter configured with a poller. Ideally, we would require no XML, even to enable component scanning this will require some changes to the module registry module initializer.",NULL,"XD Module configured for component scanning classes or methods<span class='highlight-text severity-high'>? annotated with Source, consider Processor, and Sink as well and simply provide the POJOs and dependent jars in the module lib directory. Custom processor is fairly straightforward currently, but still requires an XML module definition to wire up the POJO as a service activator or transformer to the input and output channel. A service activator works for a POJO backed sink. Writing a source that is not backed by an existing inbound channel adapter is a bit more involved and requires more than basic familiarity with SI. It should be possible for XD to automatically create a polling source by wiring a Java method to an inbound adapter configured with a poller. Ideally, we would require no XML, even to enable component scanning this will require some changes to the module registry module initializer.</span>","minimal","punctuation","high",False
18758,"We need a visual representation of the XD cluster with runtime container and deployed modules.",NULL,"We need a visual representation of the XD cluster with runtime container and deployed modules.",NULL,"Add for who this story is","well_formed","no_role","high",False
18758,"We need a visual representation of the XD cluster with runtime container and deployed modules.",NULL,"We need a visual representation of the XD cluster with runtime container and deployed modules.",NULL,"We need a visual representation of the XD cluster with runtime container<span class='highlight-text severity-high'> and </span>deployed modules.","atomic","conjunctions","high",False
18756,"For a given stream job, we need a visual representation of the stream job with any deployed modules.",NULL,"For a given stream job, we need a visual representation of the stream job with any deployed modules.",NULL,"Add for who this story is","well_formed","no_role","high",False
18748,"Currently the application allows AWS select which zone in the region to create an instance.",NULL,"Currently the application allows AWS select which zone in the region to create an instance.",NULL,"Add for who this story is","well_formed","no_role","high",False
18757,"Admin UI currently allows job to be deployed with deployment properties, we need similar way to deploy stream with the deployment properties module count, container matching criteria .",NULL,"Admin UI currently allows job to be deployed with deployment properties, we need similar way to deploy stream with the deployment properties module count, container matching criteria .",NULL,"Add for who this story is","well_formed","no_role","high",False
18760,"Design Spike Investigate various approaches to bootstrap custom modules. Can Spring Boot be leveraged? Starter POM?",NULL,"Design Spike Investigate various approaches to bootstrap custom modules. Can Spring Boot be leveraged? Starter POM?",NULL,"Add for who this story is","well_formed","no_role","high",False
18760,"Design Spike Investigate various approaches to bootstrap custom modules. Can Spring Boot be leveraged? Starter POM?",NULL,"Design Spike Investigate various approaches to bootstrap custom modules. Can Spring Boot be leveraged? Starter POM?",NULL,"Design Spike Investigate various approaches to bootstrap custom modules<span class='highlight-text severity-high'>. Can Spring Boot be leveraged? Starter POM?</span>","minimal","punctuation","high",False
18763,"h1. Summary User wants the ability to deploy an ec2 cluster where the admin containers use a pre existing ZK ensemble, Rabbit and redis instance that are deployed on different machines. ",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
18763,"h1. Summary User wants the ability to deploy an ec2 cluster where the admin containers use a pre existing ZK ensemble, Rabbit and redis instance that are deployed on different machines. ",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
18763,"h1. Summary User wants the ability to deploy an ec2 cluster where the admin containers use a pre existing ZK ensemble, Rabbit and redis instance that are deployed on different machines. ",NULL,NULL,NULL,"h1<span class='highlight-text severity-high'>. Summary User wants the ability to deploy an ec2 cluster where the admin containers use a pre existing ZK ensemble, Rabbit and redis instance that are deployed on different machines. </span>","minimal","punctuation","high",False
18761,"As a user, I'd like to have guidance to create custom modules so that I can align the development practices with recommended approach. 11 20 Update Scope of this task is to create an example to demonstrate and document the capability.","As a user",", I'd like to have guidance to create custom modules","so that I can align the development practices with recommended approach. 11 20 Update Scope of this task is to create an example to demonstrate and document the capability.","As a user, I'd like to have guidance to create custom modules so that I can align the development practices with recommended approach<span class='highlight-text severity-high'>. 11 20 Update Scope of this task is to create an example to demonstrate and document the capability.</span>","minimal","punctuation","high",False
18761,"As a user, I'd like to have guidance to create custom modules so that I can align the development practices with recommended approach. 11 20 Update Scope of this task is to create an example to demonstrate and document the capability.","As a user",", I'd like to have guidance to create custom modules","so that I can align the development practices with recommended approach. 11 20 Update Scope of this task is to create an example to demonstrate and document the capability.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18764,"Provide for retry and or dead lettering for the rabbit source similar to the rabbit message bus .",NULL,"Provide for retry and or dead lettering for the rabbit source similar to the rabbit message bus .",NULL,"Add for who this story is","well_formed","no_role","high",False
18764,"Provide for retry and or dead lettering for the rabbit source similar to the rabbit message bus .",NULL,"Provide for retry and or dead lettering for the rabbit source similar to the rabbit message bus .",NULL,"Provide for retry<span class='highlight-text severity-high'> and </span>span class='highlight-text severity-high'> or </span>dead lettering for the rabbit source similar to the rabbit message bus .","atomic","conjunctions","high",False
18765,"The http source does not provide debug logging to see information such as http headers and requests, in particular if a non OK response is returned. I updated the log4j config for org.jboss.netty but it had no effect. I suspect this is due to the need to configure the netty logging system via InternalLoggerFactory.setDefaultFactory new Log4JLoggerFactory . ",NULL,"The http source does not provide debug logging to see information such as http headers and requests, in particular if a non OK response is returned. I updated the log4j config for org.jboss.netty but it had no effect. I suspect this is due to the need to configure the netty logging system via InternalLoggerFactory.setDefaultFactory new Log4JLoggerFactory . ",NULL,"Add for who this story is","well_formed","no_role","high",False
18765,"The http source does not provide debug logging to see information such as http headers and requests, in particular if a non OK response is returned. I updated the log4j config for org.jboss.netty but it had no effect. I suspect this is due to the need to configure the netty logging system via InternalLoggerFactory.setDefaultFactory new Log4JLoggerFactory . ",NULL,"The http source does not provide debug logging to see information such as http headers and requests, in particular if a non OK response is returned. I updated the log4j config for org.jboss.netty but it had no effect. I suspect this is due to the need to configure the netty logging system via InternalLoggerFactory.setDefaultFactory new Log4JLoggerFactory . ",NULL,"The http source does not provide debug logging to see information such as http headers<span class='highlight-text severity-high'> and </span>requests, in particular if a non OK response is returned. I updated the log4j config for org.jboss.netty but it had no effect. I suspect this is due to the need to configure the netty logging system via InternalLoggerFactory.setDefaultFactory new Log4JLoggerFactory . ","atomic","conjunctions","high",False
18765,"The http source does not provide debug logging to see information such as http headers and requests, in particular if a non OK response is returned. I updated the log4j config for org.jboss.netty but it had no effect. I suspect this is due to the need to configure the netty logging system via InternalLoggerFactory.setDefaultFactory new Log4JLoggerFactory . ",NULL,"The http source does not provide debug logging to see information such as http headers and requests, in particular if a non OK response is returned. I updated the log4j config for org.jboss.netty but it had no effect. I suspect this is due to the need to configure the netty logging system via InternalLoggerFactory.setDefaultFactory new Log4JLoggerFactory . ",NULL,"The http source does not provide debug logging to see information such as http headers and requests, in particular if a non OK response is returned<span class='highlight-text severity-high'>. I updated the log4j config for org.jboss.netty but it had no effect. I suspect this is due to the need to configure the netty logging system via InternalLoggerFactory.setDefaultFactory new Log4JLoggerFactory . </span>","minimal","punctuation","high",False
18766,"Tests that use verifySendCounts to validate whether data was sent to all the modules in a stream occasionally fail. This is because, sometimes it takes 2 or more sends to get the data transmitted between modules. With the current test structure this is considered a failure. Is this the correct behavior?",NULL,"Tests that use verifySendCounts to validate whether data was sent to all the modules in a stream occasionally fail. This is because, sometimes it takes 2 or more sends to get the data transmitted between modules. With the current test structure this is considered a failure. Is this the correct behavior?",NULL,"Add for who this story is","well_formed","no_role","high",False
18766,"Tests that use verifySendCounts to validate whether data was sent to all the modules in a stream occasionally fail. This is because, sometimes it takes 2 or more sends to get the data transmitted between modules. With the current test structure this is considered a failure. Is this the correct behavior?",NULL,"Tests that use verifySendCounts to validate whether data was sent to all the modules in a stream occasionally fail. This is because, sometimes it takes 2 or more sends to get the data transmitted between modules. With the current test structure this is considered a failure. Is this the correct behavior?",NULL,"Tests that use verifySendCounts to validate whether data was sent to all the modules in a stream occasionally fail. This is because, sometimes it takes 2<span class='highlight-text severity-high'> or </span>more sends to get the data transmitted between modules. With the current test structure this is considered a failure. Is this the correct behavior?","atomic","conjunctions","high",False
18766,"Tests that use verifySendCounts to validate whether data was sent to all the modules in a stream occasionally fail. This is because, sometimes it takes 2 or more sends to get the data transmitted between modules. With the current test structure this is considered a failure. Is this the correct behavior?",NULL,"Tests that use verifySendCounts to validate whether data was sent to all the modules in a stream occasionally fail. This is because, sometimes it takes 2 or more sends to get the data transmitted between modules. With the current test structure this is considered a failure. Is this the correct behavior?",NULL,"Tests that use verifySendCounts to validate whether data was sent to all the modules in a stream occasionally fail<span class='highlight-text severity-high'>. This is because, sometimes it takes 2 or more sends to get the data transmitted between modules. With the current test structure this is considered a failure. Is this the correct behavior?</span>","minimal","punctuation","high",False
18767,"Use a baseline DIRT infrastructure to measure throughput, HA and scalability for various payload sizes. Depends on testing infrastructure setup, configuration and availability.",NULL,"Use a baseline DIRT infrastructure to measure throughput, HA and scalability for various payload sizes. Depends on testing infrastructure setup, configuration and availability.",NULL,"Add for who this story is","well_formed","no_role","high",False
18767,"Use a baseline DIRT infrastructure to measure throughput, HA and scalability for various payload sizes. Depends on testing infrastructure setup, configuration and availability.",NULL,"Use a baseline DIRT infrastructure to measure throughput, HA and scalability for various payload sizes. Depends on testing infrastructure setup, configuration and availability.",NULL,"Use a baseline DIRT infrastructure to measure throughput, HA and scalability for various payload sizes<span class='highlight-text severity-high'>. Depends on testing infrastructure setup, configuration and availability.</span>","minimal","punctuation","high",False
18768,"As an user, I'd like to have the ability to ingest data into Redis sink.","As an user",", I'd like to have the ability to ingest data into Redis sink.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18771,"As a user, I'd like to have the option to use the SFTP source module so that I can access, transfer, and mange files over any reliable data streams. Reference Spring Integration SFTP Adapter http docs.spring.io spring integration reference html sftp.html Need to consider the infrastructure for testing.","As a user",", I'd like to have the option to use the SFTP source module","so that I can access, transfer, and mange files over any reliable data streams. Reference Spring Integration SFTP Adapter http docs.spring.io spring integration reference html sftp.html Need to consider the infrastructure for testing.","As a user, I'd like to have the option to use the SFTP source module so that I can access, transfer, and mange files over any reliable data streams<span class='highlight-text severity-high'>. Reference Spring Integration SFTP Adapter http docs.spring.io spring integration reference html sftp.html Need to consider the infrastructure for testing.</span>","minimal","punctuation","high",False
18771,"As a user, I'd like to have the option to use the SFTP source module so that I can access, transfer, and mange files over any reliable data streams. Reference Spring Integration SFTP Adapter http docs.spring.io spring integration reference html sftp.html Need to consider the infrastructure for testing.","As a user",", I'd like to have the option to use the SFTP source module","so that I can access, transfer, and mange files over any reliable data streams. Reference Spring Integration SFTP Adapter http docs.spring.io spring integration reference html sftp.html Need to consider the infrastructure for testing.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18769,"As an user, I'd like to have a native JDBC source module to ingest data directly from various databases. ","As an user",", I'd like to have a native JDBC source module to ingest data directly from various databases.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18779,"If the XD admin server lets tomcat chooses random http port by setting PORT or server.port to 0 , the XD config logging and admin server context id still points to port zero as these are set before tomcat assigns the available random port. We also need to persist the admin servers ports into ZK so that this repo can be accessed by the client.",NULL,"If the XD admin server lets tomcat chooses random http port by setting PORT or server.port to 0 , the XD config logging and admin server context id still points to port zero as these are set before tomcat assigns the available random port. We also need to persist the admin servers ports into ZK","so that this repo can be accessed by the client.","Add for who this story is","well_formed","no_role","high",False
19342,"The admin UI should be polling the server to automatically pick up any new jobs, executions, and instances.",NULL,"The admin UI should be polling the server to automatically pick up any new jobs, executions, and instances.",NULL,"Add for who this story is","well_formed","no_role","high",False
18779,"If the XD admin server lets tomcat chooses random http port by setting PORT or server.port to 0 , the XD config logging and admin server context id still points to port zero as these are set before tomcat assigns the available random port. We also need to persist the admin servers ports into ZK so that this repo can be accessed by the client.",NULL,"If the XD admin server lets tomcat chooses random http port by setting PORT or server.port to 0 , the XD config logging and admin server context id still points to port zero as these are set before tomcat assigns the available random port. We also need to persist the admin servers ports into ZK","so that this repo can be accessed by the client.","If the XD admin server lets tomcat chooses random http port by setting PORT or server<span class='highlight-text severity-high'>.port to 0 , the XD config logging and admin server context id still points to port zero as these are set before tomcat assigns the available random port. We also need to persist the admin servers ports into ZK so that this repo can be accessed by the client.</span>","minimal","punctuation","high",False
18779,"If the XD admin server lets tomcat chooses random http port by setting PORT or server.port to 0 , the XD config logging and admin server context id still points to port zero as these are set before tomcat assigns the available random port. We also need to persist the admin servers ports into ZK so that this repo can be accessed by the client.",NULL,"If the XD admin server lets tomcat chooses random http port by setting PORT or server.port to 0 , the XD config logging and admin server context id still points to port zero as these are set before tomcat assigns the available random port. We also need to persist the admin servers ports into ZK","so that this repo can be accessed by the client.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18762,"h1. Run Acceptance tests on the following deployments. ",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
18762,"h1. Run Acceptance tests on the following deployments. ",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
18762,"h1. Run Acceptance tests on the following deployments. ",NULL,NULL,NULL,"h1<span class='highlight-text severity-high'>. Run Acceptance tests on the following deployments. </span>","minimal","punctuation","high",False
18759,"Currently, there is a stream list job list which shows the status of a given stream job along with the DSL. and, there is runtime modules which shows all the deployed modules with their container info. We need a better REST endpoint that gives all the deployed modules for a given stream job along with the status.",NULL,"Currently, there is a stream list job list which shows the status of a given stream job along with the DSL. and, there is runtime modules which shows all the deployed modules with their container info. We need a better REST endpoint that gives all the deployed modules for a given stream job along with the status.",NULL,"Add for who this story is","well_formed","no_role","high",False
18759,"Currently, there is a stream list job list which shows the status of a given stream job along with the DSL. and, there is runtime modules which shows all the deployed modules with their container info. We need a better REST endpoint that gives all the deployed modules for a given stream job along with the status.",NULL,"Currently, there is a stream list job list which shows the status of a given stream job along with the DSL. and, there is runtime modules which shows all the deployed modules with their container info. We need a better REST endpoint that gives all the deployed modules for a given stream job along with the status.",NULL,"Currently, there is a stream list job list which shows the status of a given stream job along with the DSL<span class='highlight-text severity-high'>. and, there is runtime modules which shows all the deployed modules with their container info. We need a better REST endpoint that gives all the deployed modules for a given stream job along with the status.</span>","minimal","punctuation","high",False
18770,"The goal is to optimize the build process and at the same time validate the feature capabilities as quickly as we can. Investigate the long running tests. Look for long timeout window declarations. ",NULL,"The goal is to optimize the build process and at the same time validate the feature capabilities as quickly as we can. Investigate the long running tests. Look for long timeout window declarations. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18770,"The goal is to optimize the build process and at the same time validate the feature capabilities as quickly as we can. Investigate the long running tests. Look for long timeout window declarations. ",NULL,"The goal is to optimize the build process and at the same time validate the feature capabilities as quickly as we can. Investigate the long running tests. Look for long timeout window declarations. ",NULL,"The goal is to optimize the build process<span class='highlight-text severity-high'> and </span>at the same time validate the feature capabilities as quickly as we can. Investigate the long running tests. Look for long timeout window declarations. ","atomic","conjunctions","high",False
18770,"The goal is to optimize the build process and at the same time validate the feature capabilities as quickly as we can. Investigate the long running tests. Look for long timeout window declarations. ",NULL,"The goal is to optimize the build process and at the same time validate the feature capabilities as quickly as we can. Investigate the long running tests. Look for long timeout window declarations. ",NULL,"The goal is to optimize the build process and at the same time validate the feature capabilities as quickly as we can<span class='highlight-text severity-high'>. Investigate the long running tests. Look for long timeout window declarations. </span>","minimal","punctuation","high",False
18773,"Maybe only have it run after the publish build instead of triggering builds directly from jdk6 7 8.",NULL,"Maybe only have it run after the publish build instead of triggering builds directly from jdk6 7 8.",NULL,"Add for who this story is","well_formed","no_role","high",False
18775,"Removed various TODO comments in code and put here for proper triage. DefaultTuple Error handling. When delegating to the conversion service, the ConversionFailedException does not have the context of which key caused the failure. Need to wrap ConversionFailedException with IllegalArgumentException and add that context back in. see method convert Ctor visibility. Consider making ctor final and package protect the ctor so as to always use TupleBuilder check for no duplicate values when initializing names values list tuple. top level methods to add. String getComponentName... somethign that would indicate which stream or job this tuple is being processed in.... TupleFieldSetMapper Only one date format? JsonStringtoTupleConverter JsonNodetoTupleConverter do we want to not map id and timestamp believe the answer is don t map, preserve original ",NULL,"Removed various TODO comments in code and put here for proper triage. DefaultTuple Error handling. When delegating to the conversion service, the ConversionFailedException does not have the context of which key caused the failure. Need to wrap ConversionFailedException with IllegalArgumentException and add that context back in. see method convert Ctor visibility. Consider making ctor final and package protect the ctor","so as to always use TupleBuilder check for no duplicate values when initializing names values list tuple. top level methods to add. String getComponentName... somethign that would indicate which stream or job this tuple is being processed in.... TupleFieldSetMapper Only one date format? JsonStringtoTupleConverter JsonNodetoTupleConverter do we want to not map id and timestamp believe the answer is don t map, preserve original","Add for who this story is","well_formed","no_role","high",False
18775,"Removed various TODO comments in code and put here for proper triage. DefaultTuple Error handling. When delegating to the conversion service, the ConversionFailedException does not have the context of which key caused the failure. Need to wrap ConversionFailedException with IllegalArgumentException and add that context back in. see method convert Ctor visibility. Consider making ctor final and package protect the ctor so as to always use TupleBuilder check for no duplicate values when initializing names values list tuple. top level methods to add. String getComponentName... somethign that would indicate which stream or job this tuple is being processed in.... TupleFieldSetMapper Only one date format? JsonStringtoTupleConverter JsonNodetoTupleConverter do we want to not map id and timestamp believe the answer is don t map, preserve original ",NULL,"Removed various TODO comments in code and put here for proper triage. DefaultTuple Error handling. When delegating to the conversion service, the ConversionFailedException does not have the context of which key caused the failure. Need to wrap ConversionFailedException with IllegalArgumentException and add that context back in. see method convert Ctor visibility. Consider making ctor final and package protect the ctor","so as to always use TupleBuilder check for no duplicate values when initializing names values list tuple. top level methods to add. String getComponentName... somethign that would indicate which stream or job this tuple is being processed in.... TupleFieldSetMapper Only one date format? JsonStringtoTupleConverter JsonNodetoTupleConverter do we want to not map id and timestamp believe the answer is don t map, preserve original","Removed various TODO comments in code<span class='highlight-text severity-high'> and </span>put here for proper triage. DefaultTuple Error handling. When delegating to the conversion service, the ConversionFailedException does not have the context of which key caused the failure. Need to wrap ConversionFailedException with IllegalArgumentException and add that context back in. see method convert Ctor visibility. Consider making ctor final and package protect the ctor so as to always use TupleBuilder check for no duplicate values when initializing names values list tuple. top level methods to add. String getComponentName... somethign that would indicate which stream<span class='highlight-text severity-high'> or </span>job this tuple is being processed in.... TupleFieldSetMapper Only one date format? JsonStringtoTupleConverter JsonNodetoTupleConverter do we want to not map id and timestamp believe the answer is don t map, preserve original ","atomic","conjunctions","high",False
18775,"Removed various TODO comments in code and put here for proper triage. DefaultTuple Error handling. When delegating to the conversion service, the ConversionFailedException does not have the context of which key caused the failure. Need to wrap ConversionFailedException with IllegalArgumentException and add that context back in. see method convert Ctor visibility. Consider making ctor final and package protect the ctor so as to always use TupleBuilder check for no duplicate values when initializing names values list tuple. top level methods to add. String getComponentName... somethign that would indicate which stream or job this tuple is being processed in.... TupleFieldSetMapper Only one date format? JsonStringtoTupleConverter JsonNodetoTupleConverter do we want to not map id and timestamp believe the answer is don t map, preserve original ",NULL,"Removed various TODO comments in code and put here for proper triage. DefaultTuple Error handling. When delegating to the conversion service, the ConversionFailedException does not have the context of which key caused the failure. Need to wrap ConversionFailedException with IllegalArgumentException and add that context back in. see method convert Ctor visibility. Consider making ctor final and package protect the ctor","so as to always use TupleBuilder check for no duplicate values when initializing names values list tuple. top level methods to add. String getComponentName... somethign that would indicate which stream or job this tuple is being processed in.... TupleFieldSetMapper Only one date format? JsonStringtoTupleConverter JsonNodetoTupleConverter do we want to not map id and timestamp believe the answer is don t map, preserve original","Removed various TODO comments in code and put here for proper triage<span class='highlight-text severity-high'>. DefaultTuple Error handling. When delegating to the conversion service, the ConversionFailedException does not have the context of which key caused the failure. Need to wrap ConversionFailedException with IllegalArgumentException and add that context back in. see method convert Ctor visibility. Consider making ctor final and package protect the ctor so as to always use TupleBuilder check for no duplicate values when initializing names values list tuple. top level methods to add. String getComponentName... somethign that would indicate which stream or job this tuple is being processed in.... TupleFieldSetMapper Only one date format? JsonStringtoTupleConverter JsonNodetoTupleConverter do we want to not map id and timestamp believe the answer is don t map, preserve original </span>","minimal","punctuation","high",False
18775,"Removed various TODO comments in code and put here for proper triage. DefaultTuple Error handling. When delegating to the conversion service, the ConversionFailedException does not have the context of which key caused the failure. Need to wrap ConversionFailedException with IllegalArgumentException and add that context back in. see method convert Ctor visibility. Consider making ctor final and package protect the ctor so as to always use TupleBuilder check for no duplicate values when initializing names values list tuple. top level methods to add. String getComponentName... somethign that would indicate which stream or job this tuple is being processed in.... TupleFieldSetMapper Only one date format? JsonStringtoTupleConverter JsonNodetoTupleConverter do we want to not map id and timestamp believe the answer is don t map, preserve original ",NULL,"Removed various TODO comments in code and put here for proper triage. DefaultTuple Error handling. When delegating to the conversion service, the ConversionFailedException does not have the context of which key caused the failure. Need to wrap ConversionFailedException with IllegalArgumentException and add that context back in. see method convert Ctor visibility. Consider making ctor final and package protect the ctor","so as to always use TupleBuilder check for no duplicate values when initializing names values list tuple. top level methods to add. String getComponentName... somethign that would indicate which stream or job this tuple is being processed in.... TupleFieldSetMapper Only one date format? JsonStringtoTupleConverter JsonNodetoTupleConverter do we want to not map id and timestamp believe the answer is don t map, preserve original","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18778,"https github.com spring projects spring xd wiki Creating a Job Item Processor should be very brief introduction to this topic, before linking to relevant spring batch documentation.",NULL,"https github.com spring projects spring xd wiki Creating a Job Item Processor should be very brief introduction to this topic, before linking to relevant spring batch documentation.",NULL,"Add for who this story is","well_formed","no_role","high",False
18774,"Noticed a few issues while reviewing the documentation The sidebar for TOC is no longer there That was really nice. Somehow the Using MQTT on XD section is giving an error. quote asciidoctor WARNING index.adoc line 167 invalid style for paragraph appendix asciidoctor WARNING index.adoc line 169 include file not found data projects spring xd build asciidoc guide Using MQTT on XD.asciidoc distZip quote but I'don t notice anything different between that appendix and the others in index.adoc. ",NULL,"Noticed a few issues while reviewing the documentation The sidebar for TOC is no longer there That was really nice. Somehow the Using MQTT on XD section is giving an error. quote asciidoctor WARNING index.adoc line 167 invalid style for paragraph appendix asciidoctor WARNING index.adoc line 169 include file not found data projects spring xd build asciidoc guide Using MQTT on XD.asciidoc distZip quote but I'don t notice anything different between that appendix and the others in index.adoc. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18774,"Noticed a few issues while reviewing the documentation The sidebar for TOC is no longer there That was really nice. Somehow the Using MQTT on XD section is giving an error. quote asciidoctor WARNING index.adoc line 167 invalid style for paragraph appendix asciidoctor WARNING index.adoc line 169 include file not found data projects spring xd build asciidoc guide Using MQTT on XD.asciidoc distZip quote but I'don t notice anything different between that appendix and the others in index.adoc. ",NULL,"Noticed a few issues while reviewing the documentation The sidebar for TOC is no longer there That was really nice. Somehow the Using MQTT on XD section is giving an error. quote asciidoctor WARNING index.adoc line 167 invalid style for paragraph appendix asciidoctor WARNING index.adoc line 169 include file not found data projects spring xd build asciidoc guide Using MQTT on XD.asciidoc distZip quote but I'don t notice anything different between that appendix and the others in index.adoc. ",NULL,"Noticed a few issues while reviewing the documentation The sidebar for TOC is no longer there That was really nice<span class='highlight-text severity-high'>. Somehow the Using MQTT on XD section is giving an error. quote asciidoctor WARNING index.adoc line 167 invalid style for paragraph appendix asciidoctor WARNING index.adoc line 169 include file not found data projects spring xd build asciidoc guide Using MQTT on XD.asciidoc distZip quote but I'don t notice anything different between that appendix and the others in index.adoc. </span>","minimal","punctuation","high",False
18776,"Spring 4.0 provides a UUID generator used by default in SI that should be used instead of the com.eaoi.uuid library in the xd tuple library",NULL,"Spring 4.0 provides a UUID generator used by default in SI that should be used instead of the com.eaoi.uuid library in the xd tuple library",NULL,"Add for who this story is","well_formed","no_role","high",False
18777,"There are a few places in the doc we can reference regarding overall lifecycle of jobs but this should provide a basic recipe for a single step job. The focus should be on creating a job item processor. In particular how List Message Tuple as the payload. this should link back to a new section in the aggregator that also mentions List Message Tuple Change title from Creating a Job Item Processor to Creating a Job Module",NULL,"There are a few places in the doc we can reference regarding overall lifecycle of jobs but this should provide a basic recipe for a single step job. The focus should be on creating a job item processor. In particular how List Message Tuple as the payload. this should link back to a new section in the aggregator that also mentions List Message Tuple Change title from Creating a Job Item Processor to Creating a Job Module",NULL,"Add for who this story is","well_formed","no_role","high",False
18777,"There are a few places in the doc we can reference regarding overall lifecycle of jobs but this should provide a basic recipe for a single step job. The focus should be on creating a job item processor. In particular how List Message Tuple as the payload. this should link back to a new section in the aggregator that also mentions List Message Tuple Change title from Creating a Job Item Processor to Creating a Job Module",NULL,"There are a few places in the doc we can reference regarding overall lifecycle of jobs but this should provide a basic recipe for a single step job. The focus should be on creating a job item processor. In particular how List Message Tuple as the payload. this should link back to a new section in the aggregator that also mentions List Message Tuple Change title from Creating a Job Item Processor to Creating a Job Module",NULL,"There are a few places in the doc we can reference regarding overall lifecycle of jobs but this should provide a basic recipe for a single step job<span class='highlight-text severity-high'>. The focus should be on creating a job item processor. In particular how List Message Tuple as the payload. this should link back to a new section in the aggregator that also mentions List Message Tuple Change title from Creating a Job Item Processor to Creating a Job Module</span>","minimal","punctuation","high",False
18782,"Here is a strawman noformat Getting Started rather meaty compared to other top level sections, maybe have a section running in SingleNode Running in Distributed Mode Running on YARN Application Configuration Message Bus Configuration Monitoring and Management Technical Documentation Architecture Distributed Runtime remove XD prefix Interactive Shell Batch Jobs Streams Modules Tuples Sources Processors Analytics Sinks Taps Type Conversion Deployment better name? Best Practices new section Admin UI DSL Reference REST API Samples noformat ",NULL,"Here is a strawman noformat Getting Started rather meaty compared to other top level sections, maybe have a section running in SingleNode Running in Distributed Mode Running on YARN Application Configuration Message Bus Configuration Monitoring and Management Technical Documentation Architecture Distributed Runtime remove XD prefix Interactive Shell Batch Jobs Streams Modules Tuples Sources Processors Analytics Sinks Taps Type Conversion Deployment better name? Best Practices new section Admin UI DSL Reference REST API Samples noformat ",NULL,"Add for who this story is","well_formed","no_role","high",False
18782,"Here is a strawman noformat Getting Started rather meaty compared to other top level sections, maybe have a section running in SingleNode Running in Distributed Mode Running on YARN Application Configuration Message Bus Configuration Monitoring and Management Technical Documentation Architecture Distributed Runtime remove XD prefix Interactive Shell Batch Jobs Streams Modules Tuples Sources Processors Analytics Sinks Taps Type Conversion Deployment better name? Best Practices new section Admin UI DSL Reference REST API Samples noformat ",NULL,"Here is a strawman noformat Getting Started rather meaty compared to other top level sections, maybe have a section running in SingleNode Running in Distributed Mode Running on YARN Application Configuration Message Bus Configuration Monitoring and Management Technical Documentation Architecture Distributed Runtime remove XD prefix Interactive Shell Batch Jobs Streams Modules Tuples Sources Processors Analytics Sinks Taps Type Conversion Deployment better name? Best Practices new section Admin UI DSL Reference REST API Samples noformat ",NULL,"Here is a strawman noformat Getting Started rather meaty compared to other top level sections, maybe have a section running in SingleNode Running in Distributed Mode Running on YARN Application Configuration Message Bus Configuration Monitoring<span class='highlight-text severity-high'> and </span>Management Technical Documentation Architecture Distributed Runtime remove XD prefix Interactive Shell Batch Jobs Streams Modules Tuples Sources Processors Analytics Sinks Taps Type Conversion Deployment better name? Best Practices new section Admin UI DSL Reference REST API Samples noformat ","atomic","conjunctions","high",False
18782,"Here is a strawman noformat Getting Started rather meaty compared to other top level sections, maybe have a section running in SingleNode Running in Distributed Mode Running on YARN Application Configuration Message Bus Configuration Monitoring and Management Technical Documentation Architecture Distributed Runtime remove XD prefix Interactive Shell Batch Jobs Streams Modules Tuples Sources Processors Analytics Sinks Taps Type Conversion Deployment better name? Best Practices new section Admin UI DSL Reference REST API Samples noformat ",NULL,"Here is a strawman noformat Getting Started rather meaty compared to other top level sections, maybe have a section running in SingleNode Running in Distributed Mode Running on YARN Application Configuration Message Bus Configuration Monitoring and Management Technical Documentation Architecture Distributed Runtime remove XD prefix Interactive Shell Batch Jobs Streams Modules Tuples Sources Processors Analytics Sinks Taps Type Conversion Deployment better name? Best Practices new section Admin UI DSL Reference REST API Samples noformat ",NULL,"Here is a strawman noformat Getting Started rather meaty compared to other top level sections, maybe have a section running in SingleNode Running in Distributed Mode Running on YARN Application Configuration Message Bus Configuration Monitoring and Management Technical Documentation Architecture Distributed Runtime remove XD prefix Interactive Shell Batch Jobs Streams Modules Tuples Sources Processors Analytics Sinks Taps Type Conversion Deployment better name<span class='highlight-text severity-high'>? Best Practices new section Admin UI DSL Reference REST API Samples noformat </span>","minimal","punctuation","high",False
18783,"When hdfsmongodb was written Spring Batch did not have a MongoItemWriter available, so Mark wrote one for XD. The hdfsmongodb module now needs to be use Spring Batches MongoItemWriter.",NULL,"When hdfsmongodb was written Spring Batch did not have a MongoItemWriter available,","so Mark wrote one for XD. The hdfsmongodb module now needs to be use Spring Batches MongoItemWriter.","Add for who this story is","well_formed","no_role","high",False
19346,"Now that the new batch admin api is taking shape, we need to rebase the XD web UI to use this. It s more than just changing the http urls sent to the xd server since the new API is not identical to the old one.",NULL,"Now that the new batch admin api is taking shape, we need to rebase the XD web UI to use this. It s more than just changing the http urls sent to the xd server since the new API is not identical to the old one.",NULL,"Add for who this story is","well_formed","no_role","high",False
18783,"When hdfsmongodb was written Spring Batch did not have a MongoItemWriter available, so Mark wrote one for XD. The hdfsmongodb module now needs to be use Spring Batches MongoItemWriter.",NULL,"When hdfsmongodb was written Spring Batch did not have a MongoItemWriter available,","so Mark wrote one for XD. The hdfsmongodb module now needs to be use Spring Batches MongoItemWriter.","When hdfsmongodb was written Spring Batch did not have a MongoItemWriter available, so Mark wrote one for XD<span class='highlight-text severity-high'>. The hdfsmongodb module now needs to be use Spring Batches MongoItemWriter.</span>","minimal","punctuation","high",False
18783,"When hdfsmongodb was written Spring Batch did not have a MongoItemWriter available, so Mark wrote one for XD. The hdfsmongodb module now needs to be use Spring Batches MongoItemWriter.",NULL,"When hdfsmongodb was written Spring Batch did not have a MongoItemWriter available,","so Mark wrote one for XD. The hdfsmongodb module now needs to be use Spring Batches MongoItemWriter.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18786,"Introduced by XD 2006, admin and container logs will have a pid suffix appended to their filename. The acceptance tests will have to identify the PID for the admin server and the container servers deployed in the cluster and then append the pid value to the filename contained in the xd container log dir.",NULL,"Introduced by XD 2006, admin and container logs will have a pid suffix appended to their filename. The acceptance tests will have to identify the PID for the admin server and the container servers deployed in the cluster and then append the pid value to the filename contained in the xd container log dir.",NULL,"Add for who this story is","well_formed","no_role","high",False
18786,"Introduced by XD 2006, admin and container logs will have a pid suffix appended to their filename. The acceptance tests will have to identify the PID for the admin server and the container servers deployed in the cluster and then append the pid value to the filename contained in the xd container log dir.",NULL,"Introduced by XD 2006, admin and container logs will have a pid suffix appended to their filename. The acceptance tests will have to identify the PID for the admin server and the container servers deployed in the cluster and then append the pid value to the filename contained in the xd container log dir.",NULL,"Introduced by XD 2006, admin<span class='highlight-text severity-high'> and </span>container logs will have a pid suffix appended to their filename. The acceptance tests will have to identify the PID for the admin server and the container servers deployed in the cluster and then append the pid value to the filename contained in the xd container log dir.","atomic","conjunctions","high",False
18786,"Introduced by XD 2006, admin and container logs will have a pid suffix appended to their filename. The acceptance tests will have to identify the PID for the admin server and the container servers deployed in the cluster and then append the pid value to the filename contained in the xd container log dir.",NULL,"Introduced by XD 2006, admin and container logs will have a pid suffix appended to their filename. The acceptance tests will have to identify the PID for the admin server and the container servers deployed in the cluster and then append the pid value to the filename contained in the xd container log dir.",NULL,"Introduced by XD 2006, admin and container logs will have a pid suffix appended to their filename<span class='highlight-text severity-high'>. The acceptance tests will have to identify the PID for the admin server and the container servers deployed in the cluster and then append the pid value to the filename contained in the xd container log dir.</span>","minimal","punctuation","high",False
18780,"issue seems to be error 25 Jul 2014 18 36 18 cat gemfire.pid No such file or directory error 25 Jul 2014 18 36 18 Usage error 25 Jul 2014 18 36 18 kill pid ... Send SIGTERM to every process listed. error 25 Jul 2014 18 36 18 kill signal pid ... Send a signal to every process listed. error 25 Jul 2014 18 36 18 kill s signal pid ... Send a signal to every process listed. error 25 Jul 2014 18 36 18 kill l List all signal names. error 25 Jul 2014 18 36 18 kill L List all signal names in a nice table. error 25 Jul 2014 18 36 18 kill l signal Convert between signal numbers and names. error 25 Jul 2014 18 36 18 rm cannot remove gemfire.pid No such file or directory",NULL,"issue seems to be error 25 Jul 2014 18 36 18 cat gemfire.pid No such file or directory error 25 Jul 2014 18 36 18 Usage error 25 Jul 2014 18 36 18 kill pid ... Send SIGTERM to every process listed. error 25 Jul 2014 18 36 18 kill signal pid ... Send a signal to every process listed. error 25 Jul 2014 18 36 18 kill s signal pid ... Send a signal to every process listed. error 25 Jul 2014 18 36 18 kill l List all signal names. error 25 Jul 2014 18 36 18 kill L List all signal names in a nice table. error 25 Jul 2014 18 36 18 kill l signal Convert between signal numbers and names. error 25 Jul 2014 18 36 18 rm cannot remove gemfire.pid No such file or directory",NULL,"Add for who this story is","well_formed","no_role","high",False
18780,"issue seems to be error 25 Jul 2014 18 36 18 cat gemfire.pid No such file or directory error 25 Jul 2014 18 36 18 Usage error 25 Jul 2014 18 36 18 kill pid ... Send SIGTERM to every process listed. error 25 Jul 2014 18 36 18 kill signal pid ... Send a signal to every process listed. error 25 Jul 2014 18 36 18 kill s signal pid ... Send a signal to every process listed. error 25 Jul 2014 18 36 18 kill l List all signal names. error 25 Jul 2014 18 36 18 kill L List all signal names in a nice table. error 25 Jul 2014 18 36 18 kill l signal Convert between signal numbers and names. error 25 Jul 2014 18 36 18 rm cannot remove gemfire.pid No such file or directory",NULL,"issue seems to be error 25 Jul 2014 18 36 18 cat gemfire.pid No such file or directory error 25 Jul 2014 18 36 18 Usage error 25 Jul 2014 18 36 18 kill pid ... Send SIGTERM to every process listed. error 25 Jul 2014 18 36 18 kill signal pid ... Send a signal to every process listed. error 25 Jul 2014 18 36 18 kill s signal pid ... Send a signal to every process listed. error 25 Jul 2014 18 36 18 kill l List all signal names. error 25 Jul 2014 18 36 18 kill L List all signal names in a nice table. error 25 Jul 2014 18 36 18 kill l signal Convert between signal numbers and names. error 25 Jul 2014 18 36 18 rm cannot remove gemfire.pid No such file or directory",NULL,"issue seems to be error 25 Jul 2014 18 36 18 cat gemfire.pid No such file<span class='highlight-text severity-high'> or </span>directory error 25 Jul 2014 18 36 18 Usage error 25 Jul 2014 18 36 18 kill pid ... Send SIGTERM to every process listed. error 25 Jul 2014 18 36 18 kill signal pid ... Send a signal to every process listed. error 25 Jul 2014 18 36 18 kill s signal pid ... Send a signal to every process listed. error 25 Jul 2014 18 36 18 kill l List all signal names. error 25 Jul 2014 18 36 18 kill L List all signal names in a nice table. error 25 Jul 2014 18 36 18 kill l signal Convert between signal numbers<span class='highlight-text severity-high'> and </span>names. error 25 Jul 2014 18 36 18 rm cannot remove gemfire.pid No such file or directory","atomic","conjunctions","high",False
18780,"issue seems to be error 25 Jul 2014 18 36 18 cat gemfire.pid No such file or directory error 25 Jul 2014 18 36 18 Usage error 25 Jul 2014 18 36 18 kill pid ... Send SIGTERM to every process listed. error 25 Jul 2014 18 36 18 kill signal pid ... Send a signal to every process listed. error 25 Jul 2014 18 36 18 kill s signal pid ... Send a signal to every process listed. error 25 Jul 2014 18 36 18 kill l List all signal names. error 25 Jul 2014 18 36 18 kill L List all signal names in a nice table. error 25 Jul 2014 18 36 18 kill l signal Convert between signal numbers and names. error 25 Jul 2014 18 36 18 rm cannot remove gemfire.pid No such file or directory",NULL,"issue seems to be error 25 Jul 2014 18 36 18 cat gemfire.pid No such file or directory error 25 Jul 2014 18 36 18 Usage error 25 Jul 2014 18 36 18 kill pid ... Send SIGTERM to every process listed. error 25 Jul 2014 18 36 18 kill signal pid ... Send a signal to every process listed. error 25 Jul 2014 18 36 18 kill s signal pid ... Send a signal to every process listed. error 25 Jul 2014 18 36 18 kill l List all signal names. error 25 Jul 2014 18 36 18 kill L List all signal names in a nice table. error 25 Jul 2014 18 36 18 kill l signal Convert between signal numbers and names. error 25 Jul 2014 18 36 18 rm cannot remove gemfire.pid No such file or directory",NULL,"issue seems to be error 25 Jul 2014 18 36 18 cat gemfire<span class='highlight-text severity-high'>.pid No such file or directory error 25 Jul 2014 18 36 18 Usage error 25 Jul 2014 18 36 18 kill pid ... Send SIGTERM to every process listed. error 25 Jul 2014 18 36 18 kill signal pid ... Send a signal to every process listed. error 25 Jul 2014 18 36 18 kill s signal pid ... Send a signal to every process listed. error 25 Jul 2014 18 36 18 kill l List all signal names. error 25 Jul 2014 18 36 18 kill L List all signal names in a nice table. error 25 Jul 2014 18 36 18 kill l signal Convert between signal numbers and names. error 25 Jul 2014 18 36 18 rm cannot remove gemfire.pid No such file or directory</span>","minimal","punctuation","high",False
18784,"Container s module deployer org.springframework.xd.dirt.module.ModuleDeployer has some unused code and container server.xml has listeners.xml which is no longer used. Also, all the extension code is moved to SharedContextConfiguration.",NULL,"Container s module deployer org.springframework.xd.dirt.module.ModuleDeployer has some unused code and container server.xml has listeners.xml which is no longer used. Also, all the extension code is moved to SharedContextConfiguration.",NULL,"Add for who this story is","well_formed","no_role","high",False
18784,"Container s module deployer org.springframework.xd.dirt.module.ModuleDeployer has some unused code and container server.xml has listeners.xml which is no longer used. Also, all the extension code is moved to SharedContextConfiguration.",NULL,"Container s module deployer org.springframework.xd.dirt.module.ModuleDeployer has some unused code and container server.xml has listeners.xml which is no longer used. Also, all the extension code is moved to SharedContextConfiguration.",NULL,"Container s module deployer org.springframework.xd.dirt.module.ModuleDeployer has some unused code<span class='highlight-text severity-high'> and </span>container server.xml has listeners.xml which is no longer used. Also, all the extension code is moved to SharedContextConfiguration.","atomic","conjunctions","high",False
18784,"Container s module deployer org.springframework.xd.dirt.module.ModuleDeployer has some unused code and container server.xml has listeners.xml which is no longer used. Also, all the extension code is moved to SharedContextConfiguration.",NULL,"Container s module deployer org.springframework.xd.dirt.module.ModuleDeployer has some unused code and container server.xml has listeners.xml which is no longer used. Also, all the extension code is moved to SharedContextConfiguration.",NULL,"Container s module deployer org<span class='highlight-text severity-high'>.springframework.xd.dirt.module.ModuleDeployer has some unused code and container server.xml has listeners.xml which is no longer used. Also, all the extension code is moved to SharedContextConfiguration.</span>","minimal","punctuation","high",False
18781,"0xData is a rich JVM based machine learning and scoring engine.",NULL,"0xData is a rich JVM based machine learning and scoring engine.",NULL,"Add for who this story is","well_formed","no_role","high",False
18781,"0xData is a rich JVM based machine learning and scoring engine.",NULL,"0xData is a rich JVM based machine learning and scoring engine.",NULL,"0xData is a rich JVM based machine learning<span class='highlight-text severity-high'> and </span>scoring engine.","atomic","conjunctions","high",False
18785,"https docs.sonatype.org display Repository Central Sync Requirements has a list of requirements. This also means that https jira.spring.io browse XD 1509 is critical to fix.",NULL,"https docs.sonatype.org display Repository Central Sync Requirements has a list of requirements. This also means that https jira.spring.io browse XD 1509 is critical to fix.",NULL,"Add for who this story is","well_formed","no_role","high",False
18785,"https docs.sonatype.org display Repository Central Sync Requirements has a list of requirements. This also means that https jira.spring.io browse XD 1509 is critical to fix.",NULL,"https docs.sonatype.org display Repository Central Sync Requirements has a list of requirements. This also means that https jira.spring.io browse XD 1509 is critical to fix.",NULL,"https docs<span class='highlight-text severity-high'>.sonatype.org display Repository Central Sync Requirements has a list of requirements. This also means that https jira.spring.io browse XD 1509 is critical to fix.</span>","minimal","punctuation","high",False
18788,"Determine a better package name for the following packages once we have a common model that applies to both stream job org.springframework.xd.dirt.stream org.springframework.xd.dirt.stream.zookeeper ",NULL,"Determine a better package name for the following packages once we have a common model that applies to both stream job org.springframework.xd.dirt.stream org.springframework.xd.dirt.stream.zookeeper ",NULL,"Add for who this story is","well_formed","no_role","high",False
18789,"The jars jersey test framework core 1.9.jar jersey test framework grizzly2 1.9.jar are incorrectly classified as compile time deps in hadoop vs. testCompile. ",NULL,"The jars jersey test framework core 1.9.jar jersey test framework grizzly2 1.9.jar are incorrectly classified as compile time deps in hadoop vs. testCompile. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18789,"The jars jersey test framework core 1.9.jar jersey test framework grizzly2 1.9.jar are incorrectly classified as compile time deps in hadoop vs. testCompile. ",NULL,"The jars jersey test framework core 1.9.jar jersey test framework grizzly2 1.9.jar are incorrectly classified as compile time deps in hadoop vs. testCompile. ",NULL,"The jars jersey test framework core 1<span class='highlight-text severity-high'>.9.jar jersey test framework grizzly2 1.9.jar are incorrectly classified as compile time deps in hadoop vs. testCompile. </span>","minimal","punctuation","high",False
18790,"The AggregateCounterTests were created to satisfy XD 1462, but currently they only have a couple tests to validate the time field processing. More comprehensive tests need to be added including the testing of the Redis based implementation in addition to in memory . For more info, see the comment here https github.com spring projects spring xd pull 1087 issuecomment 49638189",NULL,"The AggregateCounterTests were created to satisfy XD 1462, but currently they only have a couple tests to validate the time field processing. More comprehensive tests need to be added including the testing of the Redis based implementation in addition to in memory . For more info, see the comment here https github.com spring projects spring xd pull 1087 issuecomment 49638189",NULL,"Add for who this story is","well_formed","no_role","high",False
18790,"The AggregateCounterTests were created to satisfy XD 1462, but currently they only have a couple tests to validate the time field processing. More comprehensive tests need to be added including the testing of the Redis based implementation in addition to in memory . For more info, see the comment here https github.com spring projects spring xd pull 1087 issuecomment 49638189",NULL,"The AggregateCounterTests were created to satisfy XD 1462, but currently they only have a couple tests to validate the time field processing. More comprehensive tests need to be added including the testing of the Redis based implementation in addition to in memory . For more info, see the comment here https github.com spring projects spring xd pull 1087 issuecomment 49638189",NULL,"The AggregateCounterTests were created to satisfy XD 1462, but currently they only have a couple tests to validate the time field processing<span class='highlight-text severity-high'>. More comprehensive tests need to be added including the testing of the Redis based implementation in addition to in memory . For more info, see the comment here https github.com spring projects spring xd pull 1087 issuecomment 49638189</span>","minimal","punctuation","high",False
18791,"e.g., ^C09 42 00,882 ERROR localhost startStop 2 loader.WebappClassLoader The web application appears to have started a thread named Abandoned connection cleanup thread but has failed to stop it. This is very likely to create a memory leak. The thread name may be different...",NULL,"e.g., ^C09 42 00,882 ERROR localhost startStop 2 loader.WebappClassLoader The web application appears to have started a thread named Abandoned connection cleanup thread but has failed to stop it. This is very likely to create a memory leak. The thread name may be different...",NULL,"Add for who this story is","well_formed","no_role","high",False
18791,"e.g., ^C09 42 00,882 ERROR localhost startStop 2 loader.WebappClassLoader The web application appears to have started a thread named Abandoned connection cleanup thread but has failed to stop it. This is very likely to create a memory leak. The thread name may be different...",NULL,"e.g., ^C09 42 00,882 ERROR localhost startStop 2 loader.WebappClassLoader The web application appears to have started a thread named Abandoned connection cleanup thread but has failed to stop it. This is very likely to create a memory leak. The thread name may be different...",NULL,"e<span class='highlight-text severity-high'>.g., ^C09 42 00,882 ERROR localhost startStop 2 loader.WebappClassLoader The web application appears to have started a thread named Abandoned connection cleanup thread but has failed to stop it. This is very likely to create a memory leak. The thread name may be different...</span>","minimal","punctuation","high",False
18792,"http stackoverflow.com questions 24819401 how to get spring xd to deploy a predefined set of streams and taps on startup It is documented here https github.com spring projects spring xd wiki Shell executing a script But maybe it should also be at the top of the appendix? https github.com spring projects spring xd wiki ShellReference",NULL,"http stackoverflow.com questions 24819401 how to get spring xd to deploy a predefined set of streams and taps on startup It is documented here https github.com spring projects spring xd wiki Shell executing a script But maybe it should also be at the top of the appendix? https github.com spring projects spring xd wiki ShellReference",NULL,"Add for who this story is","well_formed","no_role","high",False
18792,"http stackoverflow.com questions 24819401 how to get spring xd to deploy a predefined set of streams and taps on startup It is documented here https github.com spring projects spring xd wiki Shell executing a script But maybe it should also be at the top of the appendix? https github.com spring projects spring xd wiki ShellReference",NULL,"http stackoverflow.com questions 24819401 how to get spring xd to deploy a predefined set of streams and taps on startup It is documented here https github.com spring projects spring xd wiki Shell executing a script But maybe it should also be at the top of the appendix? https github.com spring projects spring xd wiki ShellReference",NULL,"http stackoverflow.com questions 24819401 how to get spring xd to deploy a predefined set of streams<span class='highlight-text severity-high'> and </span>taps on startup It is documented here https github.com spring projects spring xd wiki Shell executing a script But maybe it should also be at the top of the appendix? https github.com spring projects spring xd wiki ShellReference","atomic","conjunctions","high",False
18792,"http stackoverflow.com questions 24819401 how to get spring xd to deploy a predefined set of streams and taps on startup It is documented here https github.com spring projects spring xd wiki Shell executing a script But maybe it should also be at the top of the appendix? https github.com spring projects spring xd wiki ShellReference",NULL,"http stackoverflow.com questions 24819401 how to get spring xd to deploy a predefined set of streams and taps on startup It is documented here https github.com spring projects spring xd wiki Shell executing a script But maybe it should also be at the top of the appendix? https github.com spring projects spring xd wiki ShellReference",NULL,"http stackoverflow.com questions 24819401 how to get spring xd to deploy a predefined set of streams and taps on startup It is documented here https github.com spring projects spring xd wiki Shell executing a script But maybe it should also be at the top of the appendix<span class='highlight-text severity-high'>? https github.com spring projects spring xd wiki ShellReference</span>","minimal","punctuation","high",False
18811,"Hadoop 2.4.1 is now a stable release and we should add support for running against it",NULL,"Hadoop 2.4.1 is now a stable release and we should add support for running against it",NULL,"Add for who this story is","well_formed","no_role","high",False
18811,"Hadoop 2.4.1 is now a stable release and we should add support for running against it",NULL,"Hadoop 2.4.1 is now a stable release and we should add support for running against it",NULL,"Hadoop 2.4.1 is now a stable release<span class='highlight-text severity-high'> and </span>we should add support for running against it","atomic","conjunctions","high",False
18813,"Now that the bus supports retry it is no longer necessary to have the retry advice in the TCP Sink.",NULL,"Now that the bus supports retry it is no longer necessary to have the retry advice in the TCP Sink.",NULL,"Add for who this story is","well_formed","no_role","high",False
18814,"When the job is in deploying state, until we decide whether the job is actually deployed or failed incomplete , there is no way to know if it is fine to launch schedule though the launching requests are going to go to the job launch request queue . We could either disable both deploy undeploy until the state changes from deploying ?",NULL,"When the job is in deploying state, until we decide whether the job is actually deployed or failed incomplete , there is no way to know if it is fine to launch schedule though the launching requests are going to go to the job launch request queue . We could either disable both deploy undeploy until the state changes from deploying ?",NULL,"Add for who this story is","well_formed","no_role","high",False
18814,"When the job is in deploying state, until we decide whether the job is actually deployed or failed incomplete , there is no way to know if it is fine to launch schedule though the launching requests are going to go to the job launch request queue . We could either disable both deploy undeploy until the state changes from deploying ?",NULL,"When the job is in deploying state, until we decide whether the job is actually deployed or failed incomplete , there is no way to know if it is fine to launch schedule though the launching requests are going to go to the job launch request queue . We could either disable both deploy undeploy until the state changes from deploying ?",NULL,"When the job is in deploying state, until we decide whether the job is actually deployed<span class='highlight-text severity-high'> or </span>failed incomplete , there is no way to know if it is fine to launch schedule though the launching requests are going to go to the job launch request queue . We could either disable both deploy undeploy until the state changes from deploying ?","atomic","conjunctions","high",False
18814,"When the job is in deploying state, until we decide whether the job is actually deployed or failed incomplete , there is no way to know if it is fine to launch schedule though the launching requests are going to go to the job launch request queue . We could either disable both deploy undeploy until the state changes from deploying ?",NULL,"When the job is in deploying state, until we decide whether the job is actually deployed or failed incomplete , there is no way to know if it is fine to launch schedule though the launching requests are going to go to the job launch request queue . We could either disable both deploy undeploy until the state changes from deploying ?",NULL,"When the job is in deploying state, until we decide whether the job is actually deployed or failed incomplete , there is no way to know if it is fine to launch schedule though the launching requests are going to go to the job launch request queue <span class='highlight-text severity-high'>. We could either disable both deploy undeploy until the state changes from deploying ?</span>","minimal","punctuation","high",False
18812,"https github.com spring guides gs spring xd issues 1",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
18812,"https github.com spring guides gs spring xd issues 1",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
18815,"As a minimum we need some common polling strategy on the client side to detect status changes of job streams etc. E.g. during deployment of streams jobs Ideally, I would like to have this addressed on the server side as well. It would be nice if we could propagate events between, containers and admin server that would inform about any changes in the system. We could then use those to notify connected UI clients.","As a minimum","we need some common polling strategy on the client side to detect status changes of job streams etc. E.g. during deployment of streams jobs Ideally, I would like to have this addressed on the server side as well. It would be nice if we could propagate events between, containers and admin server that would inform about any changes in the system. We could then use those to notify connected UI clients.",NULL,"As a minimum we need some common polling strategy on the client side to detect status changes of job streams etc. E.g. during deployment of streams jobs Ideally, I would like to have this addressed on the server side as well. It would be nice if we could propagate events between, containers<span class='highlight-text severity-high'> and </span>admin server that would inform about any changes in the system. We could then use those to notify connected UI clients.","atomic","conjunctions","high",False
18815,"As a minimum we need some common polling strategy on the client side to detect status changes of job streams etc. E.g. during deployment of streams jobs Ideally, I would like to have this addressed on the server side as well. It would be nice if we could propagate events between, containers and admin server that would inform about any changes in the system. We could then use those to notify connected UI clients.","As a minimum","we need some common polling strategy on the client side to detect status changes of job streams etc. E.g. during deployment of streams jobs Ideally, I would like to have this addressed on the server side as well. It would be nice if we could propagate events between, containers and admin server that would inform about any changes in the system. We could then use those to notify connected UI clients.",NULL,"As a minimum we need some common polling strategy on the client side to detect status changes of job streams etc<span class='highlight-text severity-high'>. E.g. during deployment of streams jobs Ideally, I would like to have this addressed on the server side as well. It would be nice if we could propagate events between, containers and admin server that would inform about any changes in the system. We could then use those to notify connected UI clients.</span>","minimal","punctuation","high",False
18815,"As a minimum we need some common polling strategy on the client side to detect status changes of job streams etc. E.g. during deployment of streams jobs Ideally, I would like to have this addressed on the server side as well. It would be nice if we could propagate events between, containers and admin server that would inform about any changes in the system. We could then use those to notify connected UI clients.","As a minimum","we need some common polling strategy on the client side to detect status changes of job streams etc. E.g. during deployment of streams jobs Ideally, I would like to have this addressed on the server side as well. It would be nice if we could propagate events between, containers and admin server that would inform about any changes in the system. We could then use those to notify connected UI clients.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18787,"In cases where the deployment requires jars that can not be included with the distribution, the user should be able to pull a jar from a http site and place it in lib xd. The use case is that when we removed the mysql jar from the distribution, the CI tests could not start the XD instances on EC2 without it. It was suggested that we use the postgresql instead, but decided to continue the use of mysql for acceptance tests.",NULL,"In cases where the deployment requires jars that can not be included with the distribution, the user should be able to pull a jar from a http site and place it in lib xd. The use case is that when we removed the mysql jar from the distribution, the CI tests could not start the XD instances on EC2 without it. It was suggested that we use the postgresql instead, but decided to continue the use of mysql for acceptance tests.",NULL,"Add for who this story is","well_formed","no_role","high",False
18787,"In cases where the deployment requires jars that can not be included with the distribution, the user should be able to pull a jar from a http site and place it in lib xd. The use case is that when we removed the mysql jar from the distribution, the CI tests could not start the XD instances on EC2 without it. It was suggested that we use the postgresql instead, but decided to continue the use of mysql for acceptance tests.",NULL,"In cases where the deployment requires jars that can not be included with the distribution, the user should be able to pull a jar from a http site and place it in lib xd. The use case is that when we removed the mysql jar from the distribution, the CI tests could not start the XD instances on EC2 without it. It was suggested that we use the postgresql instead, but decided to continue the use of mysql for acceptance tests.",NULL,"In cases where the deployment requires jars that can not be included with the distribution, the user should be able to pull a jar from a http site and place it in lib xd<span class='highlight-text severity-high'>. The use case is that when we removed the mysql jar from the distribution, the CI tests could not start the XD instances on EC2 without it. It was suggested that we use the postgresql instead, but decided to continue the use of mysql for acceptance tests.</span>","minimal","punctuation","high",False
18810,"Previously, the scripts all looked for the logging configuration in XD HOME config or XD HOME config . This caused issues because it meant that if you moved all of the configuration and overrode XD CONFIG LOCATION or XD CONFIG LOCATION , the logging configuration changes would not be found in the new location. This change updates the scripts to look for logging configuration in XD CONFIG LOCATION or XD CONFIG LOCATION .",NULL,"Previously, the scripts all looked for the logging configuration in XD HOME config or XD HOME config . This caused issues because it meant that if you moved all of the configuration and overrode XD CONFIG LOCATION or XD CONFIG LOCATION , the logging configuration changes would not be found in the new location. This change updates the scripts to look for logging configuration in XD CONFIG LOCATION or XD CONFIG LOCATION .",NULL,"Add for who this story is","well_formed","no_role","high",False
18810,"Previously, the scripts all looked for the logging configuration in XD HOME config or XD HOME config . This caused issues because it meant that if you moved all of the configuration and overrode XD CONFIG LOCATION or XD CONFIG LOCATION , the logging configuration changes would not be found in the new location. This change updates the scripts to look for logging configuration in XD CONFIG LOCATION or XD CONFIG LOCATION .",NULL,"Previously, the scripts all looked for the logging configuration in XD HOME config or XD HOME config . This caused issues because it meant that if you moved all of the configuration and overrode XD CONFIG LOCATION or XD CONFIG LOCATION , the logging configuration changes would not be found in the new location. This change updates the scripts to look for logging configuration in XD CONFIG LOCATION or XD CONFIG LOCATION .",NULL,"Previously, the scripts all looked for the logging configuration in XD HOME config<span class='highlight-text severity-high'> or </span>XD HOME config . This caused issues because it meant that if you moved all of the configuration<span class='highlight-text severity-high'> and </span>overrode XD CONFIG LOCATION or XD CONFIG LOCATION , the logging configuration changes would not be found in the new location. This change updates the scripts to look for logging configuration in XD CONFIG LOCATION or XD CONFIG LOCATION .","atomic","conjunctions","high",False
18810,"Previously, the scripts all looked for the logging configuration in XD HOME config or XD HOME config . This caused issues because it meant that if you moved all of the configuration and overrode XD CONFIG LOCATION or XD CONFIG LOCATION , the logging configuration changes would not be found in the new location. This change updates the scripts to look for logging configuration in XD CONFIG LOCATION or XD CONFIG LOCATION .",NULL,"Previously, the scripts all looked for the logging configuration in XD HOME config or XD HOME config . This caused issues because it meant that if you moved all of the configuration and overrode XD CONFIG LOCATION or XD CONFIG LOCATION , the logging configuration changes would not be found in the new location. This change updates the scripts to look for logging configuration in XD CONFIG LOCATION or XD CONFIG LOCATION .",NULL,"Previously, the scripts all looked for the logging configuration in XD HOME config or XD HOME config <span class='highlight-text severity-high'>. This caused issues because it meant that if you moved all of the configuration and overrode XD CONFIG LOCATION or XD CONFIG LOCATION , the logging configuration changes would not be found in the new location. This change updates the scripts to look for logging configuration in XD CONFIG LOCATION or XD CONFIG LOCATION .</span>","minimal","punctuation","high",False
18799,"The module info command renders text that is pretty much unreadable on a reasonably sized screen. See attached screen shot. Also all the jdbc pool settings are mixed in with module settings making for a confusing list of options. What the heck does fairQueue have to do with filejdbc jobs?",NULL,"The module info command renders text that is pretty much unreadable on a reasonably sized screen. See attached screen shot. Also all the jdbc pool settings are mixed in with module settings making for a confusing list of options. What the heck does fairQueue have to do with filejdbc jobs?",NULL,"Add for who this story is","well_formed","no_role","high",False
18799,"The module info command renders text that is pretty much unreadable on a reasonably sized screen. See attached screen shot. Also all the jdbc pool settings are mixed in with module settings making for a confusing list of options. What the heck does fairQueue have to do with filejdbc jobs?",NULL,"The module info command renders text that is pretty much unreadable on a reasonably sized screen. See attached screen shot. Also all the jdbc pool settings are mixed in with module settings making for a confusing list of options. What the heck does fairQueue have to do with filejdbc jobs?",NULL,"The module info command renders text that is pretty much unreadable on a reasonably sized screen<span class='highlight-text severity-high'>. See attached screen shot. Also all the jdbc pool settings are mixed in with module settings making for a confusing list of options. What the heck does fairQueue have to do with filejdbc jobs?</span>","minimal","punctuation","high",False
18796,"The Back button is at lower left of the page which requires scrolling all the way to the bottom could we move it to top right? Would make clicking back and forth for job executions much easier.",NULL,"The Back button is at lower left of the page which requires scrolling all the way to the bottom could we move it to top right? Would make clicking back and forth for job executions much easier.",NULL,"Add for who this story is","well_formed","no_role","high",False
18796,"The Back button is at lower left of the page which requires scrolling all the way to the bottom could we move it to top right? Would make clicking back and forth for job executions much easier.",NULL,"The Back button is at lower left of the page which requires scrolling all the way to the bottom could we move it to top right? Would make clicking back and forth for job executions much easier.",NULL,"The Back button is at lower left of the page which requires scrolling all the way to the bottom could we move it to top right<span class='highlight-text severity-high'>? Would make clicking back and forth for job executions much easier.</span>","minimal","punctuation","high",False
18798,"12 30 43,930 WARN main logging.LoggingApplicationListener Logging environment value file data projects spring xd build dist spring xd xd config xd container logger.properties cannot be opened and will be ignored There are extra slashes in there.... probably due to some recent changes related to xd config location in the scripts.",NULL,"12 30 43,930 WARN main logging.LoggingApplicationListener Logging environment value file data projects spring xd build dist spring xd xd config xd container logger.properties cannot be opened and will be ignored There are extra slashes in there.... probably due to some recent changes related to xd config location in the scripts.",NULL,"Add for who this story is","well_formed","no_role","high",False
18798,"12 30 43,930 WARN main logging.LoggingApplicationListener Logging environment value file data projects spring xd build dist spring xd xd config xd container logger.properties cannot be opened and will be ignored There are extra slashes in there.... probably due to some recent changes related to xd config location in the scripts.",NULL,"12 30 43,930 WARN main logging.LoggingApplicationListener Logging environment value file data projects spring xd build dist spring xd xd config xd container logger.properties cannot be opened and will be ignored There are extra slashes in there.... probably due to some recent changes related to xd config location in the scripts.",NULL,"12 30 43,930 WARN main logging.LoggingApplicationListener Logging environment value file data projects spring xd build dist spring xd xd config xd container logger.properties cannot be opened<span class='highlight-text severity-high'> and </span>will be ignored There are extra slashes in there.... probably due to some recent changes related to xd config location in the scripts.","atomic","conjunctions","high",False
18798,"12 30 43,930 WARN main logging.LoggingApplicationListener Logging environment value file data projects spring xd build dist spring xd xd config xd container logger.properties cannot be opened and will be ignored There are extra slashes in there.... probably due to some recent changes related to xd config location in the scripts.",NULL,"12 30 43,930 WARN main logging.LoggingApplicationListener Logging environment value file data projects spring xd build dist spring xd xd config xd container logger.properties cannot be opened and will be ignored There are extra slashes in there.... probably due to some recent changes related to xd config location in the scripts.",NULL,"12 30 43,930 WARN main logging<span class='highlight-text severity-high'>.LoggingApplicationListener Logging environment value file data projects spring xd build dist spring xd xd config xd container logger.properties cannot be opened and will be ignored There are extra slashes in there.... probably due to some recent changes related to xd config location in the scripts.</span>","minimal","punctuation","high",False
18804,"jclouds is not compatible with versions of guava higher than 15.",NULL,"jclouds is not compatible with versions of guava higher than 15.",NULL,"Add for who this story is","well_formed","no_role","high",False
18801,"The platform uses Boot version 1.1.4 so the plugin version used in build.gradle should match that.",NULL,"The platform uses Boot version 1.1.4","so the plugin version used in build.gradle should match that.","Add for who this story is","well_formed","no_role","high",False
18801,"The platform uses Boot version 1.1.4 so the plugin version used in build.gradle should match that.",NULL,"The platform uses Boot version 1.1.4","so the plugin version used in build.gradle should match that.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18800,"Should be part of the daily build. One easy way to do it would be to use the hardcoded authentication scheme as described here bamboo should mask a property whose name contains password We may want to create a dedicated github user though",NULL,"Should be part of the daily build. One easy way to do it would be to use the hardcoded authentication scheme as described here bamboo should mask a property whose name contains password We may want to create a dedicated github user though",NULL,"Add for who this story is","well_formed","no_role","high",False
18800,"Should be part of the daily build. One easy way to do it would be to use the hardcoded authentication scheme as described here bamboo should mask a property whose name contains password We may want to create a dedicated github user though",NULL,"Should be part of the daily build. One easy way to do it would be to use the hardcoded authentication scheme as described here bamboo should mask a property whose name contains password We may want to create a dedicated github user though",NULL,"Should be part of the daily build<span class='highlight-text severity-high'>. One easy way to do it would be to use the hardcoded authentication scheme as described here bamboo should mask a property whose name contains password We may want to create a dedicated github user though</span>","minimal","punctuation","high",False
18802,"See PR https github.com spring projects spring xd pull 1043 ",NULL,"See PR https github.com spring projects spring xd pull 1043 ",NULL,"Add for who this story is","well_formed","no_role","high",False
18806,"postgresql is BSD http jdbc.postgresql.org about license.html",NULL,"postgresql is BSD http jdbc.postgresql.org about license.html",NULL,"Add for who this story is","well_formed","no_role","high",False
18807,"The work here is doing the research.... mysql client jar should be removed as it is GPL http dev.mysql.com downloads connector j 5.0.html GPL postgresql is BSD so that is ok another issue will handle license file inclusion.",NULL,"The work here is doing the research.... mysql client jar should be removed as it is GPL http dev.mysql.com downloads connector j 5.0.html GPL postgresql is BSD","so that is ok another issue will handle license file inclusion.","Add for who this story is","well_formed","no_role","high",False
18807,"The work here is doing the research.... mysql client jar should be removed as it is GPL http dev.mysql.com downloads connector j 5.0.html GPL postgresql is BSD so that is ok another issue will handle license file inclusion.",NULL,"The work here is doing the research.... mysql client jar should be removed as it is GPL http dev.mysql.com downloads connector j 5.0.html GPL postgresql is BSD","so that is ok another issue will handle license file inclusion.","The work here is doing the research<span class='highlight-text severity-high'>.... mysql client jar should be removed as it is GPL http dev.mysql.com downloads connector j 5.0.html GPL postgresql is BSD so that is ok another issue will handle license file inclusion.</span>","minimal","punctuation","high",False
18807,"The work here is doing the research.... mysql client jar should be removed as it is GPL http dev.mysql.com downloads connector j 5.0.html GPL postgresql is BSD so that is ok another issue will handle license file inclusion.",NULL,"The work here is doing the research.... mysql client jar should be removed as it is GPL http dev.mysql.com downloads connector j 5.0.html GPL postgresql is BSD","so that is ok another issue will handle license file inclusion.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18794,"A possible approach is to set a configurable wait period when the first container arrives. If another container arrives during the wait period, reset the clock. When the wait period expires, start deploying modules. ",NULL,"A possible approach is to set a configurable wait period when the first container arrives. If another container arrives during the wait period, reset the clock. When the wait period expires, start deploying modules. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18794,"A possible approach is to set a configurable wait period when the first container arrives. If another container arrives during the wait period, reset the clock. When the wait period expires, start deploying modules. ",NULL,"A possible approach is to set a configurable wait period when the first container arrives. If another container arrives during the wait period, reset the clock. When the wait period expires, start deploying modules. ",NULL,"A possible approach is to set a configurable wait period when the first container arrives<span class='highlight-text severity-high'>. If another container arrives during the wait period, reset the clock. When the wait period expires, start deploying modules. </span>","minimal","punctuation","high",False
18795,"https github.com spring projects spring xd commit db66aa2a329a6fc7ef89a340dd4d562fa70d14a4 introduces org.apache.tomcat.embed tomcat embed logging log4j which is not covered by platform. Yet, we should lookup the version to use from other tomcat artifacts, using some gradle magic",NULL,"https github.com spring projects spring xd commit db66aa2a329a6fc7ef89a340dd4d562fa70d14a4 introduces org.apache.tomcat.embed tomcat embed logging log4j which is not covered by platform. Yet, we should lookup the version to use from other tomcat artifacts, using some gradle magic",NULL,"Add for who this story is","well_formed","no_role","high",False
18795,"https github.com spring projects spring xd commit db66aa2a329a6fc7ef89a340dd4d562fa70d14a4 introduces org.apache.tomcat.embed tomcat embed logging log4j which is not covered by platform. Yet, we should lookup the version to use from other tomcat artifacts, using some gradle magic",NULL,"https github.com spring projects spring xd commit db66aa2a329a6fc7ef89a340dd4d562fa70d14a4 introduces org.apache.tomcat.embed tomcat embed logging log4j which is not covered by platform. Yet, we should lookup the version to use from other tomcat artifacts, using some gradle magic",NULL,"https github<span class='highlight-text severity-high'>.com spring projects spring xd commit db66aa2a329a6fc7ef89a340dd4d562fa70d14a4 introduces org.apache.tomcat.embed tomcat embed logging log4j which is not covered by platform. Yet, we should lookup the version to use from other tomcat artifacts, using some gradle magic</span>","minimal","punctuation","high",False
18797,"The Rabbit SMLC uses a SimpleAsyncTaskExecutor by default. This makes it difficult to debug when multiple modules are in the same container because all threads are named SimpleAsyncTaskExecutor 1 .",NULL,"The Rabbit SMLC uses a SimpleAsyncTaskExecutor by default. This makes it difficult to debug when multiple modules are in the same container because all threads are named SimpleAsyncTaskExecutor 1 .",NULL,"Add for who this story is","well_formed","no_role","high",False
18797,"The Rabbit SMLC uses a SimpleAsyncTaskExecutor by default. This makes it difficult to debug when multiple modules are in the same container because all threads are named SimpleAsyncTaskExecutor 1 .",NULL,"The Rabbit SMLC uses a SimpleAsyncTaskExecutor by default. This makes it difficult to debug when multiple modules are in the same container because all threads are named SimpleAsyncTaskExecutor 1 .",NULL,"The Rabbit SMLC uses a SimpleAsyncTaskExecutor by default<span class='highlight-text severity-high'>. This makes it difficult to debug when multiple modules are in the same container because all threads are named SimpleAsyncTaskExecutor 1 .</span>","minimal","punctuation","high",False
18803,"Test to verify stream state is correct after starting stopping containers.",NULL,"Test to verify stream state is correct after starting stopping containers.",NULL,"Add for who this story is","well_formed","no_role","high",False
18805,"Remove unnecessary duplicated jars from the lib directory in spring xd yarn zip distribution",NULL,"Remove unnecessary duplicated jars from the lib directory in spring xd yarn zip distribution",NULL,"Add for who this story is","well_formed","no_role","high",False
18820,"Spring IO Compatibility",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
18820,"Spring IO Compatibility",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
18824,"https github.com spring projects spring xd blob master src test scripts tweet tests use field value counter and aggregate counter. Should do a simplified version of this so that we can assert values of the field value counter and aggregate counter.",NULL,"https github.com spring projects spring xd blob master src test scripts tweet tests use field value counter and aggregate counter. Should do a simplified version of this","so that we can assert values of the field value counter and aggregate counter.","Add for who this story is","well_formed","no_role","high",False
18824,"https github.com spring projects spring xd blob master src test scripts tweet tests use field value counter and aggregate counter. Should do a simplified version of this so that we can assert values of the field value counter and aggregate counter.",NULL,"https github.com spring projects spring xd blob master src test scripts tweet tests use field value counter and aggregate counter. Should do a simplified version of this","so that we can assert values of the field value counter and aggregate counter.","https github<span class='highlight-text severity-high'>.com spring projects spring xd blob master src test scripts tweet tests use field value counter and aggregate counter. Should do a simplified version of this so that we can assert values of the field value counter and aggregate counter.</span>","minimal","punctuation","high",False
18824,"https github.com spring projects spring xd blob master src test scripts tweet tests use field value counter and aggregate counter. Should do a simplified version of this so that we can assert values of the field value counter and aggregate counter.",NULL,"https github.com spring projects spring xd blob master src test scripts tweet tests use field value counter and aggregate counter. Should do a simplified version of this","so that we can assert values of the field value counter and aggregate counter.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18821,"Not all module options are born equal. Some are more important useful than others, and having the more expert ones show up e.g. in TAB completion is very noisy esp. given how JLine2 currently presents the whole stream definition typed so far when doing completion, as opposed to just the last bit ",NULL,"Not all module options are born equal.","Some are more important useful than others, and having the more expert ones show up e.g. in TAB completion is very noisy esp. given how JLine2 currently presents the whole stream definition typed so far when doing completion, as opposed to just the last bit","Add for who this story is","well_formed","no_role","high",False
18821,"Not all module options are born equal. Some are more important useful than others, and having the more expert ones show up e.g. in TAB completion is very noisy esp. given how JLine2 currently presents the whole stream definition typed so far when doing completion, as opposed to just the last bit ",NULL,"Not all module options are born equal.","Some are more important useful than others, and having the more expert ones show up e.g. in TAB completion is very noisy esp. given how JLine2 currently presents the whole stream definition typed so far when doing completion, as opposed to just the last bit","Not all module options are born equal<span class='highlight-text severity-high'>. Some are more important useful than others, and having the more expert ones show up e.g. in TAB completion is very noisy esp. given how JLine2 currently presents the whole stream definition typed so far when doing completion, as opposed to just the last bit </span>","minimal","punctuation","high",False
18821,"Not all module options are born equal. Some are more important useful than others, and having the more expert ones show up e.g. in TAB completion is very noisy esp. given how JLine2 currently presents the whole stream definition typed so far when doing completion, as opposed to just the last bit ",NULL,"Not all module options are born equal.","Some are more important useful than others, and having the more expert ones show up e.g. in TAB completion is very noisy esp. given how JLine2 currently presents the whole stream definition typed so far when doing completion, as opposed to just the last bit","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18825,"See https github.com spring projects spring xd blob master src test scripts jdbc tests L96 and https github.com spring projects spring xd blob master src test scripts jdbc tests L64 The second assert has initializeDb false and so there are double the number of rows running the job a second time.",NULL,"See https github.com spring projects spring xd blob master src test scripts jdbc tests L96 and https github.com spring projects spring xd blob master src test scripts jdbc tests L64 The second assert has initializeDb false and","so there are double the number of rows running the job a second time.","Add for who this story is","well_formed","no_role","high",False
18825,"See https github.com spring projects spring xd blob master src test scripts jdbc tests L96 and https github.com spring projects spring xd blob master src test scripts jdbc tests L64 The second assert has initializeDb false and so there are double the number of rows running the job a second time.",NULL,"See https github.com spring projects spring xd blob master src test scripts jdbc tests L96 and https github.com spring projects spring xd blob master src test scripts jdbc tests L64 The second assert has initializeDb false and","so there are double the number of rows running the job a second time.","See https github.com spring projects spring xd blob master src test scripts jdbc tests L96<span class='highlight-text severity-high'> and </span>https github.com spring projects spring xd blob master src test scripts jdbc tests L64 The second assert has initializeDb false and so there are double the number of rows running the job a second time.","atomic","conjunctions","high",False
18825,"See https github.com spring projects spring xd blob master src test scripts jdbc tests L96 and https github.com spring projects spring xd blob master src test scripts jdbc tests L64 The second assert has initializeDb false and so there are double the number of rows running the job a second time.",NULL,"See https github.com spring projects spring xd blob master src test scripts jdbc tests L96 and https github.com spring projects spring xd blob master src test scripts jdbc tests L64 The second assert has initializeDb false and","so there are double the number of rows running the job a second time.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18816,"When deploying a definition with a container match criteria specified, and no container could be selected the logging is ambiguous and should mention the affected module code 11 58 24,089 WARN DeploymentSupervisorCacheListener 0 cluster.DefaultContainerMatcher No currently available containers match criteria somecriteria code ",NULL,"When deploying a definition with a container match criteria specified, and no container could be selected the logging is ambiguous and should mention the affected module code 11 58 24,089 WARN DeploymentSupervisorCacheListener 0 cluster.DefaultContainerMatcher No currently available containers match criteria somecriteria code ",NULL,"Add for who this story is","well_formed","no_role","high",False
18816,"When deploying a definition with a container match criteria specified, and no container could be selected the logging is ambiguous and should mention the affected module code 11 58 24,089 WARN DeploymentSupervisorCacheListener 0 cluster.DefaultContainerMatcher No currently available containers match criteria somecriteria code ",NULL,"When deploying a definition with a container match criteria specified, and no container could be selected the logging is ambiguous and should mention the affected module code 11 58 24,089 WARN DeploymentSupervisorCacheListener 0 cluster.DefaultContainerMatcher No currently available containers match criteria somecriteria code ",NULL,"When deploying a definition with a container match criteria specified,<span class='highlight-text severity-high'> and </span>no container could be selected the logging is ambiguous and should mention the affected module code 11 58 24,089 WARN DeploymentSupervisorCacheListener 0 cluster.DefaultContainerMatcher No currently available containers match criteria somecriteria code ","atomic","conjunctions","high",False
18822,"It seems that no option validation being Spring or jsr 303 is happening anymore at stream creation time. eg noformat stream create foo definition http port bar log noformat ",NULL,"It seems that no option validation being Spring or jsr 303 is happening anymore at stream creation time. eg noformat stream create foo definition http port bar log noformat ",NULL,"Add for who this story is","well_formed","no_role","high",False
18822,"It seems that no option validation being Spring or jsr 303 is happening anymore at stream creation time. eg noformat stream create foo definition http port bar log noformat ",NULL,"It seems that no option validation being Spring or jsr 303 is happening anymore at stream creation time. eg noformat stream create foo definition http port bar log noformat ",NULL,"It seems that no option validation being Spring<span class='highlight-text severity-high'> or </span>jsr 303 is happening anymore at stream creation time. eg noformat stream create foo definition http port bar log noformat ","atomic","conjunctions","high",False
18822,"It seems that no option validation being Spring or jsr 303 is happening anymore at stream creation time. eg noformat stream create foo definition http port bar log noformat ",NULL,"It seems that no option validation being Spring or jsr 303 is happening anymore at stream creation time. eg noformat stream create foo definition http port bar log noformat ",NULL,"It seems that no option validation being Spring or jsr 303 is happening anymore at stream creation time<span class='highlight-text severity-high'>. eg noformat stream create foo definition http port bar log noformat </span>","minimal","punctuation","high",False
18817,"This should include some guidance on setting rabbit message bus paramters relating to prefetch and concurrency. It should also discuss the bypass functionality or reference another section that covers it. We should probably include how to scale out http sources, e.g. the need to use a load balancer.",NULL,"This should include some guidance on setting rabbit message bus paramters relating to prefetch and concurrency. It should also discuss the bypass functionality or reference another section that covers it. We should probably include how to scale out http sources, e.g. the need to use a load balancer.",NULL,"Add for who this story is","well_formed","no_role","high",False
18817,"This should include some guidance on setting rabbit message bus paramters relating to prefetch and concurrency. It should also discuss the bypass functionality or reference another section that covers it. We should probably include how to scale out http sources, e.g. the need to use a load balancer.",NULL,"This should include some guidance on setting rabbit message bus paramters relating to prefetch and concurrency. It should also discuss the bypass functionality or reference another section that covers it. We should probably include how to scale out http sources, e.g. the need to use a load balancer.",NULL,"This should include some guidance on setting rabbit message bus paramters relating to prefetch<span class='highlight-text severity-high'> and </span>concurrency. It should also discuss the bypass functionality<span class='highlight-text severity-high'> or </span>reference another section that covers it. We should probably include how to scale out http sources, e.g. the need to use a load balancer.","atomic","conjunctions","high",False
18817,"This should include some guidance on setting rabbit message bus paramters relating to prefetch and concurrency. It should also discuss the bypass functionality or reference another section that covers it. We should probably include how to scale out http sources, e.g. the need to use a load balancer.",NULL,"This should include some guidance on setting rabbit message bus paramters relating to prefetch and concurrency. It should also discuss the bypass functionality or reference another section that covers it. We should probably include how to scale out http sources, e.g. the need to use a load balancer.",NULL,"This should include some guidance on setting rabbit message bus paramters relating to prefetch and concurrency<span class='highlight-text severity-high'>. It should also discuss the bypass functionality or reference another section that covers it. We should probably include how to scale out http sources, e.g. the need to use a load balancer.</span>","minimal","punctuation","high",False
18818,"See https jira.spring.io browse XD 1684 Requires update to gradle 2.1",NULL,"See https jira.spring.io browse XD 1684 Requires update to gradle 2.1",NULL,"Add for who this story is","well_formed","no_role","high",False
18819,"Use external JVM launch support provided by the Oracle Tools framework https java.net projects oracletools .",NULL,"Use external JVM launch support provided by the Oracle Tools framework https java.net projects oracletools .",NULL,"Add for who this story is","well_formed","no_role","high",False
18823,"Also, the approach may not work as expected on windows.",NULL,"Also, the approach may not work as expected on windows.",NULL,"Add for who this story is","well_formed","no_role","high",False
18829,"Note that the documentation for the module options in particular for http client should be autogenerated using the following syntax see others noformat ^processor.http client processor.http client noformat ",NULL,"Note that the documentation for the module options in particular for http client should be autogenerated using the following syntax see others noformat ^processor.http client processor.http client noformat ",NULL,"Add for who this story is","well_formed","no_role","high",False
18828,"The script tests does the following. code Filter for good and bad create stream httpfilter http good filter expression jsonPath payload, .entities.hashtags .text .contains good aftergood filter expression true bad filter expression jsonPath payload, .entities.hashtags .text .contains bad goodandbad splitter expression jsonPath payload, .id file dir TEST DIR true code ",NULL,"The script tests does the following. code Filter for good and bad create stream httpfilter http good filter expression jsonPath payload, .entities.hashtags .text .contains good aftergood filter expression true bad filter expression jsonPath payload, .entities.hashtags .text .contains bad goodandbad splitter expression jsonPath payload, .id file dir TEST DIR true code ",NULL,"Add for who this story is","well_formed","no_role","high",False
18828,"The script tests does the following. code Filter for good and bad create stream httpfilter http good filter expression jsonPath payload, .entities.hashtags .text .contains good aftergood filter expression true bad filter expression jsonPath payload, .entities.hashtags .text .contains bad goodandbad splitter expression jsonPath payload, .id file dir TEST DIR true code ",NULL,"The script tests does the following. code Filter for good and bad create stream httpfilter http good filter expression jsonPath payload, .entities.hashtags .text .contains good aftergood filter expression true bad filter expression jsonPath payload, .entities.hashtags .text .contains bad goodandbad splitter expression jsonPath payload, .id file dir TEST DIR true code ",NULL,"The script tests does the following. code Filter for good<span class='highlight-text severity-high'> and </span>bad create stream httpfilter http good filter expression jsonPath payload, .entities.hashtags .text .contains good aftergood filter expression true bad filter expression jsonPath payload, .entities.hashtags .text .contains bad goodandbad splitter expression jsonPath payload, .id file dir TEST DIR true code ","atomic","conjunctions","high",False
18828,"The script tests does the following. code Filter for good and bad create stream httpfilter http good filter expression jsonPath payload, .entities.hashtags .text .contains good aftergood filter expression true bad filter expression jsonPath payload, .entities.hashtags .text .contains bad goodandbad splitter expression jsonPath payload, .id file dir TEST DIR true code ",NULL,"The script tests does the following. code Filter for good and bad create stream httpfilter http good filter expression jsonPath payload, .entities.hashtags .text .contains good aftergood filter expression true bad filter expression jsonPath payload, .entities.hashtags .text .contains bad goodandbad splitter expression jsonPath payload, .id file dir TEST DIR true code ",NULL,"The script tests does the following<span class='highlight-text severity-high'>. code Filter for good and bad create stream httpfilter http good filter expression jsonPath payload, .entities.hashtags .text .contains good aftergood filter expression true bad filter expression jsonPath payload, .entities.hashtags .text .contains bad goodandbad splitter expression jsonPath payload, .id file dir TEST DIR true code </span>","minimal","punctuation","high",False
18836,"Going forward it seems that providing Hadoop v1 will be of lesser importance and we might as well drop it now. SHDP 2.1 will also drop any v1 support. Remove support for hadoop12 Apache Hadoop 1.2.1 cdh4 Cloudera CDH 4.6.0 hdp13 Hortonworks Data Platform 1.3 Keep hadoop22 Apache Hadoop 2.2.0 default phd1 Pivotal HD 1.1 phd20 Pivotal HD 2.0 cdh5 Cloudera CDH 5.0.0 hdp21 Hortonworks Data Platform 2.1 This should make configuration and documentation easier too. Not to mention testing. This affects startup scripts and the shell plus the build script.",NULL,"Going forward it seems that providing Hadoop v1 will be of lesser importance and we might as well drop it now. SHDP 2.1 will also drop any v1 support. Remove support for hadoop12 Apache Hadoop 1.2.1 cdh4 Cloudera CDH 4.6.0 hdp13 Hortonworks Data Platform 1.3 Keep hadoop22 Apache Hadoop 2.2.0 default phd1 Pivotal HD 1.1 phd20 Pivotal HD 2.0 cdh5 Cloudera CDH 5.0.0 hdp21 Hortonworks Data Platform 2.1 This should make configuration and documentation easier too. Not to mention testing. This affects startup scripts and the shell plus the build script.",NULL,"Add for who this story is","well_formed","no_role","high",False
18949,"This will allow us to use multiple instances of hdfs file sinks and not have any filename path collisions.",NULL,"This will allow us to use multiple instances of hdfs file sinks and not have any filename path collisions.",NULL,"This will allow us to use multiple instances of hdfs file sinks<span class='highlight-text severity-high'> and </span>not have any filename path collisions.","atomic","conjunctions","high",False
18946,"See XD 1283. We ve been waiting for 1283 to change constructs like noformat attr name xd.stream.name to just noformat attr name noformat Turns out we can simply push down the xd.stream.name bit in the default value most likely initialization of a field in a POJO metadata class and it will work just fine. We can also consider providing a fake value for those placeholders to use when doing module info ie user will see name of the stream instead of xd.stream.name ",NULL,"See XD 1283. We ve been waiting for 1283 to change constructs like noformat attr name xd.stream.name to just noformat attr name noformat Turns out we can simply push down the xd.stream.name bit in the default value most likely initialization of a field in a POJO metadata class and it will work just fine. We can also consider providing a fake value for those placeholders to use when doing module info ie user will see name of the stream instead of xd.stream.name ",NULL,"Add for who this story is","well_formed","no_role","high",False
18836,"Going forward it seems that providing Hadoop v1 will be of lesser importance and we might as well drop it now. SHDP 2.1 will also drop any v1 support. Remove support for hadoop12 Apache Hadoop 1.2.1 cdh4 Cloudera CDH 4.6.0 hdp13 Hortonworks Data Platform 1.3 Keep hadoop22 Apache Hadoop 2.2.0 default phd1 Pivotal HD 1.1 phd20 Pivotal HD 2.0 cdh5 Cloudera CDH 5.0.0 hdp21 Hortonworks Data Platform 2.1 This should make configuration and documentation easier too. Not to mention testing. This affects startup scripts and the shell plus the build script.",NULL,"Going forward it seems that providing Hadoop v1 will be of lesser importance and we might as well drop it now. SHDP 2.1 will also drop any v1 support. Remove support for hadoop12 Apache Hadoop 1.2.1 cdh4 Cloudera CDH 4.6.0 hdp13 Hortonworks Data Platform 1.3 Keep hadoop22 Apache Hadoop 2.2.0 default phd1 Pivotal HD 1.1 phd20 Pivotal HD 2.0 cdh5 Cloudera CDH 5.0.0 hdp21 Hortonworks Data Platform 2.1 This should make configuration and documentation easier too. Not to mention testing. This affects startup scripts and the shell plus the build script.",NULL,"Going forward it seems that providing Hadoop v1 will be of lesser importance<span class='highlight-text severity-high'> and </span>we might as well drop it now. SHDP 2.1 will also drop any v1 support. Remove support for hadoop12 Apache Hadoop 1.2.1 cdh4 Cloudera CDH 4.6.0 hdp13 Hortonworks Data Platform 1.3 Keep hadoop22 Apache Hadoop 2.2.0 default phd1 Pivotal HD 1.1 phd20 Pivotal HD 2.0 cdh5 Cloudera CDH 5.0.0 hdp21 Hortonworks Data Platform 2.1 This should make configuration and documentation easier too. Not to mention testing. This affects startup scripts and the shell plus the build script.","atomic","conjunctions","high",False
18836,"Going forward it seems that providing Hadoop v1 will be of lesser importance and we might as well drop it now. SHDP 2.1 will also drop any v1 support. Remove support for hadoop12 Apache Hadoop 1.2.1 cdh4 Cloudera CDH 4.6.0 hdp13 Hortonworks Data Platform 1.3 Keep hadoop22 Apache Hadoop 2.2.0 default phd1 Pivotal HD 1.1 phd20 Pivotal HD 2.0 cdh5 Cloudera CDH 5.0.0 hdp21 Hortonworks Data Platform 2.1 This should make configuration and documentation easier too. Not to mention testing. This affects startup scripts and the shell plus the build script.",NULL,"Going forward it seems that providing Hadoop v1 will be of lesser importance and we might as well drop it now. SHDP 2.1 will also drop any v1 support. Remove support for hadoop12 Apache Hadoop 1.2.1 cdh4 Cloudera CDH 4.6.0 hdp13 Hortonworks Data Platform 1.3 Keep hadoop22 Apache Hadoop 2.2.0 default phd1 Pivotal HD 1.1 phd20 Pivotal HD 2.0 cdh5 Cloudera CDH 5.0.0 hdp21 Hortonworks Data Platform 2.1 This should make configuration and documentation easier too. Not to mention testing. This affects startup scripts and the shell plus the build script.",NULL,"Going forward it seems that providing Hadoop v1 will be of lesser importance and we might as well drop it now<span class='highlight-text severity-high'>. SHDP 2.1 will also drop any v1 support. Remove support for hadoop12 Apache Hadoop 1.2.1 cdh4 Cloudera CDH 4.6.0 hdp13 Hortonworks Data Platform 1.3 Keep hadoop22 Apache Hadoop 2.2.0 default phd1 Pivotal HD 1.1 phd20 Pivotal HD 2.0 cdh5 Cloudera CDH 5.0.0 hdp21 Hortonworks Data Platform 2.1 This should make configuration and documentation easier too. Not to mention testing. This affects startup scripts and the shell plus the build script.</span>","minimal","punctuation","high",False
18830,"Investigate Job Executions list page load timing based on number of job executions to load. The investigation can be of the following steps 1 Return all the size restrictions to retrieve the number of job executions. 2 Setup 5, 10, 100, 500, 1000 number of job executions and measure the page load timings. Based on this, we can address the paging support mentioned in XD 1864.",NULL,"Investigate Job Executions list page load timing based on number of job executions to load. The investigation can be of the following steps 1 Return all the size restrictions to retrieve the number of job executions. 2 Setup 5, 10, 100, 500, 1000 number of job executions and measure the page load timings. Based on this, we can address the paging support mentioned in XD 1864.",NULL,"Add for who this story is","well_formed","no_role","high",False
18830,"Investigate Job Executions list page load timing based on number of job executions to load. The investigation can be of the following steps 1 Return all the size restrictions to retrieve the number of job executions. 2 Setup 5, 10, 100, 500, 1000 number of job executions and measure the page load timings. Based on this, we can address the paging support mentioned in XD 1864.",NULL,"Investigate Job Executions list page load timing based on number of job executions to load. The investigation can be of the following steps 1 Return all the size restrictions to retrieve the number of job executions. 2 Setup 5, 10, 100, 500, 1000 number of job executions and measure the page load timings. Based on this, we can address the paging support mentioned in XD 1864.",NULL,"Investigate Job Executions list page load timing based on number of job executions to load. The investigation can be of the following steps 1 Return all the size restrictions to retrieve the number of job executions. 2 Setup 5, 10, 100, 500, 1000 number of job executions<span class='highlight-text severity-high'> and </span>measure the page load timings. Based on this, we can address the paging support mentioned in XD 1864.","atomic","conjunctions","high",False
18830,"Investigate Job Executions list page load timing based on number of job executions to load. The investigation can be of the following steps 1 Return all the size restrictions to retrieve the number of job executions. 2 Setup 5, 10, 100, 500, 1000 number of job executions and measure the page load timings. Based on this, we can address the paging support mentioned in XD 1864.",NULL,"Investigate Job Executions list page load timing based on number of job executions to load. The investigation can be of the following steps 1 Return all the size restrictions to retrieve the number of job executions. 2 Setup 5, 10, 100, 500, 1000 number of job executions and measure the page load timings. Based on this, we can address the paging support mentioned in XD 1864.",NULL,"Investigate Job Executions list page load timing based on number of job executions to load<span class='highlight-text severity-high'>. The investigation can be of the following steps 1 Return all the size restrictions to retrieve the number of job executions. 2 Setup 5, 10, 100, 500, 1000 number of job executions and measure the page load timings. Based on this, we can address the paging support mentioned in XD 1864.</span>","minimal","punctuation","high",False
18832,"Need a way for end user to package and add custom modules scripts when deploying XD on YARN. Currently we have a zip file containing all code including modules. It s not convenient to un zip re zip this archive to add custom modules scripts. See https github.com spring projects spring xd issues 931",NULL,"Need a way for end user to package and add custom modules scripts when deploying XD on YARN. Currently we have a zip file containing all code including modules. It s not convenient to un zip re zip this archive to add custom modules scripts. See https github.com spring projects spring xd issues 931",NULL,"Add for who this story is","well_formed","no_role","high",False
18832,"Need a way for end user to package and add custom modules scripts when deploying XD on YARN. Currently we have a zip file containing all code including modules. It s not convenient to un zip re zip this archive to add custom modules scripts. See https github.com spring projects spring xd issues 931",NULL,"Need a way for end user to package and add custom modules scripts when deploying XD on YARN. Currently we have a zip file containing all code including modules. It s not convenient to un zip re zip this archive to add custom modules scripts. See https github.com spring projects spring xd issues 931",NULL,"Need a way for end user to package<span class='highlight-text severity-high'> and </span>add custom modules scripts when deploying XD on YARN. Currently we have a zip file containing all code including modules. It s not convenient to un zip re zip this archive to add custom modules scripts. See https github.com spring projects spring xd issues 931","atomic","conjunctions","high",False
18832,"Need a way for end user to package and add custom modules scripts when deploying XD on YARN. Currently we have a zip file containing all code including modules. It s not convenient to un zip re zip this archive to add custom modules scripts. See https github.com spring projects spring xd issues 931",NULL,"Need a way for end user to package and add custom modules scripts when deploying XD on YARN. Currently we have a zip file containing all code including modules. It s not convenient to un zip re zip this archive to add custom modules scripts. See https github.com spring projects spring xd issues 931",NULL,"Need a way for end user to package and add custom modules scripts when deploying XD on YARN<span class='highlight-text severity-high'>. Currently we have a zip file containing all code including modules. It s not convenient to un zip re zip this archive to add custom modules scripts. See https github.com spring projects spring xd issues 931</span>","minimal","punctuation","high",False
18834,"See impacted code at https github.com spring projects spring xd pull 951",NULL,"See impacted code at https github.com spring projects spring xd pull 951",NULL,"Add for who this story is","well_formed","no_role","high",False
18833,"Currently, jobs definitions and batch jobs offer similar info related to the job configuration info. The former comes from ZKJobDefinitionRepository while the latter comes from Batch Job Repository. We need to combine this together so that it is not confusing to the end user.",NULL,"Currently, jobs definitions and batch jobs offer similar info related to the job configuration info. The former comes from ZKJobDefinitionRepository while the latter comes from Batch Job Repository. We need to combine this together","so that it is not confusing to the end user.","Add for who this story is","well_formed","no_role","high",False
18833,"Currently, jobs definitions and batch jobs offer similar info related to the job configuration info. The former comes from ZKJobDefinitionRepository while the latter comes from Batch Job Repository. We need to combine this together so that it is not confusing to the end user.",NULL,"Currently, jobs definitions and batch jobs offer similar info related to the job configuration info. The former comes from ZKJobDefinitionRepository while the latter comes from Batch Job Repository. We need to combine this together","so that it is not confusing to the end user.","Currently, jobs definitions and batch jobs offer similar info related to the job configuration info<span class='highlight-text severity-high'>. The former comes from ZKJobDefinitionRepository while the latter comes from Batch Job Repository. We need to combine this together so that it is not confusing to the end user.</span>","minimal","punctuation","high",False
18833,"Currently, jobs definitions and batch jobs offer similar info related to the job configuration info. The former comes from ZKJobDefinitionRepository while the latter comes from Batch Job Repository. We need to combine this together so that it is not confusing to the end user.",NULL,"Currently, jobs definitions and batch jobs offer similar info related to the job configuration info. The former comes from ZKJobDefinitionRepository while the latter comes from Batch Job Repository. We need to combine this together","so that it is not confusing to the end user.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18837,"The Tap fixture does not need to inherit from AbstractModuleFixture Replace moduleName method with moduleToTap. The current tap syntax is tap stream streamname . modulelabel and not tap stream streamname . modulelabel . modulename as currently implemented by the label fixture. ",NULL,"The Tap fixture does not need to inherit from AbstractModuleFixture Replace moduleName method with moduleToTap. The current tap syntax is tap stream streamname . modulelabel and not tap stream streamname . modulelabel . modulename as currently implemented by the label fixture. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18837,"The Tap fixture does not need to inherit from AbstractModuleFixture Replace moduleName method with moduleToTap. The current tap syntax is tap stream streamname . modulelabel and not tap stream streamname . modulelabel . modulename as currently implemented by the label fixture. ",NULL,"The Tap fixture does not need to inherit from AbstractModuleFixture Replace moduleName method with moduleToTap. The current tap syntax is tap stream streamname . modulelabel and not tap stream streamname . modulelabel . modulename as currently implemented by the label fixture. ",NULL,"The Tap fixture does not need to inherit from AbstractModuleFixture Replace moduleName method with moduleToTap. The current tap syntax is tap stream streamname . modulelabel<span class='highlight-text severity-high'> and </span>not tap stream streamname . modulelabel . modulename as currently implemented by the label fixture. ","atomic","conjunctions","high",False
18837,"The Tap fixture does not need to inherit from AbstractModuleFixture Replace moduleName method with moduleToTap. The current tap syntax is tap stream streamname . modulelabel and not tap stream streamname . modulelabel . modulename as currently implemented by the label fixture. ",NULL,"The Tap fixture does not need to inherit from AbstractModuleFixture Replace moduleName method with moduleToTap. The current tap syntax is tap stream streamname . modulelabel and not tap stream streamname . modulelabel . modulename as currently implemented by the label fixture. ",NULL,"The Tap fixture does not need to inherit from AbstractModuleFixture Replace moduleName method with moduleToTap<span class='highlight-text severity-high'>. The current tap syntax is tap stream streamname . modulelabel and not tap stream streamname . modulelabel . modulename as currently implemented by the label fixture. </span>","minimal","punctuation","high",False
18835,"We should have an fsUri parameter for hdfs and hdfs dataset sinks so we can write to different file systems hdfs, webhdfs ",NULL,"We should have an fsUri parameter for hdfs and hdfs dataset sinks","so we can write to different file systems hdfs, webhdfs","Add for who this story is","well_formed","no_role","high",False
18835,"We should have an fsUri parameter for hdfs and hdfs dataset sinks so we can write to different file systems hdfs, webhdfs ",NULL,"We should have an fsUri parameter for hdfs and hdfs dataset sinks","so we can write to different file systems hdfs, webhdfs","We should have an fsUri parameter for hdfs<span class='highlight-text severity-high'> and </span>hdfs dataset sinks so we can write to different file systems hdfs, webhdfs ","atomic","conjunctions","high",False
18835,"We should have an fsUri parameter for hdfs and hdfs dataset sinks so we can write to different file systems hdfs, webhdfs ",NULL,"We should have an fsUri parameter for hdfs and hdfs dataset sinks","so we can write to different file systems hdfs, webhdfs","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18838,"currently we have batch and jobs. Everything should move to jobs. See https github.com spring projects spring xd wiki REST API for details.",NULL,"currently we have batch and jobs. Everything should move to jobs. See https github.com spring projects spring xd wiki REST API for details.",NULL,"Add for who this story is","well_formed","no_role","high",False
18838,"currently we have batch and jobs. Everything should move to jobs. See https github.com spring projects spring xd wiki REST API for details.",NULL,"currently we have batch and jobs. Everything should move to jobs. See https github.com spring projects spring xd wiki REST API for details.",NULL,"currently we have batch and jobs<span class='highlight-text severity-high'>. Everything should move to jobs. See https github.com spring projects spring xd wiki REST API for details.</span>","minimal","punctuation","high",False
18831,"Update documentation related to database migration with the changes from XD 1822",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
18831,"Update documentation related to database migration with the changes from XD 1822",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
18826,"The test https github.com spring projects spring xd blob master src test scripts httpbash is very simple, it doesn t even check the results. A small change to https github.com spring projects spring xd blob master spring xd test fixtures src main java org springframework xd test generator SimpleHttpGenerator.java so that number of messages to post is specified would be part of this work.",NULL,"The test https github.com spring projects spring xd blob master src test scripts httpbash is very simple, it doesn t even check the results. A small change to https github.com spring projects spring xd blob master spring xd test fixtures src main java org springframework xd test generator SimpleHttpGenerator.java","so that number of messages to post is specified would be part of this work.","Add for who this story is","well_formed","no_role","high",False
18826,"The test https github.com spring projects spring xd blob master src test scripts httpbash is very simple, it doesn t even check the results. A small change to https github.com spring projects spring xd blob master spring xd test fixtures src main java org springframework xd test generator SimpleHttpGenerator.java so that number of messages to post is specified would be part of this work.",NULL,"The test https github.com spring projects spring xd blob master src test scripts httpbash is very simple, it doesn t even check the results. A small change to https github.com spring projects spring xd blob master spring xd test fixtures src main java org springframework xd test generator SimpleHttpGenerator.java","so that number of messages to post is specified would be part of this work.","The test https github<span class='highlight-text severity-high'>.com spring projects spring xd blob master src test scripts httpbash is very simple, it doesn t even check the results. A small change to https github.com spring projects spring xd blob master spring xd test fixtures src main java org springframework xd test generator SimpleHttpGenerator.java so that number of messages to post is specified would be part of this work.</span>","minimal","punctuation","high",False
18826,"The test https github.com spring projects spring xd blob master src test scripts httpbash is very simple, it doesn t even check the results. A small change to https github.com spring projects spring xd blob master spring xd test fixtures src main java org springframework xd test generator SimpleHttpGenerator.java so that number of messages to post is specified would be part of this work.",NULL,"The test https github.com spring projects spring xd blob master src test scripts httpbash is very simple, it doesn t even check the results. A small change to https github.com spring projects spring xd blob master spring xd test fixtures src main java org springframework xd test generator SimpleHttpGenerator.java","so that number of messages to post is specified would be part of this work.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18847,"This issue could be more involved. Proper pagination may not be implemented correctly by the REST controller making the respective service call . This would also necessitate some form of improved state management for the UI. E.g. User is on page 5 of the listing of Job Executions User views details User presses the back button on the screen The the listing of Job Executions should be still on page 5 ",NULL,"This issue could be more involved. Proper pagination may not be implemented correctly by the REST controller making the respective service call . This would also necessitate some form of improved state management for the UI. E.g. User is on page 5 of the listing of Job Executions User views details User presses the back button on the screen The the listing of Job Executions should be still on page 5 ",NULL,"Add for who this story is","well_formed","no_role","high",False
18847,"This issue could be more involved. Proper pagination may not be implemented correctly by the REST controller making the respective service call . This would also necessitate some form of improved state management for the UI. E.g. User is on page 5 of the listing of Job Executions User views details User presses the back button on the screen The the listing of Job Executions should be still on page 5 ",NULL,"This issue could be more involved. Proper pagination may not be implemented correctly by the REST controller making the respective service call . This would also necessitate some form of improved state management for the UI. E.g. User is on page 5 of the listing of Job Executions User views details User presses the back button on the screen The the listing of Job Executions should be still on page 5 ",NULL,"This issue could be more involved<span class='highlight-text severity-high'>. Proper pagination may not be implemented correctly by the REST controller making the respective service call . This would also necessitate some form of improved state management for the UI. E.g. User is on page 5 of the listing of Job Executions User views details User presses the back button on the screen The the listing of Job Executions should be still on page 5 </span>","minimal","punctuation","high",False
18840,"See the design document https docs.google.com a gopivotal.com document d 1kWtoH xEF1wMklzQ8AZaiuhBZWIlpCDi8G9 hAP8Fgc edit heading h.2rk74f16ow4i for more details. This class or perhaps a method on a repository? will need to query ZooKeeper to obtain the state of a stream job in order to pass it along to the REST controller.",NULL,"See the design document https docs.google.com a gopivotal.com document d 1kWtoH xEF1wMklzQ8AZaiuhBZWIlpCDi8G9 hAP8Fgc edit heading h.2rk74f16ow4i for more details. This class or perhaps a method on a repository? will need to query ZooKeeper to obtain the state of a stream job","in order to pass it along to the REST controller.","Add for who this story is","well_formed","no_role","high",False
19363,"related to XD 779. We need the ability to provide JSON serializable JobExecution information. Change from using JavaSerialization back to returning objects ",NULL,NULL,NULL,"related to XD 779<span class='highlight-text severity-high'>. We need the ability to provide JSON serializable JobExecution information. Change from using JavaSerialization back to returning objects </span>","minimal","punctuation","high",False
18840,"See the design document https docs.google.com a gopivotal.com document d 1kWtoH xEF1wMklzQ8AZaiuhBZWIlpCDi8G9 hAP8Fgc edit heading h.2rk74f16ow4i for more details. This class or perhaps a method on a repository? will need to query ZooKeeper to obtain the state of a stream job in order to pass it along to the REST controller.",NULL,"See the design document https docs.google.com a gopivotal.com document d 1kWtoH xEF1wMklzQ8AZaiuhBZWIlpCDi8G9 hAP8Fgc edit heading h.2rk74f16ow4i for more details. This class or perhaps a method on a repository? will need to query ZooKeeper to obtain the state of a stream job","in order to pass it along to the REST controller.","See the design document https docs<span class='highlight-text severity-high'>.google.com a gopivotal.com document d 1kWtoH xEF1wMklzQ8AZaiuhBZWIlpCDi8G9 hAP8Fgc edit heading h.2rk74f16ow4i for more details. This class or perhaps a method on a repository? will need to query ZooKeeper to obtain the state of a stream job in order to pass it along to the REST controller.</span>","minimal","punctuation","high",False
18840,"See the design document https docs.google.com a gopivotal.com document d 1kWtoH xEF1wMklzQ8AZaiuhBZWIlpCDi8G9 hAP8Fgc edit heading h.2rk74f16ow4i for more details. This class or perhaps a method on a repository? will need to query ZooKeeper to obtain the state of a stream job in order to pass it along to the REST controller.",NULL,"See the design document https docs.google.com a gopivotal.com document d 1kWtoH xEF1wMklzQ8AZaiuhBZWIlpCDi8G9 hAP8Fgc edit heading h.2rk74f16ow4i for more details. This class or perhaps a method on a repository? will need to query ZooKeeper to obtain the state of a stream job","in order to pass it along to the REST controller.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18842,"This needs closer inspection, but here are some things that currently do not work, either at the parser level, or at actual deployment time noformat xd module compose foo definition queue bar filter Command failed org.springframework.xd.rest.client.impl.SpringXDException Could not find module with name filter and type sink xd module compose foolog definition queue foo log Successfully created module foolog with type sink should fail not a module, but a full stream xd module compose foo definition queue bar filter transform Successfully created module foo with type processor should be source noformat ",NULL,"This needs closer inspection, but here are some things that currently do not work, either at the parser level, or at actual deployment time noformat xd module compose foo definition queue bar filter Command failed org.springframework.xd.rest.client.impl.SpringXDException Could not find module with name filter and type sink xd module compose foolog definition queue foo log Successfully created module foolog with type sink should fail not a module, but a full stream xd module compose foo definition queue bar filter transform Successfully created module foo with type processor should be source noformat ",NULL,"Add for who this story is","well_formed","no_role","high",False
18842,"This needs closer inspection, but here are some things that currently do not work, either at the parser level, or at actual deployment time noformat xd module compose foo definition queue bar filter Command failed org.springframework.xd.rest.client.impl.SpringXDException Could not find module with name filter and type sink xd module compose foolog definition queue foo log Successfully created module foolog with type sink should fail not a module, but a full stream xd module compose foo definition queue bar filter transform Successfully created module foo with type processor should be source noformat ",NULL,"This needs closer inspection, but here are some things that currently do not work, either at the parser level, or at actual deployment time noformat xd module compose foo definition queue bar filter Command failed org.springframework.xd.rest.client.impl.SpringXDException Could not find module with name filter and type sink xd module compose foolog definition queue foo log Successfully created module foolog with type sink should fail not a module, but a full stream xd module compose foo definition queue bar filter transform Successfully created module foo with type processor should be source noformat ",NULL,"This needs closer inspection, but here are some things that currently do not work, either at the parser level,<span class='highlight-text severity-high'> or </span>at actual deployment time noformat xd module compose foo definition queue bar filter Command failed org.springframework.xd.rest.client.impl.SpringXDException Could not find module with name filter<span class='highlight-text severity-high'> and </span>type sink xd module compose foolog definition queue foo log Successfully created module foolog with type sink should fail not a module, but a full stream xd module compose foo definition queue bar filter transform Successfully created module foo with type processor should be source noformat ","atomic","conjunctions","high",False
18849,"Generate asciidoc fragments for each module s options, this way it is always up to date.",NULL,"Generate asciidoc fragments for each module s options, this way it is always up to date.",NULL,"Add for who this story is","well_formed","no_role","high",False
18845,"The JMS Source Sink has a pluggable provider default activemq but the URL property amqUrl implies activeMQ the property name should be generic found while testing XD 1149 .",NULL,"The JMS Source Sink has a pluggable provider default activemq but the URL property amqUrl implies activeMQ the property name should be generic found while testing XD 1149 .",NULL,"Add for who this story is","well_formed","no_role","high",False
18848,"Currently, there are JOB REGISTRY NAMES, JOB REGISTRY RESTARTABLES and JOB REGISTRY INCREMENTABLES tables and we can possibly combine them into one table and have a better schema for this.",NULL,"Currently, there are JOB REGISTRY NAMES, JOB REGISTRY RESTARTABLES and JOB REGISTRY INCREMENTABLES tables and we can possibly combine them into one table and have a better schema for this.",NULL,"Add for who this story is","well_formed","no_role","high",False
18848,"Currently, there are JOB REGISTRY NAMES, JOB REGISTRY RESTARTABLES and JOB REGISTRY INCREMENTABLES tables and we can possibly combine them into one table and have a better schema for this.",NULL,"Currently, there are JOB REGISTRY NAMES, JOB REGISTRY RESTARTABLES and JOB REGISTRY INCREMENTABLES tables and we can possibly combine them into one table and have a better schema for this.",NULL,"Currently, there are JOB REGISTRY NAMES, JOB REGISTRY RESTARTABLES<span class='highlight-text severity-high'> and </span>JOB REGISTRY INCREMENTABLES tables and we can possibly combine them into one table and have a better schema for this.","atomic","conjunctions","high",False
18850,"Pass module properties from stream plugin to MessageBusAwareChannelResolver . Disallow partitioning properties.",NULL,"Pass module properties from stream plugin to MessageBusAwareChannelResolver . Disallow partitioning properties.",NULL,"Add for who this story is","well_formed","no_role","high",False
18850,"Pass module properties from stream plugin to MessageBusAwareChannelResolver . Disallow partitioning properties.",NULL,"Pass module properties from stream plugin to MessageBusAwareChannelResolver . Disallow partitioning properties.",NULL,"Pass module properties from stream plugin to MessageBusAwareChannelResolver <span class='highlight-text severity-high'>. Disallow partitioning properties.</span>","minimal","punctuation","high",False
18843,"With XD 1311, the job execution list shows the definition deployment status of the associated job. We need to show the same information for a given job execution.",NULL,"With XD 1311, the job execution list shows the definition deployment status of the associated job. We need to show the same information for a given job execution.",NULL,"Add for who this story is","well_formed","no_role","high",False
18843,"With XD 1311, the job execution list shows the definition deployment status of the associated job. We need to show the same information for a given job execution.",NULL,"With XD 1311, the job execution list shows the definition deployment status of the associated job. We need to show the same information for a given job execution.",NULL,"With XD 1311, the job execution list shows the definition deployment status of the associated job<span class='highlight-text severity-high'>. We need to show the same information for a given job execution.</span>","minimal","punctuation","high",False
18839,"See the design document https docs.google.com a gopivotal.com document d 1kWtoH xEF1wMklzQ8AZaiuhBZWIlpCDi8G9 hAP8Fgc edit heading h.2rk74f16ow4i for more details. The REST controller needs to be modified to obtain stream job state once it is available in ZooKeeper. This depends on XD 1847.",NULL,"See the design document https docs.google.com a gopivotal.com document d 1kWtoH xEF1wMklzQ8AZaiuhBZWIlpCDi8G9 hAP8Fgc edit heading h.2rk74f16ow4i for more details. The REST controller needs to be modified to obtain stream job state once it is available in ZooKeeper. This depends on XD 1847.",NULL,"Add for who this story is","well_formed","no_role","high",False
18839,"See the design document https docs.google.com a gopivotal.com document d 1kWtoH xEF1wMklzQ8AZaiuhBZWIlpCDi8G9 hAP8Fgc edit heading h.2rk74f16ow4i for more details. The REST controller needs to be modified to obtain stream job state once it is available in ZooKeeper. This depends on XD 1847.",NULL,"See the design document https docs.google.com a gopivotal.com document d 1kWtoH xEF1wMklzQ8AZaiuhBZWIlpCDi8G9 hAP8Fgc edit heading h.2rk74f16ow4i for more details. The REST controller needs to be modified to obtain stream job state once it is available in ZooKeeper. This depends on XD 1847.",NULL,"See the design document https docs<span class='highlight-text severity-high'>.google.com a gopivotal.com document d 1kWtoH xEF1wMklzQ8AZaiuhBZWIlpCDi8G9 hAP8Fgc edit heading h.2rk74f16ow4i for more details. The REST controller needs to be modified to obtain stream job state once it is available in ZooKeeper. This depends on XD 1847.</span>","minimal","punctuation","high",False
18929,"The first 2 images in the documentation section linked below should no longer show redis, rabbit, or local for the communication between Admins and Containers. Rather we need to show ZooKeeper. https github.com spring projects spring xd wiki Architecture",NULL,"The first 2 images in the documentation section linked below should no longer show redis, rabbit, or local for the communication between Admins and Containers. Rather we need to show ZooKeeper. https github.com spring projects spring xd wiki Architecture",NULL,"Add for who this story is","well_formed","no_role","high",False
18929,"The first 2 images in the documentation section linked below should no longer show redis, rabbit, or local for the communication between Admins and Containers. Rather we need to show ZooKeeper. https github.com spring projects spring xd wiki Architecture",NULL,"The first 2 images in the documentation section linked below should no longer show redis, rabbit, or local for the communication between Admins and Containers. Rather we need to show ZooKeeper. https github.com spring projects spring xd wiki Architecture",NULL,"The first 2 images in the documentation section linked below should no longer show redis, rabbit,<span class='highlight-text severity-high'> or </span>local for the communication between Admins<span class='highlight-text severity-high'> and </span>Containers. Rather we need to show ZooKeeper. https github.com spring projects spring xd wiki Architecture","atomic","conjunctions","high",False
18929,"The first 2 images in the documentation section linked below should no longer show redis, rabbit, or local for the communication between Admins and Containers. Rather we need to show ZooKeeper. https github.com spring projects spring xd wiki Architecture",NULL,"The first 2 images in the documentation section linked below should no longer show redis, rabbit, or local for the communication between Admins and Containers. Rather we need to show ZooKeeper. https github.com spring projects spring xd wiki Architecture",NULL,"The first 2 images in the documentation section linked below should no longer show redis, rabbit, or local for the communication between Admins and Containers<span class='highlight-text severity-high'>. Rather we need to show ZooKeeper. https github.com spring projects spring xd wiki Architecture</span>","minimal","punctuation","high",False
18846,"Support receiving messages from an HA cluster.",NULL,"Support receiving messages from an HA cluster.",NULL,"Add for who this story is","well_formed","no_role","high",False
18854,"So that the code format matches that of the XD Project.",NULL,"So that the code format matches that of the XD Project.",NULL,"Add for who this story is","well_formed","no_role","high",False
18855,"XML is currently required for module definitions. XD should also support Java Config and Groovy bean definitions and potentially, SI DSLs. ",NULL,"XML is currently required for module definitions. XD should also support Java Config and Groovy bean definitions and potentially, SI DSLs. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18855,"XML is currently required for module definitions. XD should also support Java Config and Groovy bean definitions and potentially, SI DSLs. ",NULL,"XML is currently required for module definitions. XD should also support Java Config and Groovy bean definitions and potentially, SI DSLs. ",NULL,"XML is currently required for module definitions<span class='highlight-text severity-high'>. XD should also support Java Config and Groovy bean definitions and potentially, SI DSLs. </span>","minimal","punctuation","high",False
18856,"The XD parser had initial support for substreams, which have been subsumed by composed modules. That legacy code is unused and not needed",NULL,"The XD parser had initial support for substreams, which have been subsumed by composed modules. That legacy code is unused and not needed",NULL,"Add for who this story is","well_formed","no_role","high",False
18856,"The XD parser had initial support for substreams, which have been subsumed by composed modules. That legacy code is unused and not needed",NULL,"The XD parser had initial support for substreams, which have been subsumed by composed modules. That legacy code is unused and not needed",NULL,"The XD parser had initial support for substreams, which have been subsumed by composed modules. That legacy code is unused<span class='highlight-text severity-high'> and </span>not needed","atomic","conjunctions","high",False
18856,"The XD parser had initial support for substreams, which have been subsumed by composed modules. That legacy code is unused and not needed",NULL,"The XD parser had initial support for substreams, which have been subsumed by composed modules. That legacy code is unused and not needed",NULL,"The XD parser had initial support for substreams, which have been subsumed by composed modules<span class='highlight-text severity-high'>. That legacy code is unused and not needed</span>","minimal","punctuation","high",False
18857,"Some REST resources lack an XmlRootElement annotation. This causes a JAXB marshalling error when trying to access the API with an Accept header of xml which is the default in most browsers This is a preliminary to XD 1800 which is much more involved , only to fix ugly exception",NULL,"Some REST resources lack an XmlRootElement annotation. This causes a JAXB marshalling error when trying to access the API with an Accept header of xml which is the default in most browsers This is a preliminary to XD 1800 which is much more involved , only to fix ugly exception",NULL,"Add for who this story is","well_formed","no_role","high",False
18857,"Some REST resources lack an XmlRootElement annotation. This causes a JAXB marshalling error when trying to access the API with an Accept header of xml which is the default in most browsers This is a preliminary to XD 1800 which is much more involved , only to fix ugly exception",NULL,"Some REST resources lack an XmlRootElement annotation. This causes a JAXB marshalling error when trying to access the API with an Accept header of xml which is the default in most browsers This is a preliminary to XD 1800 which is much more involved , only to fix ugly exception",NULL,"Some REST resources lack an XmlRootElement annotation<span class='highlight-text severity-high'>. This causes a JAXB marshalling error when trying to access the API with an Accept header of xml which is the default in most browsers This is a preliminary to XD 1800 which is much more involved , only to fix ugly exception</span>","minimal","punctuation","high",False
18881,"It would be useful to store admin server ip address in ZooKeeper leadership group node xd admins to identify admin server and it s admin port. ",NULL,"It would be useful to store admin server ip address in ZooKeeper leadership group node xd admins to identify admin server and it s admin port. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18881,"It would be useful to store admin server ip address in ZooKeeper leadership group node xd admins to identify admin server and it s admin port. ",NULL,"It would be useful to store admin server ip address in ZooKeeper leadership group node xd admins to identify admin server and it s admin port. ",NULL,"It would be useful to store admin server ip address in ZooKeeper leadership group node xd admins to identify admin server<span class='highlight-text severity-high'> and </span>it s admin port. ","atomic","conjunctions","high",False
18930,"After creating a composed module, I am unable to delete it. Steps to reproduce xd module compose doo definition filter expression payload.contains doo file Successfully created module doo with type sink xd module module compose module delete module display module info module list xd module delete name doo type sink java.lang.StringIndexOutOfBoundsException Failed to convert doo to type QualifiedModuleName for option name, String index out of range 1 xd ",NULL,"After creating a composed module, I am unable to delete it. Steps to reproduce xd module compose doo definition filter expression payload.contains doo file Successfully created module doo with type sink xd module module compose module delete module display module info module list xd module delete name doo type sink java.lang.StringIndexOutOfBoundsException Failed to convert doo to type QualifiedModuleName for option name, String index out of range 1 xd ",NULL,"Add for who this story is","well_formed","no_role","high",False
18930,"After creating a composed module, I am unable to delete it. Steps to reproduce xd module compose doo definition filter expression payload.contains doo file Successfully created module doo with type sink xd module module compose module delete module display module info module list xd module delete name doo type sink java.lang.StringIndexOutOfBoundsException Failed to convert doo to type QualifiedModuleName for option name, String index out of range 1 xd ",NULL,"After creating a composed module, I am unable to delete it. Steps to reproduce xd module compose doo definition filter expression payload.contains doo file Successfully created module doo with type sink xd module module compose module delete module display module info module list xd module delete name doo type sink java.lang.StringIndexOutOfBoundsException Failed to convert doo to type QualifiedModuleName for option name, String index out of range 1 xd ",NULL,"After creating a composed module, I am unable to delete it<span class='highlight-text severity-high'>. Steps to reproduce xd module compose doo definition filter expression payload.contains doo file Successfully created module doo with type sink xd module module compose module delete module display module info module list xd module delete name doo type sink java.lang.StringIndexOutOfBoundsException Failed to convert doo to type QualifiedModuleName for option name, String index out of range 1 xd </span>","minimal","punctuation","high",False
18853,"Add paging support for the appropriate accessor methods in ModuleMetadata Container repositories",NULL,"Add paging support for the appropriate accessor methods in ModuleMetadata Container repositories",NULL,"Add for who this story is","well_formed","no_role","high",False
18851,"If someone wants to have a dedicated location module registry for the custom modules and can be accessed from any Spring supported resource URL location, then we need to support. Currently, we use Delegating ModuleRegistry which uses ResourceModuleRegistry implementations that look for location xd.module.home and classpath modules . Maybe we can add an additional ResourceModuleRegistry with xd.custom.module.home and use it for custom modules.",NULL,"If someone wants to have a dedicated location module registry for the custom modules and can be accessed from any Spring supported resource URL location, then we need to support. Currently, we use Delegating ModuleRegistry which uses ResourceModuleRegistry implementations that look for location xd.module.home and classpath modules . Maybe we can add an additional ResourceModuleRegistry with xd.custom.module.home and use it for custom modules.",NULL,"Add for who this story is","well_formed","no_role","high",False
18851,"If someone wants to have a dedicated location module registry for the custom modules and can be accessed from any Spring supported resource URL location, then we need to support. Currently, we use Delegating ModuleRegistry which uses ResourceModuleRegistry implementations that look for location xd.module.home and classpath modules . Maybe we can add an additional ResourceModuleRegistry with xd.custom.module.home and use it for custom modules.",NULL,"If someone wants to have a dedicated location module registry for the custom modules and can be accessed from any Spring supported resource URL location, then we need to support. Currently, we use Delegating ModuleRegistry which uses ResourceModuleRegistry implementations that look for location xd.module.home and classpath modules . Maybe we can add an additional ResourceModuleRegistry with xd.custom.module.home and use it for custom modules.",NULL,"If someone wants to have a dedicated location module registry for the custom modules<span class='highlight-text severity-high'> and </span>can be accessed from any Spring supported resource URL location, then we need to support. Currently, we use Delegating ModuleRegistry which uses ResourceModuleRegistry implementations that look for location xd.module.home and classpath modules . Maybe we can add an additional ResourceModuleRegistry with xd.custom.module.home and use it for custom modules.","atomic","conjunctions","high",False
18851,"If someone wants to have a dedicated location module registry for the custom modules and can be accessed from any Spring supported resource URL location, then we need to support. Currently, we use Delegating ModuleRegistry which uses ResourceModuleRegistry implementations that look for location xd.module.home and classpath modules . Maybe we can add an additional ResourceModuleRegistry with xd.custom.module.home and use it for custom modules.",NULL,"If someone wants to have a dedicated location module registry for the custom modules and can be accessed from any Spring supported resource URL location, then we need to support. Currently, we use Delegating ModuleRegistry which uses ResourceModuleRegistry implementations that look for location xd.module.home and classpath modules . Maybe we can add an additional ResourceModuleRegistry with xd.custom.module.home and use it for custom modules.",NULL,"If someone wants to have a dedicated location module registry for the custom modules and can be accessed from any Spring supported resource URL location, then we need to support<span class='highlight-text severity-high'>. Currently, we use Delegating ModuleRegistry which uses ResourceModuleRegistry implementations that look for location xd.module.home and classpath modules . Maybe we can add an additional ResourceModuleRegistry with xd.custom.module.home and use it for custom modules.</span>","minimal","punctuation","high",False
18866,"job create bogus definition jdbchdfs sql select from bogus restartable false job deploy bogus job launch bogus http localhost 9393 admin ui jobs executions click Restart Job Execution on the failed job execution get message Job was relaunched container log has 12 36 27,231 ERROR task scheduler 10 handler.LoggingHandler 145 org.springframework.messaging.MessageHandlingException org.springframework.batch.core.repository.JobRestartException JobInstance already exists and is not restartable",NULL,"job create bogus definition jdbchdfs sql select from bogus restartable false job deploy bogus job launch bogus http localhost 9393 admin ui jobs executions click Restart Job Execution on the failed job execution get message Job was relaunched container log has 12 36 27,231 ERROR task scheduler 10 handler.LoggingHandler 145 org.springframework.messaging.MessageHandlingException org.springframework.batch.core.repository.JobRestartException JobInstance already exists and is not restartable",NULL,"Add for who this story is","well_formed","no_role","high",False
18866,"job create bogus definition jdbchdfs sql select from bogus restartable false job deploy bogus job launch bogus http localhost 9393 admin ui jobs executions click Restart Job Execution on the failed job execution get message Job was relaunched container log has 12 36 27,231 ERROR task scheduler 10 handler.LoggingHandler 145 org.springframework.messaging.MessageHandlingException org.springframework.batch.core.repository.JobRestartException JobInstance already exists and is not restartable",NULL,"job create bogus definition jdbchdfs sql select from bogus restartable false job deploy bogus job launch bogus http localhost 9393 admin ui jobs executions click Restart Job Execution on the failed job execution get message Job was relaunched container log has 12 36 27,231 ERROR task scheduler 10 handler.LoggingHandler 145 org.springframework.messaging.MessageHandlingException org.springframework.batch.core.repository.JobRestartException JobInstance already exists and is not restartable",NULL,"job create bogus definition jdbchdfs sql select from bogus restartable false job deploy bogus job launch bogus http localhost 9393 admin ui jobs executions click Restart Job Execution on the failed job execution get message Job was relaunched container log has 12 36 27,231 ERROR task scheduler 10 handler.LoggingHandler 145 org.springframework.messaging.MessageHandlingException org.springframework.batch.core.repository.JobRestartException JobInstance already exists<span class='highlight-text severity-high'> and </span>is not restartable","atomic","conjunctions","high",False
18882,"We don t support using Configuration for modules ATM. The current code was committed during the same time as improvements to handling module configuration. We should switch the reactor ip.xml to include all bean definitions and remove referencing Configuration classes or see how to add support for Configuration. Another short term hack is to put the prefix sink.reactor ip in all Value used in NetServerInboundChannelAdapterConfiguration.",NULL,"We don t support using Configuration for modules ATM. The current code was committed during the same time as improvements to handling module configuration. We should switch the reactor ip.xml to include all bean definitions and remove referencing Configuration classes or see how to add support for Configuration. Another short term hack is to put the prefix sink.reactor ip in all Value used in NetServerInboundChannelAdapterConfiguration.",NULL,"Add for who this story is","well_formed","no_role","high",False
18882,"We don t support using Configuration for modules ATM. The current code was committed during the same time as improvements to handling module configuration. We should switch the reactor ip.xml to include all bean definitions and remove referencing Configuration classes or see how to add support for Configuration. Another short term hack is to put the prefix sink.reactor ip in all Value used in NetServerInboundChannelAdapterConfiguration.",NULL,"We don t support using Configuration for modules ATM. The current code was committed during the same time as improvements to handling module configuration. We should switch the reactor ip.xml to include all bean definitions and remove referencing Configuration classes or see how to add support for Configuration. Another short term hack is to put the prefix sink.reactor ip in all Value used in NetServerInboundChannelAdapterConfiguration.",NULL,"We don t support using Configuration for modules ATM. The current code was committed during the same time as improvements to handling module configuration. We should switch the reactor ip.xml to include all bean definitions<span class='highlight-text severity-high'> and </span>remove referencing Configuration classes<span class='highlight-text severity-high'> or </span>see how to add support for Configuration. Another short term hack is to put the prefix sink.reactor ip in all Value used in NetServerInboundChannelAdapterConfiguration.","atomic","conjunctions","high",False
18882,"We don t support using Configuration for modules ATM. The current code was committed during the same time as improvements to handling module configuration. We should switch the reactor ip.xml to include all bean definitions and remove referencing Configuration classes or see how to add support for Configuration. Another short term hack is to put the prefix sink.reactor ip in all Value used in NetServerInboundChannelAdapterConfiguration.",NULL,"We don t support using Configuration for modules ATM. The current code was committed during the same time as improvements to handling module configuration. We should switch the reactor ip.xml to include all bean definitions and remove referencing Configuration classes or see how to add support for Configuration. Another short term hack is to put the prefix sink.reactor ip in all Value used in NetServerInboundChannelAdapterConfiguration.",NULL,"We don t support using Configuration for modules ATM<span class='highlight-text severity-high'>. The current code was committed during the same time as improvements to handling module configuration. We should switch the reactor ip.xml to include all bean definitions and remove referencing Configuration classes or see how to add support for Configuration. Another short term hack is to put the prefix sink.reactor ip in all Value used in NetServerInboundChannelAdapterConfiguration.</span>","minimal","punctuation","high",False
18861,"stream create foo definition bar baz stream deploy foo properties module.qux.fiz",NULL,"stream create foo definition bar baz stream deploy foo properties module.qux.fiz",NULL,"Add for who this story is","well_formed","no_role","high",False
18859,"Deploy Type Admin Container on EC2 Rabbit Transport SHA 8fba31d Steps to reproduce 1 Using Rabbit 3.3 above create a user that does not have privileges to write to sudo rabbitmqctl add user joe password omit this step sudo rabbitmqctl set permissions p joe . . . 2 Set the rabbitmq user name and password export spring rabbitmq username acctest export spring rabbitmq password acctest23 3 Start container. 4 Create stream or job foo 5 Error will occur 6 delete stream or job foo 7 stop container 8 set privilege sudo rabbitmqctl set permissions p joe . . . 9 Start container 10 create stream or job foo 11 System will report that foo exists.",NULL,"Deploy Type Admin Container on EC2 Rabbit Transport SHA 8fba31d Steps to reproduce 1 Using Rabbit 3.3 above create a user that does not have privileges to write to sudo rabbitmqctl add user joe password omit this step sudo rabbitmqctl set permissions p joe . . . 2 Set the rabbitmq user name and password export spring rabbitmq username acctest export spring rabbitmq password acctest23 3 Start container. 4 Create stream or job foo 5 Error will occur 6 delete stream or job foo 7 stop container 8 set privilege sudo rabbitmqctl set permissions p joe . . . 9 Start container 10 create stream or job foo 11 System will report that foo exists.",NULL,"Add for who this story is","well_formed","no_role","high",False
18859,"Deploy Type Admin Container on EC2 Rabbit Transport SHA 8fba31d Steps to reproduce 1 Using Rabbit 3.3 above create a user that does not have privileges to write to sudo rabbitmqctl add user joe password omit this step sudo rabbitmqctl set permissions p joe . . . 2 Set the rabbitmq user name and password export spring rabbitmq username acctest export spring rabbitmq password acctest23 3 Start container. 4 Create stream or job foo 5 Error will occur 6 delete stream or job foo 7 stop container 8 set privilege sudo rabbitmqctl set permissions p joe . . . 9 Start container 10 create stream or job foo 11 System will report that foo exists.",NULL,"Deploy Type Admin Container on EC2 Rabbit Transport SHA 8fba31d Steps to reproduce 1 Using Rabbit 3.3 above create a user that does not have privileges to write to sudo rabbitmqctl add user joe password omit this step sudo rabbitmqctl set permissions p joe . . . 2 Set the rabbitmq user name and password export spring rabbitmq username acctest export spring rabbitmq password acctest23 3 Start container. 4 Create stream or job foo 5 Error will occur 6 delete stream or job foo 7 stop container 8 set privilege sudo rabbitmqctl set permissions p joe . . . 9 Start container 10 create stream or job foo 11 System will report that foo exists.",NULL,"Deploy Type Admin Container on EC2 Rabbit Transport SHA 8fba31d Steps to reproduce 1 Using Rabbit 3.3 above create a user that does not have privileges to write to sudo rabbitmqctl add user joe password omit this step sudo rabbitmqctl set permissions p joe . . . 2 Set the rabbitmq user name<span class='highlight-text severity-high'> and </span>password export spring rabbitmq username acctest export spring rabbitmq password acctest23 3 Start container. 4 Create stream<span class='highlight-text severity-high'> or </span>job foo 5 Error will occur 6 delete stream or job foo 7 stop container 8 set privilege sudo rabbitmqctl set permissions p joe . . . 9 Start container 10 create stream or job foo 11 System will report that foo exists.","atomic","conjunctions","high",False
18859,"Deploy Type Admin Container on EC2 Rabbit Transport SHA 8fba31d Steps to reproduce 1 Using Rabbit 3.3 above create a user that does not have privileges to write to sudo rabbitmqctl add user joe password omit this step sudo rabbitmqctl set permissions p joe . . . 2 Set the rabbitmq user name and password export spring rabbitmq username acctest export spring rabbitmq password acctest23 3 Start container. 4 Create stream or job foo 5 Error will occur 6 delete stream or job foo 7 stop container 8 set privilege sudo rabbitmqctl set permissions p joe . . . 9 Start container 10 create stream or job foo 11 System will report that foo exists.",NULL,"Deploy Type Admin Container on EC2 Rabbit Transport SHA 8fba31d Steps to reproduce 1 Using Rabbit 3.3 above create a user that does not have privileges to write to sudo rabbitmqctl add user joe password omit this step sudo rabbitmqctl set permissions p joe . . . 2 Set the rabbitmq user name and password export spring rabbitmq username acctest export spring rabbitmq password acctest23 3 Start container. 4 Create stream or job foo 5 Error will occur 6 delete stream or job foo 7 stop container 8 set privilege sudo rabbitmqctl set permissions p joe . . . 9 Start container 10 create stream or job foo 11 System will report that foo exists.",NULL,"Deploy Type Admin Container on EC2 Rabbit Transport SHA 8fba31d Steps to reproduce 1 Using Rabbit 3<span class='highlight-text severity-high'>.3 above create a user that does not have privileges to write to sudo rabbitmqctl add user joe password omit this step sudo rabbitmqctl set permissions p joe . . . 2 Set the rabbitmq user name and password export spring rabbitmq username acctest export spring rabbitmq password acctest23 3 Start container. 4 Create stream or job foo 5 Error will occur 6 delete stream or job foo 7 stop container 8 set privilege sudo rabbitmqctl set permissions p joe . . . 9 Start container 10 create stream or job foo 11 System will report that foo exists.</span>","minimal","punctuation","high",False
18860,"Create OOTB batch job that executes a job on Spark as a tasklet could be something along this job create yarnJob definition sparkjob master spark localhost 7077 class SimpleApp ",NULL,"Create OOTB batch job that executes a job on Spark as a tasklet could be something along this job create yarnJob definition sparkjob master spark localhost 7077 class SimpleApp ",NULL,"Add for who this story is","well_formed","no_role","high",False
18862,"Detect properties the bus doesn t support.",NULL,"Detect properties the bus doesn t support.",NULL,"Add for who this story is","well_formed","no_role","high",False
18863,"PR https github.com spring projects spring xd pull 926",NULL,"PR https github.com spring projects spring xd pull 926",NULL,"Add for who this story is","well_formed","no_role","high",False
18864,"We need a way to access the deployment properties for the deployed modules. For example runtime module foo.sink.bar 2 ",NULL,"We need a way to access the deployment properties for the deployed modules. For example runtime module foo.sink.bar 2 ",NULL,"Add for who this story is","well_formed","no_role","high",False
18864,"We need a way to access the deployment properties for the deployed modules. For example runtime module foo.sink.bar 2 ",NULL,"We need a way to access the deployment properties for the deployed modules. For example runtime module foo.sink.bar 2 ",NULL,"We need a way to access the deployment properties for the deployed modules<span class='highlight-text severity-high'>. For example runtime module foo.sink.bar 2 </span>","minimal","punctuation","high",False
18865,"Create Acceptance Test Add Mongo to Ec2 Acceptance Test Environment.",NULL,"Create Acceptance Test Add Mongo to Ec2 Acceptance Test Environment.",NULL,"Add for who this story is","well_formed","no_role","high",False
18870,"At the job definitions page, user should be able to provide the job deployment manifest module count, criteria etc., ",NULL,"At the job definitions page, user should be able to provide the job deployment manifest module count, criteria etc., ",NULL,"Add for who this story is","well_formed","no_role","high",False
18878,"We have 13 skipped tests now...",NULL,"We have 13 skipped tests now...",NULL,"Add for who this story is","well_formed","no_role","high",False
18874,"After spring hadoop 2.0 RC4 update.",NULL,"After spring hadoop 2.0 RC4 update.",NULL,"Add for who this story is","well_formed","no_role","high",False
18879,"Add messages store optimization to the hdfs dataset ",NULL,"Add messages store optimization to the hdfs dataset ",NULL,"Add for who this story is","well_formed","no_role","high",False
18868,"Automatically close notification messages Polish UI",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
18868,"Automatically close notification messages Polish UI",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
18871,"When clicking deploy from the job definitions page, user should be able to specify the deployment manifest module count, module criteria etc., ",NULL,"When clicking deploy from the job definitions page, user should be able to specify the deployment manifest module count, module criteria etc., ",NULL,"Add for who this story is","well_formed","no_role","high",False
18869,"The changes to twitterSearch means that it will send multiple messages during the duration of the test. To support these changes 1 Remove assertReceived. Since the number of messages is indeterminate 2 Change file sink that captures the results to append mode. Because each message will overwrite the previous messages result.",NULL,"The changes to twitterSearch means that it will send multiple messages during the duration of the test. To support these changes 1 Remove assertReceived. Since the number of messages is indeterminate 2 Change file sink that captures the results to append mode. Because each message will overwrite the previous messages result.",NULL,"Add for who this story is","well_formed","no_role","high",False
18869,"The changes to twitterSearch means that it will send multiple messages during the duration of the test. To support these changes 1 Remove assertReceived. Since the number of messages is indeterminate 2 Change file sink that captures the results to append mode. Because each message will overwrite the previous messages result.",NULL,"The changes to twitterSearch means that it will send multiple messages during the duration of the test. To support these changes 1 Remove assertReceived. Since the number of messages is indeterminate 2 Change file sink that captures the results to append mode. Because each message will overwrite the previous messages result.",NULL,"The changes to twitterSearch means that it will send multiple messages during the duration of the test<span class='highlight-text severity-high'>. To support these changes 1 Remove assertReceived. Since the number of messages is indeterminate 2 Change file sink that captures the results to append mode. Because each message will overwrite the previous messages result.</span>","minimal","punctuation","high",False
18875,"Update spring data hadoop version to 2.0.0.RC4 and make necessary changes to the YARN configuration.",NULL,"Update spring data hadoop version to 2.0.0.RC4 and make necessary changes to the YARN configuration.",NULL,"Add for who this story is","well_formed","no_role","high",False
18875,"Update spring data hadoop version to 2.0.0.RC4 and make necessary changes to the YARN configuration.",NULL,"Update spring data hadoop version to 2.0.0.RC4 and make necessary changes to the YARN configuration.",NULL,"Update spring data hadoop version to 2.0.0.RC4<span class='highlight-text severity-high'> and </span>make necessary changes to the YARN configuration.","atomic","conjunctions","high",False
18872,"At the JobExecution page, if the job execution is failed and restartable, then we should enable the restart action only if the job is deployed. Please see https github.com spring projects spring xd pull 884 for the discussion related to this.",NULL,"At the JobExecution page, if the job execution is failed and restartable, then we should enable the restart action only if the job is deployed. Please see https github.com spring projects spring xd pull 884 for the discussion related to this.",NULL,"Add for who this story is","well_formed","no_role","high",False
18872,"At the JobExecution page, if the job execution is failed and restartable, then we should enable the restart action only if the job is deployed. Please see https github.com spring projects spring xd pull 884 for the discussion related to this.",NULL,"At the JobExecution page, if the job execution is failed and restartable, then we should enable the restart action only if the job is deployed. Please see https github.com spring projects spring xd pull 884 for the discussion related to this.",NULL,"At the JobExecution page, if the job execution is failed<span class='highlight-text severity-high'> and </span>restartable, then we should enable the restart action only if the job is deployed. Please see https github.com spring projects spring xd pull 884 for the discussion related to this.","atomic","conjunctions","high",False
18872,"At the JobExecution page, if the job execution is failed and restartable, then we should enable the restart action only if the job is deployed. Please see https github.com spring projects spring xd pull 884 for the discussion related to this.",NULL,"At the JobExecution page, if the job execution is failed and restartable, then we should enable the restart action only if the job is deployed. Please see https github.com spring projects spring xd pull 884 for the discussion related to this.",NULL,"At the JobExecution page, if the job execution is failed and restartable, then we should enable the restart action only if the job is deployed<span class='highlight-text severity-high'>. Please see https github.com spring projects spring xd pull 884 for the discussion related to this.</span>","minimal","punctuation","high",False
18876,"We have some places where we us a default data format specified as yyyy MM dd . In Spring for Apache Hadoop we use yyyy MM dd for partitioning path expressions. This seems more in line with ISO standard date format. For consistency we should have both SHDP and XD use the same default format.",NULL,"We have some places where we us a default data format specified as yyyy MM dd . In Spring for Apache Hadoop we use yyyy MM dd for partitioning path expressions. This seems more in line with ISO standard date format. For consistency we should have both SHDP and XD use the same default format.",NULL,"Add for who this story is","well_formed","no_role","high",False
19365,"This came up when working on email source. There is int mail imap idle channel adapter and int mail inbound channel adapter It would be nice to be able to put those in two profiles and have one of the profile being activated from module options e.g. email polling true false Don t know the runtime cost of activating profiles, but we could blindly activate profiles from all options passed explicitly beans profile profile optionname optionvalue Not sure if this is the same as XD 132",NULL,"This came up when working on email source. There is int mail imap idle channel adapter and int mail inbound channel adapter It would be nice to be able to put those in two profiles and have one of the profile being activated from module options e.g. email polling true false Don t know the runtime cost of activating profiles, but we could blindly activate profiles from all options passed explicitly beans profile profile optionname optionvalue Not sure if this is the same as XD 132",NULL,"Add for who this story is","well_formed","no_role","high",False
19473,"http static.springsource.org spring xd docs 1.0.0.BUILD SNAPSHOT reference html sources should have jms added to the list and also the corresponding section that shows some basic usage.",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
18876,"We have some places where we us a default data format specified as yyyy MM dd . In Spring for Apache Hadoop we use yyyy MM dd for partitioning path expressions. This seems more in line with ISO standard date format. For consistency we should have both SHDP and XD use the same default format.",NULL,"We have some places where we us a default data format specified as yyyy MM dd . In Spring for Apache Hadoop we use yyyy MM dd for partitioning path expressions. This seems more in line with ISO standard date format. For consistency we should have both SHDP and XD use the same default format.",NULL,"We have some places where we us a default data format specified as yyyy MM dd . In Spring for Apache Hadoop we use yyyy MM dd for partitioning path expressions. This seems more in line with ISO standard date format. For consistency we should have both SHDP<span class='highlight-text severity-high'> and </span>XD use the same default format.","atomic","conjunctions","high",False
18876,"We have some places where we us a default data format specified as yyyy MM dd . In Spring for Apache Hadoop we use yyyy MM dd for partitioning path expressions. This seems more in line with ISO standard date format. For consistency we should have both SHDP and XD use the same default format.",NULL,"We have some places where we us a default data format specified as yyyy MM dd . In Spring for Apache Hadoop we use yyyy MM dd for partitioning path expressions. This seems more in line with ISO standard date format. For consistency we should have both SHDP and XD use the same default format.",NULL,"We have some places where we us a default data format specified as yyyy MM dd <span class='highlight-text severity-high'>. In Spring for Apache Hadoop we use yyyy MM dd for partitioning path expressions. This seems more in line with ISO standard date format. For consistency we should have both SHDP and XD use the same default format.</span>","minimal","punctuation","high",False
18877,"filejdbc, hdfsjdbc, jdbchdfs jdbc modules each support a tomcat connection pool. At this time none of the configurations allowed by the tomcat connection pool are available unless the user adds them to the appropriate module xml file. We need to allow the user to configure them via yml, property file and environment variables. ",NULL,"filejdbc, hdfsjdbc, jdbchdfs jdbc modules each support a tomcat connection pool. At this time none of the configurations allowed by the tomcat connection pool are available unless the user adds them to the appropriate module xml file. We need to allow the user to configure them via yml, property file and environment variables. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18877,"filejdbc, hdfsjdbc, jdbchdfs jdbc modules each support a tomcat connection pool. At this time none of the configurations allowed by the tomcat connection pool are available unless the user adds them to the appropriate module xml file. We need to allow the user to configure them via yml, property file and environment variables. ",NULL,"filejdbc, hdfsjdbc, jdbchdfs jdbc modules each support a tomcat connection pool. At this time none of the configurations allowed by the tomcat connection pool are available unless the user adds them to the appropriate module xml file. We need to allow the user to configure them via yml, property file and environment variables. ",NULL,"filejdbc, hdfsjdbc, jdbchdfs jdbc modules each support a tomcat connection pool<span class='highlight-text severity-high'>. At this time none of the configurations allowed by the tomcat connection pool are available unless the user adds them to the appropriate module xml file. We need to allow the user to configure them via yml, property file and environment variables. </span>","minimal","punctuation","high",False
18891,"Each Hadoop distro uses different settings for yarn.application.classpath and we should provide some starting points for the distros we support running XD on YARN for. We should add a commented out stub defaultYarnClasspath entry for phd20, cdh5 and hdp21 to replace the one for hadoop22 when someone deploys on these distros. ",NULL,"Each Hadoop distro uses different settings for yarn.application.classpath and we should provide some starting points for the distros we support running XD on YARN for. We should add a commented out stub defaultYarnClasspath entry for phd20, cdh5 and hdp21 to replace the one for hadoop22 when someone deploys on these distros. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18891,"Each Hadoop distro uses different settings for yarn.application.classpath and we should provide some starting points for the distros we support running XD on YARN for. We should add a commented out stub defaultYarnClasspath entry for phd20, cdh5 and hdp21 to replace the one for hadoop22 when someone deploys on these distros. ",NULL,"Each Hadoop distro uses different settings for yarn.application.classpath and we should provide some starting points for the distros we support running XD on YARN for. We should add a commented out stub defaultYarnClasspath entry for phd20, cdh5 and hdp21 to replace the one for hadoop22 when someone deploys on these distros. ",NULL,"Each Hadoop distro uses different settings for yarn.application.classpath<span class='highlight-text severity-high'> and </span>we should provide some starting points for the distros we support running XD on YARN for. We should add a commented out stub defaultYarnClasspath entry for phd20, cdh5 and hdp21 to replace the one for hadoop22 when someone deploys on these distros. ","atomic","conjunctions","high",False
18891,"Each Hadoop distro uses different settings for yarn.application.classpath and we should provide some starting points for the distros we support running XD on YARN for. We should add a commented out stub defaultYarnClasspath entry for phd20, cdh5 and hdp21 to replace the one for hadoop22 when someone deploys on these distros. ",NULL,"Each Hadoop distro uses different settings for yarn.application.classpath and we should provide some starting points for the distros we support running XD on YARN for. We should add a commented out stub defaultYarnClasspath entry for phd20, cdh5 and hdp21 to replace the one for hadoop22 when someone deploys on these distros. ",NULL,"Each Hadoop distro uses different settings for yarn<span class='highlight-text severity-high'>.application.classpath and we should provide some starting points for the distros we support running XD on YARN for. We should add a commented out stub defaultYarnClasspath entry for phd20, cdh5 and hdp21 to replace the one for hadoop22 when someone deploys on these distros. </span>","minimal","punctuation","high",False
18931,"When a module is deployed, it doesn t use Jolokia auto configuration which requires an embedded servlet container configuration . But, the module context isn t using a servlet context. From SimpleModule application new SpringApplicationBuilder .sources PropertyPlaceholderAutoConfiguration.class .web false ; and hence, the MBeans that are exposed by the deployed modules aren t accessible via Jolokia. We definitely don t want SimpleModule to use web application context but we need to figure out if we can use the container s management port to expose the deployed modules MBeans via jolokia. ",NULL,"When a module is deployed, it doesn t use Jolokia auto configuration which requires an embedded servlet container configuration . But, the module context isn t using a servlet context. From SimpleModule application new SpringApplicationBuilder .sources PropertyPlaceholderAutoConfiguration.class .web false ; and hence, the MBeans that are exposed by the deployed modules aren t accessible via Jolokia. We definitely don t want SimpleModule to use web application context but we need to figure out if we can use the container s management port to expose the deployed modules MBeans via jolokia. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18931,"When a module is deployed, it doesn t use Jolokia auto configuration which requires an embedded servlet container configuration . But, the module context isn t using a servlet context. From SimpleModule application new SpringApplicationBuilder .sources PropertyPlaceholderAutoConfiguration.class .web false ; and hence, the MBeans that are exposed by the deployed modules aren t accessible via Jolokia. We definitely don t want SimpleModule to use web application context but we need to figure out if we can use the container s management port to expose the deployed modules MBeans via jolokia. ",NULL,"When a module is deployed, it doesn t use Jolokia auto configuration which requires an embedded servlet container configuration . But, the module context isn t using a servlet context. From SimpleModule application new SpringApplicationBuilder .sources PropertyPlaceholderAutoConfiguration.class .web false ; and hence, the MBeans that are exposed by the deployed modules aren t accessible via Jolokia. We definitely don t want SimpleModule to use web application context but we need to figure out if we can use the container s management port to expose the deployed modules MBeans via jolokia. ",NULL,"When a module is deployed, it doesn t use Jolokia auto configuration which requires an embedded servlet container configuration . But, the module context isn t using a servlet context. From SimpleModule application new SpringApplicationBuilder .sources PropertyPlaceholderAutoConfiguration.class .web false ;<span class='highlight-text severity-high'> and </span>hence, the MBeans that are exposed by the deployed modules aren t accessible via Jolokia. We definitely don t want SimpleModule to use web application context but we need to figure out if we can use the container s management port to expose the deployed modules MBeans via jolokia. ","atomic","conjunctions","high",False
18931,"When a module is deployed, it doesn t use Jolokia auto configuration which requires an embedded servlet container configuration . But, the module context isn t using a servlet context. From SimpleModule application new SpringApplicationBuilder .sources PropertyPlaceholderAutoConfiguration.class .web false ; and hence, the MBeans that are exposed by the deployed modules aren t accessible via Jolokia. We definitely don t want SimpleModule to use web application context but we need to figure out if we can use the container s management port to expose the deployed modules MBeans via jolokia. ",NULL,"When a module is deployed, it doesn t use Jolokia auto configuration which requires an embedded servlet container configuration . But, the module context isn t using a servlet context. From SimpleModule application new SpringApplicationBuilder .sources PropertyPlaceholderAutoConfiguration.class .web false ; and hence, the MBeans that are exposed by the deployed modules aren t accessible via Jolokia. We definitely don t want SimpleModule to use web application context but we need to figure out if we can use the container s management port to expose the deployed modules MBeans via jolokia. ",NULL,"When a module is deployed, it doesn t use Jolokia auto configuration which requires an embedded servlet container configuration <span class='highlight-text severity-high'>. But, the module context isn t using a servlet context. From SimpleModule application new SpringApplicationBuilder .sources PropertyPlaceholderAutoConfiguration.class .web false ; and hence, the MBeans that are exposed by the deployed modules aren t accessible via Jolokia. We definitely don t want SimpleModule to use web application context but we need to figure out if we can use the container s management port to expose the deployed modules MBeans via jolokia. </span>","minimal","punctuation","high",False
18892,"Document the different onion layers that come in play with regard to quoting and escaping shell, xd parser, SpEL expressions in some cases and provide practical examples to common scenarios ",NULL,"Document the different onion layers that come in play with regard to quoting and escaping shell, xd parser, SpEL expressions in some cases and provide practical examples to common scenarios ",NULL,"Add for who this story is","well_formed","no_role","high",False
18892,"Document the different onion layers that come in play with regard to quoting and escaping shell, xd parser, SpEL expressions in some cases and provide practical examples to common scenarios ",NULL,"Document the different onion layers that come in play with regard to quoting and escaping shell, xd parser, SpEL expressions in some cases and provide practical examples to common scenarios ",NULL,"Document the different onion layers that come in play with regard to quoting<span class='highlight-text severity-high'> and </span>escaping shell, xd parser, SpEL expressions in some cases and provide practical examples to common scenarios ","atomic","conjunctions","high",False
18894,"Not sure why the Hadoop classes are on the admin servers classpath. There is no way to select the distro, and the Hadoop classes shouldn t be needed except for module info for hdfs sink see XD 1701 ",NULL,"Not sure why the Hadoop classes are on the admin servers classpath. There is no way to select the distro, and the Hadoop classes shouldn t be needed except for module info for hdfs sink see XD 1701 ",NULL,"Add for who this story is","well_formed","no_role","high",False
18894,"Not sure why the Hadoop classes are on the admin servers classpath. There is no way to select the distro, and the Hadoop classes shouldn t be needed except for module info for hdfs sink see XD 1701 ",NULL,"Not sure why the Hadoop classes are on the admin servers classpath. There is no way to select the distro, and the Hadoop classes shouldn t be needed except for module info for hdfs sink see XD 1701 ",NULL,"Not sure why the Hadoop classes are on the admin servers classpath. There is no way to select the distro,<span class='highlight-text severity-high'> and </span>the Hadoop classes shouldn t be needed except for module info for hdfs sink see XD 1701 ","atomic","conjunctions","high",False
18894,"Not sure why the Hadoop classes are on the admin servers classpath. There is no way to select the distro, and the Hadoop classes shouldn t be needed except for module info for hdfs sink see XD 1701 ",NULL,"Not sure why the Hadoop classes are on the admin servers classpath. There is no way to select the distro, and the Hadoop classes shouldn t be needed except for module info for hdfs sink see XD 1701 ",NULL,"Not sure why the Hadoop classes are on the admin servers classpath<span class='highlight-text severity-high'>. There is no way to select the distro, and the Hadoop classes shouldn t be needed except for module info for hdfs sink see XD 1701 </span>","minimal","punctuation","high",False
18893,"We should add an easy way to configure the memory. Currently we only have the number of YARN containers configurable without diving into Spring YARN Boot specific config options. code xd adminServers 1 containers 1 code Proposing we do code xd adminServers 1 adminMemory 512M containers 1 containerMemory 512M code ",NULL,"We should add an easy way to configure the memory. Currently we only have the number of YARN containers configurable without diving into Spring YARN Boot specific config options. code xd adminServers 1 containers 1 code Proposing we do code xd adminServers 1 adminMemory 512M containers 1 containerMemory 512M code ",NULL,"Add for who this story is","well_formed","no_role","high",False
18893,"We should add an easy way to configure the memory. Currently we only have the number of YARN containers configurable without diving into Spring YARN Boot specific config options. code xd adminServers 1 containers 1 code Proposing we do code xd adminServers 1 adminMemory 512M containers 1 containerMemory 512M code ",NULL,"We should add an easy way to configure the memory. Currently we only have the number of YARN containers configurable without diving into Spring YARN Boot specific config options. code xd adminServers 1 containers 1 code Proposing we do code xd adminServers 1 adminMemory 512M containers 1 containerMemory 512M code ",NULL,"We should add an easy way to configure the memory<span class='highlight-text severity-high'>. Currently we only have the number of YARN containers configurable without diving into Spring YARN Boot specific config options. code xd adminServers 1 containers 1 code Proposing we do code xd adminServers 1 adminMemory 512M containers 1 containerMemory 512M code </span>","minimal","punctuation","high",False
18895,"Support the ability to provide deployment properties for job deploy .",NULL,"Support the ability to provide deployment properties for job deploy .",NULL,"Add for who this story is","well_formed","no_role","high",False
18932,"the transport option allows udp as well as tcp ",NULL,"the transport option allows udp as well as tcp ",NULL,"Add for who this story is","well_formed","no_role","high",False
18933,"e.g. Need to update this section maybe others https github.com spring projects spring xd wiki Running Distributed Mode Remove all mentions of Control Bus, and replace any mentions of the transport cmd line arg with the xd.transport property in yml. ",NULL,"e.g. Need to update this section maybe others https github.com spring projects spring xd wiki Running Distributed Mode Remove all mentions of Control Bus, and replace any mentions of the transport cmd line arg with the xd.transport property in yml. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18933,"e.g. Need to update this section maybe others https github.com spring projects spring xd wiki Running Distributed Mode Remove all mentions of Control Bus, and replace any mentions of the transport cmd line arg with the xd.transport property in yml. ",NULL,"e.g. Need to update this section maybe others https github.com spring projects spring xd wiki Running Distributed Mode Remove all mentions of Control Bus, and replace any mentions of the transport cmd line arg with the xd.transport property in yml. ",NULL,"e.g. Need to update this section maybe others https github.com spring projects spring xd wiki Running Distributed Mode Remove all mentions of Control Bus,<span class='highlight-text severity-high'> and </span>replace any mentions of the transport cmd line arg with the xd.transport property in yml. ","atomic","conjunctions","high",False
18933,"e.g. Need to update this section maybe others https github.com spring projects spring xd wiki Running Distributed Mode Remove all mentions of Control Bus, and replace any mentions of the transport cmd line arg with the xd.transport property in yml. ",NULL,"e.g. Need to update this section maybe others https github.com spring projects spring xd wiki Running Distributed Mode Remove all mentions of Control Bus, and replace any mentions of the transport cmd line arg with the xd.transport property in yml. ",NULL,"e<span class='highlight-text severity-high'>.g. Need to update this section maybe others https github.com spring projects spring xd wiki Running Distributed Mode Remove all mentions of Control Bus, and replace any mentions of the transport cmd line arg with the xd.transport property in yml. </span>","minimal","punctuation","high",False
18934,"XD EC2 must use environment variable XD TRANSPORT instead of transport to declare data transport for XD.",NULL,"XD EC2 must use environment variable XD TRANSPORT instead of transport to declare data transport for XD.",NULL,"Add for who this story is","well_formed","no_role","high",False
18994,"The ordering of the lookup should be described, in particular detail on how environment variables can overrride properties. Some details will necessarily change based on outcome of current discussion, but the overall ordering is going to remain.",NULL,"The ordering of the lookup should be described, in particular detail on how environment variables can overrride properties. Some details will necessarily change based on outcome of current discussion, but the overall ordering is going to remain.",NULL,"Add for who this story is","well_formed","no_role","high",False
18994,"The ordering of the lookup should be described, in particular detail on how environment variables can overrride properties. Some details will necessarily change based on outcome of current discussion, but the overall ordering is going to remain.",NULL,"The ordering of the lookup should be described, in particular detail on how environment variables can overrride properties. Some details will necessarily change based on outcome of current discussion, but the overall ordering is going to remain.",NULL,"The ordering of the lookup should be described, in particular detail on how environment variables can overrride properties<span class='highlight-text severity-high'>. Some details will necessarily change based on outcome of current discussion, but the overall ordering is going to remain.</span>","minimal","punctuation","high",False
18889,"Update StreamUtils based on Code Review comments.",NULL,"Update StreamUtils based on Code Review comments.",NULL,"Add for who this story is","well_formed","no_role","high",False
18887,"Modules can use property values in servers.yml which is very handy to keep batch and hdfs functionality working without duplication of config values in servers.yml and modules.yml or individual modules . The configuration section should highlight the common cases where this occurs, batch, hdfs, rabbitmq mqtt where using the server config values as defaults is useful and that they can still be overridden. ",NULL,"Modules can use property values in servers.yml which is very handy to keep batch and hdfs functionality working without duplication of config values in servers.yml and modules.yml or individual modules . The configuration section should highlight the common cases where this occurs, batch, hdfs, rabbitmq mqtt where using the server config values as defaults is useful and that they can still be overridden. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19167,"Even though it may be hard to come up with a mqtt broker, an easy test that should be automated is somesource mqtt topic foo with mqtt topics foo somesink And asserting that what is emitted to somesource ends up in somesink. ",NULL,"Even though it may be hard to come up with a mqtt broker, an easy test that should be automated is somesource mqtt topic foo with mqtt topics foo somesink And asserting that what is emitted to somesource ends up in somesink. ",NULL,"Even though it may be hard to come up with a mqtt broker, an easy test that should be automated is somesource mqtt topic foo with mqtt topics foo somesink<span class='highlight-text severity-high'> and </span>asserting that what is emitted to somesource ends up in somesink. ","atomic","conjunctions","high",False
18887,"Modules can use property values in servers.yml which is very handy to keep batch and hdfs functionality working without duplication of config values in servers.yml and modules.yml or individual modules . The configuration section should highlight the common cases where this occurs, batch, hdfs, rabbitmq mqtt where using the server config values as defaults is useful and that they can still be overridden. ",NULL,"Modules can use property values in servers.yml which is very handy to keep batch and hdfs functionality working without duplication of config values in servers.yml and modules.yml or individual modules . The configuration section should highlight the common cases where this occurs, batch, hdfs, rabbitmq mqtt where using the server config values as defaults is useful and that they can still be overridden. ",NULL,"Modules can use property values in servers.yml which is very handy to keep batch<span class='highlight-text severity-high'> and </span>hdfs functionality working without duplication of config values in servers.yml and modules.yml<span class='highlight-text severity-high'> or </span>individual modules . The configuration section should highlight the common cases where this occurs, batch, hdfs, rabbitmq mqtt where using the server config values as defaults is useful and that they can still be overridden. ","atomic","conjunctions","high",False
18887,"Modules can use property values in servers.yml which is very handy to keep batch and hdfs functionality working without duplication of config values in servers.yml and modules.yml or individual modules . The configuration section should highlight the common cases where this occurs, batch, hdfs, rabbitmq mqtt where using the server config values as defaults is useful and that they can still be overridden. ",NULL,"Modules can use property values in servers.yml which is very handy to keep batch and hdfs functionality working without duplication of config values in servers.yml and modules.yml or individual modules . The configuration section should highlight the common cases where this occurs, batch, hdfs, rabbitmq mqtt where using the server config values as defaults is useful and that they can still be overridden. ",NULL,"Modules can use property values in servers<span class='highlight-text severity-high'>.yml which is very handy to keep batch and hdfs functionality working without duplication of config values in servers.yml and modules.yml or individual modules . The configuration section should highlight the common cases where this occurs, batch, hdfs, rabbitmq mqtt where using the server config values as defaults is useful and that they can still be overridden. </span>","minimal","punctuation","high",False
18890,"Also check the JMX output to see that the filter rejected the entry.",NULL,"Also check the JMX output to see that the filter rejected the entry.",NULL,"Add for who this story is","well_formed","no_role","high",False
18897,"StepExecutionContext StepExecutionProgress JobScheduler Stream page Job definition XD1615 ",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
18897,"StepExecutionContext StepExecutionProgress JobScheduler Stream page Job definition XD1615 ",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
18903,"When adding streams page to the UI from XD 1667 , it is necessary to modularize the angular app modules based on the functionality components job, stream, auth etc., . As we expand into more components and use cases in the UI, this definitely makes it easier to concentrate on specific modules based on the functionality.",NULL,"When adding streams page to the UI from XD 1667 , it is necessary to modularize the angular app modules based on the functionality components job, stream, auth etc., . As we expand into more components and use cases in the UI, this definitely makes it easier to concentrate on specific modules based on the functionality.",NULL,"Add for who this story is","well_formed","no_role","high",False
18903,"When adding streams page to the UI from XD 1667 , it is necessary to modularize the angular app modules based on the functionality components job, stream, auth etc., . As we expand into more components and use cases in the UI, this definitely makes it easier to concentrate on specific modules based on the functionality.",NULL,"When adding streams page to the UI from XD 1667 , it is necessary to modularize the angular app modules based on the functionality components job, stream, auth etc., . As we expand into more components and use cases in the UI, this definitely makes it easier to concentrate on specific modules based on the functionality.",NULL,"When adding streams page to the UI from XD 1667 , it is necessary to modularize the angular app modules based on the functionality components job, stream, auth etc., . As we expand into more components<span class='highlight-text severity-high'> and </span>use cases in the UI, this definitely makes it easier to concentrate on specific modules based on the functionality.","atomic","conjunctions","high",False
18903,"When adding streams page to the UI from XD 1667 , it is necessary to modularize the angular app modules based on the functionality components job, stream, auth etc., . As we expand into more components and use cases in the UI, this definitely makes it easier to concentrate on specific modules based on the functionality.",NULL,"When adding streams page to the UI from XD 1667 , it is necessary to modularize the angular app modules based on the functionality components job, stream, auth etc., . As we expand into more components and use cases in the UI, this definitely makes it easier to concentrate on specific modules based on the functionality.",NULL,"When adding streams page to the UI from XD 1667 , it is necessary to modularize the angular app modules based on the functionality components job, stream, auth etc<span class='highlight-text severity-high'>., . As we expand into more components and use cases in the UI, this definitely makes it easier to concentrate on specific modules based on the functionality.</span>","minimal","punctuation","high",False
18908,"XD 1019 added simple stateless retry to the message bus. Use stateful retry and an AmqpRejectAndDontRequeueRecoverer enabling failed messages to be requeued on the broker until successful perhaps because another instance can handle the message ; also provides a mechanism to route failed messages to a dead letter exchange. Requires setting the message id header in bus generated messages. Also add profiles and properties for common retry backoff policies.",NULL,"XD 1019 added simple stateless retry to the message bus. Use stateful retry and an AmqpRejectAndDontRequeueRecoverer enabling failed messages to be requeued on the broker until successful perhaps because another instance can handle the message ; also provides a mechanism to route failed messages to a dead letter exchange. Requires setting the message id header in bus generated messages. Also add profiles and properties for common retry backoff policies.",NULL,"Add for who this story is","well_formed","no_role","high",False
18908,"XD 1019 added simple stateless retry to the message bus. Use stateful retry and an AmqpRejectAndDontRequeueRecoverer enabling failed messages to be requeued on the broker until successful perhaps because another instance can handle the message ; also provides a mechanism to route failed messages to a dead letter exchange. Requires setting the message id header in bus generated messages. Also add profiles and properties for common retry backoff policies.",NULL,"XD 1019 added simple stateless retry to the message bus. Use stateful retry and an AmqpRejectAndDontRequeueRecoverer enabling failed messages to be requeued on the broker until successful perhaps because another instance can handle the message ; also provides a mechanism to route failed messages to a dead letter exchange. Requires setting the message id header in bus generated messages. Also add profiles and properties for common retry backoff policies.",NULL,"XD 1019 added simple stateless retry to the message bus. Use stateful retry<span class='highlight-text severity-high'> and </span>an AmqpRejectAndDontRequeueRecoverer enabling failed messages to be requeued on the broker until successful perhaps because another instance can handle the message ; also provides a mechanism to route failed messages to a dead letter exchange. Requires setting the message id header in bus generated messages. Also add profiles and properties for common retry backoff policies.","atomic","conjunctions","high",False
18908,"XD 1019 added simple stateless retry to the message bus. Use stateful retry and an AmqpRejectAndDontRequeueRecoverer enabling failed messages to be requeued on the broker until successful perhaps because another instance can handle the message ; also provides a mechanism to route failed messages to a dead letter exchange. Requires setting the message id header in bus generated messages. Also add profiles and properties for common retry backoff policies.",NULL,"XD 1019 added simple stateless retry to the message bus. Use stateful retry and an AmqpRejectAndDontRequeueRecoverer enabling failed messages to be requeued on the broker until successful perhaps because another instance can handle the message ; also provides a mechanism to route failed messages to a dead letter exchange. Requires setting the message id header in bus generated messages. Also add profiles and properties for common retry backoff policies.",NULL,"XD 1019 added simple stateless retry to the message bus<span class='highlight-text severity-high'>. Use stateful retry and an AmqpRejectAndDontRequeueRecoverer enabling failed messages to be requeued on the broker until successful perhaps because another instance can handle the message ; also provides a mechanism to route failed messages to a dead letter exchange. Requires setting the message id header in bus generated messages. Also add profiles and properties for common retry backoff policies.</span>","minimal","punctuation","high",False
18901,"Allows looking at message headers without turning on debugging.",NULL,"Allows looking at message headers without turning on debugging.",NULL,"Add for who this story is","well_formed","no_role","high",False
18904,"The streams page needs to be added to the UI at least to show the job triggers that are created while scheduling XD jobs.",NULL,"The streams page needs to be added to the UI at least to show the job triggers that are created while scheduling XD jobs.",NULL,"Add for who this story is","well_formed","no_role","high",False
18905,"Need to update the Jackson parser.",NULL,"Need to update the Jackson parser.",NULL,"Add for who this story is","well_formed","no_role","high",False
18906,"Sonar build is currently failing.",NULL,"Sonar build is currently failing.",NULL,"Add for who this story is","well_formed","no_role","high",False
18909,"HDFS sink needs to have unique identifier for container id added as part of file name. Part of the file name in the directory will be the container id GUID like base path logfile GUID 1.txt ",NULL,"HDFS sink needs to have unique identifier for container id added as part of file name. Part of the file name in the directory will be the container id GUID like base path logfile GUID 1.txt ",NULL,"Add for who this story is","well_formed","no_role","high",False
18909,"HDFS sink needs to have unique identifier for container id added as part of file name. Part of the file name in the directory will be the container id GUID like base path logfile GUID 1.txt ",NULL,"HDFS sink needs to have unique identifier for container id added as part of file name. Part of the file name in the directory will be the container id GUID like base path logfile GUID 1.txt ",NULL,"HDFS sink needs to have unique identifier for container id added as part of file name<span class='highlight-text severity-high'>. Part of the file name in the directory will be the container id GUID like base path logfile GUID 1.txt </span>","minimal","punctuation","high",False
18950,"With the introduction of the deployment manifest setting this to false makes it easier to use the deployment manifest, which is the main use case scenario. ",NULL,"With the introduction of the deployment manifest setting this to false makes it easier to use the deployment manifest, which is the main use case scenario. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18951,"https jira.spring.io browse XD 1343 and related issues.",NULL,"https jira.spring.io browse XD 1343 and related issues.",NULL,"Add for who this story is","well_formed","no_role","high",False
18900,"The xd dirt log4j.properties includes the calling line number L which is not recommended for production. https logging.apache.org log4j 1.2 apidocs org apache log4j PatternLayout.html WARNING Generating caller location information is extremely slow and should be avoided unless execution speed is not an issue. ",NULL,"The xd dirt log4j.properties includes the calling line number L which is not recommended for production. https logging.apache.org log4j 1.2 apidocs org apache log4j PatternLayout.html WARNING Generating caller location information is extremely slow and should be avoided unless execution speed is not an issue. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18900,"The xd dirt log4j.properties includes the calling line number L which is not recommended for production. https logging.apache.org log4j 1.2 apidocs org apache log4j PatternLayout.html WARNING Generating caller location information is extremely slow and should be avoided unless execution speed is not an issue. ",NULL,"The xd dirt log4j.properties includes the calling line number L which is not recommended for production. https logging.apache.org log4j 1.2 apidocs org apache log4j PatternLayout.html WARNING Generating caller location information is extremely slow and should be avoided unless execution speed is not an issue. ",NULL,"The xd dirt log4j.properties includes the calling line number L which is not recommended for production. https logging.apache.org log4j 1.2 apidocs org apache log4j PatternLayout.html WARNING Generating caller location information is extremely slow<span class='highlight-text severity-high'> and </span>should be avoided unless execution speed is not an issue. ","atomic","conjunctions","high",False
18900,"The xd dirt log4j.properties includes the calling line number L which is not recommended for production. https logging.apache.org log4j 1.2 apidocs org apache log4j PatternLayout.html WARNING Generating caller location information is extremely slow and should be avoided unless execution speed is not an issue. ",NULL,"The xd dirt log4j.properties includes the calling line number L which is not recommended for production. https logging.apache.org log4j 1.2 apidocs org apache log4j PatternLayout.html WARNING Generating caller location information is extremely slow and should be avoided unless execution speed is not an issue. ",NULL,"The xd dirt log4j<span class='highlight-text severity-high'>.properties includes the calling line number L which is not recommended for production. https logging.apache.org log4j 1.2 apidocs org apache log4j PatternLayout.html WARNING Generating caller location information is extremely slow and should be avoided unless execution speed is not an issue. </span>","minimal","punctuation","high",False
18898,"Currently, the admin nodes that participate in the leadership election are grouped under xd admin. Since, there are multiple lock nodes that correspond to all the admin servers that participate in leadership election, we can pluralize this node name to xd admins.",NULL,"Currently, the admin nodes that participate in the leadership election are grouped under xd admin. Since, there are multiple lock nodes that correspond to all the admin servers that participate in leadership election, we can pluralize this node name to xd admins.",NULL,"Add for who this story is","well_formed","no_role","high",False
18898,"Currently, the admin nodes that participate in the leadership election are grouped under xd admin. Since, there are multiple lock nodes that correspond to all the admin servers that participate in leadership election, we can pluralize this node name to xd admins.",NULL,"Currently, the admin nodes that participate in the leadership election are grouped under xd admin. Since, there are multiple lock nodes that correspond to all the admin servers that participate in leadership election, we can pluralize this node name to xd admins.",NULL,"Currently, the admin nodes that participate in the leadership election are grouped under xd admin<span class='highlight-text severity-high'>. Since, there are multiple lock nodes that correspond to all the admin servers that participate in leadership election, we can pluralize this node name to xd admins.</span>","minimal","punctuation","high",False
18899,"I am new to Spring XD, I want to read the jboss queue message and then want to write it into text file by using stream create comment i.e stream create name TEST LOG definition jms file deploy . I am trying to configure the spring xd 1.0.0.M6 xd modules common jms jbossmq infrastructure context.xml to invoke the jboss queue and read the queue message. Can you help me to do the configuration and resolve my objective?",NULL,"I am new to Spring XD, I want to read the jboss queue message and then want to write it into text file by using stream create comment i.e stream create name TEST LOG definition jms file deploy . I am trying to configure the spring xd 1.0.0.M6 xd modules common jms jbossmq infrastructure context.xml to invoke the jboss queue and read the queue message. Can you help me to do the configuration and resolve my objective?",NULL,"Add for who this story is","well_formed","no_role","high",False
18899,"I am new to Spring XD, I want to read the jboss queue message and then want to write it into text file by using stream create comment i.e stream create name TEST LOG definition jms file deploy . I am trying to configure the spring xd 1.0.0.M6 xd modules common jms jbossmq infrastructure context.xml to invoke the jboss queue and read the queue message. Can you help me to do the configuration and resolve my objective?",NULL,"I am new to Spring XD, I want to read the jboss queue message and then want to write it into text file by using stream create comment i.e stream create name TEST LOG definition jms file deploy . I am trying to configure the spring xd 1.0.0.M6 xd modules common jms jbossmq infrastructure context.xml to invoke the jboss queue and read the queue message. Can you help me to do the configuration and resolve my objective?",NULL,"I am new to Spring XD, I want to read the jboss queue message<span class='highlight-text severity-high'> and </span>then want to write it into text file by using stream create comment i.e stream create name TEST LOG definition jms file deploy . I am trying to configure the spring xd 1.0.0.M6 xd modules common jms jbossmq infrastructure context.xml to invoke the jboss queue and read the queue message. Can you help me to do the configuration and resolve my objective?","atomic","conjunctions","high",False
18899,"I am new to Spring XD, I want to read the jboss queue message and then want to write it into text file by using stream create comment i.e stream create name TEST LOG definition jms file deploy . I am trying to configure the spring xd 1.0.0.M6 xd modules common jms jbossmq infrastructure context.xml to invoke the jboss queue and read the queue message. Can you help me to do the configuration and resolve my objective?",NULL,"I am new to Spring XD, I want to read the jboss queue message and then want to write it into text file by using stream create comment i.e stream create name TEST LOG definition jms file deploy . I am trying to configure the spring xd 1.0.0.M6 xd modules common jms jbossmq infrastructure context.xml to invoke the jboss queue and read the queue message. Can you help me to do the configuration and resolve my objective?",NULL,"I am new to Spring XD, I want to read the jboss queue message and then want to write it into text file by using stream create comment i<span class='highlight-text severity-high'>.e stream create name TEST LOG definition jms file deploy . I am trying to configure the spring xd 1.0.0.M6 xd modules common jms jbossmq infrastructure context.xml to invoke the jboss queue and read the queue message. Can you help me to do the configuration and resolve my objective?</span>","minimal","punctuation","high",False
18899,"I am new to Spring XD, I want to read the jboss queue message and then want to write it into text file by using stream create comment i.e stream create name TEST LOG definition jms file deploy . I am trying to configure the spring xd 1.0.0.M6 xd modules common jms jbossmq infrastructure context.xml to invoke the jboss queue and read the queue message. Can you help me to do the configuration and resolve my objective?",NULL,"I am new to Spring XD, I want to read the jboss queue message and then want to write it into text file by using stream create comment i.e stream create name TEST LOG definition jms file deploy . I am trying to configure the spring xd 1.0.0.M6 xd modules common jms jbossmq infrastructure context.xml to invoke the jboss queue and read the queue message. Can you help me to do the configuration and resolve my objective?",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18907,"This will reduce one extra step for getting started using XD in distributed mode out of the box ",NULL,"This will reduce one extra step for getting started using XD in distributed mode out of the box ",NULL,"Add for who this story is","well_formed","no_role","high",False
18902,"XD 1623 introduced the dependency to a SNAPSHOT version",NULL,"XD 1623 introduced the dependency to a SNAPSHOT version",NULL,"Add for who this story is","well_formed","no_role","high",False
18912,"Add JavaDocs to StreamUtils HttpTest MqttTest JmsSource Exception Handling StreamUtils stream method should throw IllegalStateException instead of a checked exception. XDEc2Validation assertReceived, assertValid should throw IllegalStateException instead of a checked exception ",NULL,"Add JavaDocs to StreamUtils HttpTest MqttTest JmsSource Exception Handling StreamUtils stream method should throw IllegalStateException instead of a checked exception. XDEc2Validation assertReceived, assertValid should throw IllegalStateException instead of a checked exception ",NULL,"Add for who this story is","well_formed","no_role","high",False
18912,"Add JavaDocs to StreamUtils HttpTest MqttTest JmsSource Exception Handling StreamUtils stream method should throw IllegalStateException instead of a checked exception. XDEc2Validation assertReceived, assertValid should throw IllegalStateException instead of a checked exception ",NULL,"Add JavaDocs to StreamUtils HttpTest MqttTest JmsSource Exception Handling StreamUtils stream method should throw IllegalStateException instead of a checked exception. XDEc2Validation assertReceived, assertValid should throw IllegalStateException instead of a checked exception ",NULL,"Add JavaDocs to StreamUtils HttpTest MqttTest JmsSource Exception Handling StreamUtils stream method should throw IllegalStateException instead of a checked exception<span class='highlight-text severity-high'>. XDEc2Validation assertReceived, assertValid should throw IllegalStateException instead of a checked exception </span>","minimal","punctuation","high",False
18917,"This may require additional support Jiras for Spring Batch",NULL,"This may require additional support Jiras for Spring Batch",NULL,"Add for who this story is","well_formed","no_role","high",False
18913,"During admin server startup, if it fails due to embedded tomcat failure, then the admin server instance still up and running. Since its tomcat isn t running it can not handle any REST client requests. In this scenario, we need fail fast the admin server process itself with better error message.",NULL,"During admin server startup, if it fails due to embedded tomcat failure, then the admin server instance still up and running. Since its tomcat isn t running it can not handle any REST client requests. In this scenario, we need fail fast the admin server process itself with better error message.",NULL,"Add for who this story is","well_formed","no_role","high",False
18913,"During admin server startup, if it fails due to embedded tomcat failure, then the admin server instance still up and running. Since its tomcat isn t running it can not handle any REST client requests. In this scenario, we need fail fast the admin server process itself with better error message.",NULL,"During admin server startup, if it fails due to embedded tomcat failure, then the admin server instance still up and running. Since its tomcat isn t running it can not handle any REST client requests. In this scenario, we need fail fast the admin server process itself with better error message.",NULL,"During admin server startup, if it fails due to embedded tomcat failure, then the admin server instance still up<span class='highlight-text severity-high'> and </span>running. Since its tomcat isn t running it can not handle any REST client requests. In this scenario, we need fail fast the admin server process itself with better error message.","atomic","conjunctions","high",False
18913,"During admin server startup, if it fails due to embedded tomcat failure, then the admin server instance still up and running. Since its tomcat isn t running it can not handle any REST client requests. In this scenario, we need fail fast the admin server process itself with better error message.",NULL,"During admin server startup, if it fails due to embedded tomcat failure, then the admin server instance still up and running. Since its tomcat isn t running it can not handle any REST client requests. In this scenario, we need fail fast the admin server process itself with better error message.",NULL,"During admin server startup, if it fails due to embedded tomcat failure, then the admin server instance still up and running<span class='highlight-text severity-high'>. Since its tomcat isn t running it can not handle any REST client requests. In this scenario, we need fail fast the admin server process itself with better error message.</span>","minimal","punctuation","high",False
18915,"Between M5 and M6 the size of the shell lib directory went up 50 MB. Investigate and remove jars from being packaged that are not used.",NULL,"Between M5 and M6 the size of the shell lib directory went up 50 MB. Investigate and remove jars from being packaged that are not used.",NULL,"Add for who this story is","well_formed","no_role","high",False
18915,"Between M5 and M6 the size of the shell lib directory went up 50 MB. Investigate and remove jars from being packaged that are not used.",NULL,"Between M5 and M6 the size of the shell lib directory went up 50 MB. Investigate and remove jars from being packaged that are not used.",NULL,"Between M5<span class='highlight-text severity-high'> and </span>M6 the size of the shell lib directory went up 50 MB. Investigate and remove jars from being packaged that are not used.","atomic","conjunctions","high",False
18915,"Between M5 and M6 the size of the shell lib directory went up 50 MB. Investigate and remove jars from being packaged that are not used.",NULL,"Between M5 and M6 the size of the shell lib directory went up 50 MB. Investigate and remove jars from being packaged that are not used.",NULL,"Between M5 and M6 the size of the shell lib directory went up 50 MB<span class='highlight-text severity-high'>. Investigate and remove jars from being packaged that are not used.</span>","minimal","punctuation","high",False
18921,"The messagebus implementations, upon registration of consumer and producer from to messagebus the corresponding endpoints start. Instead of directly calling the start on adapter consumer we can call the corresponding Binding s start which calls the underlying endpoint to start. This is in line with the way the corresponding endpoints are stopped using Binding s stop during undeploy destroy.",NULL,"The messagebus implementations, upon registration of consumer and producer from to messagebus the corresponding endpoints start. Instead of directly calling the start on adapter consumer we can call the corresponding Binding s start which calls the underlying endpoint to start. This is in line with the way the corresponding endpoints are stopped using Binding s stop during undeploy destroy.",NULL,"Add for who this story is","well_formed","no_role","high",False
18921,"The messagebus implementations, upon registration of consumer and producer from to messagebus the corresponding endpoints start. Instead of directly calling the start on adapter consumer we can call the corresponding Binding s start which calls the underlying endpoint to start. This is in line with the way the corresponding endpoints are stopped using Binding s stop during undeploy destroy.",NULL,"The messagebus implementations, upon registration of consumer and producer from to messagebus the corresponding endpoints start. Instead of directly calling the start on adapter consumer we can call the corresponding Binding s start which calls the underlying endpoint to start. This is in line with the way the corresponding endpoints are stopped using Binding s stop during undeploy destroy.",NULL,"The messagebus implementations, upon registration of consumer and producer from to messagebus the corresponding endpoints start<span class='highlight-text severity-high'>. Instead of directly calling the start on adapter consumer we can call the corresponding Binding s start which calls the underlying endpoint to start. This is in line with the way the corresponding endpoints are stopped using Binding s stop during undeploy destroy.</span>","minimal","punctuation","high",False
18918,"Some of the tests in JobCommandTests use the verification of shell command results table row on a specific row mostly first row like this String id jobExecutions.getRows .get 0 .getValue 1 ; displayJobExecution id ; It is possible that the list of table rows may have the intended row in different order. This poses inconsistent test failures. ",NULL,"Some of the tests in JobCommandTests use the verification of shell command results table row on a specific row mostly first row like this String id jobExecutions.getRows .get 0 .getValue 1 ; displayJobExecution id ; It is possible that the list of table rows may have the intended row in different order. This poses inconsistent test failures. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18918,"Some of the tests in JobCommandTests use the verification of shell command results table row on a specific row mostly first row like this String id jobExecutions.getRows .get 0 .getValue 1 ; displayJobExecution id ; It is possible that the list of table rows may have the intended row in different order. This poses inconsistent test failures. ",NULL,"Some of the tests in JobCommandTests use the verification of shell command results table row on a specific row mostly first row like this String id jobExecutions.getRows .get 0 .getValue 1 ; displayJobExecution id ; It is possible that the list of table rows may have the intended row in different order. This poses inconsistent test failures. ",NULL,"Some of the tests in JobCommandTests use the verification of shell command results table row on a specific row mostly first row like this String id jobExecutions<span class='highlight-text severity-high'>.getRows .get 0 .getValue 1 ; displayJobExecution id ; It is possible that the list of table rows may have the intended row in different order. This poses inconsistent test failures. </span>","minimal","punctuation","high",False
18924,"The analytics project has been used as a host for common repository classes because it was easily visible by both dirt and other stuff can t remember which This should be cleaned and a dedicated project for core , utility , re usable , whatever classes should be created ",NULL,"The analytics project has been used as a host for common repository classes because it was easily visible by both dirt and other stuff can t remember which This should be cleaned and a dedicated project for core , utility , re usable , whatever classes should be created ",NULL,"Add for who this story is","well_formed","no_role","high",False
18924,"The analytics project has been used as a host for common repository classes because it was easily visible by both dirt and other stuff can t remember which This should be cleaned and a dedicated project for core , utility , re usable , whatever classes should be created ",NULL,"The analytics project has been used as a host for common repository classes because it was easily visible by both dirt and other stuff can t remember which This should be cleaned and a dedicated project for core , utility , re usable , whatever classes should be created ",NULL,"The analytics project has been used as a host for common repository classes because it was easily visible by both dirt<span class='highlight-text severity-high'> and </span>other stuff can t remember which This should be cleaned and a dedicated project for core , utility , re usable , whatever classes should be created ","atomic","conjunctions","high",False
18919,"Secure Admin UI to challenge users to enter username and password to gain access.",NULL,"Secure Admin UI to challenge users to enter username and password to gain access.",NULL,"Add for who this story is","well_formed","no_role","high",False
18919,"Secure Admin UI to challenge users to enter username and password to gain access.",NULL,"Secure Admin UI to challenge users to enter username and password to gain access.",NULL,"Secure Admin UI to challenge users to enter username<span class='highlight-text severity-high'> and </span>password to gain access.","atomic","conjunctions","high",False
18920,"This is currently in the M6 pom organization name SpringSource name url http springsource.org url organization ",NULL,"This is currently in the M6 pom organization name SpringSource name url http springsource.org url organization ",NULL,"Add for who this story is","well_formed","no_role","high",False
18922,"If the rabbit source receives a message it can t convert, a MessageConversionException is thrown and the message is rejected and requeued , causing an endless loop. Add an ErrorHandler to the inbound adapter to detect and convert MCE to AmqpRejectAndDontRequeueException . Also consider adding a retry interceptor to do the same for exceptions in modules when using local transport .",NULL,"If the rabbit source receives a message it can t convert, a MessageConversionException is thrown and the message is rejected and requeued , causing an endless loop. Add an ErrorHandler to the inbound adapter to detect and convert MCE to AmqpRejectAndDontRequeueException . Also consider adding a retry interceptor to do the same for exceptions in modules when using local transport .",NULL,"Add for who this story is","well_formed","no_role","high",False
18922,"If the rabbit source receives a message it can t convert, a MessageConversionException is thrown and the message is rejected and requeued , causing an endless loop. Add an ErrorHandler to the inbound adapter to detect and convert MCE to AmqpRejectAndDontRequeueException . Also consider adding a retry interceptor to do the same for exceptions in modules when using local transport .",NULL,"If the rabbit source receives a message it can t convert, a MessageConversionException is thrown and the message is rejected and requeued , causing an endless loop. Add an ErrorHandler to the inbound adapter to detect and convert MCE to AmqpRejectAndDontRequeueException . Also consider adding a retry interceptor to do the same for exceptions in modules when using local transport .",NULL,"If the rabbit source receives a message it can t convert, a MessageConversionException is thrown<span class='highlight-text severity-high'> and </span>the message is rejected and requeued , causing an endless loop. Add an ErrorHandler to the inbound adapter to detect and convert MCE to AmqpRejectAndDontRequeueException . Also consider adding a retry interceptor to do the same for exceptions in modules when using local transport .","atomic","conjunctions","high",False
18922,"If the rabbit source receives a message it can t convert, a MessageConversionException is thrown and the message is rejected and requeued , causing an endless loop. Add an ErrorHandler to the inbound adapter to detect and convert MCE to AmqpRejectAndDontRequeueException . Also consider adding a retry interceptor to do the same for exceptions in modules when using local transport .",NULL,"If the rabbit source receives a message it can t convert, a MessageConversionException is thrown and the message is rejected and requeued , causing an endless loop. Add an ErrorHandler to the inbound adapter to detect and convert MCE to AmqpRejectAndDontRequeueException . Also consider adding a retry interceptor to do the same for exceptions in modules when using local transport .",NULL,"If the rabbit source receives a message it can t convert, a MessageConversionException is thrown and the message is rejected and requeued , causing an endless loop<span class='highlight-text severity-high'>. Add an ErrorHandler to the inbound adapter to detect and convert MCE to AmqpRejectAndDontRequeueException . Also consider adding a retry interceptor to do the same for exceptions in modules when using local transport .</span>","minimal","punctuation","high",False
18923,"acknowlege more, tx size, prefetch count, concurrency etc.",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
18923,"acknowlege more, tx size, prefetch count, concurrency etc.",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
18916,"To configure Rabbit HA a naming convention should be used to identify the queue that need to be mirrored. ",NULL,"To configure Rabbit HA a naming convention should be used to identify the queue that need to be mirrored. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18911,"This change has to be facilitated because of the XD 1456 and XD 1455 stories.",NULL,"This change has to be facilitated because of the XD 1456 and XD 1455 stories.",NULL,"Add for who this story is","well_formed","no_role","high",False
18914,"Options that are not covered codec idleTimeout inUsePrefix inUseSuffix inputType overwrite Options Renamed filename is now fileName",NULL,"Options that are not covered codec idleTimeout inUsePrefix inUseSuffix inputType overwrite Options Renamed filename is now fileName",NULL,"Add for who this story is","well_formed","no_role","high",False
18925,"Modify the PluginContextExtensionsInitializer to consume .groovy bean definitions as well as XML. ",NULL,"Modify the PluginContextExtensionsInitializer to consume .groovy bean definitions as well as XML. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18937,"from the wordcount sample code xd ! cp tmp nietzsche chapter 1.txt tmp xd input wordCountFiles You cannot specify option more than once in a single command code ",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
18937,"from the wordcount sample code xd ! cp tmp nietzsche chapter 1.txt tmp xd input wordCountFiles You cannot specify option more than once in a single command code ",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
18938,"Creating the following stream throws exception stream create s1 definition http transform script transform.groovy log Command failed org.springframework.xd.rest.client.impl.SpringXDException Error with option s for module transform of type processor valid the script and expression options are mutually exclusive The ExpressionOrScriptMixin s assertions to check if script and expression options are mutually exclusive always fails.",NULL,"Creating the following stream throws exception stream create s1 definition http transform script transform.groovy log Command failed org.springframework.xd.rest.client.impl.SpringXDException Error with option s for module transform of type processor valid the script and expression options are mutually exclusive The ExpressionOrScriptMixin s assertions to check if script and expression options are mutually exclusive always fails.",NULL,"Add for who this story is","well_formed","no_role","high",False
18938,"Creating the following stream throws exception stream create s1 definition http transform script transform.groovy log Command failed org.springframework.xd.rest.client.impl.SpringXDException Error with option s for module transform of type processor valid the script and expression options are mutually exclusive The ExpressionOrScriptMixin s assertions to check if script and expression options are mutually exclusive always fails.",NULL,"Creating the following stream throws exception stream create s1 definition http transform script transform.groovy log Command failed org.springframework.xd.rest.client.impl.SpringXDException Error with option s for module transform of type processor valid the script and expression options are mutually exclusive The ExpressionOrScriptMixin s assertions to check if script and expression options are mutually exclusive always fails.",NULL,"Creating the following stream throws exception stream create s1 definition http transform script transform.groovy log Command failed org.springframework.xd.rest.client.impl.SpringXDException Error with option s for module transform of type processor valid the script<span class='highlight-text severity-high'> and </span>expression options are mutually exclusive The ExpressionOrScriptMixin s assertions to check if script and expression options are mutually exclusive always fails.","atomic","conjunctions","high",False
18939,"need the ability to support the group option.",NULL,"need the ability to support the group option.",NULL,"Add for who this story is","well_formed","no_role","high",False
18940,"Need to update instructions to discuss the setup of the relational database requirement for the xd admin.",NULL,"Need to update instructions to discuss the setup of the relational database requirement for the xd admin.",NULL,"Add for who this story is","well_formed","no_role","high",False
18941,"Make changes to XD on YARN config that correspond to XD 1499 changes",NULL,"Make changes to XD on YARN config that correspond to XD 1499 changes",NULL,"Add for who this story is","well_formed","no_role","high",False
18945,"The setting of the xd.stream.name property is currently duplicated in StreamPlugin and JobPlugin, etc Moreover, there is no strong String constant to reference it Create a plugin dedicated to those matters namely making bits of DeploymentMetadata available to the module environment ",NULL,"The setting of the xd.stream.name property is currently duplicated in StreamPlugin and JobPlugin, etc Moreover, there is no strong String constant to reference it Create a plugin dedicated to those matters namely making bits of DeploymentMetadata available to the module environment ",NULL,"Add for who this story is","well_formed","no_role","high",False
18945,"The setting of the xd.stream.name property is currently duplicated in StreamPlugin and JobPlugin, etc Moreover, there is no strong String constant to reference it Create a plugin dedicated to those matters namely making bits of DeploymentMetadata available to the module environment ",NULL,"The setting of the xd.stream.name property is currently duplicated in StreamPlugin and JobPlugin, etc Moreover, there is no strong String constant to reference it Create a plugin dedicated to those matters namely making bits of DeploymentMetadata available to the module environment ",NULL,"The setting of the xd.stream.name property is currently duplicated in StreamPlugin<span class='highlight-text severity-high'> and </span>JobPlugin, etc Moreover, there is no strong String constant to reference it Create a plugin dedicated to those matters namely making bits of DeploymentMetadata available to the module environment ","atomic","conjunctions","high",False
18949,"This will allow us to use multiple instances of hdfs file sinks and not have any filename path collisions.",NULL,"This will allow us to use multiple instances of hdfs file sinks and not have any filename path collisions.",NULL,"Add for who this story is","well_formed","no_role","high",False
19168,"Would be nice to have some kind of regression testing on the jdbc sink, as it becomes more prominent in XD. Use of an in memory db where we expose eg a JdbcTemplate to assert state",NULL,"Would be nice to have some kind of regression testing on the jdbc sink, as it becomes more prominent in XD. Use of an in memory db where we expose eg a JdbcTemplate to assert state",NULL,"Add for who this story is","well_formed","no_role","high",False
18946,"See XD 1283. We ve been waiting for 1283 to change constructs like noformat attr name xd.stream.name to just noformat attr name noformat Turns out we can simply push down the xd.stream.name bit in the default value most likely initialization of a field in a POJO metadata class and it will work just fine. We can also consider providing a fake value for those placeholders to use when doing module info ie user will see name of the stream instead of xd.stream.name ",NULL,"See XD 1283. We ve been waiting for 1283 to change constructs like noformat attr name xd.stream.name to just noformat attr name noformat Turns out we can simply push down the xd.stream.name bit in the default value most likely initialization of a field in a POJO metadata class and it will work just fine. We can also consider providing a fake value for those placeholders to use when doing module info ie user will see name of the stream instead of xd.stream.name ",NULL,"See XD 1283. We ve been waiting for 1283 to change constructs like noformat attr name xd.stream.name to just noformat attr name noformat Turns out we can simply push down the xd.stream.name bit in the default value most likely initialization of a field in a POJO metadata class<span class='highlight-text severity-high'> and </span>it will work just fine. We can also consider providing a fake value for those placeholders to use when doing module info ie user will see name of the stream instead of xd.stream.name ","atomic","conjunctions","high",False
18946,"See XD 1283. We ve been waiting for 1283 to change constructs like noformat attr name xd.stream.name to just noformat attr name noformat Turns out we can simply push down the xd.stream.name bit in the default value most likely initialization of a field in a POJO metadata class and it will work just fine. We can also consider providing a fake value for those placeholders to use when doing module info ie user will see name of the stream instead of xd.stream.name ",NULL,"See XD 1283. We ve been waiting for 1283 to change constructs like noformat attr name xd.stream.name to just noformat attr name noformat Turns out we can simply push down the xd.stream.name bit in the default value most likely initialization of a field in a POJO metadata class and it will work just fine. We can also consider providing a fake value for those placeholders to use when doing module info ie user will see name of the stream instead of xd.stream.name ",NULL,"See XD 1283<span class='highlight-text severity-high'>. We ve been waiting for 1283 to change constructs like noformat attr name xd.stream.name to just noformat attr name noformat Turns out we can simply push down the xd.stream.name bit in the default value most likely initialization of a field in a POJO metadata class and it will work just fine. We can also consider providing a fake value for those placeholders to use when doing module info ie user will see name of the stream instead of xd.stream.name </span>","minimal","punctuation","high",False
18947,"oracle, gemfire xd derby should be the dialect , and sybase",NULL,"oracle, gemfire xd derby should be the dialect , and sybase",NULL,"Add for who this story is","well_formed","no_role","high",False
18943,"Currently, single node application assumes to use hsqldb server by default and starts the hsqldb as it starts. We need a way to provide external datasource and not to start hsqldb by default when the single node starts up.",NULL,"Currently, single node application assumes to use hsqldb server by default and starts the hsqldb as it starts. We need a way to provide external datasource and not to start hsqldb by default when the single node starts up.",NULL,"Add for who this story is","well_formed","no_role","high",False
18943,"Currently, single node application assumes to use hsqldb server by default and starts the hsqldb as it starts. We need a way to provide external datasource and not to start hsqldb by default when the single node starts up.",NULL,"Currently, single node application assumes to use hsqldb server by default and starts the hsqldb as it starts. We need a way to provide external datasource and not to start hsqldb by default when the single node starts up.",NULL,"Currently, single node application assumes to use hsqldb server by default<span class='highlight-text severity-high'> and </span>starts the hsqldb as it starts. We need a way to provide external datasource and not to start hsqldb by default when the single node starts up.","atomic","conjunctions","high",False
18943,"Currently, single node application assumes to use hsqldb server by default and starts the hsqldb as it starts. We need a way to provide external datasource and not to start hsqldb by default when the single node starts up.",NULL,"Currently, single node application assumes to use hsqldb server by default and starts the hsqldb as it starts. We need a way to provide external datasource and not to start hsqldb by default when the single node starts up.",NULL,"Currently, single node application assumes to use hsqldb server by default and starts the hsqldb as it starts<span class='highlight-text severity-high'>. We need a way to provide external datasource and not to start hsqldb by default when the single node starts up.</span>","minimal","punctuation","high",False
18935,"When trying to create the stream for the gemfire example stream create hashtags definition tap stream tweets transform script tweetSummary.groovy gemfire server keyExpression payload id deploy The shell displays Command failed org.springframework.xd.rest.client.impl.SpringXDException Error with option s for module transform of type processor valid the script and expression options are mutually exclusive I get the following error ",NULL,"When trying to create the stream for the gemfire example stream create hashtags definition tap stream tweets transform script tweetSummary.groovy gemfire server keyExpression payload id deploy The shell displays Command failed org.springframework.xd.rest.client.impl.SpringXDException Error with option s for module transform of type processor valid the script and expression options are mutually exclusive I get the following error ",NULL,"Add for who this story is","well_formed","no_role","high",False
18935,"When trying to create the stream for the gemfire example stream create hashtags definition tap stream tweets transform script tweetSummary.groovy gemfire server keyExpression payload id deploy The shell displays Command failed org.springframework.xd.rest.client.impl.SpringXDException Error with option s for module transform of type processor valid the script and expression options are mutually exclusive I get the following error ",NULL,"When trying to create the stream for the gemfire example stream create hashtags definition tap stream tweets transform script tweetSummary.groovy gemfire server keyExpression payload id deploy The shell displays Command failed org.springframework.xd.rest.client.impl.SpringXDException Error with option s for module transform of type processor valid the script and expression options are mutually exclusive I get the following error ",NULL,"When trying to create the stream for the gemfire example stream create hashtags definition tap stream tweets transform script tweetSummary.groovy gemfire server keyExpression payload id deploy The shell displays Command failed org.springframework.xd.rest.client.impl.SpringXDException Error with option s for module transform of type processor valid the script<span class='highlight-text severity-high'> and </span>expression options are mutually exclusive I get the following error ","atomic","conjunctions","high",False
18944,"Should mention jolokia, how to turn on off boot jolokia http metric monitoring and jmx. Mention the naming strategy to identify modules running in a stream.",NULL,"Should mention jolokia, how to turn on off boot jolokia http metric monitoring and jmx. Mention the naming strategy to identify modules running in a stream.",NULL,"Add for who this story is","well_formed","no_role","high",False
18944,"Should mention jolokia, how to turn on off boot jolokia http metric monitoring and jmx. Mention the naming strategy to identify modules running in a stream.",NULL,"Should mention jolokia, how to turn on off boot jolokia http metric monitoring and jmx. Mention the naming strategy to identify modules running in a stream.",NULL,"Should mention jolokia, how to turn on off boot jolokia http metric monitoring and jmx<span class='highlight-text severity-high'>. Mention the naming strategy to identify modules running in a stream.</span>","minimal","punctuation","high",False
18948,"Currently deployments with an understore in XDController should be something else. Need to segment up the url space better for stream jobs to avoid a clash.",NULL,"Currently deployments with an understore in XDController should be something else. Need to segment up the url space better for stream jobs to avoid a clash.",NULL,"Add for who this story is","well_formed","no_role","high",False
18948,"Currently deployments with an understore in XDController should be something else. Need to segment up the url space better for stream jobs to avoid a clash.",NULL,"Currently deployments with an understore in XDController should be something else. Need to segment up the url space better for stream jobs to avoid a clash.",NULL,"Currently deployments with an understore in XDController should be something else<span class='highlight-text severity-high'>. Need to segment up the url space better for stream jobs to avoid a clash.</span>","minimal","punctuation","high",False
18973,"The tests that use HDFS currently require an external Hadoop installation and is hard to set up update version in all the environments where we want to run tests, e.g. bamboo, travis. See if the mini cluster described in http docs.spring.io spring hadoop docs 2.0.0.RC1 reference html testing.html testing yarn minicluster can be used in the test cases instead.",NULL,"The tests that use HDFS currently require an external Hadoop installation and is hard to set up update version in all the environments where we want to run tests, e.g. bamboo, travis. See if the mini cluster described in http docs.spring.io spring hadoop docs 2.0.0.RC1 reference html testing.html testing yarn minicluster can be used in the test cases instead.",NULL,"Add for who this story is","well_formed","no_role","high",False
18973,"The tests that use HDFS currently require an external Hadoop installation and is hard to set up update version in all the environments where we want to run tests, e.g. bamboo, travis. See if the mini cluster described in http docs.spring.io spring hadoop docs 2.0.0.RC1 reference html testing.html testing yarn minicluster can be used in the test cases instead.",NULL,"The tests that use HDFS currently require an external Hadoop installation and is hard to set up update version in all the environments where we want to run tests, e.g. bamboo, travis. See if the mini cluster described in http docs.spring.io spring hadoop docs 2.0.0.RC1 reference html testing.html testing yarn minicluster can be used in the test cases instead.",NULL,"The tests that use HDFS currently require an external Hadoop installation<span class='highlight-text severity-high'> and </span>is hard to set up update version in all the environments where we want to run tests, e.g. bamboo, travis. See if the mini cluster described in http docs.spring.io spring hadoop docs 2.0.0.RC1 reference html testing.html testing yarn minicluster can be used in the test cases instead.","atomic","conjunctions","high",False
18973,"The tests that use HDFS currently require an external Hadoop installation and is hard to set up update version in all the environments where we want to run tests, e.g. bamboo, travis. See if the mini cluster described in http docs.spring.io spring hadoop docs 2.0.0.RC1 reference html testing.html testing yarn minicluster can be used in the test cases instead.",NULL,"The tests that use HDFS currently require an external Hadoop installation and is hard to set up update version in all the environments where we want to run tests, e.g. bamboo, travis. See if the mini cluster described in http docs.spring.io spring hadoop docs 2.0.0.RC1 reference html testing.html testing yarn minicluster can be used in the test cases instead.",NULL,"The tests that use HDFS currently require an external Hadoop installation and is hard to set up update version in all the environments where we want to run tests, e<span class='highlight-text severity-high'>.g. bamboo, travis. See if the mini cluster described in http docs.spring.io spring hadoop docs 2.0.0.RC1 reference html testing.html testing yarn minicluster can be used in the test cases instead.</span>","minimal","punctuation","high",False
18966,"The two classes in question are org.springframework.xd.dirt.cluster.Container org.springframework.xd.dirt.container.ContainerMetadata The former is currently used by the Admin when making decisions about Module deployment. That latter was a replacement for RuntimeContainerInforEntity as we migrated the various Redis InMemory Repositories to use the data that is now available in ZooKeeper instead. The ContainerRepository is currently used by the Admin leader, and the ContainerMetadataRepository is used by the REST endpoint that supports the xd shell s runtime containers command. Perhaps those can also be merged. In any case, if not addressed by a larger refactoring, the ContainerRepository should probably support an Iterable return rather than an Iterator. Having finders e.g. for attribute key values such as group foo might be convenient for various Module deployment strategies.",NULL,"The two classes in question are org.springframework.xd.dirt.cluster.Container org.springframework.xd.dirt.container.ContainerMetadata The former is currently used by the Admin when making decisions about Module deployment. That latter was a replacement for RuntimeContainerInforEntity as we migrated the various Redis InMemory Repositories to use the data that is now available in ZooKeeper instead. The ContainerRepository is currently used by the Admin leader, and the ContainerMetadataRepository is used by the REST endpoint that supports the xd shell s runtime containers command. Perhaps those can also be merged. In any case, if not addressed by a larger refactoring, the ContainerRepository should probably support an Iterable return rather than an Iterator. Having finders e.g. for attribute key values such as group foo might be convenient for various Module deployment strategies.",NULL,"Add for who this story is","well_formed","no_role","high",False
18966,"The two classes in question are org.springframework.xd.dirt.cluster.Container org.springframework.xd.dirt.container.ContainerMetadata The former is currently used by the Admin when making decisions about Module deployment. That latter was a replacement for RuntimeContainerInforEntity as we migrated the various Redis InMemory Repositories to use the data that is now available in ZooKeeper instead. The ContainerRepository is currently used by the Admin leader, and the ContainerMetadataRepository is used by the REST endpoint that supports the xd shell s runtime containers command. Perhaps those can also be merged. In any case, if not addressed by a larger refactoring, the ContainerRepository should probably support an Iterable return rather than an Iterator. Having finders e.g. for attribute key values such as group foo might be convenient for various Module deployment strategies.",NULL,"The two classes in question are org.springframework.xd.dirt.cluster.Container org.springframework.xd.dirt.container.ContainerMetadata The former is currently used by the Admin when making decisions about Module deployment. That latter was a replacement for RuntimeContainerInforEntity as we migrated the various Redis InMemory Repositories to use the data that is now available in ZooKeeper instead. The ContainerRepository is currently used by the Admin leader, and the ContainerMetadataRepository is used by the REST endpoint that supports the xd shell s runtime containers command. Perhaps those can also be merged. In any case, if not addressed by a larger refactoring, the ContainerRepository should probably support an Iterable return rather than an Iterator. Having finders e.g. for attribute key values such as group foo might be convenient for various Module deployment strategies.",NULL,"The two classes in question are org.springframework.xd.dirt.cluster.Container org.springframework.xd.dirt.container.ContainerMetadata The former is currently used by the Admin when making decisions about Module deployment. That latter was a replacement for RuntimeContainerInforEntity as we migrated the various Redis InMemory Repositories to use the data that is now available in ZooKeeper instead. The ContainerRepository is currently used by the Admin leader,<span class='highlight-text severity-high'> and </span>the ContainerMetadataRepository is used by the REST endpoint that supports the xd shell s runtime containers command. Perhaps those can also be merged. In any case, if not addressed by a larger refactoring, the ContainerRepository should probably support an Iterable return rather than an Iterator. Having finders e.g. for attribute key values such as group foo might be convenient for various Module deployment strategies.","atomic","conjunctions","high",False
18966,"The two classes in question are org.springframework.xd.dirt.cluster.Container org.springframework.xd.dirt.container.ContainerMetadata The former is currently used by the Admin when making decisions about Module deployment. That latter was a replacement for RuntimeContainerInforEntity as we migrated the various Redis InMemory Repositories to use the data that is now available in ZooKeeper instead. The ContainerRepository is currently used by the Admin leader, and the ContainerMetadataRepository is used by the REST endpoint that supports the xd shell s runtime containers command. Perhaps those can also be merged. In any case, if not addressed by a larger refactoring, the ContainerRepository should probably support an Iterable return rather than an Iterator. Having finders e.g. for attribute key values such as group foo might be convenient for various Module deployment strategies.",NULL,"The two classes in question are org.springframework.xd.dirt.cluster.Container org.springframework.xd.dirt.container.ContainerMetadata The former is currently used by the Admin when making decisions about Module deployment. That latter was a replacement for RuntimeContainerInforEntity as we migrated the various Redis InMemory Repositories to use the data that is now available in ZooKeeper instead. The ContainerRepository is currently used by the Admin leader, and the ContainerMetadataRepository is used by the REST endpoint that supports the xd shell s runtime containers command. Perhaps those can also be merged. In any case, if not addressed by a larger refactoring, the ContainerRepository should probably support an Iterable return rather than an Iterator. Having finders e.g. for attribute key values such as group foo might be convenient for various Module deployment strategies.",NULL,"The two classes in question are org<span class='highlight-text severity-high'>.springframework.xd.dirt.cluster.Container org.springframework.xd.dirt.container.ContainerMetadata The former is currently used by the Admin when making decisions about Module deployment. That latter was a replacement for RuntimeContainerInforEntity as we migrated the various Redis InMemory Repositories to use the data that is now available in ZooKeeper instead. The ContainerRepository is currently used by the Admin leader, and the ContainerMetadataRepository is used by the REST endpoint that supports the xd shell s runtime containers command. Perhaps those can also be merged. In any case, if not addressed by a larger refactoring, the ContainerRepository should probably support an Iterable return rather than an Iterator. Having finders e.g. for attribute key values such as group foo might be convenient for various Module deployment strategies.</span>","minimal","punctuation","high",False
18967,"Currently we have many catch Exception blocks that simply wrap and rethrow RuntimeExceptions. We should create at least a top level RuntimeException of our own, within the XD Exception hierarchy, and possibly a hierarchy of RuntimeExceptions extending from that, and mapping to the various checked Exceptions that can occur in ZooKeeper data access. Also, we should not be re wrapping those Exceptions that are already RuntimeExceptions, so we should consider a ZooKeeperExceptionHandler and although I m typically hesitant to recommend it, this might be a case where a static util method is the right approach . ",NULL,"Currently we have many catch Exception blocks that simply wrap and rethrow RuntimeExceptions. We should create at least a top level RuntimeException of our own, within the XD Exception hierarchy, and possibly a hierarchy of RuntimeExceptions extending from that, and mapping to the various checked Exceptions that can occur in ZooKeeper data access. Al","so, we should not be re wrapping those Exceptions that are already RuntimeExceptions, so we should consider a ZooKeeperExceptionHandler and although I m typically hesitant to recommend it, this might be a case where a static util method is the right approach .","Add for who this story is","well_formed","no_role","high",False
18967,"Currently we have many catch Exception blocks that simply wrap and rethrow RuntimeExceptions. We should create at least a top level RuntimeException of our own, within the XD Exception hierarchy, and possibly a hierarchy of RuntimeExceptions extending from that, and mapping to the various checked Exceptions that can occur in ZooKeeper data access. Also, we should not be re wrapping those Exceptions that are already RuntimeExceptions, so we should consider a ZooKeeperExceptionHandler and although I m typically hesitant to recommend it, this might be a case where a static util method is the right approach . ",NULL,"Currently we have many catch Exception blocks that simply wrap and rethrow RuntimeExceptions. We should create at least a top level RuntimeException of our own, within the XD Exception hierarchy, and possibly a hierarchy of RuntimeExceptions extending from that, and mapping to the various checked Exceptions that can occur in ZooKeeper data access. Al","so, we should not be re wrapping those Exceptions that are already RuntimeExceptions, so we should consider a ZooKeeperExceptionHandler and although I m typically hesitant to recommend it, this might be a case where a static util method is the right approach .","Currently we have many catch Exception blocks that simply wrap<span class='highlight-text severity-high'> and </span>rethrow RuntimeExceptions. We should create at least a top level RuntimeException of our own, within the XD Exception hierarchy, and possibly a hierarchy of RuntimeExceptions extending from that, and mapping to the various checked Exceptions that can occur in ZooKeeper data access. Also, we should not be re wrapping those Exceptions that are already RuntimeExceptions, so we should consider a ZooKeeperExceptionHandler and although I m typically hesitant to recommend it, this might be a case where a static util method is the right approach . ","atomic","conjunctions","high",False
19168,"Would be nice to have some kind of regression testing on the jdbc sink, as it becomes more prominent in XD. Use of an in memory db where we expose eg a JdbcTemplate to assert state",NULL,"Would be nice to have some kind of regression testing on the jdbc sink, as it becomes more prominent in XD. Use of an in memory db where we expose eg a JdbcTemplate to assert state",NULL,"Would be nice to have some kind of regression testing on the jdbc sink, as it becomes more prominent in XD<span class='highlight-text severity-high'>. Use of an in memory db where we expose eg a JdbcTemplate to assert state</span>","minimal","punctuation","high",False
18967,"Currently we have many catch Exception blocks that simply wrap and rethrow RuntimeExceptions. We should create at least a top level RuntimeException of our own, within the XD Exception hierarchy, and possibly a hierarchy of RuntimeExceptions extending from that, and mapping to the various checked Exceptions that can occur in ZooKeeper data access. Also, we should not be re wrapping those Exceptions that are already RuntimeExceptions, so we should consider a ZooKeeperExceptionHandler and although I m typically hesitant to recommend it, this might be a case where a static util method is the right approach . ",NULL,"Currently we have many catch Exception blocks that simply wrap and rethrow RuntimeExceptions. We should create at least a top level RuntimeException of our own, within the XD Exception hierarchy, and possibly a hierarchy of RuntimeExceptions extending from that, and mapping to the various checked Exceptions that can occur in ZooKeeper data access. Al","so, we should not be re wrapping those Exceptions that are already RuntimeExceptions, so we should consider a ZooKeeperExceptionHandler and although I m typically hesitant to recommend it, this might be a case where a static util method is the right approach .","Currently we have many catch Exception blocks that simply wrap and rethrow RuntimeExceptions<span class='highlight-text severity-high'>. We should create at least a top level RuntimeException of our own, within the XD Exception hierarchy, and possibly a hierarchy of RuntimeExceptions extending from that, and mapping to the various checked Exceptions that can occur in ZooKeeper data access. Also, we should not be re wrapping those Exceptions that are already RuntimeExceptions, so we should consider a ZooKeeperExceptionHandler and although I m typically hesitant to recommend it, this might be a case where a static util method is the right approach . </span>","minimal","punctuation","high",False
18967,"Currently we have many catch Exception blocks that simply wrap and rethrow RuntimeExceptions. We should create at least a top level RuntimeException of our own, within the XD Exception hierarchy, and possibly a hierarchy of RuntimeExceptions extending from that, and mapping to the various checked Exceptions that can occur in ZooKeeper data access. Also, we should not be re wrapping those Exceptions that are already RuntimeExceptions, so we should consider a ZooKeeperExceptionHandler and although I m typically hesitant to recommend it, this might be a case where a static util method is the right approach . ",NULL,"Currently we have many catch Exception blocks that simply wrap and rethrow RuntimeExceptions. We should create at least a top level RuntimeException of our own, within the XD Exception hierarchy, and possibly a hierarchy of RuntimeExceptions extending from that, and mapping to the various checked Exceptions that can occur in ZooKeeper data access. Al","so, we should not be re wrapping those Exceptions that are already RuntimeExceptions, so we should consider a ZooKeeperExceptionHandler and although I m typically hesitant to recommend it, this might be a case where a static util method is the right approach .","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18969,"change jolokia list ; to management jolokia list ; etc.",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
18969,"change jolokia list ; to management jolokia list ; etc.",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
18969,"change jolokia list ; to management jolokia list ; etc.",NULL,NULL,NULL,"change jolokia list <span class='highlight-text severity-high'>; to management jolokia list ; etc.</span>","minimal","punctuation","high",False
18972,"AggregateCounter needs a test case along the lines of of org.springframework.xd.analytics.metrics.integration.FieldValueCounterHandlerTests that will demonstrate the base functionality of the module as well as use of the timeField and dateFormat options.",NULL,"AggregateCounter needs a test case along the lines of of org.springframework.xd.analytics.metrics.integration.FieldValueCounterHandlerTests that will demonstrate the base functionality of the module as well as use of the timeField and dateFormat options.",NULL,"Add for who this story is","well_formed","no_role","high",False
18974,"After some discussion and voting, we decided to remove jmxEnabled as a command line option and have JMX enabled by default. This can be disabled from xd config.yml externally.",NULL,"After some discussion and voting, we decided to remove jmxEnabled as a command line option and have JMX enabled by default. This can be disabled from xd config.yml externally.",NULL,"Add for who this story is","well_formed","no_role","high",False
18974,"After some discussion and voting, we decided to remove jmxEnabled as a command line option and have JMX enabled by default. This can be disabled from xd config.yml externally.",NULL,"After some discussion and voting, we decided to remove jmxEnabled as a command line option and have JMX enabled by default. This can be disabled from xd config.yml externally.",NULL,"After some discussion<span class='highlight-text severity-high'> and </span>voting, we decided to remove jmxEnabled as a command line option and have JMX enabled by default. This can be disabled from xd config.yml externally.","atomic","conjunctions","high",False
18974,"After some discussion and voting, we decided to remove jmxEnabled as a command line option and have JMX enabled by default. This can be disabled from xd config.yml externally.",NULL,"After some discussion and voting, we decided to remove jmxEnabled as a command line option and have JMX enabled by default. This can be disabled from xd config.yml externally.",NULL,"After some discussion and voting, we decided to remove jmxEnabled as a command line option and have JMX enabled by default<span class='highlight-text severity-high'>. This can be disabled from xd config.yml externally.</span>","minimal","punctuation","high",False
18970,"Currently on snapshots, which is oddly pulling in groovy 2.1.0",NULL,"Currently on snapshots, which is oddly pulling in groovy 2.1.0",NULL,"Add for who this story is","well_formed","no_role","high",False
18964,"Also likely rename, remove, or replace that Module maybe can be supplanted by ModuleDescriptor when used in the refactored parser . Also, considering the url property is not necessary vestige of the prototype , all we d be left with here is the Module name and type, which are used to identify a Module uniquely. Therefore this Module could be renamed to ModuleKey or something. It could be used within the StreamDefinition itself e.g. getDescriptor moduleKey .",NULL,"Also likely rename, remove, or replace that Module maybe can be supplanted by ModuleDescriptor when used in the refactored parser . Also, considering the url property is not necessary vestige of the prototype , all we d be left with here is the Module name and type, which are used to identify a Module uniquely. Therefore this Module could be renamed to ModuleKey or something. It could be used within the StreamDefinition itself e.g. getDescriptor moduleKey .",NULL,"Add for who this story is","well_formed","no_role","high",False
18964,"Also likely rename, remove, or replace that Module maybe can be supplanted by ModuleDescriptor when used in the refactored parser . Also, considering the url property is not necessary vestige of the prototype , all we d be left with here is the Module name and type, which are used to identify a Module uniquely. Therefore this Module could be renamed to ModuleKey or something. It could be used within the StreamDefinition itself e.g. getDescriptor moduleKey .",NULL,"Also likely rename, remove, or replace that Module maybe can be supplanted by ModuleDescriptor when used in the refactored parser . Also, considering the url property is not necessary vestige of the prototype , all we d be left with here is the Module name and type, which are used to identify a Module uniquely. Therefore this Module could be renamed to ModuleKey or something. It could be used within the StreamDefinition itself e.g. getDescriptor moduleKey .",NULL,"Also likely rename, remove,<span class='highlight-text severity-high'> or </span>replace that Module maybe can be supplanted by ModuleDescriptor when used in the refactored parser . Also, considering the url property is not necessary vestige of the prototype , all we d be left with here is the Module name<span class='highlight-text severity-high'> and </span>type, which are used to identify a Module uniquely. Therefore this Module could be renamed to ModuleKey or something. It could be used within the StreamDefinition itself e.g. getDescriptor moduleKey .","atomic","conjunctions","high",False
18964,"Also likely rename, remove, or replace that Module maybe can be supplanted by ModuleDescriptor when used in the refactored parser . Also, considering the url property is not necessary vestige of the prototype , all we d be left with here is the Module name and type, which are used to identify a Module uniquely. Therefore this Module could be renamed to ModuleKey or something. It could be used within the StreamDefinition itself e.g. getDescriptor moduleKey .",NULL,"Also likely rename, remove, or replace that Module maybe can be supplanted by ModuleDescriptor when used in the refactored parser . Also, considering the url property is not necessary vestige of the prototype , all we d be left with here is the Module name and type, which are used to identify a Module uniquely. Therefore this Module could be renamed to ModuleKey or something. It could be used within the StreamDefinition itself e.g. getDescriptor moduleKey .",NULL,"Also likely rename, remove, or replace that Module maybe can be supplanted by ModuleDescriptor when used in the refactored parser <span class='highlight-text severity-high'>. Also, considering the url property is not necessary vestige of the prototype , all we d be left with here is the Module name and type, which are used to identify a Module uniquely. Therefore this Module could be renamed to ModuleKey or something. It could be used within the StreamDefinition itself e.g. getDescriptor moduleKey .</span>","minimal","punctuation","high",False
18971,"This would get rid of the CF specific post module, keeping the general abstraction of http source across CF and non CF environments.",NULL,"This would get rid of the CF specific post module, keeping the general abstraction of http source across CF and non CF environments.",NULL,"Add for who this story is","well_formed","no_role","high",False
18983,"Please see the discussion here https github.com spring projects spring xd pull 655 files r10892925",NULL,"Please see the discussion here https github.com spring projects spring xd pull 655 files r10892925",NULL,"Add for who this story is","well_formed","no_role","high",False
18976,"Originally it was placed in the code to prevent overtasking our servers with downloads when people want to install XD. And to allow for faster downloads. However XD Jars are already placed on S3, so this feature is no longer needed.",NULL,"Originally it was placed in the code to prevent overtasking our servers with downloads when people want to install XD. And to allow for faster downloads. However XD Jars are already placed on S3,","so this feature is no longer needed.","Add for who this story is","well_formed","no_role","high",False
18976,"Originally it was placed in the code to prevent overtasking our servers with downloads when people want to install XD. And to allow for faster downloads. However XD Jars are already placed on S3, so this feature is no longer needed.",NULL,"Originally it was placed in the code to prevent overtasking our servers with downloads when people want to install XD. And to allow for faster downloads. However XD Jars are already placed on S3,","so this feature is no longer needed.","Originally it was placed in the code to prevent overtasking our servers with downloads when people want to install XD.<span class='highlight-text severity-high'> and </span>to allow for faster downloads. However XD Jars are already placed on S3, so this feature is no longer needed.","atomic","conjunctions","high",False
18976,"Originally it was placed in the code to prevent overtasking our servers with downloads when people want to install XD. And to allow for faster downloads. However XD Jars are already placed on S3, so this feature is no longer needed.",NULL,"Originally it was placed in the code to prevent overtasking our servers with downloads when people want to install XD. And to allow for faster downloads. However XD Jars are already placed on S3,","so this feature is no longer needed.","Originally it was placed in the code to prevent overtasking our servers with downloads when people want to install XD<span class='highlight-text severity-high'>. And to allow for faster downloads. However XD Jars are already placed on S3, so this feature is no longer needed.</span>","minimal","punctuation","high",False
18976,"Originally it was placed in the code to prevent overtasking our servers with downloads when people want to install XD. And to allow for faster downloads. However XD Jars are already placed on S3, so this feature is no longer needed.",NULL,"Originally it was placed in the code to prevent overtasking our servers with downloads when people want to install XD. And to allow for faster downloads. However XD Jars are already placed on S3,","so this feature is no longer needed.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18977,"With the addition of sinks and sources that require connections with external entities hadoop, JMS, JDBC, ... the environment setup is getting unwieldy. Integrate SpringJUnit4ClassRunner.class into acceptance tests. Retrieve environment variables via Dependency injection from application.properties. Utilize profiles for local single node local cluster ec2 single node ec2 cluster",NULL,"With the addition of sinks and sources that require connections with external entities hadoop, JMS, JDBC, ... the environment setup is getting unwieldy. Integrate SpringJUnit4ClassRunner.class into acceptance tests. Retrieve environment variables via Dependency injection from application.properties. Utilize profiles for local single node local cluster ec2 single node ec2 cluster",NULL,"Add for who this story is","well_formed","no_role","high",False
18977,"With the addition of sinks and sources that require connections with external entities hadoop, JMS, JDBC, ... the environment setup is getting unwieldy. Integrate SpringJUnit4ClassRunner.class into acceptance tests. Retrieve environment variables via Dependency injection from application.properties. Utilize profiles for local single node local cluster ec2 single node ec2 cluster",NULL,"With the addition of sinks and sources that require connections with external entities hadoop, JMS, JDBC, ... the environment setup is getting unwieldy. Integrate SpringJUnit4ClassRunner.class into acceptance tests. Retrieve environment variables via Dependency injection from application.properties. Utilize profiles for local single node local cluster ec2 single node ec2 cluster",NULL,"With the addition of sinks and sources that require connections with external entities hadoop, JMS, JDBC, <span class='highlight-text severity-high'>... the environment setup is getting unwieldy. Integrate SpringJUnit4ClassRunner.class into acceptance tests. Retrieve environment variables via Dependency injection from application.properties. Utilize profiles for local single node local cluster ec2 single node ec2 cluster</span>","minimal","punctuation","high",False
18978,"Replace tests and throws in the XdEc2Validation with asserts in the following methods verifyTestContent verifyLogContent verifySendCounts VerifySendCounts should check for the exact number of Jmx events instead of 0.",NULL,"Replace tests and throws in the XdEc2Validation with asserts in the following methods verifyTestContent verifyLogContent verifySendCounts VerifySendCounts should check for the exact number of Jmx events instead of 0.",NULL,"Add for who this story is","well_formed","no_role","high",False
18978,"Replace tests and throws in the XdEc2Validation with asserts in the following methods verifyTestContent verifyLogContent verifySendCounts VerifySendCounts should check for the exact number of Jmx events instead of 0.",NULL,"Replace tests and throws in the XdEc2Validation with asserts in the following methods verifyTestContent verifyLogContent verifySendCounts VerifySendCounts should check for the exact number of Jmx events instead of 0.",NULL,"Replace tests<span class='highlight-text severity-high'> and </span>throws in the XdEc2Validation with asserts in the following methods verifyTestContent verifyLogContent verifySendCounts VerifySendCounts should check for the exact number of Jmx events instead of 0.","atomic","conjunctions","high",False
18979,"With the addition of zookeeper, a user does not have to specify rabbit or redis for the control channel. This story should allow a user to specify redis, rabbit or no control channel.",NULL,"With the addition of zookeeper, a user does not have to specify rabbit or redis for the control channel. This story should allow a user to specify redis, rabbit or no control channel.",NULL,"Add for who this story is","well_formed","no_role","high",False
18979,"With the addition of zookeeper, a user does not have to specify rabbit or redis for the control channel. This story should allow a user to specify redis, rabbit or no control channel.",NULL,"With the addition of zookeeper, a user does not have to specify rabbit or redis for the control channel. This story should allow a user to specify redis, rabbit or no control channel.",NULL,"With the addition of zookeeper, a user does not have to specify rabbit or redis for the control channel<span class='highlight-text severity-high'>. This story should allow a user to specify redis, rabbit or no control channel.</span>","minimal","punctuation","high",False
18980,"Get minimal compilation going in Travis. Redis Rabbit services can be done in a separate story.",NULL,"Get minimal compilation going in Travis. Redis Rabbit services can be done in a separate story.",NULL,"Add for who this story is","well_formed","no_role","high",False
18980,"Get minimal compilation going in Travis. Redis Rabbit services can be done in a separate story.",NULL,"Get minimal compilation going in Travis. Redis Rabbit services can be done in a separate story.",NULL,"Get minimal compilation going in Travis<span class='highlight-text severity-high'>. Redis Rabbit services can be done in a separate story.</span>","minimal","punctuation","high",False
18981,"In order to support ingestion from stdin, the suggested approach is to do the following. xd stream create definition tcp decoder LF log name foo cat my.log netcat localhost 1234 So while this is really a tcp based ingestion case, once can use pipe or redirect of stdin err in order to achieve the same goal. It should appear as a source module in the docs on par with other source modules in its own section. ",NULL,"In order to support ingestion from stdin, the suggested approach is to do the following. xd stream create definition tcp decoder LF log name foo cat my.log netcat localhost 1234 So while this is really a tcp based ingestion case, once can use pipe or redirect of stdin err in order to achieve the same goal. It should appear as a source module in the docs on par with other source modules in its own section. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18981,"In order to support ingestion from stdin, the suggested approach is to do the following. xd stream create definition tcp decoder LF log name foo cat my.log netcat localhost 1234 So while this is really a tcp based ingestion case, once can use pipe or redirect of stdin err in order to achieve the same goal. It should appear as a source module in the docs on par with other source modules in its own section. ",NULL,"In order to support ingestion from stdin, the suggested approach is to do the following. xd stream create definition tcp decoder LF log name foo cat my.log netcat localhost 1234 So while this is really a tcp based ingestion case, once can use pipe or redirect of stdin err in order to achieve the same goal. It should appear as a source module in the docs on par with other source modules in its own section. ",NULL,"In order to support ingestion from stdin, the suggested approach is to do the following. xd stream create definition tcp decoder LF log name foo cat my.log netcat localhost 1234 So while this is really a tcp based ingestion case, once can use pipe<span class='highlight-text severity-high'> or </span>redirect of stdin err in order to achieve the same goal. It should appear as a source module in the docs on par with other source modules in its own section. ","atomic","conjunctions","high",False
18981,"In order to support ingestion from stdin, the suggested approach is to do the following. xd stream create definition tcp decoder LF log name foo cat my.log netcat localhost 1234 So while this is really a tcp based ingestion case, once can use pipe or redirect of stdin err in order to achieve the same goal. It should appear as a source module in the docs on par with other source modules in its own section. ",NULL,"In order to support ingestion from stdin, the suggested approach is to do the following. xd stream create definition tcp decoder LF log name foo cat my.log netcat localhost 1234 So while this is really a tcp based ingestion case, once can use pipe or redirect of stdin err in order to achieve the same goal. It should appear as a source module in the docs on par with other source modules in its own section. ",NULL,"In order to support ingestion from stdin, the suggested approach is to do the following<span class='highlight-text severity-high'>. xd stream create definition tcp decoder LF log name foo cat my.log netcat localhost 1234 So while this is really a tcp based ingestion case, once can use pipe or redirect of stdin err in order to achieve the same goal. It should appear as a source module in the docs on par with other source modules in its own section. </span>","minimal","punctuation","high",False
19032,"Post boot refactoring. XDContainer lifecycle methods are not being used. Refactor by merging relevant functionality into LauncherApplication. Rename LauncherApplication to ContainerServerApplication consistent with AdminServerApplication . ",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19032,"Post boot refactoring. XDContainer lifecycle methods are not being used. Refactor by merging relevant functionality into LauncherApplication. Rename LauncherApplication to ContainerServerApplication consistent with AdminServerApplication . ",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
18981,"In order to support ingestion from stdin, the suggested approach is to do the following. xd stream create definition tcp decoder LF log name foo cat my.log netcat localhost 1234 So while this is really a tcp based ingestion case, once can use pipe or redirect of stdin err in order to achieve the same goal. It should appear as a source module in the docs on par with other source modules in its own section. ",NULL,"In order to support ingestion from stdin, the suggested approach is to do the following. xd stream create definition tcp decoder LF log name foo cat my.log netcat localhost 1234 So while this is really a tcp based ingestion case, once can use pipe or redirect of stdin err in order to achieve the same goal. It should appear as a source module in the docs on par with other source modules in its own section. ",NULL,"In order to support ingestion from stdin, the suggested approach is to do the following. xd stream create definition tcp decoder LF log name foo cat my.log netcat localhost 1234<span class='highlight-text severity-high'> So </span>while this is really a tcp based ingestion case, once can use pipe or redirect of stdin err<span class='highlight-text severity-high'> in order to </span>achieve the same goal. It should appear as a source module in the docs on par with other source modules in its own section. ","minimal","indicator_repetition","high",False
18982,"Update to Spring for Apache Hadoop 2.0 RC3 Add support for new hadoop distros Pivotal HD 2.0 phd20 Hortonworks HDP 2.1 hdp21 Cloudera CDH5 cdh5 ",NULL,"Update to Spring for Apache Hadoop 2.0 RC3 Add support for new hadoop distros Pivotal HD 2.0 phd20 Hortonworks HDP 2.1 hdp21 Cloudera CDH5 cdh5 ",NULL,"Add for who this story is","well_formed","no_role","high",False
18984,"The description in the google doc https docs.google.com a gopivotal.com document d 12Cboa7nyVVKVxDIsHLJ 68m5f78ayXn14EJLrclJYVg edit?usp sharing describes the usage of XD MODULE CONFIG LOCATION and XD MODULE CONFIG NAME",NULL,"The description in the google doc https docs.google.com a gopivotal.com document d 12Cboa7nyVVKVxDIsHLJ 68m5f78ayXn14EJLrclJYVg edit?usp sharing describes the usage of XD MODULE CONFIG LOCATION and XD MODULE CONFIG NAME",NULL,"Add for who this story is","well_formed","no_role","high",False
18985,"See report at https github.com spring projects spring xd issues 661 It would be good indeed to allow this eg by having a WeakHashMap Classloader, type name map in the global context . The caveat though, is that any statics used by the module would be shared too. We can make this an opt out though I think that sharing by default makes sense by having a flag in the module .properties manifest",NULL,"See report at https github.com spring projects spring xd issues 661 It would be good indeed to allow this eg by having a WeakHashMap Classloader, type name map in the global context . The caveat though, is that any statics used by the module would be shared too. We can make this an opt out though I think that sharing by default makes sense by having a flag in the module .properties manifest",NULL,"Add for who this story is","well_formed","no_role","high",False
18985,"See report at https github.com spring projects spring xd issues 661 It would be good indeed to allow this eg by having a WeakHashMap Classloader, type name map in the global context . The caveat though, is that any statics used by the module would be shared too. We can make this an opt out though I think that sharing by default makes sense by having a flag in the module .properties manifest",NULL,"See report at https github.com spring projects spring xd issues 661 It would be good indeed to allow this eg by having a WeakHashMap Classloader, type name map in the global context . The caveat though, is that any statics used by the module would be shared too. We can make this an opt out though I think that sharing by default makes sense by having a flag in the module .properties manifest",NULL,"See report at https github<span class='highlight-text severity-high'>.com spring projects spring xd issues 661 It would be good indeed to allow this eg by having a WeakHashMap Classloader, type name map in the global context . The caveat though, is that any statics used by the module would be shared too. We can make this an opt out though I think that sharing by default makes sense by having a flag in the module .properties manifest</span>","minimal","punctuation","high",False
18986,"See report at https github.com spring projects spring xd issues 661 This should not happen as the module holds the classes that hold the classloader, but who knows. An integration test that verifies this would be nice, albeit tricky.",NULL,"See report at https github.com spring projects spring xd issues 661 This should not happen as the module holds the classes that hold the classloader, but who knows. An integration test that verifies this would be nice, albeit tricky.",NULL,"Add for who this story is","well_formed","no_role","high",False
18986,"See report at https github.com spring projects spring xd issues 661 This should not happen as the module holds the classes that hold the classloader, but who knows. An integration test that verifies this would be nice, albeit tricky.",NULL,"See report at https github.com spring projects spring xd issues 661 This should not happen as the module holds the classes that hold the classloader, but who knows. An integration test that verifies this would be nice, albeit tricky.",NULL,"See report at https github<span class='highlight-text severity-high'>.com spring projects spring xd issues 661 This should not happen as the module holds the classes that hold the classloader, but who knows. An integration test that verifies this would be nice, albeit tricky.</span>","minimal","punctuation","high",False
18988,"1. Add quick filter 2. The table should have columns for name instance execution id Getting the name might require a bit of extra work given some limitations with JSON serialization and cycles in the current object returned from spring batch. 3. The restart action should appear only if the job is restartable and the status was failed. ",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
18988,"1. Add quick filter 2. The table should have columns for name instance execution id Getting the name might require a bit of extra work given some limitations with JSON serialization and cycles in the current object returned from spring batch. 3. The restart action should appear only if the job is restartable and the status was failed. ",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
18988,"1. Add quick filter 2. The table should have columns for name instance execution id Getting the name might require a bit of extra work given some limitations with JSON serialization and cycles in the current object returned from spring batch. 3. The restart action should appear only if the job is restartable and the status was failed. ",NULL,NULL,NULL,"1<span class='highlight-text severity-high'>. Add quick filter 2. The table should have columns for name instance execution id Getting the name might require a bit of extra work given some limitations with JSON serialization and cycles in the current object returned from spring batch. 3. The restart action should appear only if the job is restartable and the status was failed. </span>","minimal","punctuation","high",False
18987,"0. Remove home page with sign in and upper right hand corner with user login info. 1. Change the word template to modules in the tab 2. Different text for each of the tabs, modules, definition, deployments, scheduled 3. Definitions tab to have text along the lines allows you to deploy and undeploy batch job definitions add links to help on how to do that in the CLI. 4. Deployments tab a creating new definitions, parameters needs to be space on parameters, Job Parameters for Job XYZ after clicking launch. b. comment out scheduler button c. add quick filter 5. Scheduler tab a. comment out tab ",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
18987,"0. Remove home page with sign in and upper right hand corner with user login info. 1. Change the word template to modules in the tab 2. Different text for each of the tabs, modules, definition, deployments, scheduled 3. Definitions tab to have text along the lines allows you to deploy and undeploy batch job definitions add links to help on how to do that in the CLI. 4. Deployments tab a creating new definitions, parameters needs to be space on parameters, Job Parameters for Job XYZ after clicking launch. b. comment out scheduler button c. add quick filter 5. Scheduler tab a. comment out tab ",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
18987,"0. Remove home page with sign in and upper right hand corner with user login info. 1. Change the word template to modules in the tab 2. Different text for each of the tabs, modules, definition, deployments, scheduled 3. Definitions tab to have text along the lines allows you to deploy and undeploy batch job definitions add links to help on how to do that in the CLI. 4. Deployments tab a creating new definitions, parameters needs to be space on parameters, Job Parameters for Job XYZ after clicking launch. b. comment out scheduler button c. add quick filter 5. Scheduler tab a. comment out tab ",NULL,NULL,NULL,"0<span class='highlight-text severity-high'>. Remove home page with sign in and upper right hand corner with user login info. 1. Change the word template to modules in the tab 2. Different text for each of the tabs, modules, definition, deployments, scheduled 3. Definitions tab to have text along the lines allows you to deploy and undeploy batch job definitions add links to help on how to do that in the CLI. 4. Deployments tab a creating new definitions, parameters needs to be space on parameters, Job Parameters for Job XYZ after clicking launch. b. comment out scheduler button c. add quick filter 5. Scheduler tab a. comment out tab </span>","minimal","punctuation","high",False
18996,"Using the jppml evaluator, provide an implementation of the core abstractions in the spring xd machine learning. The initial code for this has been developed in a separate github repo and is located here https github.com thomasdarimont spring xd tree feature advanced analytics support spring xd analytics jpmml src main java org springframework xd analytics model jpmml ",NULL,"Using the jppml evaluator, provide an implementation of the core abstractions in the spring xd machine learning. The initial code for this has been developed in a separate github repo and is located here https github.com thomasdarimont spring xd tree feature advanced analytics support spring xd analytics jpmml src main java org springframework xd analytics model jpmml ",NULL,"Add for who this story is","well_formed","no_role","high",False
18996,"Using the jppml evaluator, provide an implementation of the core abstractions in the spring xd machine learning. The initial code for this has been developed in a separate github repo and is located here https github.com thomasdarimont spring xd tree feature advanced analytics support spring xd analytics jpmml src main java org springframework xd analytics model jpmml ",NULL,"Using the jppml evaluator, provide an implementation of the core abstractions in the spring xd machine learning. The initial code for this has been developed in a separate github repo and is located here https github.com thomasdarimont spring xd tree feature advanced analytics support spring xd analytics jpmml src main java org springframework xd analytics model jpmml ",NULL,"Using the jppml evaluator, provide an implementation of the core abstractions in the spring xd machine learning. The initial code for this has been developed in a separate github repo<span class='highlight-text severity-high'> and </span>is located here https github.com thomasdarimont spring xd tree feature advanced analytics support spring xd analytics jpmml src main java org springframework xd analytics model jpmml ","atomic","conjunctions","high",False
18996,"Using the jppml evaluator, provide an implementation of the core abstractions in the spring xd machine learning. The initial code for this has been developed in a separate github repo and is located here https github.com thomasdarimont spring xd tree feature advanced analytics support spring xd analytics jpmml src main java org springframework xd analytics model jpmml ",NULL,"Using the jppml evaluator, provide an implementation of the core abstractions in the spring xd machine learning. The initial code for this has been developed in a separate github repo and is located here https github.com thomasdarimont spring xd tree feature advanced analytics support spring xd analytics jpmml src main java org springframework xd analytics model jpmml ",NULL,"Using the jppml evaluator, provide an implementation of the core abstractions in the spring xd machine learning<span class='highlight-text severity-high'>. The initial code for this has been developed in a separate github repo and is located here https github.com thomasdarimont spring xd tree feature advanced analytics support spring xd analytics jpmml src main java org springframework xd analytics model jpmml </span>","minimal","punctuation","high",False
19000,"Acceptance tests has a direct library reference to spring xd shell. This causes problems with eclipse. It needs a intermediate main project to resolve the dependencies. This is based on the conversation I had with Mark Fisher. you cant depend on another projects src test what is needed is an intermediate project some test support project that project would depend on spring xd shell and others but then the spring xd integration test project would depend on the intermediate one lib dependencies should only be under src main src test is intended to be scoped to the project it sits in tests FOR that project not reusable base classes, etc",NULL,"Acceptance tests has a direct library reference to spring xd shell. This causes problems with eclipse. It needs a intermediate main project to resolve the dependencies. This is based on the conversation I had with Mark Fisher. you cant depend on another projects src test what is needed is an intermediate project some test support project that project would depend on spring xd shell and others but then the spring xd integration test project would depend on the intermediate one lib dependencies should only be under src main src test is intended to be scoped to the project it sits in tests FOR that project not reusable base classes, etc",NULL,"Add for who this story is","well_formed","no_role","high",False
19000,"Acceptance tests has a direct library reference to spring xd shell. This causes problems with eclipse. It needs a intermediate main project to resolve the dependencies. This is based on the conversation I had with Mark Fisher. you cant depend on another projects src test what is needed is an intermediate project some test support project that project would depend on spring xd shell and others but then the spring xd integration test project would depend on the intermediate one lib dependencies should only be under src main src test is intended to be scoped to the project it sits in tests FOR that project not reusable base classes, etc",NULL,"Acceptance tests has a direct library reference to spring xd shell. This causes problems with eclipse. It needs a intermediate main project to resolve the dependencies. This is based on the conversation I had with Mark Fisher. you cant depend on another projects src test what is needed is an intermediate project some test support project that project would depend on spring xd shell and others but then the spring xd integration test project would depend on the intermediate one lib dependencies should only be under src main src test is intended to be scoped to the project it sits in tests FOR that project not reusable base classes, etc",NULL,"Acceptance tests has a direct library reference to spring xd shell. This causes problems with eclipse. It needs a intermediate main project to resolve the dependencies. This is based on the conversation I had with Mark Fisher. you cant depend on another projects src test what is needed is an intermediate project some test support project that project would depend on spring xd shell<span class='highlight-text severity-high'> and </span>others but then the spring xd integration test project would depend on the intermediate one lib dependencies should only be under src main src test is intended to be scoped to the project it sits in tests FOR that project not reusable base classes, etc","atomic","conjunctions","high",False
19000,"Acceptance tests has a direct library reference to spring xd shell. This causes problems with eclipse. It needs a intermediate main project to resolve the dependencies. This is based on the conversation I had with Mark Fisher. you cant depend on another projects src test what is needed is an intermediate project some test support project that project would depend on spring xd shell and others but then the spring xd integration test project would depend on the intermediate one lib dependencies should only be under src main src test is intended to be scoped to the project it sits in tests FOR that project not reusable base classes, etc",NULL,"Acceptance tests has a direct library reference to spring xd shell. This causes problems with eclipse. It needs a intermediate main project to resolve the dependencies. This is based on the conversation I had with Mark Fisher. you cant depend on another projects src test what is needed is an intermediate project some test support project that project would depend on spring xd shell and others but then the spring xd integration test project would depend on the intermediate one lib dependencies should only be under src main src test is intended to be scoped to the project it sits in tests FOR that project not reusable base classes, etc",NULL,"Acceptance tests has a direct library reference to spring xd shell<span class='highlight-text severity-high'>. This causes problems with eclipse. It needs a intermediate main project to resolve the dependencies. This is based on the conversation I had with Mark Fisher. you cant depend on another projects src test what is needed is an intermediate project some test support project that project would depend on spring xd shell and others but then the spring xd integration test project would depend on the intermediate one lib dependencies should only be under src main src test is intended to be scoped to the project it sits in tests FOR that project not reusable base classes, etc</span>","minimal","punctuation","high",False
18999,"Being able to listen to a stream at any point has a significant performance impact. The reason for the impact is the message needs to be serialized transported deserialized to other members even if there is no one listening. This serialized transported deserialized processes happens for each step in a flow source process sink. Recommend creating some kind of protocol for wiretaps that allows members to know if there is someone listening in the grid so they will emit the data. Likewise we need to deregister the listener if the wiretap is deleted.",NULL,"Being able to listen to a stream at any point has a significant performance impact. The rea","son for the impact is the message needs to be serialized transported deserialized to other members even if there is no one listening. This serialized transported deserialized processes happens for each step in a flow source process sink. Recommend creating some kind of protocol for wiretaps that allows members to know if there is someone listening in the grid so they will emit the data. Likewise we need to deregister the listener if the wiretap is deleted.","Add for who this story is","well_formed","no_role","high",False
18999,"Being able to listen to a stream at any point has a significant performance impact. The reason for the impact is the message needs to be serialized transported deserialized to other members even if there is no one listening. This serialized transported deserialized processes happens for each step in a flow source process sink. Recommend creating some kind of protocol for wiretaps that allows members to know if there is someone listening in the grid so they will emit the data. Likewise we need to deregister the listener if the wiretap is deleted.",NULL,"Being able to listen to a stream at any point has a significant performance impact. The rea","son for the impact is the message needs to be serialized transported deserialized to other members even if there is no one listening. This serialized transported deserialized processes happens for each step in a flow source process sink. Recommend creating some kind of protocol for wiretaps that allows members to know if there is someone listening in the grid so they will emit the data. Likewise we need to deregister the listener if the wiretap is deleted.","Being able to listen to a stream at any point has a significant performance impact<span class='highlight-text severity-high'>. The reason for the impact is the message needs to be serialized transported deserialized to other members even if there is no one listening. This serialized transported deserialized processes happens for each step in a flow source process sink. Recommend creating some kind of protocol for wiretaps that allows members to know if there is someone listening in the grid so they will emit the data. Likewise we need to deregister the listener if the wiretap is deleted.</span>","minimal","punctuation","high",False
18999,"Being able to listen to a stream at any point has a significant performance impact. The reason for the impact is the message needs to be serialized transported deserialized to other members even if there is no one listening. This serialized transported deserialized processes happens for each step in a flow source process sink. Recommend creating some kind of protocol for wiretaps that allows members to know if there is someone listening in the grid so they will emit the data. Likewise we need to deregister the listener if the wiretap is deleted.",NULL,"Being able to listen to a stream at any point has a significant performance impact. The rea","son for the impact is the message needs to be serialized transported deserialized to other members even if there is no one listening. This serialized transported deserialized processes happens for each step in a flow source process sink. Recommend creating some kind of protocol for wiretaps that allows members to know if there is someone listening in the grid so they will emit the data. Likewise we need to deregister the listener if the wiretap is deleted.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18998,"Package SpringXD into an RPM install path opt pivotal spring xd 1.0.0.M5 with symlink opt pivotal spring xd current version init.d scripts to start stop status service springxd admin start stop status service springxd container start stop status user group springxd pivotal Host springxd rpm in Pivotal repo yum install springxd Support RHEL CentOS version 5 and 6? tested on latest updates Support for 32 and 64 bits Support Java 1.6 and 1.7 ",NULL,"Package SpringXD into an RPM install path opt pivotal spring xd 1.0.0.M5 with symlink opt pivotal spring xd current version init.d scripts to start stop status service springxd admin start stop status service springxd container start stop status user group springxd pivotal Host springxd rpm in Pivotal repo yum install springxd Support RHEL CentOS version 5 and 6? tested on latest updates Support for 32 and 64 bits Support Java 1.6 and 1.7 ",NULL,"Add for who this story is","well_formed","no_role","high",False
18998,"Package SpringXD into an RPM install path opt pivotal spring xd 1.0.0.M5 with symlink opt pivotal spring xd current version init.d scripts to start stop status service springxd admin start stop status service springxd container start stop status user group springxd pivotal Host springxd rpm in Pivotal repo yum install springxd Support RHEL CentOS version 5 and 6? tested on latest updates Support for 32 and 64 bits Support Java 1.6 and 1.7 ",NULL,"Package SpringXD into an RPM install path opt pivotal spring xd 1.0.0.M5 with symlink opt pivotal spring xd current version init.d scripts to start stop status service springxd admin start stop status service springxd container start stop status user group springxd pivotal Host springxd rpm in Pivotal repo yum install springxd Support RHEL CentOS version 5 and 6? tested on latest updates Support for 32 and 64 bits Support Java 1.6 and 1.7 ",NULL,"Package SpringXD into an RPM install path opt pivotal spring xd 1.0.0.M5 with symlink opt pivotal spring xd current version init.d scripts to start stop status service springxd admin start stop status service springxd container start stop status user group springxd pivotal Host springxd rpm in Pivotal repo yum install springxd Support RHEL CentOS version 5 and 6<span class='highlight-text severity-high'>? tested on latest updates Support for 32 and 64 bits Support Java 1.6 and 1.7 </span>","minimal","punctuation","high",False
18995,"The sample application should primarily show the round trip that is possible in that offline analysis in R can generate a pmml flle that can be imported and evaluated in an online stream definition. The specific use case can be as simple as the IRIS data set or other existing examples, such as the fraud demo. The sample application resides in https github.com spring projects spring xd samples",NULL,"The sample application should primarily show the round trip that is possible in that offline analysis in R can generate a pmml flle that can be imported and evaluated in an online stream definition. The specific use case can be as simple as the IRIS data set or other existing examples, such as the fraud demo. The sample application resides in https github.com spring projects spring xd samples",NULL,"Add for who this story is","well_formed","no_role","high",False
18995,"The sample application should primarily show the round trip that is possible in that offline analysis in R can generate a pmml flle that can be imported and evaluated in an online stream definition. The specific use case can be as simple as the IRIS data set or other existing examples, such as the fraud demo. The sample application resides in https github.com spring projects spring xd samples",NULL,"The sample application should primarily show the round trip that is possible in that offline analysis in R can generate a pmml flle that can be imported and evaluated in an online stream definition. The specific use case can be as simple as the IRIS data set or other existing examples, such as the fraud demo. The sample application resides in https github.com spring projects spring xd samples",NULL,"The sample application should primarily show the round trip that is possible in that offline analysis in R can generate a pmml flle that can be imported<span class='highlight-text severity-high'> and </span>evaluated in an online stream definition. The specific use case can be as simple as the IRIS data set<span class='highlight-text severity-high'> or </span>other existing examples, such as the fraud demo. The sample application resides in https github.com spring projects spring xd samples","atomic","conjunctions","high",False
18995,"The sample application should primarily show the round trip that is possible in that offline analysis in R can generate a pmml flle that can be imported and evaluated in an online stream definition. The specific use case can be as simple as the IRIS data set or other existing examples, such as the fraud demo. The sample application resides in https github.com spring projects spring xd samples",NULL,"The sample application should primarily show the round trip that is possible in that offline analysis in R can generate a pmml flle that can be imported and evaluated in an online stream definition. The specific use case can be as simple as the IRIS data set or other existing examples, such as the fraud demo. The sample application resides in https github.com spring projects spring xd samples",NULL,"The sample application should primarily show the round trip that is possible in that offline analysis in R can generate a pmml flle that can be imported and evaluated in an online stream definition<span class='highlight-text severity-high'>. The specific use case can be as simple as the IRIS data set or other existing examples, such as the fraud demo. The sample application resides in https github.com spring projects spring xd samples</span>","minimal","punctuation","high",False
19002,"Create an xd yarn script that is more Cloud Foundry like xd yarn push p path to unzipped yearn distro xd yarn start admin xd yarn start container ",NULL,"Create an xd yarn script that is more Cloud Foundry like xd yarn push p path to unzipped yearn distro xd yarn start admin xd yarn start container ",NULL,"Add for who this story is","well_formed","no_role","high",False
19003,"Startup zookeeper on EC2 cluster instances.",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19003,"Startup zookeeper on EC2 cluster instances.",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19007,"The application should live in in spring xd samples repository. The stream created in https jira.spring.io browse XD 1402 should be documented how to run a benchmark and made easy to execute. Can use ruby bash awk sed to generate traffic via sendfile in order to saturate the stream. ",NULL,"The application should live in in spring xd samples repository. The stream created in https jira.spring.io browse XD 1402 should be documented how to run a benchmark and made easy to execute. Can use ruby bash awk sed to generate traffic via sendfile","in order to saturate the stream.","Add for who this story is","well_formed","no_role","high",False
19007,"The application should live in in spring xd samples repository. The stream created in https jira.spring.io browse XD 1402 should be documented how to run a benchmark and made easy to execute. Can use ruby bash awk sed to generate traffic via sendfile in order to saturate the stream. ",NULL,"The application should live in in spring xd samples repository. The stream created in https jira.spring.io browse XD 1402 should be documented how to run a benchmark and made easy to execute. Can use ruby bash awk sed to generate traffic via sendfile","in order to saturate the stream.","The application should live in in spring xd samples repository. The stream created in https jira.spring.io browse XD 1402 should be documented how to run a benchmark<span class='highlight-text severity-high'> and </span>made easy to execute. Can use ruby bash awk sed to generate traffic via sendfile in order to saturate the stream. ","atomic","conjunctions","high",False
19007,"The application should live in in spring xd samples repository. The stream created in https jira.spring.io browse XD 1402 should be documented how to run a benchmark and made easy to execute. Can use ruby bash awk sed to generate traffic via sendfile in order to saturate the stream. ",NULL,"The application should live in in spring xd samples repository. The stream created in https jira.spring.io browse XD 1402 should be documented how to run a benchmark and made easy to execute. Can use ruby bash awk sed to generate traffic via sendfile","in order to saturate the stream.","The application should live in in spring xd samples repository<span class='highlight-text severity-high'>. The stream created in https jira.spring.io browse XD 1402 should be documented how to run a benchmark and made easy to execute. Can use ruby bash awk sed to generate traffic via sendfile in order to saturate the stream. </span>","minimal","punctuation","high",False
19007,"The application should live in in spring xd samples repository. The stream created in https jira.spring.io browse XD 1402 should be documented how to run a benchmark and made easy to execute. Can use ruby bash awk sed to generate traffic via sendfile in order to saturate the stream. ",NULL,"The application should live in in spring xd samples repository. The stream created in https jira.spring.io browse XD 1402 should be documented how to run a benchmark and made easy to execute. Can use ruby bash awk sed to generate traffic via sendfile","in order to saturate the stream.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19008,"A module that could be used in a stream definition such as reactor bind tcp 0.0.0.0 3000 length?codec bytes do stuff throughput sampler where throughput sampler could start measurements once a key START is found in a Message and stop when the key STOP is found in a Message, listing the number of msgs sec etc.",NULL,"A module that could be used in a stream definition such as reactor bind tcp 0.0.0.0 3000 length?codec bytes do stuff throughput sampler where throughput sampler could start measurements once a key START is found in a Message and stop when the key STOP is found in a Message, listing the number of msgs sec etc.",NULL,"Add for who this story is","well_formed","no_role","high",False
19008,"A module that could be used in a stream definition such as reactor bind tcp 0.0.0.0 3000 length?codec bytes do stuff throughput sampler where throughput sampler could start measurements once a key START is found in a Message and stop when the key STOP is found in a Message, listing the number of msgs sec etc.",NULL,"A module that could be used in a stream definition such as reactor bind tcp 0.0.0.0 3000 length?codec bytes do stuff throughput sampler where throughput sampler could start measurements once a key START is found in a Message and stop when the key STOP is found in a Message, listing the number of msgs sec etc.",NULL,"A module that could be used in a stream definition such as reactor bind tcp 0.0.0.0 3000 length?codec bytes do stuff throughput sampler where throughput sampler could start measurements once a key START is found in a Message<span class='highlight-text severity-high'> and </span>stop when the key STOP is found in a Message, listing the number of msgs sec etc.","atomic","conjunctions","high",False
19001,"I was in the process of rewriting transform using profiles see ExpressionOrScriptMixin . This broke eg ModuleCommandTests.testComposedModulesValuesInDefinition because basically composed module options activate no profile. The problem is that there is no real way to know what to activate currently, because when deploying a part of a composed module, its metadata is actually a link to the whole metadata, but does not really know which part. Long story short, something we may be able to do is to activate the union of all profiles, but this breaks very easily module compose foo definition transform expr foo transform script bar would try to activate both script and expression profiles for both modules.",NULL,"I was in the process of rewriting transform using profiles see ExpressionOrScriptMixin . This broke eg ModuleCommandTests.testComposedModulesValuesInDefinition because basically composed module options activate no profile. The problem is that there is no real way to know what to activate currently, because when deploying a part of a composed module, its metadata is actually a link to the whole metadata, but does not really know which part. Long story short, something we may be able to do is to activate the union of all profiles, but this breaks very easily module compose foo definition transform expr foo transform script bar would try to activate both script and expression profiles for both modules.",NULL,"Add for who this story is","well_formed","no_role","high",False
19001,"I was in the process of rewriting transform using profiles see ExpressionOrScriptMixin . This broke eg ModuleCommandTests.testComposedModulesValuesInDefinition because basically composed module options activate no profile. The problem is that there is no real way to know what to activate currently, because when deploying a part of a composed module, its metadata is actually a link to the whole metadata, but does not really know which part. Long story short, something we may be able to do is to activate the union of all profiles, but this breaks very easily module compose foo definition transform expr foo transform script bar would try to activate both script and expression profiles for both modules.",NULL,"I was in the process of rewriting transform using profiles see ExpressionOrScriptMixin . This broke eg ModuleCommandTests.testComposedModulesValuesInDefinition because basically composed module options activate no profile. The problem is that there is no real way to know what to activate currently, because when deploying a part of a composed module, its metadata is actually a link to the whole metadata, but does not really know which part. Long story short, something we may be able to do is to activate the union of all profiles, but this breaks very easily module compose foo definition transform expr foo transform script bar would try to activate both script and expression profiles for both modules.",NULL,"I was in the process of rewriting transform using profiles see ExpressionOrScriptMixin <span class='highlight-text severity-high'>. This broke eg ModuleCommandTests.testComposedModulesValuesInDefinition because basically composed module options activate no profile. The problem is that there is no real way to know what to activate currently, because when deploying a part of a composed module, its metadata is actually a link to the whole metadata, but does not really know which part. Long story short, something we may be able to do is to activate the union of all profiles, but this breaks very easily module compose foo definition transform expr foo transform script bar would try to activate both script and expression profiles for both modules.</span>","minimal","punctuation","high",False
19004,"The documentation lists the gemfire server sink module s attributes to be gemfireHost and gemfirePort . In the module code they are host and port . The other attributes are correct. ",NULL,"The documentation lists the gemfire server sink module s attributes to be gemfireHost and gemfirePort . In the module code they are host and port . The other attributes are correct. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19004,"The documentation lists the gemfire server sink module s attributes to be gemfireHost and gemfirePort . In the module code they are host and port . The other attributes are correct. ",NULL,"The documentation lists the gemfire server sink module s attributes to be gemfireHost and gemfirePort . In the module code they are host and port . The other attributes are correct. ",NULL,"The documentation lists the gemfire server sink module s attributes to be gemfireHost and gemfirePort <span class='highlight-text severity-high'>. In the module code they are host and port . The other attributes are correct. </span>","minimal","punctuation","high",False
19005,"The throughput module would expect a payload of the type Message byte and look for the byte to be START or STOP strings to trigger a throughput measurement. https github.com spring projects spring xd tree master extensions would be the place for the module to live.",NULL,"The throughput module would expect a payload of the type Message byte and look for the byte to be START or STOP strings to trigger a throughput measurement. https github.com spring projects spring xd tree master extensions would be the place for the module to live.",NULL,"Add for who this story is","well_formed","no_role","high",False
19005,"The throughput module would expect a payload of the type Message byte and look for the byte to be START or STOP strings to trigger a throughput measurement. https github.com spring projects spring xd tree master extensions would be the place for the module to live.",NULL,"The throughput module would expect a payload of the type Message byte and look for the byte to be START or STOP strings to trigger a throughput measurement. https github.com spring projects spring xd tree master extensions would be the place for the module to live.",NULL,"The throughput module would expect a payload of the type Message byte<span class='highlight-text severity-high'> and </span>look for the byte to be START<span class='highlight-text severity-high'> or </span>STOP strings to trigger a throughput measurement. https github.com spring projects spring xd tree master extensions would be the place for the module to live.","atomic","conjunctions","high",False
19005,"The throughput module would expect a payload of the type Message byte and look for the byte to be START or STOP strings to trigger a throughput measurement. https github.com spring projects spring xd tree master extensions would be the place for the module to live.",NULL,"The throughput module would expect a payload of the type Message byte and look for the byte to be START or STOP strings to trigger a throughput measurement. https github.com spring projects spring xd tree master extensions would be the place for the module to live.",NULL,"The throughput module would expect a payload of the type Message byte and look for the byte to be START or STOP strings to trigger a throughput measurement<span class='highlight-text severity-high'>. https github.com spring projects spring xd tree master extensions would be the place for the module to live.</span>","minimal","punctuation","high",False
19006,"Currently, XD hast testCompile dependency to use org.codehaus.groovy groovy all 2.2.1. The spring integration groovy uses 2.2.2 and it is what spring IO platform uses as well. To keep it all same, we can change this testCompile dependency to use 2.2.2 ",NULL,"Currently, XD hast testCompile dependency to use org.codehaus.groovy groovy all 2.2.1. The spring integration groovy uses 2.2.2 and it is what spring IO platform uses as well. To keep it all same, we can change this testCompile dependency to use 2.2.2 ",NULL,"Add for who this story is","well_formed","no_role","high",False
19006,"Currently, XD hast testCompile dependency to use org.codehaus.groovy groovy all 2.2.1. The spring integration groovy uses 2.2.2 and it is what spring IO platform uses as well. To keep it all same, we can change this testCompile dependency to use 2.2.2 ",NULL,"Currently, XD hast testCompile dependency to use org.codehaus.groovy groovy all 2.2.1. The spring integration groovy uses 2.2.2 and it is what spring IO platform uses as well. To keep it all same, we can change this testCompile dependency to use 2.2.2 ",NULL,"Currently, XD hast testCompile dependency to use org.codehaus.groovy groovy all 2.2.1. The spring integration groovy uses 2.2.2<span class='highlight-text severity-high'> and </span>it is what spring IO platform uses as well. To keep it all same, we can change this testCompile dependency to use 2.2.2 ","atomic","conjunctions","high",False
19006,"Currently, XD hast testCompile dependency to use org.codehaus.groovy groovy all 2.2.1. The spring integration groovy uses 2.2.2 and it is what spring IO platform uses as well. To keep it all same, we can change this testCompile dependency to use 2.2.2 ",NULL,"Currently, XD hast testCompile dependency to use org.codehaus.groovy groovy all 2.2.1. The spring integration groovy uses 2.2.2 and it is what spring IO platform uses as well. To keep it all same, we can change this testCompile dependency to use 2.2.2 ",NULL,"Currently, XD hast testCompile dependency to use org<span class='highlight-text severity-high'>.codehaus.groovy groovy all 2.2.1. The spring integration groovy uses 2.2.2 and it is what spring IO platform uses as well. To keep it all same, we can change this testCompile dependency to use 2.2.2 </span>","minimal","punctuation","high",False
19011,"The Stream deployment requests will be written to xd streams streamname and the data on that node will include the state or a boolean indicator for now of whether it should be deployed. When a Stream is deployed, the leader will consult its Container cache and write the modules to the various xd deployments child nodes see XD 1400 .",NULL,"The Stream deployment requests will be written to xd streams streamname and the data on that node will include the state or a boolean indicator for now of whether it should be deployed. When a Stream is deployed, the leader will consult its Container cache and write the modules to the various xd deployments child nodes see XD 1400 .",NULL,"Add for who this story is","well_formed","no_role","high",False
19011,"The Stream deployment requests will be written to xd streams streamname and the data on that node will include the state or a boolean indicator for now of whether it should be deployed. When a Stream is deployed, the leader will consult its Container cache and write the modules to the various xd deployments child nodes see XD 1400 .",NULL,"The Stream deployment requests will be written to xd streams streamname and the data on that node will include the state or a boolean indicator for now of whether it should be deployed. When a Stream is deployed, the leader will consult its Container cache and write the modules to the various xd deployments child nodes see XD 1400 .",NULL,"The Stream deployment requests will be written to xd streams streamname<span class='highlight-text severity-high'> and </span>the data on that node will include the state<span class='highlight-text severity-high'> or </span>a boolean indicator for now of whether it should be deployed. When a Stream is deployed, the leader will consult its Container cache and write the modules to the various xd deployments child nodes see XD 1400 .","atomic","conjunctions","high",False
19011,"The Stream deployment requests will be written to xd streams streamname and the data on that node will include the state or a boolean indicator for now of whether it should be deployed. When a Stream is deployed, the leader will consult its Container cache and write the modules to the various xd deployments child nodes see XD 1400 .",NULL,"The Stream deployment requests will be written to xd streams streamname and the data on that node will include the state or a boolean indicator for now of whether it should be deployed. When a Stream is deployed, the leader will consult its Container cache and write the modules to the various xd deployments child nodes see XD 1400 .",NULL,"The Stream deployment requests will be written to xd streams streamname and the data on that node will include the state or a boolean indicator for now of whether it should be deployed<span class='highlight-text severity-high'>. When a Stream is deployed, the leader will consult its Container cache and write the modules to the various xd deployments child nodes see XD 1400 .</span>","minimal","punctuation","high",False
19013,"This will require a ZooKeeperConnection in AdminServerApplication, based on singlenode vs. distributed via profiles, see ContainerServerApplication for an example . Then a PathChildrenCache should be established for the xd containers node.",NULL,"This will require a ZooKeeperConnection in AdminServerApplication, based on singlenode vs. distributed via profiles, see ContainerServerApplication for an example . Then a PathChildrenCache should be established for the xd containers node.",NULL,"Add for who this story is","well_formed","no_role","high",False
19013,"This will require a ZooKeeperConnection in AdminServerApplication, based on singlenode vs. distributed via profiles, see ContainerServerApplication for an example . Then a PathChildrenCache should be established for the xd containers node.",NULL,"This will require a ZooKeeperConnection in AdminServerApplication, based on singlenode vs. distributed via profiles, see ContainerServerApplication for an example . Then a PathChildrenCache should be established for the xd containers node.",NULL,"This will require a ZooKeeperConnection in AdminServerApplication, based on singlenode vs<span class='highlight-text severity-high'>. distributed via profiles, see ContainerServerApplication for an example . Then a PathChildrenCache should be established for the xd containers node.</span>","minimal","punctuation","high",False
19016,"When the job is run with its jobParameters by SimpleJobLauncher, its lastJobExecution s stepExecutions are checked for UNKNOWN status to throw JobRestartException. It looks like the stepExecutions for the lastJobExecution are never set and the collection stepExecutions is not fetched from job repository. Hence, not sure if the following condition in SimpleJobLauncher s run final Job job, final JobParameters jobParameters would ever get executed for StepExecution execution lastExecution.getStepExecutions if execution.getStatus BatchStatus.UNKNOWN throw throw new JobRestartException Step execution.getStepName is of status UNKNOWN ; end if end for",NULL,"When the job is run with its jobParameters by SimpleJobLauncher, its lastJobExecution s stepExecutions are checked for UNKNOWN status to throw JobRestartException. It looks like the stepExecutions for the lastJobExecution are never set and the collection stepExecutions is not fetched from job repository. Hence, not sure if the following condition in SimpleJobLauncher s run final Job job, final JobParameters jobParameters would ever get executed for StepExecution execution lastExecution.getStepExecutions if execution.getStatus BatchStatus.UNKNOWN throw throw new JobRestartException Step execution.getStepName is of status UNKNOWN ; end if end for",NULL,"Add for who this story is","well_formed","no_role","high",False
19474,"http static.springsource.org spring xd docs 1.0.0.BUILD SNAPSHOT reference html sources should have rabbit added to the list and also the corresponding section that shows some basic usage.",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19016,"When the job is run with its jobParameters by SimpleJobLauncher, its lastJobExecution s stepExecutions are checked for UNKNOWN status to throw JobRestartException. It looks like the stepExecutions for the lastJobExecution are never set and the collection stepExecutions is not fetched from job repository. Hence, not sure if the following condition in SimpleJobLauncher s run final Job job, final JobParameters jobParameters would ever get executed for StepExecution execution lastExecution.getStepExecutions if execution.getStatus BatchStatus.UNKNOWN throw throw new JobRestartException Step execution.getStepName is of status UNKNOWN ; end if end for",NULL,"When the job is run with its jobParameters by SimpleJobLauncher, its lastJobExecution s stepExecutions are checked for UNKNOWN status to throw JobRestartException. It looks like the stepExecutions for the lastJobExecution are never set and the collection stepExecutions is not fetched from job repository. Hence, not sure if the following condition in SimpleJobLauncher s run final Job job, final JobParameters jobParameters would ever get executed for StepExecution execution lastExecution.getStepExecutions if execution.getStatus BatchStatus.UNKNOWN throw throw new JobRestartException Step execution.getStepName is of status UNKNOWN ; end if end for",NULL,"When the job is run with its jobParameters by SimpleJobLauncher, its lastJobExecution s stepExecutions are checked for UNKNOWN status to throw JobRestartException. It looks like the stepExecutions for the lastJobExecution are never set<span class='highlight-text severity-high'> and </span>the collection stepExecutions is not fetched from job repository. Hence, not sure if the following condition in SimpleJobLauncher s run final Job job, final JobParameters jobParameters would ever get executed for StepExecution execution lastExecution.getStepExecutions if execution.getStatus BatchStatus.UNKNOWN throw throw new JobRestartException Step execution.getStepName is of status UNKNOWN ; end if end for","atomic","conjunctions","high",False
19016,"When the job is run with its jobParameters by SimpleJobLauncher, its lastJobExecution s stepExecutions are checked for UNKNOWN status to throw JobRestartException. It looks like the stepExecutions for the lastJobExecution are never set and the collection stepExecutions is not fetched from job repository. Hence, not sure if the following condition in SimpleJobLauncher s run final Job job, final JobParameters jobParameters would ever get executed for StepExecution execution lastExecution.getStepExecutions if execution.getStatus BatchStatus.UNKNOWN throw throw new JobRestartException Step execution.getStepName is of status UNKNOWN ; end if end for",NULL,"When the job is run with its jobParameters by SimpleJobLauncher, its lastJobExecution s stepExecutions are checked for UNKNOWN status to throw JobRestartException. It looks like the stepExecutions for the lastJobExecution are never set and the collection stepExecutions is not fetched from job repository. Hence, not sure if the following condition in SimpleJobLauncher s run final Job job, final JobParameters jobParameters would ever get executed for StepExecution execution lastExecution.getStepExecutions if execution.getStatus BatchStatus.UNKNOWN throw throw new JobRestartException Step execution.getStepName is of status UNKNOWN ; end if end for",NULL,"When the job is run with its jobParameters by SimpleJobLauncher, its lastJobExecution s stepExecutions are checked for UNKNOWN status to throw JobRestartException<span class='highlight-text severity-high'>. It looks like the stepExecutions for the lastJobExecution are never set and the collection stepExecutions is not fetched from job repository. Hence, not sure if the following condition in SimpleJobLauncher s run final Job job, final JobParameters jobParameters would ever get executed for StepExecution execution lastExecution.getStepExecutions if execution.getStatus BatchStatus.UNKNOWN throw throw new JobRestartException Step execution.getStepName is of status UNKNOWN ; end if end for</span>","minimal","punctuation","high",False
19014,"The container will not start with JMX enabled and the management port set. The stacktrace is attached and the settings for the container are enumerated below export endpoints jmx enabled true export endpoints jmx uniqueNames true export endpoints jolokia enabled true export XD JMX ENABLED true export management port 15005",NULL,"The container will not start with JMX enabled and the management port set. The stacktrace is attached and the settings for the container are enumerated below export endpoints jmx enabled true export endpoints jmx uniqueNames true export endpoints jolokia enabled true export XD JMX ENABLED true export management port 15005",NULL,"Add for who this story is","well_formed","no_role","high",False
19014,"The container will not start with JMX enabled and the management port set. The stacktrace is attached and the settings for the container are enumerated below export endpoints jmx enabled true export endpoints jmx uniqueNames true export endpoints jolokia enabled true export XD JMX ENABLED true export management port 15005",NULL,"The container will not start with JMX enabled and the management port set. The stacktrace is attached and the settings for the container are enumerated below export endpoints jmx enabled true export endpoints jmx uniqueNames true export endpoints jolokia enabled true export XD JMX ENABLED true export management port 15005",NULL,"The container will not start with JMX enabled<span class='highlight-text severity-high'> and </span>the management port set. The stacktrace is attached and the settings for the container are enumerated below export endpoints jmx enabled true export endpoints jmx uniqueNames true export endpoints jolokia enabled true export XD JMX ENABLED true export management port 15005","atomic","conjunctions","high",False
19014,"The container will not start with JMX enabled and the management port set. The stacktrace is attached and the settings for the container are enumerated below export endpoints jmx enabled true export endpoints jmx uniqueNames true export endpoints jolokia enabled true export XD JMX ENABLED true export management port 15005",NULL,"The container will not start with JMX enabled and the management port set. The stacktrace is attached and the settings for the container are enumerated below export endpoints jmx enabled true export endpoints jmx uniqueNames true export endpoints jolokia enabled true export XD JMX ENABLED true export management port 15005",NULL,"The container will not start with JMX enabled and the management port set<span class='highlight-text severity-high'>. The stacktrace is attached and the settings for the container are enumerated below export endpoints jmx enabled true export endpoints jmx uniqueNames true export endpoints jolokia enabled true export XD JMX ENABLED true export management port 15005</span>","minimal","punctuation","high",False
19010,"The Admin leader will write each Module deployment request to a child node of xd deployments for a selected Container see XD 1399 . That Container specific persistent child node needs to be created by the Container at the same time as it creates its ephemeral node under xd containers. The Container should then deploy the Module. If that same node is subsequently deleted, the Container should undeploy the Module.",NULL,"The Admin leader will write each Module deployment request to a child node of xd deployments for a selected Container see XD 1399 . That Container specific persistent child node needs to be created by the Container at the same time as it creates its ephemeral node under xd containers. The Container should then deploy the Module. If that same node is subsequently deleted, the Container should undeploy the Module.",NULL,"Add for who this story is","well_formed","no_role","high",False
19010,"The Admin leader will write each Module deployment request to a child node of xd deployments for a selected Container see XD 1399 . That Container specific persistent child node needs to be created by the Container at the same time as it creates its ephemeral node under xd containers. The Container should then deploy the Module. If that same node is subsequently deleted, the Container should undeploy the Module.",NULL,"The Admin leader will write each Module deployment request to a child node of xd deployments for a selected Container see XD 1399 . That Container specific persistent child node needs to be created by the Container at the same time as it creates its ephemeral node under xd containers. The Container should then deploy the Module. If that same node is subsequently deleted, the Container should undeploy the Module.",NULL,"The Admin leader will write each Module deployment request to a child node of xd deployments for a selected Container see XD 1399 <span class='highlight-text severity-high'>. That Container specific persistent child node needs to be created by the Container at the same time as it creates its ephemeral node under xd containers. The Container should then deploy the Module. If that same node is subsequently deleted, the Container should undeploy the Module.</span>","minimal","punctuation","high",False
19012,"This should also enable removal of any StreamDefinitionRepository code. The state should be written as a data node at the stream level e.g. xd streams mystream state ... For now we at least need to support the boolean deploy true false flag. If that is true, then the leader Admin will deploy the modules of the stream across available containers XD 1399 ",NULL,"This should also enable removal of any StreamDefinitionRepository code. The state should be written as a data node at the stream level e.g. xd streams mystream state ... For now we at least need to support the boolean deploy true false flag. If that is true, then the leader Admin will deploy the modules of the stream across available containers XD 1399 ",NULL,"Add for who this story is","well_formed","no_role","high",False
19012,"This should also enable removal of any StreamDefinitionRepository code. The state should be written as a data node at the stream level e.g. xd streams mystream state ... For now we at least need to support the boolean deploy true false flag. If that is true, then the leader Admin will deploy the modules of the stream across available containers XD 1399 ",NULL,"This should also enable removal of any StreamDefinitionRepository code. The state should be written as a data node at the stream level e.g. xd streams mystream state ... For now we at least need to support the boolean deploy true false flag. If that is true, then the leader Admin will deploy the modules of the stream across available containers XD 1399 ",NULL,"This should also enable removal of any StreamDefinitionRepository code<span class='highlight-text severity-high'>. The state should be written as a data node at the stream level e.g. xd streams mystream state ... For now we at least need to support the boolean deploy true false flag. If that is true, then the leader Admin will deploy the modules of the stream across available containers XD 1399 </span>","minimal","punctuation","high",False
19015,"After the container singlenode is up and running, the application does not update the singlenode.log or containernode.log. create a stream time log . you will see the timestamp in your console, but it will not be in the log.",NULL,"After the container singlenode is up and running, the application does not update the singlenode.log or containernode.log. create a stream time log . you will see the timestamp in your console, but it will not be in the log.",NULL,"Add for who this story is","well_formed","no_role","high",False
19015,"After the container singlenode is up and running, the application does not update the singlenode.log or containernode.log. create a stream time log . you will see the timestamp in your console, but it will not be in the log.",NULL,"After the container singlenode is up and running, the application does not update the singlenode.log or containernode.log. create a stream time log . you will see the timestamp in your console, but it will not be in the log.",NULL,"After the container singlenode is up<span class='highlight-text severity-high'> and </span>running, the application does not update the singlenode.log<span class='highlight-text severity-high'> or </span>containernode.log. create a stream time log . you will see the timestamp in your console, but it will not be in the log.","atomic","conjunctions","high",False
19015,"After the container singlenode is up and running, the application does not update the singlenode.log or containernode.log. create a stream time log . you will see the timestamp in your console, but it will not be in the log.",NULL,"After the container singlenode is up and running, the application does not update the singlenode.log or containernode.log. create a stream time log . you will see the timestamp in your console, but it will not be in the log.",NULL,"After the container singlenode is up and running, the application does not update the singlenode<span class='highlight-text severity-high'>.log or containernode.log. create a stream time log . you will see the timestamp in your console, but it will not be in the log.</span>","minimal","punctuation","high",False
19021,"e.g. rabbit.xml source. context property placeholder location xd.config.home configProperties rabbit .properties ignore resource not found true would be removed and rabbit connection factory id rabbitConnectionFactory host host spring.rabbitmq.host localhost port port spring.rabbitmq.port 5672 virtual host vhost spring.rabbitmq.virtualHost username username spring.rabbitmq.username guest password password spring.rabbitmq.password guest would look like port port and a property file in the module directory or POJO in the module lib would specify the default value of the port. For POJO it would be options class org.springframework.xd.dirt.modules.metadata.RabbitSourceOptionsMetadata For property file it would be option.port.default 5672 option.port.description cool port number This needs to be consistently done across all the modules. ",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19021,"e.g. rabbit.xml source. context property placeholder location xd.config.home configProperties rabbit .properties ignore resource not found true would be removed and rabbit connection factory id rabbitConnectionFactory host host spring.rabbitmq.host localhost port port spring.rabbitmq.port 5672 virtual host vhost spring.rabbitmq.virtualHost username username spring.rabbitmq.username guest password password spring.rabbitmq.password guest would look like port port and a property file in the module directory or POJO in the module lib would specify the default value of the port. For POJO it would be options class org.springframework.xd.dirt.modules.metadata.RabbitSourceOptionsMetadata For property file it would be option.port.default 5672 option.port.description cool port number This needs to be consistently done across all the modules. ",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19021,"e.g. rabbit.xml source. context property placeholder location xd.config.home configProperties rabbit .properties ignore resource not found true would be removed and rabbit connection factory id rabbitConnectionFactory host host spring.rabbitmq.host localhost port port spring.rabbitmq.port 5672 virtual host vhost spring.rabbitmq.virtualHost username username spring.rabbitmq.username guest password password spring.rabbitmq.password guest would look like port port and a property file in the module directory or POJO in the module lib would specify the default value of the port. For POJO it would be options class org.springframework.xd.dirt.modules.metadata.RabbitSourceOptionsMetadata For property file it would be option.port.default 5672 option.port.description cool port number This needs to be consistently done across all the modules. ",NULL,NULL,NULL,"e<span class='highlight-text severity-high'>.g. rabbit.xml source. context property placeholder location xd.config.home configProperties rabbit .properties ignore resource not found true would be removed and rabbit connection factory id rabbitConnectionFactory host host spring.rabbitmq.host localhost port port spring.rabbitmq.port 5672 virtual host vhost spring.rabbitmq.virtualHost username username spring.rabbitmq.username guest password password spring.rabbitmq.password guest would look like port port and a property file in the module directory or POJO in the module lib would specify the default value of the port. For POJO it would be options class org.springframework.xd.dirt.modules.metadata.RabbitSourceOptionsMetadata For property file it would be option.port.default 5672 option.port.description cool port number This needs to be consistently done across all the modules. </span>","minimal","punctuation","high",False
19024,"I am trying to use HDFS as sink while creating streams and I am encountering the following error Please refer attached document Exception localhost 8020.txt The fs.default.name set in hadoop.properties is fs.default.name hdfs localhost 8020 I have also tried the following variation in the hadoop.properties file fs.default.name hdfs 127.0.0.2 8020 fs.default.name hdfs 127.0.0.2 50070 Using these values are also throwing me exceptions as mentioned in the following files attached Exception 127.0.0.2 8020.txt Exception 127.0.0.2 50070.txt We are using Pivotal HD 1.0.1 spring xd 1.0.0.M5 dist Kindly let us know the way around for this issue. ",NULL,"I am trying to use HDFS as sink while creating streams and I am encountering the following error Please refer attached document Exception localhost 8020.txt The fs.default.name set in hadoop.properties is fs.default.name hdfs localhost 8020 I have also tried the following variation in the hadoop.properties file fs.default.name hdfs 127.0.0.2 8020 fs.default.name hdfs 127.0.0.2 50070 Using these values are also throwing me exceptions as mentioned in the following files attached Exception 127.0.0.2 8020.txt Exception 127.0.0.2 50070.txt We are using Pivotal HD 1.0.1 spring xd 1.0.0.M5 dist Kindly let us know the way around for this issue. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19024,"I am trying to use HDFS as sink while creating streams and I am encountering the following error Please refer attached document Exception localhost 8020.txt The fs.default.name set in hadoop.properties is fs.default.name hdfs localhost 8020 I have also tried the following variation in the hadoop.properties file fs.default.name hdfs 127.0.0.2 8020 fs.default.name hdfs 127.0.0.2 50070 Using these values are also throwing me exceptions as mentioned in the following files attached Exception 127.0.0.2 8020.txt Exception 127.0.0.2 50070.txt We are using Pivotal HD 1.0.1 spring xd 1.0.0.M5 dist Kindly let us know the way around for this issue. ",NULL,"I am trying to use HDFS as sink while creating streams and I am encountering the following error Please refer attached document Exception localhost 8020.txt The fs.default.name set in hadoop.properties is fs.default.name hdfs localhost 8020 I have also tried the following variation in the hadoop.properties file fs.default.name hdfs 127.0.0.2 8020 fs.default.name hdfs 127.0.0.2 50070 Using these values are also throwing me exceptions as mentioned in the following files attached Exception 127.0.0.2 8020.txt Exception 127.0.0.2 50070.txt We are using Pivotal HD 1.0.1 spring xd 1.0.0.M5 dist Kindly let us know the way around for this issue. ",NULL,"I am trying to use HDFS as sink while creating streams<span class='highlight-text severity-high'> and </span>I am encountering the following error Please refer attached document Exception localhost 8020.txt The fs.default.name set in hadoop.properties is fs.default.name hdfs localhost 8020 I have also tried the following variation in the hadoop.properties file fs.default.name hdfs 127.0.0.2 8020 fs.default.name hdfs 127.0.0.2 50070 Using these values are also throwing me exceptions as mentioned in the following files attached Exception 127.0.0.2 8020.txt Exception 127.0.0.2 50070.txt We are using Pivotal HD 1.0.1 spring xd 1.0.0.M5 dist Kindly let us know the way around for this issue. ","atomic","conjunctions","high",False
19028,"Currently, the job notification channels are direct channels. We need to make these pub sub channels. With XD 885 allowing automatic job listeners registration , this would allow us to create named channel syntax like topic job myjobname jobExecution log topic job myjobname stepExecution log ",NULL,"Currently, the job notification channels are direct channels. We need to make these pub sub channels. With XD 885 allowing automatic job listeners registration , this would allow us to create named channel syntax like topic job myjobname jobExecution log topic job myjobname stepExecution log ",NULL,"Add for who this story is","well_formed","no_role","high",False
19028,"Currently, the job notification channels are direct channels. We need to make these pub sub channels. With XD 885 allowing automatic job listeners registration , this would allow us to create named channel syntax like topic job myjobname jobExecution log topic job myjobname stepExecution log ",NULL,"Currently, the job notification channels are direct channels. We need to make these pub sub channels. With XD 885 allowing automatic job listeners registration , this would allow us to create named channel syntax like topic job myjobname jobExecution log topic job myjobname stepExecution log ",NULL,"Currently, the job notification channels are direct channels<span class='highlight-text severity-high'>. We need to make these pub sub channels. With XD 885 allowing automatic job listeners registration , this would allow us to create named channel syntax like topic job myjobname jobExecution log topic job myjobname stepExecution log </span>","minimal","punctuation","high",False
19025,"The main container context becomes the shared context for modules.",NULL,"The main container context becomes the shared context for modules.",NULL,"Add for who this story is","well_formed","no_role","high",False
19026,"xd lib includes jcl over slf4j so we don t need additional commons logging jars in the modules or extensions https github.com spring projects spring xd pull 610",NULL,"xd lib includes jcl over slf4j","so we don t need additional commons logging jars in the modules or extensions https github.com spring projects spring xd pull 610","Add for who this story is","well_formed","no_role","high",False
19026,"xd lib includes jcl over slf4j so we don t need additional commons logging jars in the modules or extensions https github.com spring projects spring xd pull 610",NULL,"xd lib includes jcl over slf4j","so we don t need additional commons logging jars in the modules or extensions https github.com spring projects spring xd pull 610","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19019,"If stream fails to create while using sources or sinks that need ports. Ports are not released. If while creating a stream you see Command failed org.springframework.xd.rest.client.impl.SpringXDException Failed to bind to 0.0.0.0 0.0.0.0 9000. Possibly the port is already in use. then destroy the stream or all streams for that matter you will not be able to create another stream that uses that port. The work around is to cycle the container.",NULL,"If stream fails to create while using sources or sinks that need ports. Ports are not released. If while creating a stream you see Command failed org.springframework.xd.rest.client.impl.SpringXDException Failed to bind to 0.0.0.0 0.0.0.0 9000. Possibly the port is already in use. then destroy the stream or all streams for that matter you will not be able to create another stream that uses that port. The work around is to cycle the container.",NULL,"Add for who this story is","well_formed","no_role","high",False
19382,"UDP and Legacy syslog sources emit a Map ; reactor emits a POJO. Make them consistent and emit Tuple s.",NULL,"UDP and Legacy syslog sources emit a Map ; reactor emits a POJO. Make them consistent and emit Tuple s.",NULL,"UDP and Legacy syslog sources emit a Map <span class='highlight-text severity-high'>; reactor emits a POJO. Make them consistent and emit Tuple s.</span>","minimal","punctuation","high",False
19019,"If stream fails to create while using sources or sinks that need ports. Ports are not released. If while creating a stream you see Command failed org.springframework.xd.rest.client.impl.SpringXDException Failed to bind to 0.0.0.0 0.0.0.0 9000. Possibly the port is already in use. then destroy the stream or all streams for that matter you will not be able to create another stream that uses that port. The work around is to cycle the container.",NULL,"If stream fails to create while using sources or sinks that need ports. Ports are not released. If while creating a stream you see Command failed org.springframework.xd.rest.client.impl.SpringXDException Failed to bind to 0.0.0.0 0.0.0.0 9000. Possibly the port is already in use. then destroy the stream or all streams for that matter you will not be able to create another stream that uses that port. The work around is to cycle the container.",NULL,"If stream fails to create while using sources<span class='highlight-text severity-high'> or </span>sinks that need ports. Ports are not released. If while creating a stream you see Command failed org.springframework.xd.rest.client.impl.SpringXDException Failed to bind to 0.0.0.0 0.0.0.0 9000. Possibly the port is already in use. then destroy the stream or all streams for that matter you will not be able to create another stream that uses that port. The work around is to cycle the container.","atomic","conjunctions","high",False
19019,"If stream fails to create while using sources or sinks that need ports. Ports are not released. If while creating a stream you see Command failed org.springframework.xd.rest.client.impl.SpringXDException Failed to bind to 0.0.0.0 0.0.0.0 9000. Possibly the port is already in use. then destroy the stream or all streams for that matter you will not be able to create another stream that uses that port. The work around is to cycle the container.",NULL,"If stream fails to create while using sources or sinks that need ports. Ports are not released. If while creating a stream you see Command failed org.springframework.xd.rest.client.impl.SpringXDException Failed to bind to 0.0.0.0 0.0.0.0 9000. Possibly the port is already in use. then destroy the stream or all streams for that matter you will not be able to create another stream that uses that port. The work around is to cycle the container.",NULL,"If stream fails to create while using sources or sinks that need ports<span class='highlight-text severity-high'>. Ports are not released. If while creating a stream you see Command failed org.springframework.xd.rest.client.impl.SpringXDException Failed to bind to 0.0.0.0 0.0.0.0 9000. Possibly the port is already in use. then destroy the stream or all streams for that matter you will not be able to create another stream that uses that port. The work around is to cycle the container.</span>","minimal","punctuation","high",False
19022,"This applies to a few places when addressing the issue, a search should be done to uncover any others , e.g. ContainerServerApplication public static final String NODE PROFILE node ; Options classes The transport to use for data messages from node to node The transport to use for control messages between admin and nodes ",NULL,"This applies to a few places when addressing the issue, a search should be done to uncover any others , e.g. ContainerServerApplication public static final String NODE PROFILE node ; Options classes The transport to use for data messages from node to node The transport to use for control messages between admin and nodes ",NULL,"Add for who this story is","well_formed","no_role","high",False
19022,"This applies to a few places when addressing the issue, a search should be done to uncover any others , e.g. ContainerServerApplication public static final String NODE PROFILE node ; Options classes The transport to use for data messages from node to node The transport to use for control messages between admin and nodes ",NULL,"This applies to a few places when addressing the issue, a search should be done to uncover any others , e.g. ContainerServerApplication public static final String NODE PROFILE node ; Options classes The transport to use for data messages from node to node The transport to use for control messages between admin and nodes ",NULL,"This applies to a few places when addressing the issue, a search should be done to uncover any others , e<span class='highlight-text severity-high'>.g. ContainerServerApplication public static final String NODE PROFILE node ; Options classes The transport to use for data messages from node to node The transport to use for control messages between admin and nodes </span>","minimal","punctuation","high",False
19023,"Need to update the spring xd extension reactor support to reflect the changes to Reactor refactorings introduced in v1.1.",NULL,"Need to update the spring xd extension reactor support to reflect the changes to Reactor refactorings introduced in v1.1.",NULL,"Add for who this story is","well_formed","no_role","high",False
19027,"The YARN support in M6 changes most of the config properties, need to update XD to use new ones.",NULL,"The YARN support in M6 changes most of the config properties, need to update XD to use new ones.",NULL,"Add for who this story is","well_formed","no_role","high",False
19030,"The xd containers node is the parent where each Container will write an ephemeral child node. The node name should be the Container s ID, and the node data should be the Container s attributes host, pid, and much more to be added later . When a Container shuts down cleanly, it should eagerly delete the ephemeral node so that watchers are notified immediately. For any other case including a network partition , the ephemeral node will disappear after the timeout elapses.",NULL,"The xd containers node is the parent where each Container will write an ephemeral child node. The node name should be the Container s ID, and the node data should be the Container s attributes host, pid, and much more to be added later . When a Container shuts down cleanly, it should eagerly delete the ephemeral node","so that watchers are notified immediately. For any other case including a network partition , the ephemeral node will disappear after the timeout elapses.","Add for who this story is","well_formed","no_role","high",False
19030,"The xd containers node is the parent where each Container will write an ephemeral child node. The node name should be the Container s ID, and the node data should be the Container s attributes host, pid, and much more to be added later . When a Container shuts down cleanly, it should eagerly delete the ephemeral node so that watchers are notified immediately. For any other case including a network partition , the ephemeral node will disappear after the timeout elapses.",NULL,"The xd containers node is the parent where each Container will write an ephemeral child node. The node name should be the Container s ID, and the node data should be the Container s attributes host, pid, and much more to be added later . When a Container shuts down cleanly, it should eagerly delete the ephemeral node","so that watchers are notified immediately. For any other case including a network partition , the ephemeral node will disappear after the timeout elapses.","The xd containers node is the parent where each Container will write an ephemeral child node. The node name should be the Container s ID,<span class='highlight-text severity-high'> and </span>the node data should be the Container s attributes host, pid, and much more to be added later . When a Container shuts down cleanly, it should eagerly delete the ephemeral node so that watchers are notified immediately. For any other case including a network partition , the ephemeral node will disappear after the timeout elapses.","atomic","conjunctions","high",False
19030,"The xd containers node is the parent where each Container will write an ephemeral child node. The node name should be the Container s ID, and the node data should be the Container s attributes host, pid, and much more to be added later . When a Container shuts down cleanly, it should eagerly delete the ephemeral node so that watchers are notified immediately. For any other case including a network partition , the ephemeral node will disappear after the timeout elapses.",NULL,"The xd containers node is the parent where each Container will write an ephemeral child node. The node name should be the Container s ID, and the node data should be the Container s attributes host, pid, and much more to be added later . When a Container shuts down cleanly, it should eagerly delete the ephemeral node","so that watchers are notified immediately. For any other case including a network partition , the ephemeral node will disappear after the timeout elapses.","The xd containers node is the parent where each Container will write an ephemeral child node<span class='highlight-text severity-high'>. The node name should be the Container s ID, and the node data should be the Container s attributes host, pid, and much more to be added later . When a Container shuts down cleanly, it should eagerly delete the ephemeral node so that watchers are notified immediately. For any other case including a network partition , the ephemeral node will disappear after the timeout elapses.</span>","minimal","punctuation","high",False
19030,"The xd containers node is the parent where each Container will write an ephemeral child node. The node name should be the Container s ID, and the node data should be the Container s attributes host, pid, and much more to be added later . When a Container shuts down cleanly, it should eagerly delete the ephemeral node so that watchers are notified immediately. For any other case including a network partition , the ephemeral node will disappear after the timeout elapses.",NULL,"The xd containers node is the parent where each Container will write an ephemeral child node. The node name should be the Container s ID, and the node data should be the Container s attributes host, pid, and much more to be added later . When a Container shuts down cleanly, it should eagerly delete the ephemeral node","so that watchers are notified immediately. For any other case including a network partition , the ephemeral node will disappear after the timeout elapses.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19036,"We currently pull all hdfs config properties from hdfs.properties change that to use application.yml",NULL,"We currently pull all hdfs config properties from hdfs.properties change that to use application.yml",NULL,"Add for who this story is","well_formed","no_role","high",False
19034,"This will allow us to control the order of plugins and use plugin s to manage the common module context, replacing BeanDefinitionAddingBeanPostProcesser",NULL,"This will allow us to control the order of plugins and use plugin s to manage the common module context, replacing BeanDefinitionAddingBeanPostProcesser",NULL,"Add for who this story is","well_formed","no_role","high",False
19034,"This will allow us to control the order of plugins and use plugin s to manage the common module context, replacing BeanDefinitionAddingBeanPostProcesser",NULL,"This will allow us to control the order of plugins and use plugin s to manage the common module context, replacing BeanDefinitionAddingBeanPostProcesser",NULL,"This will allow us to control the order of plugins<span class='highlight-text severity-high'> and </span>use plugin s to manage the common module context, replacing BeanDefinitionAddingBeanPostProcesser","atomic","conjunctions","high",False
19031,"When the container shuts down, ContainerStoppedEvent should be published so that appropriate listeners would act on. Please refer to this discussion here https github.com spring projects spring xd pull 612",NULL,"When the container shuts down, ContainerStoppedEvent should be published","so that appropriate listeners would act on. Please refer to this discussion here https github.com spring projects spring xd pull 612","Add for who this story is","well_formed","no_role","high",False
19031,"When the container shuts down, ContainerStoppedEvent should be published so that appropriate listeners would act on. Please refer to this discussion here https github.com spring projects spring xd pull 612",NULL,"When the container shuts down, ContainerStoppedEvent should be published","so that appropriate listeners would act on. Please refer to this discussion here https github.com spring projects spring xd pull 612","When the container shuts down, ContainerStoppedEvent should be published so that appropriate listeners would act on<span class='highlight-text severity-high'>. Please refer to this discussion here https github.com spring projects spring xd pull 612</span>","minimal","punctuation","high",False
19031,"When the container shuts down, ContainerStoppedEvent should be published so that appropriate listeners would act on. Please refer to this discussion here https github.com spring projects spring xd pull 612",NULL,"When the container shuts down, ContainerStoppedEvent should be published","so that appropriate listeners would act on. Please refer to this discussion here https github.com spring projects spring xd pull 612","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19035,"The enum unnecessarily restricts the ability to add new transport MessageBus implementations whereas the config location path is open for extensions since it uses wildcards in message bus.xml code import resource .. .. transports XD TRANSPORT bus.xml code ",NULL,"The enum unnecessarily restricts the ability to add new transport MessageBus implementations whereas the config location path is open for extensions since it uses wildcards in message bus.xml code import resource .. .. transports XD TRANSPORT bus.xml code ",NULL,"Add for who this story is","well_formed","no_role","high",False
19035,"The enum unnecessarily restricts the ability to add new transport MessageBus implementations whereas the config location path is open for extensions since it uses wildcards in message bus.xml code import resource .. .. transports XD TRANSPORT bus.xml code ",NULL,"The enum unnecessarily restricts the ability to add new transport MessageBus implementations whereas the config location path is open for extensions since it uses wildcards in message bus.xml code import resource .. .. transports XD TRANSPORT bus.xml code ",NULL,"The enum unnecessarily restricts the ability to add new transport MessageBus implementations whereas the config location path is open for extensions since it uses wildcards in message bus<span class='highlight-text severity-high'>.xml code import resource .. .. transports XD TRANSPORT bus.xml code </span>","minimal","punctuation","high",False
19037,"This requires exposing properties to the ListenerContainer. Probably cleaner to inject the ListenerContainer into the RabbitMessageBus and expose property placeholders on the LC. Maybe do the same for RabbitAdmin as well. ",NULL,"This requires exposing properties to the ListenerContainer. Probably cleaner to inject the ListenerContainer into the RabbitMessageBus and expose property placeholders on the LC. Maybe do the same for RabbitAdmin as well. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19037,"This requires exposing properties to the ListenerContainer. Probably cleaner to inject the ListenerContainer into the RabbitMessageBus and expose property placeholders on the LC. Maybe do the same for RabbitAdmin as well. ",NULL,"This requires exposing properties to the ListenerContainer. Probably cleaner to inject the ListenerContainer into the RabbitMessageBus and expose property placeholders on the LC. Maybe do the same for RabbitAdmin as well. ",NULL,"This requires exposing properties to the ListenerContainer. Probably cleaner to inject the ListenerContainer into the RabbitMessageBus<span class='highlight-text severity-high'> and </span>expose property placeholders on the LC. Maybe do the same for RabbitAdmin as well. ","atomic","conjunctions","high",False
19037,"This requires exposing properties to the ListenerContainer. Probably cleaner to inject the ListenerContainer into the RabbitMessageBus and expose property placeholders on the LC. Maybe do the same for RabbitAdmin as well. ",NULL,"This requires exposing properties to the ListenerContainer. Probably cleaner to inject the ListenerContainer into the RabbitMessageBus and expose property placeholders on the LC. Maybe do the same for RabbitAdmin as well. ",NULL,"This requires exposing properties to the ListenerContainer<span class='highlight-text severity-high'>. Probably cleaner to inject the ListenerContainer into the RabbitMessageBus and expose property placeholders on the LC. Maybe do the same for RabbitAdmin as well. </span>","minimal","punctuation","high",False
19038,"Following merge of https github.com spring projects spring xd pull 601, expose a unique id under which a module is known inside a stream. That id which defaults to the module name is what should be used as the qualifier for an option name inside a composed module, ie noformat module compose foo definition f1 filter filter f1.expression and filter.expression are available noformat ",NULL,"Following merge of https github.com spring projects spring xd pull 601, expose a unique id under which a module is known inside a stream. That id which defaults to the module name is what should be used as the qualifier for an option name inside a composed module, ie noformat module compose foo definition f1 filter filter f1.expression and filter.expression are available noformat ",NULL,"Add for who this story is","well_formed","no_role","high",False
19038,"Following merge of https github.com spring projects spring xd pull 601, expose a unique id under which a module is known inside a stream. That id which defaults to the module name is what should be used as the qualifier for an option name inside a composed module, ie noformat module compose foo definition f1 filter filter f1.expression and filter.expression are available noformat ",NULL,"Following merge of https github.com spring projects spring xd pull 601, expose a unique id under which a module is known inside a stream. That id which defaults to the module name is what should be used as the qualifier for an option name inside a composed module, ie noformat module compose foo definition f1 filter filter f1.expression and filter.expression are available noformat ",NULL,"Following merge of https github.com spring projects spring xd pull 601, expose a unique id under which a module is known inside a stream. That id which defaults to the module name is what should be used as the qualifier for an option name inside a composed module, ie noformat module compose foo definition f1 filter filter f1.expression<span class='highlight-text severity-high'> and </span>filter.expression are available noformat ","atomic","conjunctions","high",False
19038,"Following merge of https github.com spring projects spring xd pull 601, expose a unique id under which a module is known inside a stream. That id which defaults to the module name is what should be used as the qualifier for an option name inside a composed module, ie noformat module compose foo definition f1 filter filter f1.expression and filter.expression are available noformat ",NULL,"Following merge of https github.com spring projects spring xd pull 601, expose a unique id under which a module is known inside a stream. That id which defaults to the module name is what should be used as the qualifier for an option name inside a composed module, ie noformat module compose foo definition f1 filter filter f1.expression and filter.expression are available noformat ",NULL,"Following merge of https github<span class='highlight-text severity-high'>.com spring projects spring xd pull 601, expose a unique id under which a module is known inside a stream. That id which defaults to the module name is what should be used as the qualifier for an option name inside a composed module, ie noformat module compose foo definition f1 filter filter f1.expression and filter.expression are available noformat </span>","minimal","punctuation","high",False
19039,"Following merge of https github.com spring projects spring xd pull 601, allow a module option to be known by several names and elect a short name in composed module options when there is no ambiguity",NULL,"Following merge of https github.com spring projects spring xd pull 601, allow a module option to be known by several names and elect a short name in composed module options when there is no ambiguity",NULL,"Add for who this story is","well_formed","no_role","high",False
19039,"Following merge of https github.com spring projects spring xd pull 601, allow a module option to be known by several names and elect a short name in composed module options when there is no ambiguity",NULL,"Following merge of https github.com spring projects spring xd pull 601, allow a module option to be known by several names and elect a short name in composed module options when there is no ambiguity",NULL,"Following merge of https github.com spring projects spring xd pull 601, allow a module option to be known by several names<span class='highlight-text severity-high'> and </span>elect a short name in composed module options when there is no ambiguity","atomic","conjunctions","high",False
19040,"Following merge of https github.com spring projects spring xd pull 601, use dot as the separator for a composed module option. Need change to the parser to accept dots",NULL,"Following merge of https github.com spring projects spring xd pull 601, use dot as the separator for a composed module option. Need change to the parser to accept dots",NULL,"Add for who this story is","well_formed","no_role","high",False
19040,"Following merge of https github.com spring projects spring xd pull 601, use dot as the separator for a composed module option. Need change to the parser to accept dots",NULL,"Following merge of https github.com spring projects spring xd pull 601, use dot as the separator for a composed module option. Need change to the parser to accept dots",NULL,"Following merge of https github<span class='highlight-text severity-high'>.com spring projects spring xd pull 601, use dot as the separator for a composed module option. Need change to the parser to accept dots</span>","minimal","punctuation","high",False
19167,"Even though it may be hard to come up with a mqtt broker, an easy test that should be automated is somesource mqtt topic foo with mqtt topics foo somesink And asserting that what is emitted to somesource ends up in somesink. ",NULL,"Even though it may be hard to come up with a mqtt broker, an easy test that should be automated is somesource mqtt topic foo with mqtt topics foo somesink And asserting that what is emitted to somesource ends up in somesink. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19385,"We need to have a properties section documented as well so that users can setup their jdbc connections for the various components.",NULL,"We need to have a properties section documented as well","so that users can setup their jdbc connections for the various components.","Add for who this story is","well_formed","no_role","high",False
19032,"Post boot refactoring. XDContainer lifecycle methods are not being used. Refactor by merging relevant functionality into LauncherApplication. Rename LauncherApplication to ContainerServerApplication consistent with AdminServerApplication . ",NULL,NULL,NULL,"Post boot refactoring<span class='highlight-text severity-high'>. XDContainer lifecycle methods are not being used. Refactor by merging relevant functionality into LauncherApplication. Rename LauncherApplication to ContainerServerApplication consistent with AdminServerApplication . </span>","minimal","punctuation","high",False
19033,"The Spring Data team recommends using the Jedis driver since the Lettuce driver hasn t had any update activity for several months. Jedis is actively maintained. We might also want to investigate Redisson which is a fork of Lettuce https github.com mrniko redisson",NULL,"The Spring Data team recommends using the Jedis driver since the Lettuce driver hasn t had any update activity for several months. Jedis is actively maintained. We might also want to investigate Redisson which is a fork of Lettuce https github.com mrniko redisson",NULL,"Add for who this story is","well_formed","no_role","high",False
19033,"The Spring Data team recommends using the Jedis driver since the Lettuce driver hasn t had any update activity for several months. Jedis is actively maintained. We might also want to investigate Redisson which is a fork of Lettuce https github.com mrniko redisson",NULL,"The Spring Data team recommends using the Jedis driver since the Lettuce driver hasn t had any update activity for several months. Jedis is actively maintained. We might also want to investigate Redisson which is a fork of Lettuce https github.com mrniko redisson",NULL,"The Spring Data team recommends using the Jedis driver since the Lettuce driver hasn t had any update activity for several months<span class='highlight-text severity-high'>. Jedis is actively maintained. We might also want to investigate Redisson which is a fork of Lettuce https github.com mrniko redisson</span>","minimal","punctuation","high",False
19045,"When importing XD as a gradle project into STS, it fails with Missing directories spring xd hadoop hdp20 and spring xd yarn mkdir on these directories solves the problem. The hdp20 case relates to XD 599 it is not clear why spring xd yarn is needed. ",NULL,"When importing XD as a gradle project into STS, it fails with Missing directories spring xd hadoop hdp20 and spring xd yarn mkdir on these directories solves the problem. The hdp20 case relates to XD 599 it is not clear why spring xd yarn is needed. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19045,"When importing XD as a gradle project into STS, it fails with Missing directories spring xd hadoop hdp20 and spring xd yarn mkdir on these directories solves the problem. The hdp20 case relates to XD 599 it is not clear why spring xd yarn is needed. ",NULL,"When importing XD as a gradle project into STS, it fails with Missing directories spring xd hadoop hdp20 and spring xd yarn mkdir on these directories solves the problem. The hdp20 case relates to XD 599 it is not clear why spring xd yarn is needed. ",NULL,"When importing XD as a gradle project into STS, it fails with Missing directories spring xd hadoop hdp20<span class='highlight-text severity-high'> and </span>spring xd yarn mkdir on these directories solves the problem. The hdp20 case relates to XD 599 it is not clear why spring xd yarn is needed. ","atomic","conjunctions","high",False
19045,"When importing XD as a gradle project into STS, it fails with Missing directories spring xd hadoop hdp20 and spring xd yarn mkdir on these directories solves the problem. The hdp20 case relates to XD 599 it is not clear why spring xd yarn is needed. ",NULL,"When importing XD as a gradle project into STS, it fails with Missing directories spring xd hadoop hdp20 and spring xd yarn mkdir on these directories solves the problem. The hdp20 case relates to XD 599 it is not clear why spring xd yarn is needed. ",NULL,"When importing XD as a gradle project into STS, it fails with Missing directories spring xd hadoop hdp20 and spring xd yarn mkdir on these directories solves the problem<span class='highlight-text severity-high'>. The hdp20 case relates to XD 599 it is not clear why spring xd yarn is needed. </span>","minimal","punctuation","high",False
19043,"Currently, the XD container uses embedded tomcat only to support management server that acts as the RESTful server for boot s actuator MVC and jolokia endpoints. If the configuration is set to use a specific management port using the fixes addressed via XD 1122 , then we don t need to have embedded tomcat servlet container in XD container. ",NULL,"Currently, the XD container uses embedded tomcat only to support management server that acts as the RESTful server for boot s actuator MVC and jolokia endpoints. If the configuration is set to use a specific management port using the fixes addressed via XD 1122 , then we don t need to have embedded tomcat servlet container in XD container. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19043,"Currently, the XD container uses embedded tomcat only to support management server that acts as the RESTful server for boot s actuator MVC and jolokia endpoints. If the configuration is set to use a specific management port using the fixes addressed via XD 1122 , then we don t need to have embedded tomcat servlet container in XD container. ",NULL,"Currently, the XD container uses embedded tomcat only to support management server that acts as the RESTful server for boot s actuator MVC and jolokia endpoints. If the configuration is set to use a specific management port using the fixes addressed via XD 1122 , then we don t need to have embedded tomcat servlet container in XD container. ",NULL,"Currently, the XD container uses embedded tomcat only to support management server that acts as the RESTful server for boot s actuator MVC and jolokia endpoints<span class='highlight-text severity-high'>. If the configuration is set to use a specific management port using the fixes addressed via XD 1122 , then we don t need to have embedded tomcat servlet container in XD container. </span>","minimal","punctuation","high",False
19044,"We need a methodology for providing partitioning hints. A current proposal uses message headers to provide partition key the item to partition on destination region what to target In the proposal the developer used partition key to route the stream message to the node where data was stored in process. This was done so downstream stream operations could work on the data with out suffering any network IO. The destination region was used to target the type of data the downstream streams were going to use in their stream processing. ",NULL,"We need a methodology for providing partitioning hints. A current proposal uses message headers to provide partition key the item to partition on destination region what to target In the proposal the developer used partition key to route the stream message to the node where data was stored in process. This was done","so downstream stream operations could work on the data with out suffering any network IO. The destination region was used to target the type of data the downstream streams were going to use in their stream processing.","Add for who this story is","well_formed","no_role","high",False
19044,"We need a methodology for providing partitioning hints. A current proposal uses message headers to provide partition key the item to partition on destination region what to target In the proposal the developer used partition key to route the stream message to the node where data was stored in process. This was done so downstream stream operations could work on the data with out suffering any network IO. The destination region was used to target the type of data the downstream streams were going to use in their stream processing. ",NULL,"We need a methodology for providing partitioning hints. A current proposal uses message headers to provide partition key the item to partition on destination region what to target In the proposal the developer used partition key to route the stream message to the node where data was stored in process. This was done","so downstream stream operations could work on the data with out suffering any network IO. The destination region was used to target the type of data the downstream streams were going to use in their stream processing.","We need a methodology for providing partitioning hints<span class='highlight-text severity-high'>. A current proposal uses message headers to provide partition key the item to partition on destination region what to target In the proposal the developer used partition key to route the stream message to the node where data was stored in process. This was done so downstream stream operations could work on the data with out suffering any network IO. The destination region was used to target the type of data the downstream streams were going to use in their stream processing. </span>","minimal","punctuation","high",False
19044,"We need a methodology for providing partitioning hints. A current proposal uses message headers to provide partition key the item to partition on destination region what to target In the proposal the developer used partition key to route the stream message to the node where data was stored in process. This was done so downstream stream operations could work on the data with out suffering any network IO. The destination region was used to target the type of data the downstream streams were going to use in their stream processing. ",NULL,"We need a methodology for providing partitioning hints. A current proposal uses message headers to provide partition key the item to partition on destination region what to target In the proposal the developer used partition key to route the stream message to the node where data was stored in process. This was done","so downstream stream operations could work on the data with out suffering any network IO. The destination region was used to target the type of data the downstream streams were going to use in their stream processing.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19046,"While the current setting work while running from your laptop to local deployments. Or running from your laptop to ec2, they are not long enough for CI to Ec2. This should have good defaults and have CI set them to what it needs.",NULL,"While the current setting work while running from your laptop to local deployments. Or running from your laptop to ec2, they are not long enough for CI to Ec2. This should have good defaults and have CI set them to what it needs.",NULL,"Add for who this story is","well_formed","no_role","high",False
19046,"While the current setting work while running from your laptop to local deployments. Or running from your laptop to ec2, they are not long enough for CI to Ec2. This should have good defaults and have CI set them to what it needs.",NULL,"While the current setting work while running from your laptop to local deployments. Or running from your laptop to ec2, they are not long enough for CI to Ec2. This should have good defaults and have CI set them to what it needs.",NULL,"While the current setting work while running from your laptop to local deployments.<span class='highlight-text severity-high'> or </span>running from your laptop to ec2, they are not long enough for CI to Ec2. This should have good defaults<span class='highlight-text severity-high'> and </span>have CI set them to what it needs.","atomic","conjunctions","high",False
19046,"While the current setting work while running from your laptop to local deployments. Or running from your laptop to ec2, they are not long enough for CI to Ec2. This should have good defaults and have CI set them to what it needs.",NULL,"While the current setting work while running from your laptop to local deployments. Or running from your laptop to ec2, they are not long enough for CI to Ec2. This should have good defaults and have CI set them to what it needs.",NULL,"While the current setting work while running from your laptop to local deployments<span class='highlight-text severity-high'>. Or running from your laptop to ec2, they are not long enough for CI to Ec2. This should have good defaults and have CI set them to what it needs.</span>","minimal","punctuation","high",False
19047,"See also XD 1320.",NULL,"See also XD 1320.",NULL,"Add for who this story is","well_formed","no_role","high",False
19051,"Add YARN specific code based on Janne s prototyping Add YARN Client and AppMaster implementations and startup config files This includes shell scripts to deploy XD to YARN Test working on Apache 2.2 distribution We can modify config files, everything should be possible to override by providing command line args or env variables. . xd yarn deploy zipFile tmp spring xd yarn.zip config tmp spring xd yarn.yml ",NULL,"Add YARN specific code based on Janne s prototyping Add YARN Client and AppMaster implementations and startup config files This includes shell scripts to deploy XD to YARN Test working on Apache 2.2 distribution We can modify config files, everything should be possible to override by providing command line args or env variables. . xd yarn deploy zipFile tmp spring xd yarn.zip config tmp spring xd yarn.yml ",NULL,"Add for who this story is","well_formed","no_role","high",False
19051,"Add YARN specific code based on Janne s prototyping Add YARN Client and AppMaster implementations and startup config files This includes shell scripts to deploy XD to YARN Test working on Apache 2.2 distribution We can modify config files, everything should be possible to override by providing command line args or env variables. . xd yarn deploy zipFile tmp spring xd yarn.zip config tmp spring xd yarn.yml ",NULL,"Add YARN specific code based on Janne s prototyping Add YARN Client and AppMaster implementations and startup config files This includes shell scripts to deploy XD to YARN Test working on Apache 2.2 distribution We can modify config files, everything should be possible to override by providing command line args or env variables. . xd yarn deploy zipFile tmp spring xd yarn.zip config tmp spring xd yarn.yml ",NULL,"Add YARN specific code based on Janne s prototyping Add YARN Client<span class='highlight-text severity-high'> and </span>AppMaster implementations and startup config files This includes shell scripts to deploy XD to YARN Test working on Apache 2.2 distribution We can modify config files, everything should be possible to override by providing command line args<span class='highlight-text severity-high'> or </span>env variables. . xd yarn deploy zipFile tmp spring xd yarn.zip config tmp spring xd yarn.yml ","atomic","conjunctions","high",False
19051,"Add YARN specific code based on Janne s prototyping Add YARN Client and AppMaster implementations and startup config files This includes shell scripts to deploy XD to YARN Test working on Apache 2.2 distribution We can modify config files, everything should be possible to override by providing command line args or env variables. . xd yarn deploy zipFile tmp spring xd yarn.zip config tmp spring xd yarn.yml ",NULL,"Add YARN specific code based on Janne s prototyping Add YARN Client and AppMaster implementations and startup config files This includes shell scripts to deploy XD to YARN Test working on Apache 2.2 distribution We can modify config files, everything should be possible to override by providing command line args or env variables. . xd yarn deploy zipFile tmp spring xd yarn.zip config tmp spring xd yarn.yml ",NULL,"Add YARN specific code based on Janne s prototyping Add YARN Client and AppMaster implementations and startup config files This includes shell scripts to deploy XD to YARN Test working on Apache 2<span class='highlight-text severity-high'>.2 distribution We can modify config files, everything should be possible to override by providing command line args or env variables. . xd yarn deploy zipFile tmp spring xd yarn.zip config tmp spring xd yarn.yml </span>","minimal","punctuation","high",False
19052,"Job restart fails with NPE. See PR for XD 1090 https github.com spring projects spring xd pull 572 ",NULL,"Job restart fails with NPE. See PR for XD 1090 https github.com spring projects spring xd pull 572 ",NULL,"Add for who this story is","well_formed","no_role","high",False
19052,"Job restart fails with NPE. See PR for XD 1090 https github.com spring projects spring xd pull 572 ",NULL,"Job restart fails with NPE. See PR for XD 1090 https github.com spring projects spring xd pull 572 ",NULL,"Job restart fails with NPE<span class='highlight-text severity-high'>. See PR for XD 1090 https github.com spring projects spring xd pull 572 </span>","minimal","punctuation","high",False
19050,"Suggest trying with Hortonworks 2.0",NULL,"Suggest trying with Hortonworks 2.0",NULL,"Add for who this story is","well_formed","no_role","high",False
19092,"See https quickstart.atlassian.com download bamboo get started bamboo elements Stages are comprised of one or more Jobs, which run in parallel we would like the tests across the rabbit and redis transport to occur in parallel.",NULL,"See https quickstart.atlassian.com download bamboo get started bamboo elements Stages are comprised of one or more Jobs, which run in parallel we would like the tests across the rabbit and redis transport to occur in parallel.",NULL,"Add for who this story is","well_formed","no_role","high",False
19092,"See https quickstart.atlassian.com download bamboo get started bamboo elements Stages are comprised of one or more Jobs, which run in parallel we would like the tests across the rabbit and redis transport to occur in parallel.",NULL,"See https quickstart.atlassian.com download bamboo get started bamboo elements Stages are comprised of one or more Jobs, which run in parallel we would like the tests across the rabbit and redis transport to occur in parallel.",NULL,"See https quickstart.atlassian.com download bamboo get started bamboo elements Stages are comprised of one<span class='highlight-text severity-high'> or </span>more Jobs, which run in parallel we would like the tests across the rabbit<span class='highlight-text severity-high'> and </span>redis transport to occur in parallel.","atomic","conjunctions","high",False
19093,"See https quickstart.atlassian.com download bamboo get started bamboo elements Stages are comprised of one or more Jobs, which run in parallel we would like the tests across the rabbit and redis transport to occur in parallel. ",NULL,"See https quickstart.atlassian.com download bamboo get started bamboo elements Stages are comprised of one or more Jobs, which run in parallel we would like the tests across the rabbit and redis transport to occur in parallel. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19093,"See https quickstart.atlassian.com download bamboo get started bamboo elements Stages are comprised of one or more Jobs, which run in parallel we would like the tests across the rabbit and redis transport to occur in parallel. ",NULL,"See https quickstart.atlassian.com download bamboo get started bamboo elements Stages are comprised of one or more Jobs, which run in parallel we would like the tests across the rabbit and redis transport to occur in parallel. ",NULL,"See https quickstart.atlassian.com download bamboo get started bamboo elements Stages are comprised of one<span class='highlight-text severity-high'> or </span>more Jobs, which run in parallel we would like the tests across the rabbit<span class='highlight-text severity-high'> and </span>redis transport to occur in parallel. ","atomic","conjunctions","high",False
19094,"Run test application developed in XD 1245 ",NULL,"Run test application developed in XD 1245 ",NULL,"Add for who this story is","well_formed","no_role","high",False
19048,"From https jira.springsource.org browse XD 1231 we understand the importance of modularizing client side javascript code. This story tracks modularization of XD UI.",NULL,"From https jira.springsource.org browse XD 1231 we understand the importance of modularizing client side javascript code. This story tracks modularization of XD UI.",NULL,"Add for who this story is","well_formed","no_role","high",False
19048,"From https jira.springsource.org browse XD 1231 we understand the importance of modularizing client side javascript code. This story tracks modularization of XD UI.",NULL,"From https jira.springsource.org browse XD 1231 we understand the importance of modularizing client side javascript code. This story tracks modularization of XD UI.",NULL,"From https jira<span class='highlight-text severity-high'>.springsource.org browse XD 1231 we understand the importance of modularizing client side javascript code. This story tracks modularization of XD UI.</span>","minimal","punctuation","high",False
19041,"Provide an easy way for users to add beans e.g., Gemfire cache configuration or modify default XD configuration such as serializers, and message converters. A simple approach is to add a well known resource selector such as classpath META INF spring xd extensions or include this path in an extensible Configuration base class. In addition, we should adopt conventional names for beans that are meant to be extended, e.g. use an xd. prefix.",NULL,"Provide an easy way for users to add beans e.g., Gemfire cache configuration or modify default XD configuration such as serializers, and message converters. A simple approach is to add a well known resource selector such as classpath META INF spring xd extensions or include this path in an extensible Configuration base class. In addition, we should adopt conventional names for beans that are meant to be extended, e.g. use an xd. prefix.",NULL,"Add for who this story is","well_formed","no_role","high",False
19074,"When doing dsl completion in a stream definition and a module option accepts a value that has a closed set of possible values eg booleans, enums , we can provide completions for those. ",NULL,"When doing dsl completion in a stream definition and a module option accepts a value that has a closed set of possible values eg booleans, enums , we can provide completions for those. ",NULL,"When doing dsl completion in a stream definition<span class='highlight-text severity-high'> and </span>a module option accepts a value that has a closed set of possible values eg booleans, enums , we can provide completions for those. ","atomic","conjunctions","high",False
19076,"Will likely involve having the module identity type name be part of the OptionsMetadata identity cache key",NULL,"Will likely involve having the module identity type name be part of the OptionsMetadata identity cache key",NULL,"Add for who this story is","well_formed","no_role","high",False
19041,"Provide an easy way for users to add beans e.g., Gemfire cache configuration or modify default XD configuration such as serializers, and message converters. A simple approach is to add a well known resource selector such as classpath META INF spring xd extensions or include this path in an extensible Configuration base class. In addition, we should adopt conventional names for beans that are meant to be extended, e.g. use an xd. prefix.",NULL,"Provide an easy way for users to add beans e.g., Gemfire cache configuration or modify default XD configuration such as serializers, and message converters. A simple approach is to add a well known resource selector such as classpath META INF spring xd extensions or include this path in an extensible Configuration base class. In addition, we should adopt conventional names for beans that are meant to be extended, e.g. use an xd. prefix.",NULL,"Provide an easy way for users to add beans e.g., Gemfire cache configuration<span class='highlight-text severity-high'> or </span>modify default XD configuration such as serializers,<span class='highlight-text severity-high'> and </span>message converters. A simple approach is to add a well known resource selector such as classpath META INF spring xd extensions or include this path in an extensible Configuration base class. In addition, we should adopt conventional names for beans that are meant to be extended, e.g. use an xd. prefix.","atomic","conjunctions","high",False
19041,"Provide an easy way for users to add beans e.g., Gemfire cache configuration or modify default XD configuration such as serializers, and message converters. A simple approach is to add a well known resource selector such as classpath META INF spring xd extensions or include this path in an extensible Configuration base class. In addition, we should adopt conventional names for beans that are meant to be extended, e.g. use an xd. prefix.",NULL,"Provide an easy way for users to add beans e.g., Gemfire cache configuration or modify default XD configuration such as serializers, and message converters. A simple approach is to add a well known resource selector such as classpath META INF spring xd extensions or include this path in an extensible Configuration base class. In addition, we should adopt conventional names for beans that are meant to be extended, e.g. use an xd. prefix.",NULL,"Provide an easy way for users to add beans e<span class='highlight-text severity-high'>.g., Gemfire cache configuration or modify default XD configuration such as serializers, and message converters. A simple approach is to add a well known resource selector such as classpath META INF spring xd extensions or include this path in an extensible Configuration base class. In addition, we should adopt conventional names for beans that are meant to be extended, e.g. use an xd. prefix.</span>","minimal","punctuation","high",False
19055,"Create XD .zip distribution for YARN that adds an additional sub project to the spring xd repo for building the xd YARN.zip Link into main build file Produce a new artifact spring xd v xyz yarn.zip as part of the nightly CI process will now have 2 artifacts, main xd.zip distribution and xd yarn.zip Does not include any Hadoop distribution libraries Does include spring hadoop jars for Apache22 unflavored ",NULL,"Create XD .zip distribution for YARN that adds an additional sub project to the spring xd repo for building the xd YARN.zip Link into main build file Produce a new artifact spring xd v xyz yarn.zip as part of the nightly CI process will now have 2 artifacts, main xd.zip distribution and xd yarn.zip Does not include any Hadoop distribution libraries Does include spring hadoop jars for Apache22 unflavored ",NULL,"Add for who this story is","well_formed","no_role","high",False
19055,"Create XD .zip distribution for YARN that adds an additional sub project to the spring xd repo for building the xd YARN.zip Link into main build file Produce a new artifact spring xd v xyz yarn.zip as part of the nightly CI process will now have 2 artifacts, main xd.zip distribution and xd yarn.zip Does not include any Hadoop distribution libraries Does include spring hadoop jars for Apache22 unflavored ",NULL,"Create XD .zip distribution for YARN that adds an additional sub project to the spring xd repo for building the xd YARN.zip Link into main build file Produce a new artifact spring xd v xyz yarn.zip as part of the nightly CI process will now have 2 artifacts, main xd.zip distribution and xd yarn.zip Does not include any Hadoop distribution libraries Does include spring hadoop jars for Apache22 unflavored ",NULL,"Create XD .zip distribution for YARN that adds an additional sub project to the spring xd repo for building the xd YARN.zip Link into main build file Produce a new artifact spring xd v xyz yarn.zip as part of the nightly CI process will now have 2 artifacts, main xd.zip distribution<span class='highlight-text severity-high'> and </span>xd yarn.zip Does not include any Hadoop distribution libraries Does include spring hadoop jars for Apache22 unflavored ","atomic","conjunctions","high",False
19057,"Create a job, execute it a couple of times, destroy it and then invoke job execution list. The job name column should mention that a job is defunct even though a job with the same name could have been re created in the interim ",NULL,"Create a job, execute it a couple of times, destroy it and then invoke job execution list. The job name column should mention that a job is defunct even though a job with the same name could have been re created in the interim ",NULL,"Add for who this story is","well_formed","no_role","high",False
19057,"Create a job, execute it a couple of times, destroy it and then invoke job execution list. The job name column should mention that a job is defunct even though a job with the same name could have been re created in the interim ",NULL,"Create a job, execute it a couple of times, destroy it and then invoke job execution list. The job name column should mention that a job is defunct even though a job with the same name could have been re created in the interim ",NULL,"Create a job, execute it a couple of times, destroy it<span class='highlight-text severity-high'> and </span>then invoke job execution list. The job name column should mention that a job is defunct even though a job with the same name could have been re created in the interim ","atomic","conjunctions","high",False
19057,"Create a job, execute it a couple of times, destroy it and then invoke job execution list. The job name column should mention that a job is defunct even though a job with the same name could have been re created in the interim ",NULL,"Create a job, execute it a couple of times, destroy it and then invoke job execution list. The job name column should mention that a job is defunct even though a job with the same name could have been re created in the interim ",NULL,"Create a job, execute it a couple of times, destroy it and then invoke job execution list<span class='highlight-text severity-high'>. The job name column should mention that a job is defunct even though a job with the same name could have been re created in the interim </span>","minimal","punctuation","high",False
19056,"See discussion at https github.com spring projects spring xd pull 572",NULL,"See discussion at https github.com spring projects spring xd pull 572",NULL,"Add for who this story is","well_formed","no_role","high",False
19059,"When trying some of the examples of XD 159, came up with weird behavior of the transform module. This boils down to noformat stream create foo definition http transform expression new java.lang.Integer payload transform expression payload.getClass log http post data 42 Integer OK http post data WTH WTH ! noformat Seems that when the expression can not be evaluated, the incoming payload is transmitted as is",NULL,"When trying some of the examples of XD 159, came up with weird behavior of the transform module. This boils down to noformat stream create foo definition http transform expression new java.lang.Integer payload transform expression payload.getClass log http post data 42 Integer OK http post data WTH WTH ! noformat Seems that when the expression can not be evaluated, the incoming payload is transmitted as is",NULL,"Add for who this story is","well_formed","no_role","high",False
19059,"When trying some of the examples of XD 159, came up with weird behavior of the transform module. This boils down to noformat stream create foo definition http transform expression new java.lang.Integer payload transform expression payload.getClass log http post data 42 Integer OK http post data WTH WTH ! noformat Seems that when the expression can not be evaluated, the incoming payload is transmitted as is",NULL,"When trying some of the examples of XD 159, came up with weird behavior of the transform module. This boils down to noformat stream create foo definition http transform expression new java.lang.Integer payload transform expression payload.getClass log http post data 42 Integer OK http post data WTH WTH ! noformat Seems that when the expression can not be evaluated, the incoming payload is transmitted as is",NULL,"When trying some of the examples of XD 159, came up with weird behavior of the transform module<span class='highlight-text severity-high'>. This boils down to noformat stream create foo definition http transform expression new java.lang.Integer payload transform expression payload.getClass log http post data 42 Integer OK http post data WTH WTH ! noformat Seems that when the expression can not be evaluated, the incoming payload is transmitted as is</span>","minimal","punctuation","high",False
19058,"When using a JSR303 annotated class for module options, the binding failures should be bypassed, as they interfere with completion proposals. ",NULL,"When using a JSR303 annotated class for module options, the binding failures should be bypassed, as they interfere with completion proposals. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19063,"There are few boolean type module option properties whose default values are specified in the module definitions than their corresponding ModuleOptionsMetaData. Also, when using boolean we need to have module option using primitive type boolean than Boolean type. Currently, these are some of the module options that require this change initializeDatabase in modules filejdbc, hdfsjdbc job modules, aggregator processor module, jdbc sink module restartable in all the job modules deleteFiles in filejdbc, filepollhdfs job modules ",NULL,"There are few boolean type module option properties whose default values are specified in the module definitions than their corresponding ModuleOptionsMetaData. Also, when using boolean we need to have module option using primitive type boolean than Boolean type. Currently, these are some of the module options that require this change initializeDatabase in modules filejdbc, hdfsjdbc job modules, aggregator processor module, jdbc sink module restartable in all the job modules deleteFiles in filejdbc, filepollhdfs job modules ",NULL,"Add for who this story is","well_formed","no_role","high",False
19063,"There are few boolean type module option properties whose default values are specified in the module definitions than their corresponding ModuleOptionsMetaData. Also, when using boolean we need to have module option using primitive type boolean than Boolean type. Currently, these are some of the module options that require this change initializeDatabase in modules filejdbc, hdfsjdbc job modules, aggregator processor module, jdbc sink module restartable in all the job modules deleteFiles in filejdbc, filepollhdfs job modules ",NULL,"There are few boolean type module option properties whose default values are specified in the module definitions than their corresponding ModuleOptionsMetaData. Also, when using boolean we need to have module option using primitive type boolean than Boolean type. Currently, these are some of the module options that require this change initializeDatabase in modules filejdbc, hdfsjdbc job modules, aggregator processor module, jdbc sink module restartable in all the job modules deleteFiles in filejdbc, filepollhdfs job modules ",NULL,"There are few boolean type module option properties whose default values are specified in the module definitions than their corresponding ModuleOptionsMetaData<span class='highlight-text severity-high'>. Also, when using boolean we need to have module option using primitive type boolean than Boolean type. Currently, these are some of the module options that require this change initializeDatabase in modules filejdbc, hdfsjdbc job modules, aggregator processor module, jdbc sink module restartable in all the job modules deleteFiles in filejdbc, filepollhdfs job modules </span>","minimal","punctuation","high",False
19060,"HATEOAS 0.9 introduced some support for templated links. This should be leveraged to properly handle eg streams id instead of using string concatenation",NULL,"HATEOAS 0.9 introduced some support for templated links. This should be leveraged to properly handle eg streams id instead of using string concatenation",NULL,"Add for who this story is","well_formed","no_role","high",False
19060,"HATEOAS 0.9 introduced some support for templated links. This should be leveraged to properly handle eg streams id instead of using string concatenation",NULL,"HATEOAS 0.9 introduced some support for templated links. This should be leveraged to properly handle eg streams id instead of using string concatenation",NULL,"HATEOAS 0<span class='highlight-text severity-high'>.9 introduced some support for templated links. This should be leveraged to properly handle eg streams id instead of using string concatenation</span>","minimal","punctuation","high",False
19054,"This is so that we can have an ENUM that can show the possible values and autocomplete. Using just the XML the user has a greater chance to enter an invalid mode. ",NULL,"This is","so that we can have an ENUM that can show the possible values and autocomplete. Using just the XML the user has a greater chance to enter an invalid mode.","Add for who this story is","well_formed","no_role","high",False
19054,"This is so that we can have an ENUM that can show the possible values and autocomplete. Using just the XML the user has a greater chance to enter an invalid mode. ",NULL,"This is","so that we can have an ENUM that can show the possible values and autocomplete. Using just the XML the user has a greater chance to enter an invalid mode.","This is so that we can have an ENUM that can show the possible values and autocomplete<span class='highlight-text severity-high'>. Using just the XML the user has a greater chance to enter an invalid mode. </span>","minimal","punctuation","high",False
19054,"This is so that we can have an ENUM that can show the possible values and autocomplete. Using just the XML the user has a greater chance to enter an invalid mode. ",NULL,"This is","so that we can have an ENUM that can show the possible values and autocomplete. Using just the XML the user has a greater chance to enter an invalid mode.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19053,"A lot of modules have similar options. Moreover, job modules often have options that belong to at least two domains eg jdbc hdfs . I think that by using FlattenedCompositeModuleOptionsMetadata, we could come up with a way to combine several options POJOs into one. Something like public class JdbcHdfsOptionsMetadata OptionsMixin private JdbcOptionsMetadata jdbc; OptionsMixin private HdfsOptionsMetadata hdfs; this would expose eg driverClass as well as rolloverSize as top level options. Values could be actually injected into the fields, so that eg custom validation could occur default validation for the mixin class would occur by default ",NULL,"A lot of modules have similar options. Moreover, job modules often have options that belong to at least two domains eg jdbc hdfs . I think that by using FlattenedCompositeModuleOptionsMetadata, we could come up with a way to combine several options POJOs into one. Something like public class JdbcHdfsOptionsMetadata OptionsMixin private JdbcOptionsMetadata jdbc; OptionsMixin private HdfsOptionsMetadata hdfs; this would expose eg driverClass as well as rolloverSize as top level options. Values could be actually injected into the fields,","so that eg custom validation could occur default validation for the mixin class would occur by default","Add for who this story is","well_formed","no_role","high",False
19053,"A lot of modules have similar options. Moreover, job modules often have options that belong to at least two domains eg jdbc hdfs . I think that by using FlattenedCompositeModuleOptionsMetadata, we could come up with a way to combine several options POJOs into one. Something like public class JdbcHdfsOptionsMetadata OptionsMixin private JdbcOptionsMetadata jdbc; OptionsMixin private HdfsOptionsMetadata hdfs; this would expose eg driverClass as well as rolloverSize as top level options. Values could be actually injected into the fields, so that eg custom validation could occur default validation for the mixin class would occur by default ",NULL,"A lot of modules have similar options. Moreover, job modules often have options that belong to at least two domains eg jdbc hdfs . I think that by using FlattenedCompositeModuleOptionsMetadata, we could come up with a way to combine several options POJOs into one. Something like public class JdbcHdfsOptionsMetadata OptionsMixin private JdbcOptionsMetadata jdbc; OptionsMixin private HdfsOptionsMetadata hdfs; this would expose eg driverClass as well as rolloverSize as top level options. Values could be actually injected into the fields,","so that eg custom validation could occur default validation for the mixin class would occur by default","A lot of modules have similar options<span class='highlight-text severity-high'>. Moreover, job modules often have options that belong to at least two domains eg jdbc hdfs . I think that by using FlattenedCompositeModuleOptionsMetadata, we could come up with a way to combine several options POJOs into one. Something like public class JdbcHdfsOptionsMetadata OptionsMixin private JdbcOptionsMetadata jdbc; OptionsMixin private HdfsOptionsMetadata hdfs; this would expose eg driverClass as well as rolloverSize as top level options. Values could be actually injected into the fields, so that eg custom validation could occur default validation for the mixin class would occur by default </span>","minimal","punctuation","high",False
19053,"A lot of modules have similar options. Moreover, job modules often have options that belong to at least two domains eg jdbc hdfs . I think that by using FlattenedCompositeModuleOptionsMetadata, we could come up with a way to combine several options POJOs into one. Something like public class JdbcHdfsOptionsMetadata OptionsMixin private JdbcOptionsMetadata jdbc; OptionsMixin private HdfsOptionsMetadata hdfs; this would expose eg driverClass as well as rolloverSize as top level options. Values could be actually injected into the fields, so that eg custom validation could occur default validation for the mixin class would occur by default ",NULL,"A lot of modules have similar options. Moreover, job modules often have options that belong to at least two domains eg jdbc hdfs . I think that by using FlattenedCompositeModuleOptionsMetadata, we could come up with a way to combine several options POJOs into one. Something like public class JdbcHdfsOptionsMetadata OptionsMixin private JdbcOptionsMetadata jdbc; OptionsMixin private HdfsOptionsMetadata hdfs; this would expose eg driverClass as well as rolloverSize as top level options. Values could be actually injected into the fields,","so that eg custom validation could occur default validation for the mixin class would occur by default","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19062,"currently the file sink only supports append. User should support an overwrite feature.",NULL,"currently the file sink only supports append. User should support an overwrite feature.",NULL,"Add for who this story is","well_formed","no_role","high",False
19062,"currently the file sink only supports append. User should support an overwrite feature.",NULL,"currently the file sink only supports append. User should support an overwrite feature.",NULL,"currently the file sink only supports append<span class='highlight-text severity-high'>. User should support an overwrite feature.</span>","minimal","punctuation","high",False
19066,"If JMX is enabled, some of the integration tests fail. This is similar to what we see in XD 1295. One example of this case is, the test classes that extend StreamTestSupport. In StreamTestSupport, the BeforeClass has this line moduleDeployer containerContext.getBean ModuleDeployer.class ; When JMX is enable, the IntegrationMBeanExporter creates JdkDynamicProxy for the ModuleDeployer since it is of type MessageHandler and thereby the above line to get bean by the implementing class type ModuleDeployer fails. There are few other places where we use to refer the implementing classes on getBean . Looks like we need to fix those as well. ",NULL,"If JMX is enabled, some of the integration tests fail. This is similar to what we see in XD 1295. One example of this case is, the test classes that extend StreamTestSupport. In StreamTestSupport, the BeforeClass has this line moduleDeployer containerContext.getBean ModuleDeployer.class ; When JMX is enable, the IntegrationMBeanExporter creates JdkDynamicProxy for the ModuleDeployer since it is of type MessageHandler and thereby the above line to get bean by the implementing class type ModuleDeployer fails. There are few other places where we use to refer the implementing classes on getBean . Looks like we need to fix those as well. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19066,"If JMX is enabled, some of the integration tests fail. This is similar to what we see in XD 1295. One example of this case is, the test classes that extend StreamTestSupport. In StreamTestSupport, the BeforeClass has this line moduleDeployer containerContext.getBean ModuleDeployer.class ; When JMX is enable, the IntegrationMBeanExporter creates JdkDynamicProxy for the ModuleDeployer since it is of type MessageHandler and thereby the above line to get bean by the implementing class type ModuleDeployer fails. There are few other places where we use to refer the implementing classes on getBean . Looks like we need to fix those as well. ",NULL,"If JMX is enabled, some of the integration tests fail. This is similar to what we see in XD 1295. One example of this case is, the test classes that extend StreamTestSupport. In StreamTestSupport, the BeforeClass has this line moduleDeployer containerContext.getBean ModuleDeployer.class ; When JMX is enable, the IntegrationMBeanExporter creates JdkDynamicProxy for the ModuleDeployer since it is of type MessageHandler and thereby the above line to get bean by the implementing class type ModuleDeployer fails. There are few other places where we use to refer the implementing classes on getBean . Looks like we need to fix those as well. ",NULL,"If JMX is enabled, some of the integration tests fail. This is similar to what we see in XD 1295. One example of this case is, the test classes that extend StreamTestSupport. In StreamTestSupport, the BeforeClass has this line moduleDeployer containerContext.getBean ModuleDeployer.class ; When JMX is enable, the IntegrationMBeanExporter creates JdkDynamicProxy for the ModuleDeployer since it is of type MessageHandler<span class='highlight-text severity-high'> and </span>thereby the above line to get bean by the implementing class type ModuleDeployer fails. There are few other places where we use to refer the implementing classes on getBean . Looks like we need to fix those as well. ","atomic","conjunctions","high",False
19071,"xd stream create name simple definition http transform filter transform file Created new stream simple xd stream create name tapSimple definition tap stream mystream.transform file Created new stream tapSimple There isn t a stream named mystream ... I'don t remember if we want to allow for this set up taps before there is a stream or if it should be an error. Otherwise, works as expected xd stream create name tapSimple2 definition tap stream simple.transform file Command failed org.springframework.xd.rest.client.impl.SpringXDException XD144E pos 11 Reference to transform is not unique in the target stream http transform filter transform file , please label the relevant module and use the label, or use a suffix index to indicate which occurrence of the module, e.g. transform.0 tap simple.transform",NULL,"xd stream create name simple definition http transform filter transform file Created new stream simple xd stream create name tapSimple definition tap stream mystream.transform file Created new stream tapSimple There isn t a stream named mystream ... I'don t remember if we want to allow for this set up taps before there is a stream or if it should be an error. Otherwise, works as expected xd stream create name tapSimple2 definition tap stream simple.transform file Command failed org.springframework.xd.rest.client.impl.SpringXDException XD144E pos 11 Reference to transform is not unique in the target stream http transform filter transform file , please label the relevant module and use the label, or use a suffix index to indicate which occurrence of the module, e.g. transform.0 tap simple.transform",NULL,"Add for who this story is","well_formed","no_role","high",False
19170,"The way we now include various Hadoop distributions is cumbersome to maintain. Need a better way of managing and isolating these dependencies on a module level rather than container level.",NULL,"The way we now include various Hadoop distributions is cumbersome to maintain. Need a better way of managing and isolating these dependencies on a module level rather than container level.",NULL,"The way we now include various Hadoop distributions is cumbersome to maintain. Need a better way of managing<span class='highlight-text severity-high'> and </span>isolating these dependencies on a module level rather than container level.","atomic","conjunctions","high",False
19385,"We need to have a properties section documented as well so that users can setup their jdbc connections for the various components.",NULL,"We need to have a properties section documented as well","so that users can setup their jdbc connections for the various components.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19066,"If JMX is enabled, some of the integration tests fail. This is similar to what we see in XD 1295. One example of this case is, the test classes that extend StreamTestSupport. In StreamTestSupport, the BeforeClass has this line moduleDeployer containerContext.getBean ModuleDeployer.class ; When JMX is enable, the IntegrationMBeanExporter creates JdkDynamicProxy for the ModuleDeployer since it is of type MessageHandler and thereby the above line to get bean by the implementing class type ModuleDeployer fails. There are few other places where we use to refer the implementing classes on getBean . Looks like we need to fix those as well. ",NULL,"If JMX is enabled, some of the integration tests fail. This is similar to what we see in XD 1295. One example of this case is, the test classes that extend StreamTestSupport. In StreamTestSupport, the BeforeClass has this line moduleDeployer containerContext.getBean ModuleDeployer.class ; When JMX is enable, the IntegrationMBeanExporter creates JdkDynamicProxy for the ModuleDeployer since it is of type MessageHandler and thereby the above line to get bean by the implementing class type ModuleDeployer fails. There are few other places where we use to refer the implementing classes on getBean . Looks like we need to fix those as well. ",NULL,"If JMX is enabled, some of the integration tests fail<span class='highlight-text severity-high'>. This is similar to what we see in XD 1295. One example of this case is, the test classes that extend StreamTestSupport. In StreamTestSupport, the BeforeClass has this line moduleDeployer containerContext.getBean ModuleDeployer.class ; When JMX is enable, the IntegrationMBeanExporter creates JdkDynamicProxy for the ModuleDeployer since it is of type MessageHandler and thereby the above line to get bean by the implementing class type ModuleDeployer fails. There are few other places where we use to refer the implementing classes on getBean . Looks like we need to fix those as well. </span>","minimal","punctuation","high",False
19067,"Currently, the reactorEnv bean is defined in module common context and the spring xd dirt has the runtime dependency over spring xd extension reactor project. This enables boot s ReactorAutoConfiguration to initialize the reactor environment, we have the reactor setup configured for both admin and container server applications. Since reactor environment is not being used by container and only used by the reactor syslog module, we can move the reactorEnv bean definition in reactor syslog module. There is one caveat in this approach as the reactor environment gets setup everytime a new reactor syslog module is deployed.",NULL,"Currently, the reactorEnv bean is defined in module common context and the spring xd dirt has the runtime dependency over spring xd extension reactor project. This enables boot s ReactorAutoConfiguration to initialize the reactor environment, we have the reactor setup configured for both admin and container server applications. Since reactor environment is not being used by container and only used by the reactor syslog module, we can move the reactorEnv bean definition in reactor syslog module. There is one caveat in this approach as the reactor environment gets setup everytime a new reactor syslog module is deployed.",NULL,"Add for who this story is","well_formed","no_role","high",False
19067,"Currently, the reactorEnv bean is defined in module common context and the spring xd dirt has the runtime dependency over spring xd extension reactor project. This enables boot s ReactorAutoConfiguration to initialize the reactor environment, we have the reactor setup configured for both admin and container server applications. Since reactor environment is not being used by container and only used by the reactor syslog module, we can move the reactorEnv bean definition in reactor syslog module. There is one caveat in this approach as the reactor environment gets setup everytime a new reactor syslog module is deployed.",NULL,"Currently, the reactorEnv bean is defined in module common context and the spring xd dirt has the runtime dependency over spring xd extension reactor project. This enables boot s ReactorAutoConfiguration to initialize the reactor environment, we have the reactor setup configured for both admin and container server applications. Since reactor environment is not being used by container and only used by the reactor syslog module, we can move the reactorEnv bean definition in reactor syslog module. There is one caveat in this approach as the reactor environment gets setup everytime a new reactor syslog module is deployed.",NULL,"Currently, the reactorEnv bean is defined in module common context<span class='highlight-text severity-high'> and </span>the spring xd dirt has the runtime dependency over spring xd extension reactor project. This enables boot s ReactorAutoConfiguration to initialize the reactor environment, we have the reactor setup configured for both admin and container server applications. Since reactor environment is not being used by container and only used by the reactor syslog module, we can move the reactorEnv bean definition in reactor syslog module. There is one caveat in this approach as the reactor environment gets setup everytime a new reactor syslog module is deployed.","atomic","conjunctions","high",False
19067,"Currently, the reactorEnv bean is defined in module common context and the spring xd dirt has the runtime dependency over spring xd extension reactor project. This enables boot s ReactorAutoConfiguration to initialize the reactor environment, we have the reactor setup configured for both admin and container server applications. Since reactor environment is not being used by container and only used by the reactor syslog module, we can move the reactorEnv bean definition in reactor syslog module. There is one caveat in this approach as the reactor environment gets setup everytime a new reactor syslog module is deployed.",NULL,"Currently, the reactorEnv bean is defined in module common context and the spring xd dirt has the runtime dependency over spring xd extension reactor project. This enables boot s ReactorAutoConfiguration to initialize the reactor environment, we have the reactor setup configured for both admin and container server applications. Since reactor environment is not being used by container and only used by the reactor syslog module, we can move the reactorEnv bean definition in reactor syslog module. There is one caveat in this approach as the reactor environment gets setup everytime a new reactor syslog module is deployed.",NULL,"Currently, the reactorEnv bean is defined in module common context and the spring xd dirt has the runtime dependency over spring xd extension reactor project<span class='highlight-text severity-high'>. This enables boot s ReactorAutoConfiguration to initialize the reactor environment, we have the reactor setup configured for both admin and container server applications. Since reactor environment is not being used by container and only used by the reactor syslog module, we can move the reactorEnv bean definition in reactor syslog module. There is one caveat in this approach as the reactor environment gets setup everytime a new reactor syslog module is deployed.</span>","minimal","punctuation","high",False
19069,"There is no point in providing completion with something that will fail when the user tries it. The information about a module being a composed is now available at the REST layer, so should be used",NULL,"There is no point in providing completion with","something that will fail when the user tries it. The information about a module being a composed is now available at the REST layer, so should be used","Add for who this story is","well_formed","no_role","high",False
19069,"There is no point in providing completion with something that will fail when the user tries it. The information about a module being a composed is now available at the REST layer, so should be used",NULL,"There is no point in providing completion with","something that will fail when the user tries it. The information about a module being a composed is now available at the REST layer, so should be used","There is no point in providing completion with something that will fail when the user tries it<span class='highlight-text severity-high'>. The information about a module being a composed is now available at the REST layer, so should be used</span>","minimal","punctuation","high",False
19069,"There is no point in providing completion with something that will fail when the user tries it. The information about a module being a composed is now available at the REST layer, so should be used",NULL,"There is no point in providing completion with","something that will fail when the user tries it. The information about a module being a composed is now available at the REST layer, so should be used","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19065,"Commit 96de9fcfaf32719413015a1a6bace1b30b6b9610 strengthened module type inference, but some corner cases remain marked as TODOs and commented out assertions in tests . To be effective, we need to look at the whole deployed stream or composed module . Modify ParsinContext accordingly.",NULL,"Commit 96de9fcfaf32719413015a1a6bace1b30b6b9610 strengthened module type inference, but some corner cases remain marked as TODOs and commented out assertions in tests . To be effective, we need to look at the whole deployed stream or composed module . Modify ParsinContext accordingly.",NULL,"Add for who this story is","well_formed","no_role","high",False
19065,"Commit 96de9fcfaf32719413015a1a6bace1b30b6b9610 strengthened module type inference, but some corner cases remain marked as TODOs and commented out assertions in tests . To be effective, we need to look at the whole deployed stream or composed module . Modify ParsinContext accordingly.",NULL,"Commit 96de9fcfaf32719413015a1a6bace1b30b6b9610 strengthened module type inference, but some corner cases remain marked as TODOs and commented out assertions in tests . To be effective, we need to look at the whole deployed stream or composed module . Modify ParsinContext accordingly.",NULL,"Commit 96de9fcfaf32719413015a1a6bace1b30b6b9610 strengthened module type inference, but some corner cases remain marked as TODOs<span class='highlight-text severity-high'> and </span>commented out assertions in tests . To be effective, we need to look at the whole deployed stream<span class='highlight-text severity-high'> or </span>composed module . Modify ParsinContext accordingly.","atomic","conjunctions","high",False
19065,"Commit 96de9fcfaf32719413015a1a6bace1b30b6b9610 strengthened module type inference, but some corner cases remain marked as TODOs and commented out assertions in tests . To be effective, we need to look at the whole deployed stream or composed module . Modify ParsinContext accordingly.",NULL,"Commit 96de9fcfaf32719413015a1a6bace1b30b6b9610 strengthened module type inference, but some corner cases remain marked as TODOs and commented out assertions in tests . To be effective, we need to look at the whole deployed stream or composed module . Modify ParsinContext accordingly.",NULL,"Commit 96de9fcfaf32719413015a1a6bace1b30b6b9610 strengthened module type inference, but some corner cases remain marked as TODOs and commented out assertions in tests <span class='highlight-text severity-high'>. To be effective, we need to look at the whole deployed stream or composed module . Modify ParsinContext accordingly.</span>","minimal","punctuation","high",False
19070,"The current configuration prevents modules to have default values that evaluate to null. The workaround is to either have the module have its own PPC which allows nulls rid the placeholders with foo ",NULL,"The current configuration prevents modules to have default values that evaluate to null. The workaround is to either have the module have its own PPC which allows nulls rid the placeholders with foo ",NULL,"Add for who this story is","well_formed","no_role","high",False
19070,"The current configuration prevents modules to have default values that evaluate to null. The workaround is to either have the module have its own PPC which allows nulls rid the placeholders with foo ",NULL,"The current configuration prevents modules to have default values that evaluate to null. The workaround is to either have the module have its own PPC which allows nulls rid the placeholders with foo ",NULL,"The current configuration prevents modules to have default values that evaluate to null<span class='highlight-text severity-high'>. The workaround is to either have the module have its own PPC which allows nulls rid the placeholders with foo </span>","minimal","punctuation","high",False
19080,"To show best practice, our batch jobs should include these listeners so that notifications can be sent. In particular it is desirable that jobs with file item readers can send the files that were processed so another stream can be sent the file name when the job completes to move delete the file",NULL,"To show best practice, our batch jobs should include these listeners","so that notifications can be sent. In particular it is desirable that jobs with file item readers can send the files that were processed so another stream can be sent the file name when the job completes to move delete the file","Add for who this story is","well_formed","no_role","high",False
19080,"To show best practice, our batch jobs should include these listeners so that notifications can be sent. In particular it is desirable that jobs with file item readers can send the files that were processed so another stream can be sent the file name when the job completes to move delete the file",NULL,"To show best practice, our batch jobs should include these listeners","so that notifications can be sent. In particular it is desirable that jobs with file item readers can send the files that were processed so another stream can be sent the file name when the job completes to move delete the file","To show best practice, our batch jobs should include these listeners so that notifications can be sent<span class='highlight-text severity-high'>. In particular it is desirable that jobs with file item readers can send the files that were processed so another stream can be sent the file name when the job completes to move delete the file</span>","minimal","punctuation","high",False
19080,"To show best practice, our batch jobs should include these listeners so that notifications can be sent. In particular it is desirable that jobs with file item readers can send the files that were processed so another stream can be sent the file name when the job completes to move delete the file",NULL,"To show best practice, our batch jobs should include these listeners","so that notifications can be sent. In particular it is desirable that jobs with file item readers can send the files that were processed so another stream can be sent the file name when the job completes to move delete the file","To show best practice, our batch jobs should include these listeners<span class='highlight-text severity-high'> so that </span>notifications can be sent. In particular it is desirable that jobs with file item readers can send the files that were processed<span class='highlight-text severity-high'> so </span>another stream can be sent the file name when the job completes to move delete the file","minimal","indicator_repetition","high",False
19080,"To show best practice, our batch jobs should include these listeners so that notifications can be sent. In particular it is desirable that jobs with file item readers can send the files that were processed so another stream can be sent the file name when the job completes to move delete the file",NULL,"To show best practice, our batch jobs should include these listeners","so that notifications can be sent. In particular it is desirable that jobs with file item readers can send the files that were processed so another stream can be sent the file name when the job completes to move delete the file","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19081,"Since makeUnique, dateFormat and numberFormat now have their own module options now, they are nicely advertised as options to a job and will benefit from code completion soon , so they can be removed from the shell where they currently allow for a misconfiguration if set at both the job and shell level ",NULL,"Since makeUnique, dateFormat and numberFormat now have their own module options now, they are nicely advertised as options to a job and will benefit from code completion","soon , so they can be removed from the shell where they currently allow for a misconfiguration if set at both the job and shell level","Add for who this story is","well_formed","no_role","high",False
19081,"Since makeUnique, dateFormat and numberFormat now have their own module options now, they are nicely advertised as options to a job and will benefit from code completion soon , so they can be removed from the shell where they currently allow for a misconfiguration if set at both the job and shell level ",NULL,"Since makeUnique, dateFormat and numberFormat now have their own module options now, they are nicely advertised as options to a job and will benefit from code completion","soon , so they can be removed from the shell where they currently allow for a misconfiguration if set at both the job and shell level","Since makeUnique, dateFormat<span class='highlight-text severity-high'> and </span>numberFormat now have their own module options now, they are nicely advertised as options to a job and will benefit from code completion soon , so they can be removed from the shell where they currently allow for a misconfiguration if set at both the job and shell level ","atomic","conjunctions","high",False
19081,"Since makeUnique, dateFormat and numberFormat now have their own module options now, they are nicely advertised as options to a job and will benefit from code completion soon , so they can be removed from the shell where they currently allow for a misconfiguration if set at both the job and shell level ",NULL,"Since makeUnique, dateFormat and numberFormat now have their own module options now, they are nicely advertised as options to a job and will benefit from code completion","soon , so they can be removed from the shell where they currently allow for a misconfiguration if set at both the job and shell level","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19068,"Currently, the job module s batch job s bean id should be job . This also causes the job name to be actual job name .job and the batch job controllers require to search for job with suffix .job . Removing this constraint would help us avoiding these.",NULL,"Currently, the job module s batch job s bean id should be job . This also causes the job name to be actual job name .job and the batch job controllers require to search for job with suffix .job . Removing this constraint would help us avoiding these.",NULL,"Add for who this story is","well_formed","no_role","high",False
19068,"Currently, the job module s batch job s bean id should be job . This also causes the job name to be actual job name .job and the batch job controllers require to search for job with suffix .job . Removing this constraint would help us avoiding these.",NULL,"Currently, the job module s batch job s bean id should be job . This also causes the job name to be actual job name .job and the batch job controllers require to search for job with suffix .job . Removing this constraint would help us avoiding these.",NULL,"Currently, the job module s batch job s bean id should be job . This also causes the job name to be actual job name .job<span class='highlight-text severity-high'> and </span>the batch job controllers require to search for job with suffix .job . Removing this constraint would help us avoiding these.","atomic","conjunctions","high",False
19068,"Currently, the job module s batch job s bean id should be job . This also causes the job name to be actual job name .job and the batch job controllers require to search for job with suffix .job . Removing this constraint would help us avoiding these.",NULL,"Currently, the job module s batch job s bean id should be job . This also causes the job name to be actual job name .job and the batch job controllers require to search for job with suffix .job . Removing this constraint would help us avoiding these.",NULL,"Currently, the job module s batch job s bean id should be job <span class='highlight-text severity-high'>. This also causes the job name to be actual job name .job and the batch job controllers require to search for job with suffix .job . Removing this constraint would help us avoiding these.</span>","minimal","punctuation","high",False
19079,"May want to fix properly when tackling the module options for composed modules, but this is currently broken and wasn t at some point module compose upperHttp definition http transform expression payload.toUpperCase stream create foo definition upperHttp log This will fail saying that port can t be resolved This will work though module compose upperHttp definition http port 9000 transform expression payload.toUpperCase stream create foo definition upperHttp log Note that stream create foo definition upperHttp port xxx log should work too wut won t, but that s another bug will create after this one ",NULL,"May want to fix properly when tackling the module options for composed modules, but this is currently broken and wasn t at some point module compose upperHttp definition http transform expression payload.toUpperCase stream create foo definition upperHttp log This will fail saying that port can t be resolved This will work though module compose upperHttp definition http port 9000 transform expression payload.toUpperCase stream create foo definition upperHttp log Note that stream create foo definition upperHttp port xxx log should work too wut won t, but that s another bug will create after this one ",NULL,"Add for who this story is","well_formed","no_role","high",False
19079,"May want to fix properly when tackling the module options for composed modules, but this is currently broken and wasn t at some point module compose upperHttp definition http transform expression payload.toUpperCase stream create foo definition upperHttp log This will fail saying that port can t be resolved This will work though module compose upperHttp definition http port 9000 transform expression payload.toUpperCase stream create foo definition upperHttp log Note that stream create foo definition upperHttp port xxx log should work too wut won t, but that s another bug will create after this one ",NULL,"May want to fix properly when tackling the module options for composed modules, but this is currently broken and wasn t at some point module compose upperHttp definition http transform expression payload.toUpperCase stream create foo definition upperHttp log This will fail saying that port can t be resolved This will work though module compose upperHttp definition http port 9000 transform expression payload.toUpperCase stream create foo definition upperHttp log Note that stream create foo definition upperHttp port xxx log should work too wut won t, but that s another bug will create after this one ",NULL,"May want to fix properly when tackling the module options for composed modules, but this is currently broken<span class='highlight-text severity-high'> and </span>wasn t at some point module compose upperHttp definition http transform expression payload.toUpperCase stream create foo definition upperHttp log This will fail saying that port can t be resolved This will work though module compose upperHttp definition http port 9000 transform expression payload.toUpperCase stream create foo definition upperHttp log Note that stream create foo definition upperHttp port xxx log should work too wut won t, but that s another bug will create after this one ","atomic","conjunctions","high",False
19074,"When doing dsl completion in a stream definition and a module option accepts a value that has a closed set of possible values eg booleans, enums , we can provide completions for those. ",NULL,"When doing dsl completion in a stream definition and a module option accepts a value that has a closed set of possible values eg booleans, enums , we can provide completions for those. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19071,"xd stream create name simple definition http transform filter transform file Created new stream simple xd stream create name tapSimple definition tap stream mystream.transform file Created new stream tapSimple There isn t a stream named mystream ... I'don t remember if we want to allow for this set up taps before there is a stream or if it should be an error. Otherwise, works as expected xd stream create name tapSimple2 definition tap stream simple.transform file Command failed org.springframework.xd.rest.client.impl.SpringXDException XD144E pos 11 Reference to transform is not unique in the target stream http transform filter transform file , please label the relevant module and use the label, or use a suffix index to indicate which occurrence of the module, e.g. transform.0 tap simple.transform",NULL,"xd stream create name simple definition http transform filter transform file Created new stream simple xd stream create name tapSimple definition tap stream mystream.transform file Created new stream tapSimple There isn t a stream named mystream ... I'don t remember if we want to allow for this set up taps before there is a stream or if it should be an error. Otherwise, works as expected xd stream create name tapSimple2 definition tap stream simple.transform file Command failed org.springframework.xd.rest.client.impl.SpringXDException XD144E pos 11 Reference to transform is not unique in the target stream http transform filter transform file , please label the relevant module and use the label, or use a suffix index to indicate which occurrence of the module, e.g. transform.0 tap simple.transform",NULL,"xd stream create name simple definition http transform filter transform file Created new stream simple xd stream create name tapSimple definition tap stream mystream.transform file Created new stream tapSimple There isn t a stream named mystream ... I'don t remember if we want to allow for this set up taps before there is a stream<span class='highlight-text severity-high'> or </span>if it should be an error. Otherwise, works as expected xd stream create name tapSimple2 definition tap stream simple.transform file Command failed org.springframework.xd.rest.client.impl.SpringXDException XD144E pos 11 Reference to transform is not unique in the target stream http transform filter transform file , please label the relevant module<span class='highlight-text severity-high'> and </span>use the label, or use a suffix index to indicate which occurrence of the module, e.g. transform.0 tap simple.transform","atomic","conjunctions","high",False
19071,"xd stream create name simple definition http transform filter transform file Created new stream simple xd stream create name tapSimple definition tap stream mystream.transform file Created new stream tapSimple There isn t a stream named mystream ... I'don t remember if we want to allow for this set up taps before there is a stream or if it should be an error. Otherwise, works as expected xd stream create name tapSimple2 definition tap stream simple.transform file Command failed org.springframework.xd.rest.client.impl.SpringXDException XD144E pos 11 Reference to transform is not unique in the target stream http transform filter transform file , please label the relevant module and use the label, or use a suffix index to indicate which occurrence of the module, e.g. transform.0 tap simple.transform",NULL,"xd stream create name simple definition http transform filter transform file Created new stream simple xd stream create name tapSimple definition tap stream mystream.transform file Created new stream tapSimple There isn t a stream named mystream ... I'don t remember if we want to allow for this set up taps before there is a stream or if it should be an error. Otherwise, works as expected xd stream create name tapSimple2 definition tap stream simple.transform file Command failed org.springframework.xd.rest.client.impl.SpringXDException XD144E pos 11 Reference to transform is not unique in the target stream http transform filter transform file , please label the relevant module and use the label, or use a suffix index to indicate which occurrence of the module, e.g. transform.0 tap simple.transform",NULL,"xd stream create name simple definition http transform filter transform file Created new stream simple xd stream create name tapSimple definition tap stream mystream<span class='highlight-text severity-high'>.transform file Created new stream tapSimple There isn t a stream named mystream ... I'don t remember if we want to allow for this set up taps before there is a stream or if it should be an error. Otherwise, works as expected xd stream create name tapSimple2 definition tap stream simple.transform file Command failed org.springframework.xd.rest.client.impl.SpringXDException XD144E pos 11 Reference to transform is not unique in the target stream http transform filter transform file , please label the relevant module and use the label, or use a suffix index to indicate which occurrence of the module, e.g. transform.0 tap simple.transform</span>","minimal","punctuation","high",False
19075,"This is about computing the value to support expressions such as xd.stream.name as a default. Initial discussion suggested to leverage the work done in XD 1175 by having a custom LateValue or DeployTimeValue, etc be resolved at deployment time",NULL,"This is about computing the value to support expressions such as xd.stream.name as a default. Initial discussion suggested to leverage the work done in XD 1175 by having a custom LateValue or DeployTimeValue, etc be resolved at deployment time",NULL,"Add for who this story is","well_formed","no_role","high",False
19075,"This is about computing the value to support expressions such as xd.stream.name as a default. Initial discussion suggested to leverage the work done in XD 1175 by having a custom LateValue or DeployTimeValue, etc be resolved at deployment time",NULL,"This is about computing the value to support expressions such as xd.stream.name as a default. Initial discussion suggested to leverage the work done in XD 1175 by having a custom LateValue or DeployTimeValue, etc be resolved at deployment time",NULL,"This is about computing the value to support expressions such as xd.stream.name as a default. Initial discussion suggested to leverage the work done in XD 1175 by having a custom LateValue<span class='highlight-text severity-high'> or </span>DeployTimeValue, etc be resolved at deployment time","atomic","conjunctions","high",False
19075,"This is about computing the value to support expressions such as xd.stream.name as a default. Initial discussion suggested to leverage the work done in XD 1175 by having a custom LateValue or DeployTimeValue, etc be resolved at deployment time",NULL,"This is about computing the value to support expressions such as xd.stream.name as a default. Initial discussion suggested to leverage the work done in XD 1175 by having a custom LateValue or DeployTimeValue, etc be resolved at deployment time",NULL,"This is about computing the value to support expressions such as xd<span class='highlight-text severity-high'>.stream.name as a default. Initial discussion suggested to leverage the work done in XD 1175 by having a custom LateValue or DeployTimeValue, etc be resolved at deployment time</span>","minimal","punctuation","high",False
19072,"see discussion at https github.com spring projects spring xd pull 495 discussion diff 9291037",NULL,"see discussion at https github.com spring projects spring xd pull 495 discussion diff 9291037",NULL,"Add for who this story is","well_formed","no_role","high",False
19077,"Create a better UI instead of Boot s default Whitelabel Error Page ",NULL,"Create a better UI instead of Boot s default Whitelabel Error Page ",NULL,"Add for who this story is","well_formed","no_role","high",False
19073,"see CompletionProviderTests testUnfinishedModuleNameShouldReturnCompletions Ideally, would require a change in the parser so that it knows which kind of module was expected when it failed.",NULL,"see CompletionProviderTests testUnfinishedModuleNameShouldReturnCompletions Ideally, would require a change in the parser","so that it knows which kind of module was expected when it failed.","Add for who this story is","well_formed","no_role","high",False
19073,"see CompletionProviderTests testUnfinishedModuleNameShouldReturnCompletions Ideally, would require a change in the parser so that it knows which kind of module was expected when it failed.",NULL,"see CompletionProviderTests testUnfinishedModuleNameShouldReturnCompletions Ideally, would require a change in the parser","so that it knows which kind of module was expected when it failed.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19078,"The kite SDK is used to write Avro records in a Kite specific format and also it support the parquet format for which we will eventually support.",NULL,"The kite SDK is used to write Avro records in a Kite specific format and also it support the parquet format for which we will eventually support.",NULL,"Add for who this story is","well_formed","no_role","high",False
19078,"The kite SDK is used to write Avro records in a Kite specific format and also it support the parquet format for which we will eventually support.",NULL,"The kite SDK is used to write Avro records in a Kite specific format and also it support the parquet format for which we will eventually support.",NULL,"The kite SDK is used to write Avro records in a Kite specific format<span class='highlight-text severity-high'> and </span>also it support the parquet format for which we will eventually support.","atomic","conjunctions","high",False
19083,"Improve how the state of the stream is managed. A deploy command moves the stream from the undeployed state to the deploying state. If all modules in the stream are successfully deployed, the stream state is deployed If one or more module deployments failed, the stream state is failed. Any modules that were successfully deployed, are still running. Sending an undeploy command will stop all modules of the stream and return the stream to the undeployed state. For the individual modules that failed, we will be able to find out which ones failed. Not yet sure if we can try to redeploy just those parts of the stream that failed. See the design doc https docs.google.com a gopivotal.com document d 1kWtoH xEF1wMklzQ8AZaiuhBZWIlpCDi8G9 hAP8Fgc edit heading h.2rk74f16ow4i for more details. Story points for this issue are the total of all the story points for the subtasks.",NULL,"Improve how the state of the stream is managed. A deploy command moves the stream from the undeployed state to the deploying state. If all modules in the stream are successfully deployed, the stream state is deployed If one or more module deployments failed, the stream state is failed. Any modules that were successfully deployed, are still running. Sending an undeploy command will stop all modules of the stream and return the stream to the undeployed state. For the individual modules that failed, we will be able to find out which ones failed. Not yet sure if we can try to redeploy just those parts of the stream that failed. See the design doc https docs.google.com a gopivotal.com document d 1kWtoH xEF1wMklzQ8AZaiuhBZWIlpCDi8G9 hAP8Fgc edit heading h.2rk74f16ow4i for more details. Story points for this issue are the total of all the story points for the subtasks.",NULL,"Add for who this story is","well_formed","no_role","high",False
19083,"Improve how the state of the stream is managed. A deploy command moves the stream from the undeployed state to the deploying state. If all modules in the stream are successfully deployed, the stream state is deployed If one or more module deployments failed, the stream state is failed. Any modules that were successfully deployed, are still running. Sending an undeploy command will stop all modules of the stream and return the stream to the undeployed state. For the individual modules that failed, we will be able to find out which ones failed. Not yet sure if we can try to redeploy just those parts of the stream that failed. See the design doc https docs.google.com a gopivotal.com document d 1kWtoH xEF1wMklzQ8AZaiuhBZWIlpCDi8G9 hAP8Fgc edit heading h.2rk74f16ow4i for more details. Story points for this issue are the total of all the story points for the subtasks.",NULL,"Improve how the state of the stream is managed. A deploy command moves the stream from the undeployed state to the deploying state. If all modules in the stream are successfully deployed, the stream state is deployed If one or more module deployments failed, the stream state is failed. Any modules that were successfully deployed, are still running. Sending an undeploy command will stop all modules of the stream and return the stream to the undeployed state. For the individual modules that failed, we will be able to find out which ones failed. Not yet sure if we can try to redeploy just those parts of the stream that failed. See the design doc https docs.google.com a gopivotal.com document d 1kWtoH xEF1wMklzQ8AZaiuhBZWIlpCDi8G9 hAP8Fgc edit heading h.2rk74f16ow4i for more details. Story points for this issue are the total of all the story points for the subtasks.",NULL,"Improve how the state of the stream is managed. A deploy command moves the stream from the undeployed state to the deploying state. If all modules in the stream are successfully deployed, the stream state is deployed If one<span class='highlight-text severity-high'> or </span>more module deployments failed, the stream state is failed. Any modules that were successfully deployed, are still running. Sending an undeploy command will stop all modules of the stream<span class='highlight-text severity-high'> and </span>return the stream to the undeployed state. For the individual modules that failed, we will be able to find out which ones failed. Not yet sure if we can try to redeploy just those parts of the stream that failed. See the design doc https docs.google.com a gopivotal.com document d 1kWtoH xEF1wMklzQ8AZaiuhBZWIlpCDi8G9 hAP8Fgc edit heading h.2rk74f16ow4i for more details. Story points for this issue are the total of all the story points for the subtasks.","atomic","conjunctions","high",False
19083,"Improve how the state of the stream is managed. A deploy command moves the stream from the undeployed state to the deploying state. If all modules in the stream are successfully deployed, the stream state is deployed If one or more module deployments failed, the stream state is failed. Any modules that were successfully deployed, are still running. Sending an undeploy command will stop all modules of the stream and return the stream to the undeployed state. For the individual modules that failed, we will be able to find out which ones failed. Not yet sure if we can try to redeploy just those parts of the stream that failed. See the design doc https docs.google.com a gopivotal.com document d 1kWtoH xEF1wMklzQ8AZaiuhBZWIlpCDi8G9 hAP8Fgc edit heading h.2rk74f16ow4i for more details. Story points for this issue are the total of all the story points for the subtasks.",NULL,"Improve how the state of the stream is managed. A deploy command moves the stream from the undeployed state to the deploying state. If all modules in the stream are successfully deployed, the stream state is deployed If one or more module deployments failed, the stream state is failed. Any modules that were successfully deployed, are still running. Sending an undeploy command will stop all modules of the stream and return the stream to the undeployed state. For the individual modules that failed, we will be able to find out which ones failed. Not yet sure if we can try to redeploy just those parts of the stream that failed. See the design doc https docs.google.com a gopivotal.com document d 1kWtoH xEF1wMklzQ8AZaiuhBZWIlpCDi8G9 hAP8Fgc edit heading h.2rk74f16ow4i for more details. Story points for this issue are the total of all the story points for the subtasks.",NULL,"Improve how the state of the stream is managed<span class='highlight-text severity-high'>. A deploy command moves the stream from the undeployed state to the deploying state. If all modules in the stream are successfully deployed, the stream state is deployed If one or more module deployments failed, the stream state is failed. Any modules that were successfully deployed, are still running. Sending an undeploy command will stop all modules of the stream and return the stream to the undeployed state. For the individual modules that failed, we will be able to find out which ones failed. Not yet sure if we can try to redeploy just those parts of the stream that failed. See the design doc https docs.google.com a gopivotal.com document d 1kWtoH xEF1wMklzQ8AZaiuhBZWIlpCDi8G9 hAP8Fgc edit heading h.2rk74f16ow4i for more details. Story points for this issue are the total of all the story points for the subtasks.</span>","minimal","punctuation","high",False
19084,"The logic can be found in DefaultModuleOptionsMetadataCollector but caused problems in the initial PR. Revisit if needed",NULL,"The logic can be found in DefaultModuleOptionsMetadataCollector but caused problems in the initial PR. Revisit if needed",NULL,"Add for who this story is","well_formed","no_role","high",False
19084,"The logic can be found in DefaultModuleOptionsMetadataCollector but caused problems in the initial PR. Revisit if needed",NULL,"The logic can be found in DefaultModuleOptionsMetadataCollector but caused problems in the initial PR. Revisit if needed",NULL,"The logic can be found in DefaultModuleOptionsMetadataCollector but caused problems in the initial PR<span class='highlight-text severity-high'>. Revisit if needed</span>","minimal","punctuation","high",False
19085,"The modules are exposed via JMX and in turn exposed over http via jolokia. See https jira.springsource.org browse XD 343. This issue is to develop a helper method that given a stream id and or module name, assert that the number of messages processed after sending stimulus messages is as expected. e.g. int originalCount getCount testStream , file ; do stuff that generates 100 messages assertCount testStream , file , 100, originalCount For now we can assume we know the location of where the modules are located by assuming we have only one container deployed.",NULL,"The modules are exposed via JMX and in turn exposed over http via jolokia. See https jira.springsource.org browse XD 343. This issue is to develop a helper method that given a stream id and or module name, assert that the number of messages processed after sending stimulus messages is as expected. e.g. int originalCount getCount testStream , file ; do stuff that generates 100 messages assertCount testStream , file , 100, originalCount For now we can assume we know the location of where the modules are located by assuming we have only one container deployed.",NULL,"Add for who this story is","well_formed","no_role","high",False
19085,"The modules are exposed via JMX and in turn exposed over http via jolokia. See https jira.springsource.org browse XD 343. This issue is to develop a helper method that given a stream id and or module name, assert that the number of messages processed after sending stimulus messages is as expected. e.g. int originalCount getCount testStream , file ; do stuff that generates 100 messages assertCount testStream , file , 100, originalCount For now we can assume we know the location of where the modules are located by assuming we have only one container deployed.",NULL,"The modules are exposed via JMX and in turn exposed over http via jolokia. See https jira.springsource.org browse XD 343. This issue is to develop a helper method that given a stream id and or module name, assert that the number of messages processed after sending stimulus messages is as expected. e.g. int originalCount getCount testStream , file ; do stuff that generates 100 messages assertCount testStream , file , 100, originalCount For now we can assume we know the location of where the modules are located by assuming we have only one container deployed.",NULL,"The modules are exposed via JMX<span class='highlight-text severity-high'> and </span>in turn exposed over http via jolokia. See https jira.springsource.org browse XD 343. This issue is to develop a helper method that given a stream id and<span class='highlight-text severity-high'> or </span>module name, assert that the number of messages processed after sending stimulus messages is as expected. e.g. int originalCount getCount testStream , file ; do stuff that generates 100 messages assertCount testStream , file , 100, originalCount For now we can assume we know the location of where the modules are located by assuming we have only one container deployed.","atomic","conjunctions","high",False
19085,"The modules are exposed via JMX and in turn exposed over http via jolokia. See https jira.springsource.org browse XD 343. This issue is to develop a helper method that given a stream id and or module name, assert that the number of messages processed after sending stimulus messages is as expected. e.g. int originalCount getCount testStream , file ; do stuff that generates 100 messages assertCount testStream , file , 100, originalCount For now we can assume we know the location of where the modules are located by assuming we have only one container deployed.",NULL,"The modules are exposed via JMX and in turn exposed over http via jolokia. See https jira.springsource.org browse XD 343. This issue is to develop a helper method that given a stream id and or module name, assert that the number of messages processed after sending stimulus messages is as expected. e.g. int originalCount getCount testStream , file ; do stuff that generates 100 messages assertCount testStream , file , 100, originalCount For now we can assume we know the location of where the modules are located by assuming we have only one container deployed.",NULL,"The modules are exposed via JMX and in turn exposed over http via jolokia<span class='highlight-text severity-high'>. See https jira.springsource.org browse XD 343. This issue is to develop a helper method that given a stream id and or module name, assert that the number of messages processed after sending stimulus messages is as expected. e.g. int originalCount getCount testStream , file ; do stuff that generates 100 messages assertCount testStream , file , 100, originalCount For now we can assume we know the location of where the modules are located by assuming we have only one container deployed.</span>","minimal","punctuation","high",False
19087,"See also XD 451 as reference.",NULL,"See also XD 451 as reference.",NULL,"Add for who this story is","well_formed","no_role","high",False
19086,"Currently, if we want to bind values to script variables we need to put them in a properties file like so xd stream create name groovyprocessortest definition http port 9006 script location custom processor.groovy properties location custom processor.properties log Ideally it should be xd stream create name groovyprocessortest definition http port 9006 script location custom processor.groovy foo bar baz boo log ",NULL,"Currently, if we want to bind values to script variables we need to put them in a properties file like","so xd stream create name groovyprocessortest definition http port 9006 script location custom processor.groovy properties location custom processor.properties log Ideally it should be xd stream create name groovyprocessortest definition http port 9006 script location custom processor.groovy foo bar baz boo log","Add for who this story is","well_formed","no_role","high",False
19086,"Currently, if we want to bind values to script variables we need to put them in a properties file like so xd stream create name groovyprocessortest definition http port 9006 script location custom processor.groovy properties location custom processor.properties log Ideally it should be xd stream create name groovyprocessortest definition http port 9006 script location custom processor.groovy foo bar baz boo log ",NULL,"Currently, if we want to bind values to script variables we need to put them in a properties file like","so xd stream create name groovyprocessortest definition http port 9006 script location custom processor.groovy properties location custom processor.properties log Ideally it should be xd stream create name groovyprocessortest definition http port 9006 script location custom processor.groovy foo bar baz boo log","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19088,"Create a first pass at an acceptance test app for a stream definition of http log. This will involve creating two new projects in xd 1. spring xd integration test 2. spring xd acceptance tests 1 will contain generally useful utility methods for acceptance test, such as sending data over http, obtaining and asserting JMX values of specific modules. 2 will contain tests that use 1 to test the various out of the box modules provides in XD.",NULL,"Create a first pass at an acceptance test app for a stream definition of http log. This will involve creating two new projects in xd 1. spring xd integration test 2. spring xd acceptance tests 1 will contain generally useful utility methods for acceptance test, such as sending data over http, obtaining and asserting JMX values of specific modules. 2 will contain tests that use 1 to test the various out of the box modules provides in XD.",NULL,"Create a first pass at an acceptance test app for a stream definition of http log. This will involve creating two new projects in xd 1. spring xd integration test 2. spring xd acceptance tests 1 will contain generally useful utility methods for acceptance test, such as sending data over http, obtaining<span class='highlight-text severity-high'> and </span>asserting JMX values of specific modules. 2 will contain tests that use 1 to test the various out of the box modules provides in XD.","atomic","conjunctions","high",False
19088,"Create a first pass at an acceptance test app for a stream definition of http log. This will involve creating two new projects in xd 1. spring xd integration test 2. spring xd acceptance tests 1 will contain generally useful utility methods for acceptance test, such as sending data over http, obtaining and asserting JMX values of specific modules. 2 will contain tests that use 1 to test the various out of the box modules provides in XD.",NULL,"Create a first pass at an acceptance test app for a stream definition of http log. This will involve creating two new projects in xd 1. spring xd integration test 2. spring xd acceptance tests 1 will contain generally useful utility methods for acceptance test, such as sending data over http, obtaining and asserting JMX values of specific modules. 2 will contain tests that use 1 to test the various out of the box modules provides in XD.",NULL,"Create a first pass at an acceptance test app for a stream definition of http log<span class='highlight-text severity-high'>. This will involve creating two new projects in xd 1. spring xd integration test 2. spring xd acceptance tests 1 will contain generally useful utility methods for acceptance test, such as sending data over http, obtaining and asserting JMX values of specific modules. 2 will contain tests that use 1 to test the various out of the box modules provides in XD.</span>","minimal","punctuation","high",False
19089,"Use the cleanup app from XD 1243 to stop previous CI runs of XD on EC2. Build XD EC2 deployer from github. Use XD EC2 Deployer to deploy the CI XD Instance Should produce artifact that contains the URL admin server of the XD cluster. container servers of the XD cluster",NULL,"Use the cleanup app from XD 1243 to stop previous CI runs of XD on EC2. Build XD EC2 deployer from github. Use XD EC2 Deployer to deploy the CI XD Instance Should produce artifact that contains the URL admin server of the XD cluster. container servers of the XD cluster",NULL,"Add for who this story is","well_formed","no_role","high",False
19089,"Use the cleanup app from XD 1243 to stop previous CI runs of XD on EC2. Build XD EC2 deployer from github. Use XD EC2 Deployer to deploy the CI XD Instance Should produce artifact that contains the URL admin server of the XD cluster. container servers of the XD cluster",NULL,"Use the cleanup app from XD 1243 to stop previous CI runs of XD on EC2. Build XD EC2 deployer from github. Use XD EC2 Deployer to deploy the CI XD Instance Should produce artifact that contains the URL admin server of the XD cluster. container servers of the XD cluster",NULL,"Use the cleanup app from XD 1243 to stop previous CI runs of XD on EC2<span class='highlight-text severity-high'>. Build XD EC2 deployer from github. Use XD EC2 Deployer to deploy the CI XD Instance Should produce artifact that contains the URL admin server of the XD cluster. container servers of the XD cluster</span>","minimal","punctuation","high",False
19090,"Application will shutdown all servers with a specific name. Application will take a cluster name parameter. Generate artifact to state success or failure ",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19090,"Application will shutdown all servers with a specific name. Application will take a cluster name parameter. Generate artifact to state success or failure ",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19090,"Application will shutdown all servers with a specific name. Application will take a cluster name parameter. Generate artifact to state success or failure ",NULL,NULL,NULL,"Application will shutdown all servers with a specific name<span class='highlight-text severity-high'>. Application will take a cluster name parameter. Generate artifact to state success or failure </span>","minimal","punctuation","high",False
19091,"The deployment of nodes is sequential, we can reduce the time to deploy a cluster greatly by having these tasks execute in parallel.",NULL,"The deployment of nodes is sequential, we can reduce the time to deploy a cluster greatly by having these tasks execute in parallel.",NULL,"Add for who this story is","well_formed","no_role","high",False
19169,"would be good to have a general feel for the general performance of these two options. Redis can run on the same node as the benchmark.",NULL,"would be good to have a general feel for the general performance of these two options. Redis can run on the same node as the benchmark.",NULL,"Add for who this story is","well_formed","no_role","high",False
19169,"would be good to have a general feel for the general performance of these two options. Redis can run on the same node as the benchmark.",NULL,"would be good to have a general feel for the general performance of these two options. Redis can run on the same node as the benchmark.",NULL,"would be good to have a general feel for the general performance of these two options<span class='highlight-text severity-high'>. Redis can run on the same node as the benchmark.</span>","minimal","punctuation","high",False
19098,"Create a project that includes build and test automation for a new Angular based UI. This work is independent of calling the UI build step from gradle. A super minimal UI, just to have some basic code, is all that is required. This should use Grunt Jasmine Karma Bower YUIdoc separate story? UI Components from backbone should be available in the base project. ",NULL,"Create a project that includes build and test automation for a new Angular based UI. This work is independent of calling the UI build step from gradle. A super minimal UI, just to have some basic code, is all that is required. This should use Grunt Jasmine Karma Bower YUIdoc separate story? UI Components from backbone should be available in the base project. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19098,"Create a project that includes build and test automation for a new Angular based UI. This work is independent of calling the UI build step from gradle. A super minimal UI, just to have some basic code, is all that is required. This should use Grunt Jasmine Karma Bower YUIdoc separate story? UI Components from backbone should be available in the base project. ",NULL,"Create a project that includes build and test automation for a new Angular based UI. This work is independent of calling the UI build step from gradle. A super minimal UI, just to have some basic code, is all that is required. This should use Grunt Jasmine Karma Bower YUIdoc separate story? UI Components from backbone should be available in the base project. ",NULL,"Create a project that includes build<span class='highlight-text severity-high'> and </span>test automation for a new Angular based UI. This work is independent of calling the UI build step from gradle. A super minimal UI, just to have some basic code, is all that is required. This should use Grunt Jasmine Karma Bower YUIdoc separate story? UI Components from backbone should be available in the base project. ","atomic","conjunctions","high",False
19098,"Create a project that includes build and test automation for a new Angular based UI. This work is independent of calling the UI build step from gradle. A super minimal UI, just to have some basic code, is all that is required. This should use Grunt Jasmine Karma Bower YUIdoc separate story? UI Components from backbone should be available in the base project. ",NULL,"Create a project that includes build and test automation for a new Angular based UI. This work is independent of calling the UI build step from gradle. A super minimal UI, just to have some basic code, is all that is required. This should use Grunt Jasmine Karma Bower YUIdoc separate story? UI Components from backbone should be available in the base project. ",NULL,"Create a project that includes build and test automation for a new Angular based UI<span class='highlight-text severity-high'>. This work is independent of calling the UI build step from gradle. A super minimal UI, just to have some basic code, is all that is required. This should use Grunt Jasmine Karma Bower YUIdoc separate story? UI Components from backbone should be available in the base project. </span>","minimal","punctuation","high",False
19097,"Blog post http naleid.com blog 2013 01 24 calling gruntjs tasks from gradle seems to be the definitive reference....",NULL,"Blog post http naleid.com blog 2013 01 24 calling gruntjs tasks from gradle seems to be the definitive reference....",NULL,"Add for who this story is","well_formed","no_role","high",False
19101,"Build should be able to generate code coverage reports. After a quick tour of the intertubes it seems that JaCoCo is a well maintained project and has first class support inside gradle. http www.gradle.org docs current userguide jacoco plugin.html ",NULL,"Build should be able to generate code coverage reports. After a quick tour of the intertubes it seems that JaCoCo is a well maintained project and has first class support inside gradle. http www.gradle.org docs current userguide jacoco plugin.html ",NULL,"Add for who this story is","well_formed","no_role","high",False
19101,"Build should be able to generate code coverage reports. After a quick tour of the intertubes it seems that JaCoCo is a well maintained project and has first class support inside gradle. http www.gradle.org docs current userguide jacoco plugin.html ",NULL,"Build should be able to generate code coverage reports. After a quick tour of the intertubes it seems that JaCoCo is a well maintained project and has first class support inside gradle. http www.gradle.org docs current userguide jacoco plugin.html ",NULL,"Build should be able to generate code coverage reports. After a quick tour of the intertubes it seems that JaCoCo is a well maintained project<span class='highlight-text severity-high'> and </span>has first class support inside gradle. http www.gradle.org docs current userguide jacoco plugin.html ","atomic","conjunctions","high",False
19101,"Build should be able to generate code coverage reports. After a quick tour of the intertubes it seems that JaCoCo is a well maintained project and has first class support inside gradle. http www.gradle.org docs current userguide jacoco plugin.html ",NULL,"Build should be able to generate code coverage reports. After a quick tour of the intertubes it seems that JaCoCo is a well maintained project and has first class support inside gradle. http www.gradle.org docs current userguide jacoco plugin.html ",NULL,"Build should be able to generate code coverage reports<span class='highlight-text severity-high'>. After a quick tour of the intertubes it seems that JaCoCo is a well maintained project and has first class support inside gradle. http www.gradle.org docs current userguide jacoco plugin.html </span>","minimal","punctuation","high",False
19102,"Currently, there are test data e.g stream name not being cleaned up during teardown especially when there is a test case failure. This breaks the test suite when the same test data is used by other tests. We need to move the cleanup logic at an appropriate level so that the test data is always cleaned up irrespective of the test result.",NULL,"Currently, there are test data e.g stream name not being cleaned up during teardown especially when there is a test case failure. This breaks the test suite when the same test data is used by other tests. We need to move the cleanup logic at an appropriate level","so that the test data is always cleaned up irrespective of the test result.","Add for who this story is","well_formed","no_role","high",False
19102,"Currently, there are test data e.g stream name not being cleaned up during teardown especially when there is a test case failure. This breaks the test suite when the same test data is used by other tests. We need to move the cleanup logic at an appropriate level so that the test data is always cleaned up irrespective of the test result.",NULL,"Currently, there are test data e.g stream name not being cleaned up during teardown especially when there is a test case failure. This breaks the test suite when the same test data is used by other tests. We need to move the cleanup logic at an appropriate level","so that the test data is always cleaned up irrespective of the test result.","Currently, there are test data e<span class='highlight-text severity-high'>.g stream name not being cleaned up during teardown especially when there is a test case failure. This breaks the test suite when the same test data is used by other tests. We need to move the cleanup logic at an appropriate level so that the test data is always cleaned up irrespective of the test result.</span>","minimal","punctuation","high",False
19102,"Currently, there are test data e.g stream name not being cleaned up during teardown especially when there is a test case failure. This breaks the test suite when the same test data is used by other tests. We need to move the cleanup logic at an appropriate level so that the test data is always cleaned up irrespective of the test result.",NULL,"Currently, there are test data e.g stream name not being cleaned up during teardown especially when there is a test case failure. This breaks the test suite when the same test data is used by other tests. We need to move the cleanup logic at an appropriate level","so that the test data is always cleaned up irrespective of the test result.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19103,"The MBeanServer is referred by XD admin launcher when JMX is enabled by setting jmxEnabled option and this is defined in xd global beans.xml. The MBeans that are exposed by the modules also use the same MBeanServer define above and there is a duplicate MBeanServer definition from jmx common.xml which the MBeanExportingPlugin adds as a component to the module which doesn t seem to be needed. Also, currently the flag jmxEnabled is generic and used by adminserver launcher as well as the module MBeanExportingPlugin. If we have a separate flag to enable the JMX only for modules then, a separate definition of MBeanServer could be necessary. ",NULL,"The MBeanServer is referred by XD admin launcher when JMX is enabled by setting jmxEnabled option and this is defined in xd global beans.xml. The MBeans that are exposed by the modules also use the same MBeanServer define above and there is a duplicate MBeanServer definition from jmx common.xml which the MBeanExportingPlugin adds as a component to the module which doesn t seem to be needed. Also, currently the flag jmxEnabled is generic and used by adminserver launcher as well as the module MBeanExportingPlugin. If we have a separate flag to enable the JMX only for modules then, a separate definition of MBeanServer could be necessary. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19103,"The MBeanServer is referred by XD admin launcher when JMX is enabled by setting jmxEnabled option and this is defined in xd global beans.xml. The MBeans that are exposed by the modules also use the same MBeanServer define above and there is a duplicate MBeanServer definition from jmx common.xml which the MBeanExportingPlugin adds as a component to the module which doesn t seem to be needed. Also, currently the flag jmxEnabled is generic and used by adminserver launcher as well as the module MBeanExportingPlugin. If we have a separate flag to enable the JMX only for modules then, a separate definition of MBeanServer could be necessary. ",NULL,"The MBeanServer is referred by XD admin launcher when JMX is enabled by setting jmxEnabled option and this is defined in xd global beans.xml. The MBeans that are exposed by the modules also use the same MBeanServer define above and there is a duplicate MBeanServer definition from jmx common.xml which the MBeanExportingPlugin adds as a component to the module which doesn t seem to be needed. Also, currently the flag jmxEnabled is generic and used by adminserver launcher as well as the module MBeanExportingPlugin. If we have a separate flag to enable the JMX only for modules then, a separate definition of MBeanServer could be necessary. ",NULL,"The MBeanServer is referred by XD admin launcher when JMX is enabled by setting jmxEnabled option<span class='highlight-text severity-high'> and </span>this is defined in xd global beans.xml. The MBeans that are exposed by the modules also use the same MBeanServer define above and there is a duplicate MBeanServer definition from jmx common.xml which the MBeanExportingPlugin adds as a component to the module which doesn t seem to be needed. Also, currently the flag jmxEnabled is generic and used by adminserver launcher as well as the module MBeanExportingPlugin. If we have a separate flag to enable the JMX only for modules then, a separate definition of MBeanServer could be necessary. ","atomic","conjunctions","high",False
19103,"The MBeanServer is referred by XD admin launcher when JMX is enabled by setting jmxEnabled option and this is defined in xd global beans.xml. The MBeans that are exposed by the modules also use the same MBeanServer define above and there is a duplicate MBeanServer definition from jmx common.xml which the MBeanExportingPlugin adds as a component to the module which doesn t seem to be needed. Also, currently the flag jmxEnabled is generic and used by adminserver launcher as well as the module MBeanExportingPlugin. If we have a separate flag to enable the JMX only for modules then, a separate definition of MBeanServer could be necessary. ",NULL,"The MBeanServer is referred by XD admin launcher when JMX is enabled by setting jmxEnabled option and this is defined in xd global beans.xml. The MBeans that are exposed by the modules also use the same MBeanServer define above and there is a duplicate MBeanServer definition from jmx common.xml which the MBeanExportingPlugin adds as a component to the module which doesn t seem to be needed. Also, currently the flag jmxEnabled is generic and used by adminserver launcher as well as the module MBeanExportingPlugin. If we have a separate flag to enable the JMX only for modules then, a separate definition of MBeanServer could be necessary. ",NULL,"The MBeanServer is referred by XD admin launcher when JMX is enabled by setting jmxEnabled option and this is defined in xd global beans<span class='highlight-text severity-high'>.xml. The MBeans that are exposed by the modules also use the same MBeanServer define above and there is a duplicate MBeanServer definition from jmx common.xml which the MBeanExportingPlugin adds as a component to the module which doesn t seem to be needed. Also, currently the flag jmxEnabled is generic and used by adminserver launcher as well as the module MBeanExportingPlugin. If we have a separate flag to enable the JMX only for modules then, a separate definition of MBeanServer could be necessary. </span>","minimal","punctuation","high",False
19105,"Batch jobs should use application.yml provided connection as default. They now have their own configuration in batch jdbc.properties. This config needs to account for any changes made to application.yml settings so the data is written to the batch metadata database by default.",NULL,"Batch jobs should use application.yml provided connection as default. They now have their own configuration in batch jdbc.properties. This config needs to account for any changes made to application.yml settings","so the data is written to the batch metadata database by default.","Add for who this story is","well_formed","no_role","high",False
19105,"Batch jobs should use application.yml provided connection as default. They now have their own configuration in batch jdbc.properties. This config needs to account for any changes made to application.yml settings so the data is written to the batch metadata database by default.",NULL,"Batch jobs should use application.yml provided connection as default. They now have their own configuration in batch jdbc.properties. This config needs to account for any changes made to application.yml settings","so the data is written to the batch metadata database by default.","Batch jobs should use application<span class='highlight-text severity-high'>.yml provided connection as default. They now have their own configuration in batch jdbc.properties. This config needs to account for any changes made to application.yml settings so the data is written to the batch metadata database by default.</span>","minimal","punctuation","high",False
19105,"Batch jobs should use application.yml provided connection as default. They now have their own configuration in batch jdbc.properties. This config needs to account for any changes made to application.yml settings so the data is written to the batch metadata database by default.",NULL,"Batch jobs should use application.yml provided connection as default. They now have their own configuration in batch jdbc.properties. This config needs to account for any changes made to application.yml settings","so the data is written to the batch metadata database by default.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19170,"The way we now include various Hadoop distributions is cumbersome to maintain. Need a better way of managing and isolating these dependencies on a module level rather than container level.",NULL,"The way we now include various Hadoop distributions is cumbersome to maintain. Need a better way of managing and isolating these dependencies on a module level rather than container level.",NULL,"Add for who this story is","well_formed","no_role","high",False
19111,"batchHashtagCount and batchWordCount projects need hadoop fs ls instructions need to be updated. ",NULL,"batchHashtagCount and batchWordCount projects need hadoop fs ls instructions need to be updated. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19124,"Update to spring data hadoop 2.0.0.M5 when it is released and remove the temporary DatasetTemplateAllowingNulls in spring xd hadoop We should also review the supported hadoop distros think we should support anything that is current stable hadoop12 hadoop22 phd1 PHD 1.1 hdp13 hdp20 cdh4 ",NULL,"Update to spring data hadoop 2.0.0.M5 when it is released and remove the temporary DatasetTemplateAllowingNulls in spring xd hadoop We should also review the supported hadoop distros think we should support anything that is current stable hadoop12 hadoop22 phd1 PHD 1.1 hdp13 hdp20 cdh4 ",NULL,"Add for who this story is","well_formed","no_role","high",False
19170,"The way we now include various Hadoop distributions is cumbersome to maintain. Need a better way of managing and isolating these dependencies on a module level rather than container level.",NULL,"The way we now include various Hadoop distributions is cumbersome to maintain. Need a better way of managing and isolating these dependencies on a module level rather than container level.",NULL,"The way we now include various Hadoop distributions is cumbersome to maintain<span class='highlight-text severity-high'>. Need a better way of managing and isolating these dependencies on a module level rather than container level.</span>","minimal","punctuation","high",False
19171,"Currently, the BatchJobsController and BatchJobExecutionsController are not HATEOAS compliant and we need make them so.",NULL,"Currently, the BatchJobsController and BatchJobExecutionsController are not HATEOAS compliant and we need make them so.",NULL,"Add for who this story is","well_formed","no_role","high",False
19171,"Currently, the BatchJobsController and BatchJobExecutionsController are not HATEOAS compliant and we need make them so.",NULL,"Currently, the BatchJobsController and BatchJobExecutionsController are not HATEOAS compliant and we need make them so.",NULL,"Currently, the BatchJobsController<span class='highlight-text severity-high'> and </span>BatchJobExecutionsController are not HATEOAS compliant and we need make them so.","atomic","conjunctions","high",False
19100,"Not sure if this is best done via Sonar our sonar build plan, the nightly one, or the frequent one off master... Open question is if we want to fail a build do to code coverage levels.",NULL,"Not sure if this is best done via Sonar our sonar build plan, the nightly one, or the frequent one off master... Open question is if we want to fail a build do to code coverage levels.",NULL,"Add for who this story is","well_formed","no_role","high",False
19100,"Not sure if this is best done via Sonar our sonar build plan, the nightly one, or the frequent one off master... Open question is if we want to fail a build do to code coverage levels.",NULL,"Not sure if this is best done via Sonar our sonar build plan, the nightly one, or the frequent one off master... Open question is if we want to fail a build do to code coverage levels.",NULL,"Not sure if this is best done via Sonar our sonar build plan, the nightly one,<span class='highlight-text severity-high'> or </span>the frequent one off master... Open question is if we want to fail a build do to code coverage levels.","atomic","conjunctions","high",False
19100,"Not sure if this is best done via Sonar our sonar build plan, the nightly one, or the frequent one off master... Open question is if we want to fail a build do to code coverage levels.",NULL,"Not sure if this is best done via Sonar our sonar build plan, the nightly one, or the frequent one off master... Open question is if we want to fail a build do to code coverage levels.",NULL,"Not sure if this is best done via Sonar our sonar build plan, the nightly one, or the frequent one off master<span class='highlight-text severity-high'>... Open question is if we want to fail a build do to code coverage levels.</span>","minimal","punctuation","high",False
19096,"This functionality should be added as a command line option to the main app in the spring xd ec2 project",NULL,"This functionality should be added as a command line option to the main app in the spring xd ec2 project",NULL,"Add for who this story is","well_formed","no_role","high",False
19104,"Currently, the modules project is marked as java project to enable eclipse idea metadata files generation. But it generates a build directory with a jar that has empty MANIFEST file. This build directory also gets copied into the bundle after dist .",NULL,"Currently, the modules project is marked as java project to enable eclipse idea metadata files generation. But it generates a build directory with a jar that has empty MANIFEST file. This build directory also gets copied into the bundle after dist .",NULL,"Add for who this story is","well_formed","no_role","high",False
19104,"Currently, the modules project is marked as java project to enable eclipse idea metadata files generation. But it generates a build directory with a jar that has empty MANIFEST file. This build directory also gets copied into the bundle after dist .",NULL,"Currently, the modules project is marked as java project to enable eclipse idea metadata files generation. But it generates a build directory with a jar that has empty MANIFEST file. This build directory also gets copied into the bundle after dist .",NULL,"Currently, the modules project is marked as java project to enable eclipse idea metadata files generation<span class='highlight-text severity-high'>. But it generates a build directory with a jar that has empty MANIFEST file. This build directory also gets copied into the bundle after dist .</span>","minimal","punctuation","high",False
19114,"We should use fileName, fileExtension properties and default to xd jobname as directory",NULL,"We should use fileName, fileExtension properties and default to xd jobname as directory",NULL,"Add for who this story is","well_formed","no_role","high",False
19110,"Some boot classes we compile against have changed or been replaced.",NULL,"Some boot classes we compile against have changed or been replaced.",NULL,"Add for who this story is","well_formed","no_role","high",False
19110,"Some boot classes we compile against have changed or been replaced.",NULL,"Some boot classes we compile against have changed or been replaced.",NULL,"Some boot classes we compile against have changed<span class='highlight-text severity-high'> or </span>been replaced.","atomic","conjunctions","high",False
19115,"xd shell is using the default spring shell help command. Need to create a help command specific to XD so that it can list the hadoopDistro command line option. Note, the hadoopDistro command line option is actually processed by the xd shell bash script",NULL,"xd shell is using the default spring shell help command. Need to create a help command specific to XD","so that it can list the hadoopDistro command line option. Note, the hadoopDistro command line option is actually processed by the xd shell bash script","Add for who this story is","well_formed","no_role","high",False
19115,"xd shell is using the default spring shell help command. Need to create a help command specific to XD so that it can list the hadoopDistro command line option. Note, the hadoopDistro command line option is actually processed by the xd shell bash script",NULL,"xd shell is using the default spring shell help command. Need to create a help command specific to XD","so that it can list the hadoopDistro command line option. Note, the hadoopDistro command line option is actually processed by the xd shell bash script","xd shell is using the default spring shell help command<span class='highlight-text severity-high'>. Need to create a help command specific to XD so that it can list the hadoopDistro command line option. Note, the hadoopDistro command line option is actually processed by the xd shell bash script</span>","minimal","punctuation","high",False
19115,"xd shell is using the default spring shell help command. Need to create a help command specific to XD so that it can list the hadoopDistro command line option. Note, the hadoopDistro command line option is actually processed by the xd shell bash script",NULL,"xd shell is using the default spring shell help command. Need to create a help command specific to XD","so that it can list the hadoopDistro command line option. Note, the hadoopDistro command line option is actually processed by the xd shell bash script","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19112,"This was added in bash scripts as part of XD 1186.",NULL,"This was added in bash scripts as part of XD 1186.",NULL,"Add for who this story is","well_formed","no_role","high",False
19130,"Expected benefits are key space value as well as key value on the command line eg XD 1108 nice usage screen ",NULL,"Expected benefits are key space value as well as key value on the command line eg XD 1108 nice usage screen ",NULL,"Add for who this story is","well_formed","no_role","high",False
19127,"Labels should only be used help uniquely identify a module in a stream. The stream definition a x b x c d should throw an error since the label x is applied to two modules. Also ambiguity exists with a stream definition such as http transform transform filter hdfs since a module with the name transform will exist twice, meaning a tap on transform will result in message from both transform modules. To make the naming unique and the usage with taps, the following naming strategy is proposed demonstrated by example. http transform transform filter hdfs http, transform.0, transform.1, filter, hdfs If a use tries to tap on transform, an error would be shown saying that you could try to tap on transform.0 or transform.1 or alternatively use labels. http transform filter transform hdfs http, transform.0, filter, transform.1, hdfs http x transform filter transform hdfs http, x, filter, transform, hdfs ",NULL,"Labels should only be used help uniquely identify a module in a stream. The stream definition a x b x c d should throw an error since the label x is applied to two modules. Also ambiguity exists with a stream definition such as http transform transform filter hdfs since a module with the name transform will exist twice, meaning a tap on transform will result in message from both transform modules. To make the naming unique and the usage with taps, the following naming strategy is proposed demonstrated by example. http transform transform filter hdfs http, transform.0, transform.1, filter, hdfs If a use tries to tap on transform, an error would be shown saying that you could try to tap on transform.0 or transform.1 or alternatively use labels. http transform filter transform hdfs http, transform.0, filter, transform.1, hdfs http x transform filter transform hdfs http, x, filter, transform, hdfs ",NULL,"Add for who this story is","well_formed","no_role","high",False
19127,"Labels should only be used help uniquely identify a module in a stream. The stream definition a x b x c d should throw an error since the label x is applied to two modules. Also ambiguity exists with a stream definition such as http transform transform filter hdfs since a module with the name transform will exist twice, meaning a tap on transform will result in message from both transform modules. To make the naming unique and the usage with taps, the following naming strategy is proposed demonstrated by example. http transform transform filter hdfs http, transform.0, transform.1, filter, hdfs If a use tries to tap on transform, an error would be shown saying that you could try to tap on transform.0 or transform.1 or alternatively use labels. http transform filter transform hdfs http, transform.0, filter, transform.1, hdfs http x transform filter transform hdfs http, x, filter, transform, hdfs ",NULL,"Labels should only be used help uniquely identify a module in a stream. The stream definition a x b x c d should throw an error since the label x is applied to two modules. Also ambiguity exists with a stream definition such as http transform transform filter hdfs since a module with the name transform will exist twice, meaning a tap on transform will result in message from both transform modules. To make the naming unique and the usage with taps, the following naming strategy is proposed demonstrated by example. http transform transform filter hdfs http, transform.0, transform.1, filter, hdfs If a use tries to tap on transform, an error would be shown saying that you could try to tap on transform.0 or transform.1 or alternatively use labels. http transform filter transform hdfs http, transform.0, filter, transform.1, hdfs http x transform filter transform hdfs http, x, filter, transform, hdfs ",NULL,"Labels should only be used help uniquely identify a module in a stream. The stream definition a x b x c d should throw an error since the label x is applied to two modules. Also ambiguity exists with a stream definition such as http transform transform filter hdfs since a module with the name transform will exist twice, meaning a tap on transform will result in message from both transform modules. To make the naming unique<span class='highlight-text severity-high'> and </span>the usage with taps, the following naming strategy is proposed demonstrated by example. http transform transform filter hdfs http, transform.0, transform.1, filter, hdfs If a use tries to tap on transform, an error would be shown saying that you could try to tap on transform.0<span class='highlight-text severity-high'> or </span>transform.1 or alternatively use labels. http transform filter transform hdfs http, transform.0, filter, transform.1, hdfs http x transform filter transform hdfs http, x, filter, transform, hdfs ","atomic","conjunctions","high",False
19127,"Labels should only be used help uniquely identify a module in a stream. The stream definition a x b x c d should throw an error since the label x is applied to two modules. Also ambiguity exists with a stream definition such as http transform transform filter hdfs since a module with the name transform will exist twice, meaning a tap on transform will result in message from both transform modules. To make the naming unique and the usage with taps, the following naming strategy is proposed demonstrated by example. http transform transform filter hdfs http, transform.0, transform.1, filter, hdfs If a use tries to tap on transform, an error would be shown saying that you could try to tap on transform.0 or transform.1 or alternatively use labels. http transform filter transform hdfs http, transform.0, filter, transform.1, hdfs http x transform filter transform hdfs http, x, filter, transform, hdfs ",NULL,"Labels should only be used help uniquely identify a module in a stream. The stream definition a x b x c d should throw an error since the label x is applied to two modules. Also ambiguity exists with a stream definition such as http transform transform filter hdfs since a module with the name transform will exist twice, meaning a tap on transform will result in message from both transform modules. To make the naming unique and the usage with taps, the following naming strategy is proposed demonstrated by example. http transform transform filter hdfs http, transform.0, transform.1, filter, hdfs If a use tries to tap on transform, an error would be shown saying that you could try to tap on transform.0 or transform.1 or alternatively use labels. http transform filter transform hdfs http, transform.0, filter, transform.1, hdfs http x transform filter transform hdfs http, x, filter, transform, hdfs ",NULL,"Labels should only be used help uniquely identify a module in a stream<span class='highlight-text severity-high'>. The stream definition a x b x c d should throw an error since the label x is applied to two modules. Also ambiguity exists with a stream definition such as http transform transform filter hdfs since a module with the name transform will exist twice, meaning a tap on transform will result in message from both transform modules. To make the naming unique and the usage with taps, the following naming strategy is proposed demonstrated by example. http transform transform filter hdfs http, transform.0, transform.1, filter, hdfs If a use tries to tap on transform, an error would be shown saying that you could try to tap on transform.0 or transform.1 or alternatively use labels. http transform filter transform hdfs http, transform.0, filter, transform.1, hdfs http x transform filter transform hdfs http, x, filter, transform, hdfs </span>","minimal","punctuation","high",False
19129,"The current behavior when there are global external defaults to module options is to set them in the placeholder construct foo the default where the default is sourced from some properties file. The downside is that the module options infrastructure is unaware of them. Provide support for such defaults at least when using PojoModuleOptions in the form of Annotate a field or the setter? with Value the default maybe Annotate the pojo with PropertySource to indicate the location of the properties file maybe come up with a general naming scheme for the properties file",NULL,"The current behavior when there are global external defaults to module options is to set them in the placeholder construct foo the default where the default is sourced from some properties file. The downside is that the module options infrastructure is unaware of them. Provide support for such defaults at least when using PojoModuleOptions in the form of Annotate a field or the setter? with Value the default maybe Annotate the pojo with PropertySource to indicate the location of the properties file maybe come up with a general naming scheme for the properties file",NULL,"Add for who this story is","well_formed","no_role","high",False
19129,"The current behavior when there are global external defaults to module options is to set them in the placeholder construct foo the default where the default is sourced from some properties file. The downside is that the module options infrastructure is unaware of them. Provide support for such defaults at least when using PojoModuleOptions in the form of Annotate a field or the setter? with Value the default maybe Annotate the pojo with PropertySource to indicate the location of the properties file maybe come up with a general naming scheme for the properties file",NULL,"The current behavior when there are global external defaults to module options is to set them in the placeholder construct foo the default where the default is sourced from some properties file. The downside is that the module options infrastructure is unaware of them. Provide support for such defaults at least when using PojoModuleOptions in the form of Annotate a field or the setter? with Value the default maybe Annotate the pojo with PropertySource to indicate the location of the properties file maybe come up with a general naming scheme for the properties file",NULL,"The current behavior when there are global external defaults to module options is to set them in the placeholder construct foo the default where the default is sourced from some properties file<span class='highlight-text severity-high'>. The downside is that the module options infrastructure is unaware of them. Provide support for such defaults at least when using PojoModuleOptions in the form of Annotate a field or the setter? with Value the default maybe Annotate the pojo with PropertySource to indicate the location of the properties file maybe come up with a general naming scheme for the properties file</span>","minimal","punctuation","high",False
19128,"Update dependencies to spring data hadoop 2.0.0.M4",NULL,"Update dependencies to spring data hadoop 2.0.0.M4",NULL,"Add for who this story is","well_formed","no_role","high",False
19108,"The job names used by the tests should be unique across the tests when use the same JobRepository. ",NULL,"The job names used by the tests should be unique across the tests when use the same JobRepository. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19106,"Currently twitterstream emits native twitter json whereas twittersearch uses SI Spring Social and emits spring social Tweet types. This makes it difficult to replace twitter sources and reuse XD stream definitions. This requires coordination with SS 1.1.0 and SI 4.0 GA releases. NOTE I think it s a good idea to continue to support native twitter JSON, keep as an option for twitterstream, but the default should be Tweet types.",NULL,"Currently twitterstream emits native twitter json whereas twittersearch uses SI Spring Social and emits spring social Tweet types. This makes it difficult to replace twitter sources and reuse XD stream definitions. This requires coordination with SS 1.1.0 and SI 4.0 GA releases. NOTE I think it s a good idea to continue to support native twitter JSON, keep as an option for twitterstream, but the default should be Tweet types.",NULL,"Add for who this story is","well_formed","no_role","high",False
19106,"Currently twitterstream emits native twitter json whereas twittersearch uses SI Spring Social and emits spring social Tweet types. This makes it difficult to replace twitter sources and reuse XD stream definitions. This requires coordination with SS 1.1.0 and SI 4.0 GA releases. NOTE I think it s a good idea to continue to support native twitter JSON, keep as an option for twitterstream, but the default should be Tweet types.",NULL,"Currently twitterstream emits native twitter json whereas twittersearch uses SI Spring Social and emits spring social Tweet types. This makes it difficult to replace twitter sources and reuse XD stream definitions. This requires coordination with SS 1.1.0 and SI 4.0 GA releases. NOTE I think it s a good idea to continue to support native twitter JSON, keep as an option for twitterstream, but the default should be Tweet types.",NULL,"Currently twitterstream emits native twitter json whereas twittersearch uses SI Spring Social<span class='highlight-text severity-high'> and </span>emits spring social Tweet types. This makes it difficult to replace twitter sources and reuse XD stream definitions. This requires coordination with SS 1.1.0 and SI 4.0 GA releases. NOTE I think it s a good idea to continue to support native twitter JSON, keep as an option for twitterstream, but the default should be Tweet types.","atomic","conjunctions","high",False
19106,"Currently twitterstream emits native twitter json whereas twittersearch uses SI Spring Social and emits spring social Tweet types. This makes it difficult to replace twitter sources and reuse XD stream definitions. This requires coordination with SS 1.1.0 and SI 4.0 GA releases. NOTE I think it s a good idea to continue to support native twitter JSON, keep as an option for twitterstream, but the default should be Tweet types.",NULL,"Currently twitterstream emits native twitter json whereas twittersearch uses SI Spring Social and emits spring social Tweet types. This makes it difficult to replace twitter sources and reuse XD stream definitions. This requires coordination with SS 1.1.0 and SI 4.0 GA releases. NOTE I think it s a good idea to continue to support native twitter JSON, keep as an option for twitterstream, but the default should be Tweet types.",NULL,"Currently twitterstream emits native twitter json whereas twittersearch uses SI Spring Social and emits spring social Tweet types<span class='highlight-text severity-high'>. This makes it difficult to replace twitter sources and reuse XD stream definitions. This requires coordination with SS 1.1.0 and SI 4.0 GA releases. NOTE I think it s a good idea to continue to support native twitter JSON, keep as an option for twitterstream, but the default should be Tweet types.</span>","minimal","punctuation","high",False
19113,"The filejdbc jobs isn t included in the test scripts",NULL,"The filejdbc jobs isn t included in the test scripts",NULL,"Add for who this story is","well_formed","no_role","high",False
19109,"Using JSR303 groups, which should be derived from injected values",NULL,"Using JSR303 groups, which should be derived from injected values",NULL,"Add for who this story is","well_formed","no_role","high",False
19124,"Update to spring data hadoop 2.0.0.M5 when it is released and remove the temporary DatasetTemplateAllowingNulls in spring xd hadoop We should also review the supported hadoop distros think we should support anything that is current stable hadoop12 hadoop22 phd1 PHD 1.1 hdp13 hdp20 cdh4 ",NULL,"Update to spring data hadoop 2.0.0.M5 when it is released and remove the temporary DatasetTemplateAllowingNulls in spring xd hadoop We should also review the supported hadoop distros think we should support anything that is current stable hadoop12 hadoop22 phd1 PHD 1.1 hdp13 hdp20 cdh4 ",NULL,"Update to spring data hadoop 2.0.0.M5 when it is released<span class='highlight-text severity-high'> and </span>remove the temporary DatasetTemplateAllowingNulls in spring xd hadoop We should also review the supported hadoop distros think we should support anything that is current stable hadoop12 hadoop22 phd1 PHD 1.1 hdp13 hdp20 cdh4 ","atomic","conjunctions","high",False
19119,"see discussion at https github.com spring projects spring xd commit 2f0e80b5e337b71c9c70de510a44d2f050d10fa7",NULL,"see discussion at https github.com spring projects spring xd commit 2f0e80b5e337b71c9c70de510a44d2f050d10fa7",NULL,"Add for who this story is","well_formed","no_role","high",False
19121,"Add docs to section https github.com spring projects spring xd wiki Batch Jobs pre packaged batch jobs",NULL,"Add docs to section https github.com spring projects spring xd wiki Batch Jobs pre packaged batch jobs",NULL,"Add for who this story is","well_formed","no_role","high",False
19126,"However this is triggered depending on whether https github.com spring projects spring xd pull 477 files is merged yet or not , jmx seems to be broken because of duplicate beans mbeans names",NULL,"However this is triggered depending on whether https github.com spring projects spring xd pull 477 files is merged yet or not , jmx seems to be broken because of duplicate beans mbeans names",NULL,"Add for who this story is","well_formed","no_role","high",False
19126,"However this is triggered depending on whether https github.com spring projects spring xd pull 477 files is merged yet or not , jmx seems to be broken because of duplicate beans mbeans names",NULL,"However this is triggered depending on whether https github.com spring projects spring xd pull 477 files is merged yet or not , jmx seems to be broken because of duplicate beans mbeans names",NULL,"However this is triggered depending on whether https github.com spring projects spring xd pull 477 files is merged yet<span class='highlight-text severity-high'> or </span>not , jmx seems to be broken because of duplicate beans mbeans names","atomic","conjunctions","high",False
19125,"When using Value for providing a default value in a module options POJO, make it so that the REST API and hence the module info command advertises that 1 the expression was foo.something 2 to the best extent possible value may come from another property source , tell which config file it came from introspecting the PropertySource annotation ",NULL,"When using Value for providing a default value in a module options POJO, make it","so that the REST API and hence the module info command advertises that 1 the expression was foo.something 2 to the best extent possible value may come from another property source , tell which config file it came from introspecting the PropertySource annotation","Add for who this story is","well_formed","no_role","high",False
19125,"When using Value for providing a default value in a module options POJO, make it so that the REST API and hence the module info command advertises that 1 the expression was foo.something 2 to the best extent possible value may come from another property source , tell which config file it came from introspecting the PropertySource annotation ",NULL,"When using Value for providing a default value in a module options POJO, make it","so that the REST API and hence the module info command advertises that 1 the expression was foo.something 2 to the best extent possible value may come from another property source , tell which config file it came from introspecting the PropertySource annotation","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19117,"filejdbc throws an exception code java.lang.IllegalArgumentException Could not resolve resource location pattern mycsvdir .csv class path resource mycsvdir cannot be resolved to URL because it does not exist code This can be solved by using a file prefix Maybe just update the docs? ",NULL,"filejdbc throws an exception code java.lang.IllegalArgumentException Could not resolve resource location pattern mycsvdir .csv class path resource mycsvdir cannot be resolved to URL because it does not exist code This can be solved by using a file prefix Maybe just update the docs? ",NULL,"Add for who this story is","well_formed","no_role","high",False
19122,"The PropertyResolver needs to follow the below precedence order on PropertySources when resolving the module properties From lowest to the highest order, 0 application.yml 1 applicaiton.yml fragment 2 property placeholders 2a property placeholder under shared config directory 2b property placeholder under module source sink processor config directory 3. environment variables 4. system properties 5. command line ",NULL,"The PropertyResolver needs to follow the below precedence order on PropertySources when resolving the module properties From lowest to the highest order, 0 application.yml 1 applicaiton.yml fragment 2 property placeholders 2a property placeholder under shared config directory 2b property placeholder under module source sink processor config directory 3. environment variables 4. system properties 5. command line ",NULL,"Add for who this story is","well_formed","no_role","high",False
19122,"The PropertyResolver needs to follow the below precedence order on PropertySources when resolving the module properties From lowest to the highest order, 0 application.yml 1 applicaiton.yml fragment 2 property placeholders 2a property placeholder under shared config directory 2b property placeholder under module source sink processor config directory 3. environment variables 4. system properties 5. command line ",NULL,"The PropertyResolver needs to follow the below precedence order on PropertySources when resolving the module properties From lowest to the highest order, 0 application.yml 1 applicaiton.yml fragment 2 property placeholders 2a property placeholder under shared config directory 2b property placeholder under module source sink processor config directory 3. environment variables 4. system properties 5. command line ",NULL,"The PropertyResolver needs to follow the below precedence order on PropertySources when resolving the module properties From lowest to the highest order, 0 application<span class='highlight-text severity-high'>.yml 1 applicaiton.yml fragment 2 property placeholders 2a property placeholder under shared config directory 2b property placeholder under module source sink processor config directory 3. environment variables 4. system properties 5. command line </span>","minimal","punctuation","high",False
19118,"The batch jobs use different defaults compared to some of the sink source modules. filehdfs puts data in a data directory with files named after the stream using a .log file extension. The hdfs sink puts files in an xd streamname directory using .txt as the default file extension. filehdfs needs a more descriptive naming",NULL,"The batch jobs use different defaults compared to some of the sink source modules. filehdfs puts data in a data directory with files named after the stream using a .log file extension. The hdfs sink puts files in an xd streamname directory using .txt as the default file extension. filehdfs needs a more descriptive naming",NULL,"Add for who this story is","well_formed","no_role","high",False
19118,"The batch jobs use different defaults compared to some of the sink source modules. filehdfs puts data in a data directory with files named after the stream using a .log file extension. The hdfs sink puts files in an xd streamname directory using .txt as the default file extension. filehdfs needs a more descriptive naming",NULL,"The batch jobs use different defaults compared to some of the sink source modules. filehdfs puts data in a data directory with files named after the stream using a .log file extension. The hdfs sink puts files in an xd streamname directory using .txt as the default file extension. filehdfs needs a more descriptive naming",NULL,"The batch jobs use different defaults compared to some of the sink source modules<span class='highlight-text severity-high'>. filehdfs puts data in a data directory with files named after the stream using a .log file extension. The hdfs sink puts files in an xd streamname directory using .txt as the default file extension. filehdfs needs a more descriptive naming</span>","minimal","punctuation","high",False
19123,"NAME?",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19123,"NAME?",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19133,"We should standardize on the options between modules idleTimeout timeout rolloverSize rollover Also, need to standardize on unit used for timeout should this be s or ms? ",NULL,"We should standardize on the options between modules idleTimeout timeout rolloverSize rollover Also, need to standardize on unit used for timeout should this be s or ms? ",NULL,"Add for who this story is","well_formed","no_role","high",False
19136,"Support all available options",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19136,"Support all available options",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19140,"The TriggerSourceOptionsMetadata class should be able to use an actual Date object, thanks to Spring binding conversion. BUT, the date construct will receive a toString version of it. Make sure this works properly",NULL,"The TriggerSourceOptionsMetadata class should be able to use an actual Date object, thanks to Spring binding conversion. BUT, the date construct will receive a toString version of it. Make sure this works properly",NULL,"Add for who this story is","well_formed","no_role","high",False
19140,"The TriggerSourceOptionsMetadata class should be able to use an actual Date object, thanks to Spring binding conversion. BUT, the date construct will receive a toString version of it. Make sure this works properly",NULL,"The TriggerSourceOptionsMetadata class should be able to use an actual Date object, thanks to Spring binding conversion. BUT, the date construct will receive a toString version of it. Make sure this works properly",NULL,"The TriggerSourceOptionsMetadata class should be able to use an actual Date object, thanks to Spring binding conversion<span class='highlight-text severity-high'>. BUT, the date construct will receive a toString version of it. Make sure this works properly</span>","minimal","punctuation","high",False
19137,"Instead of using jobExecutionId and stepExecutionId as two separate options for the job execution step progress command, we can have a single option with id mentioned as jobExecutionId stepExecutionId ",NULL,"Instead of using jobExecutionId and stepExecutionId as two separate options for the job execution step progress command, we can have a single option with id mentioned as jobExecutionId stepExecutionId ",NULL,"Add for who this story is","well_formed","no_role","high",False
19137,"Instead of using jobExecutionId and stepExecutionId as two separate options for the job execution step progress command, we can have a single option with id mentioned as jobExecutionId stepExecutionId ",NULL,"Instead of using jobExecutionId and stepExecutionId as two separate options for the job execution step progress command, we can have a single option with id mentioned as jobExecutionId stepExecutionId ",NULL,"Instead of using jobExecutionId<span class='highlight-text severity-high'> and </span>stepExecutionId as two separate options for the job execution step progress command, we can have a single option with id mentioned as jobExecutionId stepExecutionId ","atomic","conjunctions","high",False
19138,"Currently local is not a supported data transport for the container. It should be an option. Note that local is not valid for control transport for a standalone container. So we need to revisit the current semantics that default the control transport to be the same as the data transport, i.e. , transport local should fail. ",NULL,"Currently local is not a supported data transport for the container. It should be an option. Note that local is not valid for control transport for a standalone container.","So we need to revisit the current semantics that default the control transport to be the same as the data transport, i.e. , transport local should fail.","Add for who this story is","well_formed","no_role","high",False
19138,"Currently local is not a supported data transport for the container. It should be an option. Note that local is not valid for control transport for a standalone container. So we need to revisit the current semantics that default the control transport to be the same as the data transport, i.e. , transport local should fail. ",NULL,"Currently local is not a supported data transport for the container. It should be an option. Note that local is not valid for control transport for a standalone container.","So we need to revisit the current semantics that default the control transport to be the same as the data transport, i.e. , transport local should fail.","Currently local is not a supported data transport for the container<span class='highlight-text severity-high'>. It should be an option. Note that local is not valid for control transport for a standalone container. So we need to revisit the current semantics that default the control transport to be the same as the data transport, i.e. , transport local should fail. </span>","minimal","punctuation","high",False
19138,"Currently local is not a supported data transport for the container. It should be an option. Note that local is not valid for control transport for a standalone container. So we need to revisit the current semantics that default the control transport to be the same as the data transport, i.e. , transport local should fail. ",NULL,"Currently local is not a supported data transport for the container. It should be an option. Note that local is not valid for control transport for a standalone container.","So we need to revisit the current semantics that default the control transport to be the same as the data transport, i.e. , transport local should fail.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19141,"The initial parsing of a stream job definitions into a list of ModuleDeploymentRequests needs to be transformed into a Physical Deployment Model that takes into account 1 module co location 2 partitioning 3 number of instances 4 node assignment an potentially other data related to the runtime properties of a module e.g. concurrency settings The an external DeploymentManifest will be used to capture this information. A deployment of a stream or job will then need to optionally provide a reference to deployment manifest.",NULL,"The initial parsing of a stream job definitions into a list of ModuleDeploymentRequests needs to be transformed into a Physical Deployment Model that takes into account 1 module co location 2 partitioning 3 number of instances 4 node assignment an potentially other data related to the runtime properties of a module e.g. concurrency settings The an external DeploymentManifest will be used to capture this information. A deployment of a stream or job will then need to optionally provide a reference to deployment manifest.",NULL,"Add for who this story is","well_formed","no_role","high",False
19141,"The initial parsing of a stream job definitions into a list of ModuleDeploymentRequests needs to be transformed into a Physical Deployment Model that takes into account 1 module co location 2 partitioning 3 number of instances 4 node assignment an potentially other data related to the runtime properties of a module e.g. concurrency settings The an external DeploymentManifest will be used to capture this information. A deployment of a stream or job will then need to optionally provide a reference to deployment manifest.",NULL,"The initial parsing of a stream job definitions into a list of ModuleDeploymentRequests needs to be transformed into a Physical Deployment Model that takes into account 1 module co location 2 partitioning 3 number of instances 4 node assignment an potentially other data related to the runtime properties of a module e.g. concurrency settings The an external DeploymentManifest will be used to capture this information. A deployment of a stream or job will then need to optionally provide a reference to deployment manifest.",NULL,"The initial parsing of a stream job definitions into a list of ModuleDeploymentRequests needs to be transformed into a Physical Deployment Model that takes into account 1 module co location 2 partitioning 3 number of instances 4 node assignment an potentially other data related to the runtime properties of a module e.g. concurrency settings The an external DeploymentManifest will be used to capture this information. A deployment of a stream<span class='highlight-text severity-high'> or </span>job will then need to optionally provide a reference to deployment manifest.","atomic","conjunctions","high",False
19141,"The initial parsing of a stream job definitions into a list of ModuleDeploymentRequests needs to be transformed into a Physical Deployment Model that takes into account 1 module co location 2 partitioning 3 number of instances 4 node assignment an potentially other data related to the runtime properties of a module e.g. concurrency settings The an external DeploymentManifest will be used to capture this information. A deployment of a stream or job will then need to optionally provide a reference to deployment manifest.",NULL,"The initial parsing of a stream job definitions into a list of ModuleDeploymentRequests needs to be transformed into a Physical Deployment Model that takes into account 1 module co location 2 partitioning 3 number of instances 4 node assignment an potentially other data related to the runtime properties of a module e.g. concurrency settings The an external DeploymentManifest will be used to capture this information. A deployment of a stream or job will then need to optionally provide a reference to deployment manifest.",NULL,"The initial parsing of a stream job definitions into a list of ModuleDeploymentRequests needs to be transformed into a Physical Deployment Model that takes into account 1 module co location 2 partitioning 3 number of instances 4 node assignment an potentially other data related to the runtime properties of a module e<span class='highlight-text severity-high'>.g. concurrency settings The an external DeploymentManifest will be used to capture this information. A deployment of a stream or job will then need to optionally provide a reference to deployment manifest.</span>","minimal","punctuation","high",False
19139,"Need to clarify if this means alternate transports within a stream, e.g source rabbit processor redis sink or specifying that a stream use an alternate transport to the one configured for the container. ",NULL,"Need to clarify if this means alternate transports within a stream, e.g source rabbit processor redis sink or specifying that a stream use an alternate transport to the one configured for the container. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19139,"Need to clarify if this means alternate transports within a stream, e.g source rabbit processor redis sink or specifying that a stream use an alternate transport to the one configured for the container. ",NULL,"Need to clarify if this means alternate transports within a stream, e.g source rabbit processor redis sink or specifying that a stream use an alternate transport to the one configured for the container. ",NULL,"Need to clarify if this means alternate transports within a stream, e.g source rabbit processor redis sink<span class='highlight-text severity-high'> or </span>specifying that a stream use an alternate transport to the one configured for the container. ","atomic","conjunctions","high",False
19172,"The automatic deployment of the job makes it harder to understand the lifecycle of the job and also does not allow for the opportunity to define any additional deployment metadata for how that job runs, e.g is it partitioned etc.",NULL,"The automatic deployment of the job makes it harder to understand the lifecycle of the job and also does not allow for the opportunity to define any additional deployment metadata for how that job runs, e.g is it partitioned etc.",NULL,"Add for who this story is","well_formed","no_role","high",False
19172,"The automatic deployment of the job makes it harder to understand the lifecycle of the job and also does not allow for the opportunity to define any additional deployment metadata for how that job runs, e.g is it partitioned etc.",NULL,"The automatic deployment of the job makes it harder to understand the lifecycle of the job and also does not allow for the opportunity to define any additional deployment metadata for how that job runs, e.g is it partitioned etc.",NULL,"The automatic deployment of the job makes it harder to understand the lifecycle of the job<span class='highlight-text severity-high'> and </span>also does not allow for the opportunity to define any additional deployment metadata for how that job runs, e.g is it partitioned etc.","atomic","conjunctions","high",False
18080,"As a developer, I want to be able to set a partitioning key for the Kafka bus even when there is a single downstream module, so that I can take advantage of the native Kafka partitioning and message ordering support.","As a developer,","I want to be able to set a partitioning key for the Kafka bus even when there is a single downstream module,","so that I can take advantage of the native Kafka partitioning and message ordering support.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19132,"The Gemfire CQ source needs some enhancements enable locator configuration consider decoupling from JSON. Currently designed to work with gemfire json server to avoid dependence on specific domain objects on the client and server side. So produces json strings from PdxInstance s stored in the cache. ",NULL,NULL,"source needs some enhancements enable locator configuration consider decoupling from JSON. Currently designed to work with gemfire json server to avoid dependence on specific domain objects on the client and server side. So produces json strings from PdxInstance s stored in the cache.","Add what you want to achieve","well_formed","no_means","high",False
19132,"The Gemfire CQ source needs some enhancements enable locator configuration consider decoupling from JSON. Currently designed to work with gemfire json server to avoid dependence on specific domain objects on the client and server side. So produces json strings from PdxInstance s stored in the cache. ",NULL,NULL,"source needs some enhancements enable locator configuration consider decoupling from JSON. Currently designed to work with gemfire json server to avoid dependence on specific domain objects on the client and server side. So produces json strings from PdxInstance s stored in the cache.","Add for who this story is","well_formed","no_role","high",False
19132,"The Gemfire CQ source needs some enhancements enable locator configuration consider decoupling from JSON. Currently designed to work with gemfire json server to avoid dependence on specific domain objects on the client and server side. So produces json strings from PdxInstance s stored in the cache. ",NULL,NULL,"source needs some enhancements enable locator configuration consider decoupling from JSON. Currently designed to work with gemfire json server to avoid dependence on specific domain objects on the client and server side. So produces json strings from PdxInstance s stored in the cache.","The Gemfire CQ source needs some enhancements enable locator configuration consider decoupling from JSON<span class='highlight-text severity-high'>. Currently designed to work with gemfire json server to avoid dependence on specific domain objects on the client and server side. So produces json strings from PdxInstance s stored in the cache. </span>","minimal","punctuation","high",False
19132,"The Gemfire CQ source needs some enhancements enable locator configuration consider decoupling from JSON. Currently designed to work with gemfire json server to avoid dependence on specific domain objects on the client and server side. So produces json strings from PdxInstance s stored in the cache. ",NULL,NULL,"source needs some enhancements enable locator configuration consider decoupling from JSON. Currently designed to work with gemfire json server to avoid dependence on specific domain objects on the client and server side. So produces json strings from PdxInstance s stored in the cache.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19173,"BatchJobsController.listForJob should be executionsForJob BatchJobsController.jobInstances should be instancesForJob The JavaDoc for the class and each method should be more descriptive about their functionality ",NULL,"BatchJobsController.listForJob should be executionsForJob BatchJobsController.jobInstances should be instancesForJob The JavaDoc for the class and each method should be more descriptive about their functionality ",NULL,"BatchJobsController.listForJob should be executionsForJob BatchJobsController.jobInstances should be instancesForJob The JavaDoc for the class<span class='highlight-text severity-high'> and </span>each method should be more descriptive about their functionality ","atomic","conjunctions","high",False
19135,"Currently, the JobPlugin extends AbstractPlugin and the AbstractPlugin has got lots of unused code like it doesn t do anything with preProcessSharedContext in there. ",NULL,"Currently, the JobPlugin extends AbstractPlugin and the AbstractPlugin has got lots of unused code like it doesn t do anything with preProcessSharedContext in there. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19135,"Currently, the JobPlugin extends AbstractPlugin and the AbstractPlugin has got lots of unused code like it doesn t do anything with preProcessSharedContext in there. ",NULL,"Currently, the JobPlugin extends AbstractPlugin and the AbstractPlugin has got lots of unused code like it doesn t do anything with preProcessSharedContext in there. ",NULL,"Currently, the JobPlugin extends AbstractPlugin<span class='highlight-text severity-high'> and </span>the AbstractPlugin has got lots of unused code like it doesn t do anything with preProcessSharedContext in there. ","atomic","conjunctions","high",False
19145,"When a ModuleOption is backed by an enum, change the currently String type representation to be the possible values ie java.lang.String String but traffic.Light Red Green Orange",NULL,"When a ModuleOption is backed by an enum, change the currently String type representation to be the possible values ie java.lang.String String but traffic.Light Red Green Orange",NULL,"Add for who this story is","well_formed","no_role","high",False
19146,"This causes intermittent test failures when testing streams with other transports since ModuleDeployer receives duplicate requests from multiple threads. Fix is to check for local transport before invoking setUpControlChannels ",NULL,"This causes intermittent test failures when testing streams with other transports since ModuleDeployer receives duplicate requests from multiple threads. Fix is to check for local transport before invoking setUpControlChannels ",NULL,"Add for who this story is","well_formed","no_role","high",False
19146,"This causes intermittent test failures when testing streams with other transports since ModuleDeployer receives duplicate requests from multiple threads. Fix is to check for local transport before invoking setUpControlChannels ",NULL,"This causes intermittent test failures when testing streams with other transports since ModuleDeployer receives duplicate requests from multiple threads. Fix is to check for local transport before invoking setUpControlChannels ",NULL,"This causes intermittent test failures when testing streams with other transports since ModuleDeployer receives duplicate requests from multiple threads<span class='highlight-text severity-high'>. Fix is to check for local transport before invoking setUpControlChannels </span>","minimal","punctuation","high",False
19142,"Base matching algorithm on the model used in http research.cs.wisc.edu htcondor Jobs Stream specify their requirements and preferences Nodes specify their requirements and preferences e.g. A job stream module requires a linux x85 64 platform and prefers the machine with the most free memory A node requires that only only can run jobs when there is 25 or more free memory and it prefers to run stream modules over job modules. The requirements and preferences are represented as SpEL expressions in a Advertisement data structure. There is a require expression and a preference expression. Matching occurs between Node Ads and Job Stream Ads so that the requirements of both Ads evaluate to true and the matching nodes are ranked according to the preference expression. The Job Streams Ads can make use of well defined attributes about the nodes, such as it s memory cpu usage.",NULL,"Base matching algorithm on the model used in http research.cs.wisc.edu htcondor Jobs Stream specify their requirements and preferences Nodes specify their requirements and preferences e.g. A job stream module requires a linux x85 64 platform and prefers the machine with the most free memory A node requires that only only can run jobs when there is 25 or more free memory and it prefers to run stream modules over job modules. The requirements and preferences are represented as SpEL expressions in a Advertisement data structure. There is a require expression and a preference expression. Matching occurs between Node Ads and Job Stream Ads","so that the requirements of both Ads evaluate to true and the matching nodes are ranked according to the preference expression. The Job Streams Ads can make use of well defined attributes about the nodes, such as it s memory cpu usage.","Add for who this story is","well_formed","no_role","high",False
19142,"Base matching algorithm on the model used in http research.cs.wisc.edu htcondor Jobs Stream specify their requirements and preferences Nodes specify their requirements and preferences e.g. A job stream module requires a linux x85 64 platform and prefers the machine with the most free memory A node requires that only only can run jobs when there is 25 or more free memory and it prefers to run stream modules over job modules. The requirements and preferences are represented as SpEL expressions in a Advertisement data structure. There is a require expression and a preference expression. Matching occurs between Node Ads and Job Stream Ads so that the requirements of both Ads evaluate to true and the matching nodes are ranked according to the preference expression. The Job Streams Ads can make use of well defined attributes about the nodes, such as it s memory cpu usage.",NULL,"Base matching algorithm on the model used in http research.cs.wisc.edu htcondor Jobs Stream specify their requirements and preferences Nodes specify their requirements and preferences e.g. A job stream module requires a linux x85 64 platform and prefers the machine with the most free memory A node requires that only only can run jobs when there is 25 or more free memory and it prefers to run stream modules over job modules. The requirements and preferences are represented as SpEL expressions in a Advertisement data structure. There is a require expression and a preference expression. Matching occurs between Node Ads and Job Stream Ads","so that the requirements of both Ads evaluate to true and the matching nodes are ranked according to the preference expression. The Job Streams Ads can make use of well defined attributes about the nodes, such as it s memory cpu usage.","Base matching algorithm on the model used in http research.cs.wisc.edu htcondor Jobs Stream specify their requirements<span class='highlight-text severity-high'> and </span>preferences Nodes specify their requirements and preferences e.g. A job stream module requires a linux x85 64 platform and prefers the machine with the most free memory A node requires that only only can run jobs when there is 25<span class='highlight-text severity-high'> or </span>more free memory and it prefers to run stream modules over job modules. The requirements and preferences are represented as SpEL expressions in a Advertisement data structure. There is a require expression and a preference expression. Matching occurs between Node Ads and Job Stream Ads so that the requirements of both Ads evaluate to true and the matching nodes are ranked according to the preference expression. The Job Streams Ads can make use of well defined attributes about the nodes, such as it s memory cpu usage.","atomic","conjunctions","high",False
19142,"Base matching algorithm on the model used in http research.cs.wisc.edu htcondor Jobs Stream specify their requirements and preferences Nodes specify their requirements and preferences e.g. A job stream module requires a linux x85 64 platform and prefers the machine with the most free memory A node requires that only only can run jobs when there is 25 or more free memory and it prefers to run stream modules over job modules. The requirements and preferences are represented as SpEL expressions in a Advertisement data structure. There is a require expression and a preference expression. Matching occurs between Node Ads and Job Stream Ads so that the requirements of both Ads evaluate to true and the matching nodes are ranked according to the preference expression. The Job Streams Ads can make use of well defined attributes about the nodes, such as it s memory cpu usage.",NULL,"Base matching algorithm on the model used in http research.cs.wisc.edu htcondor Jobs Stream specify their requirements and preferences Nodes specify their requirements and preferences e.g. A job stream module requires a linux x85 64 platform and prefers the machine with the most free memory A node requires that only only can run jobs when there is 25 or more free memory and it prefers to run stream modules over job modules. The requirements and preferences are represented as SpEL expressions in a Advertisement data structure. There is a require expression and a preference expression. Matching occurs between Node Ads and Job Stream Ads","so that the requirements of both Ads evaluate to true and the matching nodes are ranked according to the preference expression. The Job Streams Ads can make use of well defined attributes about the nodes, such as it s memory cpu usage.","Base matching algorithm on the model used in http research<span class='highlight-text severity-high'>.cs.wisc.edu htcondor Jobs Stream specify their requirements and preferences Nodes specify their requirements and preferences e.g. A job stream module requires a linux x85 64 platform and prefers the machine with the most free memory A node requires that only only can run jobs when there is 25 or more free memory and it prefers to run stream modules over job modules. The requirements and preferences are represented as SpEL expressions in a Advertisement data structure. There is a require expression and a preference expression. Matching occurs between Node Ads and Job Stream Ads so that the requirements of both Ads evaluate to true and the matching nodes are ranked according to the preference expression. The Job Streams Ads can make use of well defined attributes about the nodes, such as it s memory cpu usage.</span>","minimal","punctuation","high",False
19142,"Base matching algorithm on the model used in http research.cs.wisc.edu htcondor Jobs Stream specify their requirements and preferences Nodes specify their requirements and preferences e.g. A job stream module requires a linux x85 64 platform and prefers the machine with the most free memory A node requires that only only can run jobs when there is 25 or more free memory and it prefers to run stream modules over job modules. The requirements and preferences are represented as SpEL expressions in a Advertisement data structure. There is a require expression and a preference expression. Matching occurs between Node Ads and Job Stream Ads so that the requirements of both Ads evaluate to true and the matching nodes are ranked according to the preference expression. The Job Streams Ads can make use of well defined attributes about the nodes, such as it s memory cpu usage.",NULL,"Base matching algorithm on the model used in http research.cs.wisc.edu htcondor Jobs Stream specify their requirements and preferences Nodes specify their requirements and preferences e.g. A job stream module requires a linux x85 64 platform and prefers the machine with the most free memory A node requires that only only can run jobs when there is 25 or more free memory and it prefers to run stream modules over job modules. The requirements and preferences are represented as SpEL expressions in a Advertisement data structure. There is a require expression and a preference expression. Matching occurs between Node Ads and Job Stream Ads","so that the requirements of both Ads evaluate to true and the matching nodes are ranked according to the preference expression. The Job Streams Ads can make use of well defined attributes about the nodes, such as it s memory cpu usage.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19143,"Each node in the cluster advertises itself and the admin node listens to these ads and creates groups out of them. The deployment of jobs and stream processing can then be deployed onto a specific group and specific nodes within the group. The project https github.com spring projects spring data grid is the start of this model.",NULL,"Each node in the cluster advertises itself and the admin node listens to these ads and creates groups out of them. The deployment of jobs and stream processing can then be deployed onto a specific group and specific nodes within the group. The project https github.com spring projects spring data grid is the start of this model.",NULL,"Add for who this story is","well_formed","no_role","high",False
19143,"Each node in the cluster advertises itself and the admin node listens to these ads and creates groups out of them. The deployment of jobs and stream processing can then be deployed onto a specific group and specific nodes within the group. The project https github.com spring projects spring data grid is the start of this model.",NULL,"Each node in the cluster advertises itself and the admin node listens to these ads and creates groups out of them. The deployment of jobs and stream processing can then be deployed onto a specific group and specific nodes within the group. The project https github.com spring projects spring data grid is the start of this model.",NULL,"Each node in the cluster advertises itself<span class='highlight-text severity-high'> and </span>the admin node listens to these ads and creates groups out of them. The deployment of jobs and stream processing can then be deployed onto a specific group and specific nodes within the group. The project https github.com spring projects spring data grid is the start of this model.","atomic","conjunctions","high",False
19143,"Each node in the cluster advertises itself and the admin node listens to these ads and creates groups out of them. The deployment of jobs and stream processing can then be deployed onto a specific group and specific nodes within the group. The project https github.com spring projects spring data grid is the start of this model.",NULL,"Each node in the cluster advertises itself and the admin node listens to these ads and creates groups out of them. The deployment of jobs and stream processing can then be deployed onto a specific group and specific nodes within the group. The project https github.com spring projects spring data grid is the start of this model.",NULL,"Each node in the cluster advertises itself and the admin node listens to these ads and creates groups out of them<span class='highlight-text severity-high'>. The deployment of jobs and stream processing can then be deployed onto a specific group and specific nodes within the group. The project https github.com spring projects spring data grid is the start of this model.</span>","minimal","punctuation","high",False
19144,"For several Batch Job related JSON endpoints, we serialize too much information.",NULL,"For several Batch Job related JSON endpoints, we serialize too much information.",NULL,"Add for who this story is","well_formed","no_role","high",False
19147,"need to add a Dxd.home XD HOME to the start scripts else all files will write to the logs directory. Need to update .bat files to use for env variables instead of . Need to rename logger.config to logging.config so that boot will pick up the log config files. Admin needs to use xd admin logger configs instead of xd container logger Renamed logging file for singlenode from admin.log to singlenode.log",NULL,"need to add a Dxd.home XD HOME to the start scripts else all files will write to the logs directory. Need to update .bat files to use for env variables instead of . Need to rename logger.config to logging.config","so that boot will pick up the log config files. Admin needs to use xd admin logger configs instead of xd container logger Renamed logging file for singlenode from admin.log to singlenode.log","Add for who this story is","well_formed","no_role","high",False
19147,"need to add a Dxd.home XD HOME to the start scripts else all files will write to the logs directory. Need to update .bat files to use for env variables instead of . Need to rename logger.config to logging.config so that boot will pick up the log config files. Admin needs to use xd admin logger configs instead of xd container logger Renamed logging file for singlenode from admin.log to singlenode.log",NULL,"need to add a Dxd.home XD HOME to the start scripts else all files will write to the logs directory. Need to update .bat files to use for env variables instead of . Need to rename logger.config to logging.config","so that boot will pick up the log config files. Admin needs to use xd admin logger configs instead of xd container logger Renamed logging file for singlenode from admin.log to singlenode.log","need to add a Dxd<span class='highlight-text severity-high'>.home XD HOME to the start scripts else all files will write to the logs directory. Need to update .bat files to use for env variables instead of . Need to rename logger.config to logging.config so that boot will pick up the log config files. Admin needs to use xd admin logger configs instead of xd container logger Renamed logging file for singlenode from admin.log to singlenode.log</span>","minimal","punctuation","high",False
19147,"need to add a Dxd.home XD HOME to the start scripts else all files will write to the logs directory. Need to update .bat files to use for env variables instead of . Need to rename logger.config to logging.config so that boot will pick up the log config files. Admin needs to use xd admin logger configs instead of xd container logger Renamed logging file for singlenode from admin.log to singlenode.log",NULL,"need to add a Dxd.home XD HOME to the start scripts else all files will write to the logs directory. Need to update .bat files to use for env variables instead of . Need to rename logger.config to logging.config","so that boot will pick up the log config files. Admin needs to use xd admin logger configs instead of xd container logger Renamed logging file for singlenode from admin.log to singlenode.log","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19152,"Following merge of XD 953, provide module options using the simple approach where applicable",NULL,"Following merge of XD 953, provide module options using the simple approach where applicable",NULL,"Add for who this story is","well_formed","no_role","high",False
19151,"This is a very big story, needs some planning discussion before starting work. Should be able to be implemented as a plugin. ",NULL,"This is a very big story, needs some planning discussion before starting work. Should be able to be implemented as a plugin. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19151,"This is a very big story, needs some planning discussion before starting work. Should be able to be implemented as a plugin. ",NULL,"This is a very big story, needs some planning discussion before starting work. Should be able to be implemented as a plugin. ",NULL,"This is a very big story, needs some planning discussion before starting work<span class='highlight-text severity-high'>. Should be able to be implemented as a plugin. </span>","minimal","punctuation","high",False
19154,"Following merge of XD 1109. See discussion at https github.com spring projects spring xd commit eaf886eab3b2ef07da55575029ccabb2c8a36af9 commitcomment 4701947",NULL,"Following merge of XD 1109. See discussion at https github.com spring projects spring xd commit eaf886eab3b2ef07da55575029ccabb2c8a36af9 commitcomment 4701947",NULL,"Add for who this story is","well_formed","no_role","high",False
19154,"Following merge of XD 1109. See discussion at https github.com spring projects spring xd commit eaf886eab3b2ef07da55575029ccabb2c8a36af9 commitcomment 4701947",NULL,"Following merge of XD 1109. See discussion at https github.com spring projects spring xd commit eaf886eab3b2ef07da55575029ccabb2c8a36af9 commitcomment 4701947",NULL,"Following merge of XD 1109<span class='highlight-text severity-high'>. See discussion at https github.com spring projects spring xd commit eaf886eab3b2ef07da55575029ccabb2c8a36af9 commitcomment 4701947</span>","minimal","punctuation","high",False
19153,"Example noformat xd module delete name sink file 12 31 19,495 WARN Spring Shell client.RestTemplate 566 DELETE request for http localhost 9393 modules sink file resulted in 500 Internal Server Error ; invoking error handler Command failed org.springframework.xd.rest.client.impl.SpringXDException Cannot delete non composed module sink file noformat The WARN log is redundant with the command result and should be silenced",NULL,"Example noformat xd module delete name sink file 12 31 19,495 WARN Spring Shell client.RestTemplate 566 DELETE request for http localhost 9393 modules sink file resulted in 500 Internal Server Error ; invoking error handler Command failed org.springframework.xd.rest.client.impl.SpringXDException Cannot delete non composed module sink file noformat The WARN log is redundant with the command result and should be silenced",NULL,"Add for who this story is","well_formed","no_role","high",False
19153,"Example noformat xd module delete name sink file 12 31 19,495 WARN Spring Shell client.RestTemplate 566 DELETE request for http localhost 9393 modules sink file resulted in 500 Internal Server Error ; invoking error handler Command failed org.springframework.xd.rest.client.impl.SpringXDException Cannot delete non composed module sink file noformat The WARN log is redundant with the command result and should be silenced",NULL,"Example noformat xd module delete name sink file 12 31 19,495 WARN Spring Shell client.RestTemplate 566 DELETE request for http localhost 9393 modules sink file resulted in 500 Internal Server Error ; invoking error handler Command failed org.springframework.xd.rest.client.impl.SpringXDException Cannot delete non composed module sink file noformat The WARN log is redundant with the command result and should be silenced",NULL,"Example noformat xd module delete name sink file 12 31 19,495 WARN Spring Shell client.RestTemplate 566 DELETE request for http localhost 9393 modules sink file resulted in 500 Internal Server Error ; invoking error handler Command failed org.springframework.xd.rest.client.impl.SpringXDException Cannot delete non composed module sink file noformat The WARN log is redundant with the command result<span class='highlight-text severity-high'> and </span>should be silenced","atomic","conjunctions","high",False
19153,"Example noformat xd module delete name sink file 12 31 19,495 WARN Spring Shell client.RestTemplate 566 DELETE request for http localhost 9393 modules sink file resulted in 500 Internal Server Error ; invoking error handler Command failed org.springframework.xd.rest.client.impl.SpringXDException Cannot delete non composed module sink file noformat The WARN log is redundant with the command result and should be silenced",NULL,"Example noformat xd module delete name sink file 12 31 19,495 WARN Spring Shell client.RestTemplate 566 DELETE request for http localhost 9393 modules sink file resulted in 500 Internal Server Error ; invoking error handler Command failed org.springframework.xd.rest.client.impl.SpringXDException Cannot delete non composed module sink file noformat The WARN log is redundant with the command result and should be silenced",NULL,"Example noformat xd module delete name sink file 12 31 19,495 WARN Spring Shell client.RestTemplate 566 DELETE request for http localhost 9393 modules sink file resulted in 500 Internal Server Error <span class='highlight-text severity-high'>; invoking error handler Command failed org.springframework.xd.rest.client.impl.SpringXDException Cannot delete non composed module sink file noformat The WARN log is redundant with the command result and should be silenced</span>","minimal","punctuation","high",False
19155,"2 forces at heand here Spring binding validation itself jsr303 for eg NotNull ",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19155,"2 forces at heand here Spring binding validation itself jsr303 for eg NotNull ",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19158,"We no longer validate the hadoopDistro options in the xd scripts. Seem sthe classes doing this validation were removed for boot. We do this validation in the xd shell script",NULL,"We no longer validate the hadoopDistro options in the xd scripts. Seem sthe classes doing this validation were removed for boot. We do this validation in the xd shell script",NULL,"Add for who this story is","well_formed","no_role","high",False
19158,"We no longer validate the hadoopDistro options in the xd scripts. Seem sthe classes doing this validation were removed for boot. We do this validation in the xd shell script",NULL,"We no longer validate the hadoopDistro options in the xd scripts. Seem sthe classes doing this validation were removed for boot. We do this validation in the xd shell script",NULL,"We no longer validate the hadoopDistro options in the xd scripts<span class='highlight-text severity-high'>. Seem sthe classes doing this validation were removed for boot. We do this validation in the xd shell script</span>","minimal","punctuation","high",False
19156,"We should we centrally standardize on date time formats so that we don t create inconsistencies, and follow ISO 8601 internally. Internally we should only work with UTC or make that the default config option . Ultimately, whatever the user sees is just a formatting concern.",NULL,"We should we centrally standardize on date time formats","so that we don t create inconsistencies, and follow ISO 8601 internally. Internally we should only work with UTC or make that the default config option . Ultimately, whatever the user sees is just a formatting concern.","Add for who this story is","well_formed","no_role","high",False
19156,"We should we centrally standardize on date time formats so that we don t create inconsistencies, and follow ISO 8601 internally. Internally we should only work with UTC or make that the default config option . Ultimately, whatever the user sees is just a formatting concern.",NULL,"We should we centrally standardize on date time formats","so that we don t create inconsistencies, and follow ISO 8601 internally. Internally we should only work with UTC or make that the default config option . Ultimately, whatever the user sees is just a formatting concern.","We should we centrally standardize on date time formats so that we don t create inconsistencies, and follow ISO 8601 internally<span class='highlight-text severity-high'>. Internally we should only work with UTC or make that the default config option . Ultimately, whatever the user sees is just a formatting concern.</span>","minimal","punctuation","high",False
19156,"We should we centrally standardize on date time formats so that we don t create inconsistencies, and follow ISO 8601 internally. Internally we should only work with UTC or make that the default config option . Ultimately, whatever the user sees is just a formatting concern.",NULL,"We should we centrally standardize on date time formats","so that we don t create inconsistencies, and follow ISO 8601 internally. Internally we should only work with UTC or make that the default config option . Ultimately, whatever the user sees is just a formatting concern.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19160,"ApplicationContext ID generation is difficult in general. Cloud Foundry solves this problem for us by providing unique instance ids to all running instances of an app. I'don t suppose that helps much in the general case though, and we need something a rule of thumb that is unique and preferably deterministic, so that nodes retain their ID across process and connector restarts. In Cloud Foundry the instance id plays a vital role and will be automatically picked up by the app and applied need to look at how that plays in a context hierarchy . User can set the context id manually using spring.application.name and spring.application.index .",NULL,"ApplicationContext ID generation is difficult in general. Cloud Foundry solves this problem for us by providing unique instance ids to all running instances of an app. I'don t suppose that helps much in the general case though, and we need something a rule of thumb that is unique and preferably deterministic,","so that nodes retain their ID across process and connector restarts. In Cloud Foundry the instance id plays a vital role and will be automatically picked up by the app and applied need to look at how that plays in a context hierarchy . User can set the context id manually using spring.application.name and spring.application.index .","Add for who this story is","well_formed","no_role","high",False
19160,"ApplicationContext ID generation is difficult in general. Cloud Foundry solves this problem for us by providing unique instance ids to all running instances of an app. I'don t suppose that helps much in the general case though, and we need something a rule of thumb that is unique and preferably deterministic, so that nodes retain their ID across process and connector restarts. In Cloud Foundry the instance id plays a vital role and will be automatically picked up by the app and applied need to look at how that plays in a context hierarchy . User can set the context id manually using spring.application.name and spring.application.index .",NULL,"ApplicationContext ID generation is difficult in general. Cloud Foundry solves this problem for us by providing unique instance ids to all running instances of an app. I'don t suppose that helps much in the general case though, and we need something a rule of thumb that is unique and preferably deterministic,","so that nodes retain their ID across process and connector restarts. In Cloud Foundry the instance id plays a vital role and will be automatically picked up by the app and applied need to look at how that plays in a context hierarchy . User can set the context id manually using spring.application.name and spring.application.index .","ApplicationContext ID generation is difficult in general. Cloud Foundry solves this problem for us by providing unique instance ids to all running instances of an app. I'don t suppose that helps much in the general case though,<span class='highlight-text severity-high'> and </span>we need something a rule of thumb that is unique and preferably deterministic, so that nodes retain their ID across process and connector restarts. In Cloud Foundry the instance id plays a vital role and will be automatically picked up by the app and applied need to look at how that plays in a context hierarchy . User can set the context id manually using spring.application.name and spring.application.index .","atomic","conjunctions","high",False
19160,"ApplicationContext ID generation is difficult in general. Cloud Foundry solves this problem for us by providing unique instance ids to all running instances of an app. I'don t suppose that helps much in the general case though, and we need something a rule of thumb that is unique and preferably deterministic, so that nodes retain their ID across process and connector restarts. In Cloud Foundry the instance id plays a vital role and will be automatically picked up by the app and applied need to look at how that plays in a context hierarchy . User can set the context id manually using spring.application.name and spring.application.index .",NULL,"ApplicationContext ID generation is difficult in general. Cloud Foundry solves this problem for us by providing unique instance ids to all running instances of an app. I'don t suppose that helps much in the general case though, and we need something a rule of thumb that is unique and preferably deterministic,","so that nodes retain their ID across process and connector restarts. In Cloud Foundry the instance id plays a vital role and will be automatically picked up by the app and applied need to look at how that plays in a context hierarchy . User can set the context id manually using spring.application.name and spring.application.index .","ApplicationContext ID generation is difficult in general<span class='highlight-text severity-high'>. Cloud Foundry solves this problem for us by providing unique instance ids to all running instances of an app. I'don t suppose that helps much in the general case though, and we need something a rule of thumb that is unique and preferably deterministic, so that nodes retain their ID across process and connector restarts. In Cloud Foundry the instance id plays a vital role and will be automatically picked up by the app and applied need to look at how that plays in a context hierarchy . User can set the context id manually using spring.application.name and spring.application.index .</span>","minimal","punctuation","high",False
19160,"ApplicationContext ID generation is difficult in general. Cloud Foundry solves this problem for us by providing unique instance ids to all running instances of an app. I'don t suppose that helps much in the general case though, and we need something a rule of thumb that is unique and preferably deterministic, so that nodes retain their ID across process and connector restarts. In Cloud Foundry the instance id plays a vital role and will be automatically picked up by the app and applied need to look at how that plays in a context hierarchy . User can set the context id manually using spring.application.name and spring.application.index .",NULL,"ApplicationContext ID generation is difficult in general. Cloud Foundry solves this problem for us by providing unique instance ids to all running instances of an app. I'don t suppose that helps much in the general case though, and we need something a rule of thumb that is unique and preferably deterministic,","so that nodes retain their ID across process and connector restarts. In Cloud Foundry the instance id plays a vital role and will be automatically picked up by the app and applied need to look at how that plays in a context hierarchy . User can set the context id manually using spring.application.name and spring.application.index .","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19161,"Spring Boot support port scanning if you set server.port 0 and disable with 1 , so we could make that the default for the container node.",NULL,"Spring Boot support port scanning if you set server.port 0 and disable with 1 ,","so we could make that the default for the container node.","Add for who this story is","well_formed","no_role","high",False
19161,"Spring Boot support port scanning if you set server.port 0 and disable with 1 , so we could make that the default for the container node.",NULL,"Spring Boot support port scanning if you set server.port 0 and disable with 1 ,","so we could make that the default for the container node.","Spring Boot support port scanning if you set server.port 0<span class='highlight-text severity-high'> and </span>disable with 1 , so we could make that the default for the container node.","atomic","conjunctions","high",False
19161,"Spring Boot support port scanning if you set server.port 0 and disable with 1 , so we could make that the default for the container node.",NULL,"Spring Boot support port scanning if you set server.port 0 and disable with 1 ,","so we could make that the default for the container node.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19162,"The following keys remain after running the test suite code redis 127.0.0.1 6379 keys 1 containers 2 containers.application 9292 code ",NULL,"The following keys remain after running the test suite code redis 127.0.0.1 6379 keys 1 containers 2 containers.application 9292 code ",NULL,"Add for who this story is","well_formed","no_role","high",False
19149,"EC2 Deployer Needs to change its configuration behavior to use environment variables vs. the property files Remove ConfigureSystem class, since we will use environment variables instead Allow users to set environment variables via the xd ec2 properties. If properties are not present use those that are available in the application.yml Utilize JClouds environment variable setup features to implement this behavior.",NULL,"EC2 Deployer Needs to change its configuration behavior to use environment variables vs. the property files Remove ConfigureSystem class, since we will use environment variables instead Allow users to set environment variables via the xd ec2 properties. If properties are not present use those that are available in the application.yml Utilize JClouds environment variable setup features to implement this behavior.",NULL,"Add for who this story is","well_formed","no_role","high",False
19149,"EC2 Deployer Needs to change its configuration behavior to use environment variables vs. the property files Remove ConfigureSystem class, since we will use environment variables instead Allow users to set environment variables via the xd ec2 properties. If properties are not present use those that are available in the application.yml Utilize JClouds environment variable setup features to implement this behavior.",NULL,"EC2 Deployer Needs to change its configuration behavior to use environment variables vs. the property files Remove ConfigureSystem class, since we will use environment variables instead Allow users to set environment variables via the xd ec2 properties. If properties are not present use those that are available in the application.yml Utilize JClouds environment variable setup features to implement this behavior.",NULL,"EC2 Deployer Needs to change its configuration behavior to use environment variables vs<span class='highlight-text severity-high'>. the property files Remove ConfigureSystem class, since we will use environment variables instead Allow users to set environment variables via the xd ec2 properties. If properties are not present use those that are available in the application.yml Utilize JClouds environment variable setup features to implement this behavior.</span>","minimal","punctuation","high",False
19157,"We need to create a shell script that calls the batch DB creation sql files for the JDBC option selected. ",NULL,"We need to create a shell script that calls the batch DB creation sql files for the JDBC option selected. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19177,"Summary says it all. When starting, we now get noformat SLF4J Class path contains multiple SLF4J bindings. SLF4J Found binding in jar file Users ebottard .gradle caches artifacts 24 filestore org.slf4j slf4j log4j12 1.7.5 jar 6edffc576ce104ec769d954618764f39f0f0f10d slf4j log4j12 1.7.5.jar! org slf4j impl StaticLoggerBinder.class SLF4J Found binding in jar file Users ebottard .gradle caches artifacts 24 filestore org.slf4j slf4j log4j12 1.6.1 jar bd245d6746cdd4e6203e976e21d597a46f115802 slf4j log4j12 1.6.1.jar! org slf4j impl StaticLoggerBinder.class SLF4J See http www.slf4j.org codes.html multiple bindings for an explanation. SLF4J Actual binding is of type org.slf4j.impl.Log4jLoggerFactory noformat Most certainly introduced by 1c4817ee60ae9325f6394dcc78aa803c47818546 or 72baec92f2e7bbe860b4a9dc6c994536c1670881 ",NULL,"Summary says it all. When starting, we now get noformat SLF4J Class path contains multiple SLF4J bindings. SLF4J Found binding in jar file Users ebottard .gradle caches artifacts 24 filestore org.slf4j slf4j log4j12 1.7.5 jar 6edffc576ce104ec769d954618764f39f0f0f10d slf4j log4j12 1.7.5.jar! org slf4j impl StaticLoggerBinder.class SLF4J Found binding in jar file Users ebottard .gradle caches artifacts 24 filestore org.slf4j slf4j log4j12 1.6.1 jar bd245d6746cdd4e6203e976e21d597a46f115802 slf4j log4j12 1.6.1.jar! org slf4j impl StaticLoggerBinder.class SLF4J See http www.slf4j.org codes.html multiple bindings for an explanation. SLF4J Actual binding is of type org.slf4j.impl.Log4jLoggerFactory noformat Most certainly introduced by 1c4817ee60ae9325f6394dcc78aa803c47818546 or 72baec92f2e7bbe860b4a9dc6c994536c1670881 ",NULL,"Add for who this story is","well_formed","no_role","high",False
19177,"Summary says it all. When starting, we now get noformat SLF4J Class path contains multiple SLF4J bindings. SLF4J Found binding in jar file Users ebottard .gradle caches artifacts 24 filestore org.slf4j slf4j log4j12 1.7.5 jar 6edffc576ce104ec769d954618764f39f0f0f10d slf4j log4j12 1.7.5.jar! org slf4j impl StaticLoggerBinder.class SLF4J Found binding in jar file Users ebottard .gradle caches artifacts 24 filestore org.slf4j slf4j log4j12 1.6.1 jar bd245d6746cdd4e6203e976e21d597a46f115802 slf4j log4j12 1.6.1.jar! org slf4j impl StaticLoggerBinder.class SLF4J See http www.slf4j.org codes.html multiple bindings for an explanation. SLF4J Actual binding is of type org.slf4j.impl.Log4jLoggerFactory noformat Most certainly introduced by 1c4817ee60ae9325f6394dcc78aa803c47818546 or 72baec92f2e7bbe860b4a9dc6c994536c1670881 ",NULL,"Summary says it all. When starting, we now get noformat SLF4J Class path contains multiple SLF4J bindings. SLF4J Found binding in jar file Users ebottard .gradle caches artifacts 24 filestore org.slf4j slf4j log4j12 1.7.5 jar 6edffc576ce104ec769d954618764f39f0f0f10d slf4j log4j12 1.7.5.jar! org slf4j impl StaticLoggerBinder.class SLF4J Found binding in jar file Users ebottard .gradle caches artifacts 24 filestore org.slf4j slf4j log4j12 1.6.1 jar bd245d6746cdd4e6203e976e21d597a46f115802 slf4j log4j12 1.6.1.jar! org slf4j impl StaticLoggerBinder.class SLF4J See http www.slf4j.org codes.html multiple bindings for an explanation. SLF4J Actual binding is of type org.slf4j.impl.Log4jLoggerFactory noformat Most certainly introduced by 1c4817ee60ae9325f6394dcc78aa803c47818546 or 72baec92f2e7bbe860b4a9dc6c994536c1670881 ",NULL,"Summary says it all<span class='highlight-text severity-high'>. When starting, we now get noformat SLF4J Class path contains multiple SLF4J bindings. SLF4J Found binding in jar file Users ebottard .gradle caches artifacts 24 filestore org.slf4j slf4j log4j12 1.7.5 jar 6edffc576ce104ec769d954618764f39f0f0f10d slf4j log4j12 1.7.5.jar! org slf4j impl StaticLoggerBinder.class SLF4J Found binding in jar file Users ebottard .gradle caches artifacts 24 filestore org.slf4j slf4j log4j12 1.6.1 jar bd245d6746cdd4e6203e976e21d597a46f115802 slf4j log4j12 1.6.1.jar! org slf4j impl StaticLoggerBinder.class SLF4J See http www.slf4j.org codes.html multiple bindings for an explanation. SLF4J Actual binding is of type org.slf4j.impl.Log4jLoggerFactory noformat Most certainly introduced by 1c4817ee60ae9325f6394dcc78aa803c47818546 or 72baec92f2e7bbe860b4a9dc6c994536c1670881 </span>","minimal","punctuation","high",False
19178,"Change the default port since it conflicts with the default port for the http source",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19178,"Change the default port since it conflicts with the default port for the http source",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19200,"Add a module that can act as a tcp client as opposed to our current tcp module, which acts as a server, waiting for an incoming connection Also, the module should allow to send commands to the remote server. The typical minimal case for such a protocol is to send PING messages, but a stateful mechanism should be put in place for more complex cases.",NULL,"Add a module that can act as a tcp client as opposed to our current tcp module, which acts as a server, waiting for an incoming connection Also, the module should allow to send commands to the remote server. The typical minimal case for such a protocol is to send PING messages, but a stateful mechanism should be put in place for more complex cases.",NULL,"Add for who this story is","well_formed","no_role","high",False
19200,"Add a module that can act as a tcp client as opposed to our current tcp module, which acts as a server, waiting for an incoming connection Also, the module should allow to send commands to the remote server. The typical minimal case for such a protocol is to send PING messages, but a stateful mechanism should be put in place for more complex cases.",NULL,"Add a module that can act as a tcp client as opposed to our current tcp module, which acts as a server, waiting for an incoming connection Also, the module should allow to send commands to the remote server. The typical minimal case for such a protocol is to send PING messages, but a stateful mechanism should be put in place for more complex cases.",NULL,"Add a module that can act as a tcp client as opposed to our current tcp module, which acts as a server, waiting for an incoming connection Also, the module should allow to send commands to the remote server<span class='highlight-text severity-high'>. The typical minimal case for such a protocol is to send PING messages, but a stateful mechanism should be put in place for more complex cases.</span>","minimal","punctuation","high",False
19176,"Migrating to boot dropped the XD banner and its info. Can be restored using eg a boot Initializer and removing the default boot banner",NULL,"Migrating to boot dropped the XD banner and its info. Can be restored using eg a boot Initializer and removing the default boot banner",NULL,"Add for who this story is","well_formed","no_role","high",False
19176,"Migrating to boot dropped the XD banner and its info. Can be restored using eg a boot Initializer and removing the default boot banner",NULL,"Migrating to boot dropped the XD banner and its info. Can be restored using eg a boot Initializer and removing the default boot banner",NULL,"Migrating to boot dropped the XD banner<span class='highlight-text severity-high'> and </span>its info. Can be restored using eg a boot Initializer and removing the default boot banner","atomic","conjunctions","high",False
19176,"Migrating to boot dropped the XD banner and its info. Can be restored using eg a boot Initializer and removing the default boot banner",NULL,"Migrating to boot dropped the XD banner and its info. Can be restored using eg a boot Initializer and removing the default boot banner",NULL,"Migrating to boot dropped the XD banner and its info<span class='highlight-text severity-high'>. Can be restored using eg a boot Initializer and removing the default boot banner</span>","minimal","punctuation","high",False
19174,"Develop out a Proof of concept for review that allows users to easily create a new module to Spring XD. Key features to be enabled 1. Allow existing modules to be included as a classpath dependency 2. Consolidate shared libraries in existing modules e.g. a shared lib for all gemfire modules 3. User develops a module as a normal Java project 4. User s project can then be added to a runtime container in the same way as a dependency Nice to have 1. at development time container can run continuously and pick up changes from user s module project. 2. Java config for a module. 3. command line functionality to create project scaffolding. ",NULL,"Develop out a Proof of concept for review that allows users to easily create a new module to Spring XD. Key features to be enabled 1. Allow existing modules to be included as a classpath dependency 2. Consolidate shared libraries in existing modules e.g. a shared lib for all gemfire modules 3. User develops a module as a normal Java project 4. User s project can then be added to a runtime container in the same way as a dependency Nice to have 1. at development time container can run continuously and pick up changes from user s module project. 2. Java config for a module. 3. command line functionality to create project scaffolding. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19174,"Develop out a Proof of concept for review that allows users to easily create a new module to Spring XD. Key features to be enabled 1. Allow existing modules to be included as a classpath dependency 2. Consolidate shared libraries in existing modules e.g. a shared lib for all gemfire modules 3. User develops a module as a normal Java project 4. User s project can then be added to a runtime container in the same way as a dependency Nice to have 1. at development time container can run continuously and pick up changes from user s module project. 2. Java config for a module. 3. command line functionality to create project scaffolding. ",NULL,"Develop out a Proof of concept for review that allows users to easily create a new module to Spring XD. Key features to be enabled 1. Allow existing modules to be included as a classpath dependency 2. Consolidate shared libraries in existing modules e.g. a shared lib for all gemfire modules 3. User develops a module as a normal Java project 4. User s project can then be added to a runtime container in the same way as a dependency Nice to have 1. at development time container can run continuously and pick up changes from user s module project. 2. Java config for a module. 3. command line functionality to create project scaffolding. ",NULL,"Develop out a Proof of concept for review that allows users to easily create a new module to Spring XD. Key features to be enabled 1. Allow existing modules to be included as a classpath dependency 2. Consolidate shared libraries in existing modules e.g. a shared lib for all gemfire modules 3. User develops a module as a normal Java project 4. User s project can then be added to a runtime container in the same way as a dependency Nice to have 1. at development time container can run continuously<span class='highlight-text severity-high'> and </span>pick up changes from user s module project. 2. Java config for a module. 3. command line functionality to create project scaffolding. ","atomic","conjunctions","high",False
19210,"ModuleDeployer has many methods with very similar names that are hard to understand. Moreover, there is a substantial amount of duplicated code that should be extracted in sub methods with descriptive names. One should even consider splitting the class ",NULL,"ModuleDeployer has many methods with very similar names that are hard to understand. Moreover, there is a substantial amount of duplicated code that should be extracted in sub methods with descriptive names. One should even consider splitting the class ",NULL,"ModuleDeployer has many methods with very similar names that are hard to understand<span class='highlight-text severity-high'>. Moreover, there is a substantial amount of duplicated code that should be extracted in sub methods with descriptive names. One should even consider splitting the class </span>","minimal","punctuation","high",False
19211,"Probably the cleanest approach is to provide a properties file in the xd config directory that enables this globally, e.g., json.pretty.print true. This will require some refactoring of the ModuleTypeConversion plugin, i.e., use DI in streams.xml",NULL,"Probably the cleanest approach is to provide a properties file in the xd config directory that enables this globally, e.g., json.pretty.print true. This will require some refactoring of the ModuleTypeConversion plugin, i.e., use DI in streams.xml",NULL,"Add for who this story is","well_formed","no_role","high",False
19174,"Develop out a Proof of concept for review that allows users to easily create a new module to Spring XD. Key features to be enabled 1. Allow existing modules to be included as a classpath dependency 2. Consolidate shared libraries in existing modules e.g. a shared lib for all gemfire modules 3. User develops a module as a normal Java project 4. User s project can then be added to a runtime container in the same way as a dependency Nice to have 1. at development time container can run continuously and pick up changes from user s module project. 2. Java config for a module. 3. command line functionality to create project scaffolding. ",NULL,"Develop out a Proof of concept for review that allows users to easily create a new module to Spring XD. Key features to be enabled 1. Allow existing modules to be included as a classpath dependency 2. Consolidate shared libraries in existing modules e.g. a shared lib for all gemfire modules 3. User develops a module as a normal Java project 4. User s project can then be added to a runtime container in the same way as a dependency Nice to have 1. at development time container can run continuously and pick up changes from user s module project. 2. Java config for a module. 3. command line functionality to create project scaffolding. ",NULL,"Develop out a Proof of concept for review that allows users to easily create a new module to Spring XD<span class='highlight-text severity-high'>. Key features to be enabled 1. Allow existing modules to be included as a classpath dependency 2. Consolidate shared libraries in existing modules e.g. a shared lib for all gemfire modules 3. User develops a module as a normal Java project 4. User s project can then be added to a runtime container in the same way as a dependency Nice to have 1. at development time container can run continuously and pick up changes from user s module project. 2. Java config for a module. 3. command line functionality to create project scaffolding. </span>","minimal","punctuation","high",False
19180,"Use of dot in property name prevents the user from specifying a value in stream definition Also, defaults are repeated at .xml and .properties level",NULL,"Use of dot in property name prevents the user from specifying a value in stream definition Also, defaults are repeated at .xml and .properties level",NULL,"Add for who this story is","well_formed","no_role","high",False
19181,"Looks like we need to spend a cycle on Asciidoc as we still have the author tag issue I thought we can simply upgrade the asciidoctor gradle plugin to 0.7.0 currently 0.4.1 but that breaks the docs being generated.",NULL,"Looks like we need to spend a cycle on Asciidoc as we still have the author tag issue I thought we can simply upgrade the asciidoctor gradle plugin to 0.7.0 currently 0.4.1 but that breaks the docs being generated.",NULL,"Add for who this story is","well_formed","no_role","high",False
19185,"Job gets a empty key value pair when launching the job from the admin ui.",NULL,"Job gets a empty key value pair when launching the job from the admin ui.",NULL,"Add for who this story is","well_formed","no_role","high",False
19187,"The deployment tab does not reflect the current state of the jobs. User must hit browser refresh button to make it work.",NULL,"The deployment tab does not reflect the current state of the jobs. User must hit browser refresh button to make it work.",NULL,"Add for who this story is","well_formed","no_role","high",False
19187,"The deployment tab does not reflect the current state of the jobs. User must hit browser refresh button to make it work.",NULL,"The deployment tab does not reflect the current state of the jobs. User must hit browser refresh button to make it work.",NULL,"The deployment tab does not reflect the current state of the jobs<span class='highlight-text severity-high'>. User must hit browser refresh button to make it work.</span>","minimal","punctuation","high",False
19192,"see comments on this PR which is part of the code that needs to be refactored https github.com spring projects spring xd pull 390 ",NULL,"see comments on this PR which is part of the code that needs to be refactored https github.com spring projects spring xd pull 390 ",NULL,"Add for who this story is","well_formed","no_role","high",False
19190,"note from PR 365 which has been merged providing the initial level of support... Pending issues to be addressed in another PR? x complex case x default values for complex case, when option is not surfaced back to the module eg suffix in our canonical example plugin provided options and values descriptive defaults instead of actual defaults e.g. use stream name JSR303 Validation ",NULL,"note from PR 365 which has been merged providing the initial level of support... Pending issues to be addressed in another PR? x complex case x default values for complex case, when option is not surfaced back to the module eg suffix in our canonical example plugin provided options and values descriptive defaults instead of actual defaults e.g. use stream name JSR303 Validation ",NULL,"Add for who this story is","well_formed","no_role","high",False
19190,"note from PR 365 which has been merged providing the initial level of support... Pending issues to be addressed in another PR? x complex case x default values for complex case, when option is not surfaced back to the module eg suffix in our canonical example plugin provided options and values descriptive defaults instead of actual defaults e.g. use stream name JSR303 Validation ",NULL,"note from PR 365 which has been merged providing the initial level of support... Pending issues to be addressed in another PR? x complex case x default values for complex case, when option is not surfaced back to the module eg suffix in our canonical example plugin provided options and values descriptive defaults instead of actual defaults e.g. use stream name JSR303 Validation ",NULL,"note from PR 365 which has been merged providing the initial level of support<span class='highlight-text severity-high'>... Pending issues to be addressed in another PR? x complex case x default values for complex case, when option is not surfaced back to the module eg suffix in our canonical example plugin provided options and values descriptive defaults instead of actual defaults e.g. use stream name JSR303 Validation </span>","minimal","punctuation","high",False
19191,"this would elminate dependencies that are currently in the codebase, such as RESTModuleType and ModuleType enums ModuleOption and DetailedModuleDefinitionResource.Option ",NULL,"this would elminate dependencies that are currently in the codebase, such as RESTModuleType and ModuleType enums ModuleOption and DetailedModuleDefinitionResource.Option ",NULL,"Add for who this story is","well_formed","no_role","high",False
19191,"this would elminate dependencies that are currently in the codebase, such as RESTModuleType and ModuleType enums ModuleOption and DetailedModuleDefinitionResource.Option ",NULL,"this would elminate dependencies that are currently in the codebase, such as RESTModuleType and ModuleType enums ModuleOption and DetailedModuleDefinitionResource.Option ",NULL,"this would elminate dependencies that are currently in the codebase, such as RESTModuleType<span class='highlight-text severity-high'> and </span>ModuleType enums ModuleOption and DetailedModuleDefinitionResource.Option ","atomic","conjunctions","high",False
19201,"rename spring xd extension hdfs to something else, as it seems it is all spring data stuff and is not coupled to xd. But leave it in extensions for now rename and move spring xd hadoop inside extensions maybe to spring xd extension hadoop or hdfs make hadoop related modules depend on the latter which itself will depend on the former ",NULL,"rename spring xd extension hdfs to something else, as it seems it is all spring data stuff and is not coupled to xd. But leave it in extensions for now rename and move spring xd hadoop inside extensions maybe to spring xd extension hadoop or hdfs make hadoop related modules depend on the latter which itself will depend on the former ",NULL,"Add for who this story is","well_formed","no_role","high",False
19201,"rename spring xd extension hdfs to something else, as it seems it is all spring data stuff and is not coupled to xd. But leave it in extensions for now rename and move spring xd hadoop inside extensions maybe to spring xd extension hadoop or hdfs make hadoop related modules depend on the latter which itself will depend on the former ",NULL,"rename spring xd extension hdfs to something else, as it seems it is all spring data stuff and is not coupled to xd. But leave it in extensions for now rename and move spring xd hadoop inside extensions maybe to spring xd extension hadoop or hdfs make hadoop related modules depend on the latter which itself will depend on the former ",NULL,"rename spring xd extension hdfs to something else, as it seems it is all spring data stuff<span class='highlight-text severity-high'> and </span>is not coupled to xd. But leave it in extensions for now rename and move spring xd hadoop inside extensions maybe to spring xd extension hadoop<span class='highlight-text severity-high'> or </span>hdfs make hadoop related modules depend on the latter which itself will depend on the former ","atomic","conjunctions","high",False
19201,"rename spring xd extension hdfs to something else, as it seems it is all spring data stuff and is not coupled to xd. But leave it in extensions for now rename and move spring xd hadoop inside extensions maybe to spring xd extension hadoop or hdfs make hadoop related modules depend on the latter which itself will depend on the former ",NULL,"rename spring xd extension hdfs to something else, as it seems it is all spring data stuff and is not coupled to xd. But leave it in extensions for now rename and move spring xd hadoop inside extensions maybe to spring xd extension hadoop or hdfs make hadoop related modules depend on the latter which itself will depend on the former ",NULL,"rename spring xd extension hdfs to something else, as it seems it is all spring data stuff and is not coupled to xd<span class='highlight-text severity-high'>. But leave it in extensions for now rename and move spring xd hadoop inside extensions maybe to spring xd extension hadoop or hdfs make hadoop related modules depend on the latter which itself will depend on the former </span>","minimal","punctuation","high",False
19183,"Currently, the bootstrap.less file has all the styles that the bootstrap supports. But we should only add compile the LESS that are needed by XD UI.",NULL,"Currently, the bootstrap.less file has all the styles that the bootstrap supports. But we should only add compile the LESS that are needed by XD UI.",NULL,"Add for who this story is","well_formed","no_role","high",False
19183,"Currently, the bootstrap.less file has all the styles that the bootstrap supports. But we should only add compile the LESS that are needed by XD UI.",NULL,"Currently, the bootstrap.less file has all the styles that the bootstrap supports. But we should only add compile the LESS that are needed by XD UI.",NULL,"Currently, the bootstrap<span class='highlight-text severity-high'>.less file has all the styles that the bootstrap supports. But we should only add compile the LESS that are needed by XD UI.</span>","minimal","punctuation","high",False
19188,"e.g. see comment on PR 390 https github.com spring projects spring xd pull 390 files r7563787 In that case, it s delete in one place and destroy in another. There are other cases as well.",NULL,"e.g. see comment on PR 390 https github.com spring projects spring xd pull 390 files r7563787 In that case, it s delete in one place and destroy in another. There are other cases as well.",NULL,"Add for who this story is","well_formed","no_role","high",False
19188,"e.g. see comment on PR 390 https github.com spring projects spring xd pull 390 files r7563787 In that case, it s delete in one place and destroy in another. There are other cases as well.",NULL,"e.g. see comment on PR 390 https github.com spring projects spring xd pull 390 files r7563787 In that case, it s delete in one place and destroy in another. There are other cases as well.",NULL,"e.g. see comment on PR 390 https github.com spring projects spring xd pull 390 files r7563787 In that case, it s delete in one place<span class='highlight-text severity-high'> and </span>destroy in another. There are other cases as well.","atomic","conjunctions","high",False
19188,"e.g. see comment on PR 390 https github.com spring projects spring xd pull 390 files r7563787 In that case, it s delete in one place and destroy in another. There are other cases as well.",NULL,"e.g. see comment on PR 390 https github.com spring projects spring xd pull 390 files r7563787 In that case, it s delete in one place and destroy in another. There are other cases as well.",NULL,"e<span class='highlight-text severity-high'>.g. see comment on PR 390 https github.com spring projects spring xd pull 390 files r7563787 In that case, it s delete in one place and destroy in another. There are other cases as well.</span>","minimal","punctuation","high",False
19184,"The table background color on the Job Definition Tab is green while the others tabs have a white background. They should be consistent.",NULL,"The table background color on the Job Definition Tab is green while the others tabs have a white background. They should be consistent.",NULL,"Add for who this story is","well_formed","no_role","high",False
19184,"The table background color on the Job Definition Tab is green while the others tabs have a white background. They should be consistent.",NULL,"The table background color on the Job Definition Tab is green while the others tabs have a white background. They should be consistent.",NULL,"The table background color on the Job Definition Tab is green while the others tabs have a white background<span class='highlight-text severity-high'>. They should be consistent.</span>","minimal","punctuation","high",False
19186,"I know that the feature is not ready, but we either should post a message to the user that the feature is not available or hide the button",NULL,"I know that the feature is not ready, but we either should post a message to the user that the feature is not available or hide the button",NULL,"Add for who this story is","well_formed","no_role","high",False
19186,"I know that the feature is not ready, but we either should post a message to the user that the feature is not available or hide the button",NULL,"I know that the feature is not ready, but we either should post a message to the user that the feature is not available or hide the button",NULL,"I know that the feature is not ready, but we either should post a message to the user that the feature is not available<span class='highlight-text severity-high'> or </span>hide the button","atomic","conjunctions","high",False
19671,"As the XD system, I need to be able to execute a job or potentially a stream based on a given condition time, data existence, etc . This story is intended is for a local trigger implementation but remote triggers will also need to exist. ","As the XD system",", I need to be able to execute a job or potentially a stream based on a given condition time, data existence, etc . This story is intended is for a local trigger implementation but remote triggers will also need to exist.",NULL,"As the XD system, I need to be able to execute a job<span class='highlight-text severity-high'> or </span>potentially a stream based on a given condition time, data existence, etc . This story is intended is for a local trigger implementation but remote triggers will also need to exist. ","atomic","conjunctions","high",False
19671,"As the XD system, I need to be able to execute a job or potentially a stream based on a given condition time, data existence, etc . This story is intended is for a local trigger implementation but remote triggers will also need to exist. ","As the XD system",", I need to be able to execute a job or potentially a stream based on a given condition time, data existence, etc . This story is intended is for a local trigger implementation but remote triggers will also need to exist.",NULL,"As the XD system, I need to be able to execute a job or potentially a stream based on a given condition time, data existence, etc <span class='highlight-text severity-high'>. This story is intended is for a local trigger implementation but remote triggers will also need to exist. </span>","minimal","punctuation","high",False
19671,"As the XD system, I need to be able to execute a job or potentially a stream based on a given condition time, data existence, etc . This story is intended is for a local trigger implementation but remote triggers will also need to exist. ","As the XD system",", I need to be able to execute a job or potentially a stream based on a given condition time, data existence, etc . This story is intended is for a local trigger implementation but remote triggers will also need to exist.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19198,"As per discussion, any attempt to create something that would collide with existing should fail II Attempt to create a composed module with same name as an already existing, not composed module, with same type should fail EB, MF should work and shadow previous module from now on III Attempt to create a composed module with same name as an already existing composed module, with same type should fail EB, MF should work and shadow previous module from now on should work and retroactively change definitions of streams with that module ","As per discussion, any at","tempt to create something that would collide with existing should fail II Attempt to create a composed module with same name as an already existing, not composed module, with same type should fail EB, MF should work and shadow previous module from now on III Attempt to create a composed module with same name as an already existing composed module, with same type should fail EB, MF should work and shadow previous module from now on should work and retroactively change definitions of streams with that module",NULL,"As per discussion, any attempt to create something that would collide with existing should fail II Attempt to create a composed module with same name as an already existing, not composed module, with same type should fail EB, MF should work<span class='highlight-text severity-high'> and </span>shadow previous module from now on III Attempt to create a composed module with same name as an already existing composed module, with same type should fail EB, MF should work and shadow previous module from now on should work and retroactively change definitions of streams with that module ","atomic","conjunctions","high",False
19198,"As per discussion, any attempt to create something that would collide with existing should fail II Attempt to create a composed module with same name as an already existing, not composed module, with same type should fail EB, MF should work and shadow previous module from now on III Attempt to create a composed module with same name as an already existing composed module, with same type should fail EB, MF should work and shadow previous module from now on should work and retroactively change definitions of streams with that module ","As per discussion, any at","tempt to create something that would collide with existing should fail II Attempt to create a composed module with same name as an already existing, not composed module, with same type should fail EB, MF should work and shadow previous module from now on III Attempt to create a composed module with same name as an already existing composed module, with same type should fail EB, MF should work and shadow previous module from now on should work and retroactively change definitions of streams with that module",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19199,"This relates to XD 871 which provides a good scenario. stream create s1 definition http log deploy stream create s2 definition http log deploy The second command results in an error message that the port is in use but the stream definition is still saved. Since create deploy is a logical unit of work, it should follow transactional semantics. In other words if the deploy fails, the repository should be rolled back or a compensating destroy should be performed . Note this should not be handled the same way if create and deploy happen separately. In that case, the stream definition should remain.",NULL,"This relates to XD 871 which provides a good scenario. stream create s1 definition http log deploy stream create s2 definition http log deploy The second command results in an error message that the port is in use but the stream definition is still saved. Since create deploy is a logical unit of work, it should follow transactional semantics. In other words if the deploy fails, the repository should be rolled back or a compensating destroy should be performed . Note this should not be handled the same way if create and deploy happen separately. In that case, the stream definition should remain.",NULL,"Add for who this story is","well_formed","no_role","high",False
19211,"Probably the cleanest approach is to provide a properties file in the xd config directory that enables this globally, e.g., json.pretty.print true. This will require some refactoring of the ModuleTypeConversion plugin, i.e., use DI in streams.xml",NULL,"Probably the cleanest approach is to provide a properties file in the xd config directory that enables this globally, e.g., json.pretty.print true. This will require some refactoring of the ModuleTypeConversion plugin, i.e., use DI in streams.xml",NULL,"Probably the cleanest approach is to provide a properties file in the xd config directory that enables this globally, e<span class='highlight-text severity-high'>.g., json.pretty.print true. This will require some refactoring of the ModuleTypeConversion plugin, i.e., use DI in streams.xml</span>","minimal","punctuation","high",False
19212,"Create a reusable responsive UI layout to render the PagedResources returned from REST endpoint. As part of this, try upgrading the bootstrap to 3.0.0 and use the responsive styles offered in it.",NULL,"Create a reusable responsive UI layout to render the PagedResources returned from REST endpoint. As part of this, try upgrading the bootstrap to 3.0.0 and use the responsive styles offered in it.",NULL,"Add for who this story is","well_formed","no_role","high",False
19384,"container and event . XDContainer references and is referenced by ContainerStartedEvent and stopped . https sonar.springsource.org drilldown measures 7173?metric package cycles rids 5B 5D 7717 ",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19199,"This relates to XD 871 which provides a good scenario. stream create s1 definition http log deploy stream create s2 definition http log deploy The second command results in an error message that the port is in use but the stream definition is still saved. Since create deploy is a logical unit of work, it should follow transactional semantics. In other words if the deploy fails, the repository should be rolled back or a compensating destroy should be performed . Note this should not be handled the same way if create and deploy happen separately. In that case, the stream definition should remain.",NULL,"This relates to XD 871 which provides a good scenario. stream create s1 definition http log deploy stream create s2 definition http log deploy The second command results in an error message that the port is in use but the stream definition is still saved. Since create deploy is a logical unit of work, it should follow transactional semantics. In other words if the deploy fails, the repository should be rolled back or a compensating destroy should be performed . Note this should not be handled the same way if create and deploy happen separately. In that case, the stream definition should remain.",NULL,"This relates to XD 871 which provides a good scenario. stream create s1 definition http log deploy stream create s2 definition http log deploy The second command results in an error message that the port is in use but the stream definition is still saved. Since create deploy is a logical unit of work, it should follow transactional semantics. In other words if the deploy fails, the repository should be rolled back<span class='highlight-text severity-high'> or </span>a compensating destroy should be performed . Note this should not be handled the same way if create<span class='highlight-text severity-high'> and </span>deploy happen separately. In that case, the stream definition should remain.","atomic","conjunctions","high",False
19199,"This relates to XD 871 which provides a good scenario. stream create s1 definition http log deploy stream create s2 definition http log deploy The second command results in an error message that the port is in use but the stream definition is still saved. Since create deploy is a logical unit of work, it should follow transactional semantics. In other words if the deploy fails, the repository should be rolled back or a compensating destroy should be performed . Note this should not be handled the same way if create and deploy happen separately. In that case, the stream definition should remain.",NULL,"This relates to XD 871 which provides a good scenario. stream create s1 definition http log deploy stream create s2 definition http log deploy The second command results in an error message that the port is in use but the stream definition is still saved. Since create deploy is a logical unit of work, it should follow transactional semantics. In other words if the deploy fails, the repository should be rolled back or a compensating destroy should be performed . Note this should not be handled the same way if create and deploy happen separately. In that case, the stream definition should remain.",NULL,"This relates to XD 871 which provides a good scenario<span class='highlight-text severity-high'>. stream create s1 definition http log deploy stream create s2 definition http log deploy The second command results in an error message that the port is in use but the stream definition is still saved. Since create deploy is a logical unit of work, it should follow transactional semantics. In other words if the deploy fails, the repository should be rolled back or a compensating destroy should be performed . Note this should not be handled the same way if create and deploy happen separately. In that case, the stream definition should remain.</span>","minimal","punctuation","high",False
19195,"Make sure the sinks and jobs work against Pivotal HD 1.1",NULL,"Make sure the sinks and jobs work against Pivotal HD 1.1",NULL,"Add for who this story is","well_formed","no_role","high",False
19195,"Make sure the sinks and jobs work against Pivotal HD 1.1",NULL,"Make sure the sinks and jobs work against Pivotal HD 1.1",NULL,"Make sure the sinks<span class='highlight-text severity-high'> and </span>jobs work against Pivotal HD 1.1","atomic","conjunctions","high",False
19227,"Writing POJOs using Kite SDK ",NULL,"Writing POJOs using Kite SDK ",NULL,"Add for who this story is","well_formed","no_role","high",False
19193,"Currently the parser returns a List ModuleDeploymentRequest , and the deployer works with that list directly. We need a higher level parser result e.g. DeployableStream or probably a better name after some thought that can encapsulate that list while also enabling metadata to be added. That metadata may be helpful for composite module information as well as the module dependencies of a given stream including any composed modules within that stream .",NULL,"Currently the parser returns a List ModuleDeploymentRequest , and the deployer works with that list directly. We need a higher level parser result e.g. DeployableStream or probably a better name after some thought that can encapsulate that list while also enabling metadata to be added. That metadata may be helpful for composite module information as well as the module dependencies of a given stream including any composed modules within that stream .",NULL,"Add for who this story is","well_formed","no_role","high",False
19193,"Currently the parser returns a List ModuleDeploymentRequest , and the deployer works with that list directly. We need a higher level parser result e.g. DeployableStream or probably a better name after some thought that can encapsulate that list while also enabling metadata to be added. That metadata may be helpful for composite module information as well as the module dependencies of a given stream including any composed modules within that stream .",NULL,"Currently the parser returns a List ModuleDeploymentRequest , and the deployer works with that list directly. We need a higher level parser result e.g. DeployableStream or probably a better name after some thought that can encapsulate that list while also enabling metadata to be added. That metadata may be helpful for composite module information as well as the module dependencies of a given stream including any composed modules within that stream .",NULL,"Currently the parser returns a List ModuleDeploymentRequest ,<span class='highlight-text severity-high'> and </span>the deployer works with that list directly. We need a higher level parser result e.g. DeployableStream<span class='highlight-text severity-high'> or </span>probably a better name after some thought that can encapsulate that list while also enabling metadata to be added. That metadata may be helpful for composite module information as well as the module dependencies of a given stream including any composed modules within that stream .","atomic","conjunctions","high",False
19193,"Currently the parser returns a List ModuleDeploymentRequest , and the deployer works with that list directly. We need a higher level parser result e.g. DeployableStream or probably a better name after some thought that can encapsulate that list while also enabling metadata to be added. That metadata may be helpful for composite module information as well as the module dependencies of a given stream including any composed modules within that stream .",NULL,"Currently the parser returns a List ModuleDeploymentRequest , and the deployer works with that list directly. We need a higher level parser result e.g. DeployableStream or probably a better name after some thought that can encapsulate that list while also enabling metadata to be added. That metadata may be helpful for composite module information as well as the module dependencies of a given stream including any composed modules within that stream .",NULL,"Currently the parser returns a List ModuleDeploymentRequest , and the deployer works with that list directly<span class='highlight-text severity-high'>. We need a higher level parser result e.g. DeployableStream or probably a better name after some thought that can encapsulate that list while also enabling metadata to be added. That metadata may be helpful for composite module information as well as the module dependencies of a given stream including any composed modules within that stream .</span>","minimal","punctuation","high",False
19194,"The ModuleOptions PR currently uses FQN for types eg java.lang.String Would be nice to have support for short names for common types, both in the properties files and the annotation",NULL,"The ModuleOptions PR currently uses FQN for types eg java.lang.String Would be nice to have support for short names for common types, both in the properties files and the annotation",NULL,"Add for who this story is","well_formed","no_role","high",False
19196,"See the discussion in https github.com spring projects spring xd pull 370 There are now various superseded classes and tests which we no longer need.",NULL,"See the discussion in https github.com spring projects spring xd pull 370 There are now various superseded classes and tests which we no longer need.",NULL,"Add for who this story is","well_formed","no_role","high",False
19196,"See the discussion in https github.com spring projects spring xd pull 370 There are now various superseded classes and tests which we no longer need.",NULL,"See the discussion in https github.com spring projects spring xd pull 370 There are now various superseded classes and tests which we no longer need.",NULL,"See the discussion in https github.com spring projects spring xd pull 370 There are now various superseded classes<span class='highlight-text severity-high'> and </span>tests which we no longer need.","atomic","conjunctions","high",False
19197,"Provided it is not currently used in any stream V Attempt to destroy a composed module Should not be supported at all Should not be supported if involved in at least one stream EB?, MF! Should be supported and have no other consequences whatsoever see IV EB? Should be supported and invalidate destroy streams involving it ",NULL,"Provided it is not currently used in any stream V Attempt to destroy a composed module Should not be supported at all Should not be supported if involved in at least one stream EB?, MF! Should be supported and have no other consequences whatsoever see IV EB? Should be supported and invalidate destroy streams involving it ",NULL,"Add for who this story is","well_formed","no_role","high",False
19197,"Provided it is not currently used in any stream V Attempt to destroy a composed module Should not be supported at all Should not be supported if involved in at least one stream EB?, MF! Should be supported and have no other consequences whatsoever see IV EB? Should be supported and invalidate destroy streams involving it ",NULL,"Provided it is not currently used in any stream V Attempt to destroy a composed module Should not be supported at all Should not be supported if involved in at least one stream EB?, MF! Should be supported and have no other consequences whatsoever see IV EB? Should be supported and invalidate destroy streams involving it ",NULL,"Provided it is not currently used in any stream V Attempt to destroy a composed module Should not be supported at all Should not be supported if involved in at least one stream EB?, MF! Should be supported<span class='highlight-text severity-high'> and </span>have no other consequences whatsoever see IV EB? Should be supported and invalidate destroy streams involving it ","atomic","conjunctions","high",False
19197,"Provided it is not currently used in any stream V Attempt to destroy a composed module Should not be supported at all Should not be supported if involved in at least one stream EB?, MF! Should be supported and have no other consequences whatsoever see IV EB? Should be supported and invalidate destroy streams involving it ",NULL,"Provided it is not currently used in any stream V Attempt to destroy a composed module Should not be supported at all Should not be supported if involved in at least one stream EB?, MF! Should be supported and have no other consequences whatsoever see IV EB? Should be supported and invalidate destroy streams involving it ",NULL,"Provided it is not currently used in any stream V Attempt to destroy a composed module Should not be supported at all Should not be supported if involved in at least one stream EB<span class='highlight-text severity-high'>?, MF! Should be supported and have no other consequences whatsoever see IV EB? Should be supported and invalidate destroy streams involving it </span>","minimal","punctuation","high",False
19229,"Support for writing Sequence Files Without Compression Need a means to specify the key value to be used ",NULL,"Support for writing Sequence Files Without Compression Need a means to specify the key value to be used ",NULL,"Add for who this story is","well_formed","no_role","high",False
19205,"From XD 1023, the job status deployed undeployed is available from JobInstance Repository and a job can be deployed undeployed correctly. Implement Job deploy undeploy for a given job from JobDefinitions page and indicate status of the job definition deployed undeployed .",NULL,"From XD 1023, the job status deployed undeployed is available from JobInstance Repository and a job can be deployed undeployed correctly. Implement Job deploy undeploy for a given job from JobDefinitions page and indicate status of the job definition deployed undeployed .",NULL,"Add for who this story is","well_formed","no_role","high",False
19205,"From XD 1023, the job status deployed undeployed is available from JobInstance Repository and a job can be deployed undeployed correctly. Implement Job deploy undeploy for a given job from JobDefinitions page and indicate status of the job definition deployed undeployed .",NULL,"From XD 1023, the job status deployed undeployed is available from JobInstance Repository and a job can be deployed undeployed correctly. Implement Job deploy undeploy for a given job from JobDefinitions page and indicate status of the job definition deployed undeployed .",NULL,"From XD 1023, the job status deployed undeployed is available from JobInstance Repository<span class='highlight-text severity-high'> and </span>a job can be deployed undeployed correctly. Implement Job deploy undeploy for a given job from JobDefinitions page and indicate status of the job definition deployed undeployed .","atomic","conjunctions","high",False
19205,"From XD 1023, the job status deployed undeployed is available from JobInstance Repository and a job can be deployed undeployed correctly. Implement Job deploy undeploy for a given job from JobDefinitions page and indicate status of the job definition deployed undeployed .",NULL,"From XD 1023, the job status deployed undeployed is available from JobInstance Repository and a job can be deployed undeployed correctly. Implement Job deploy undeploy for a given job from JobDefinitions page and indicate status of the job definition deployed undeployed .",NULL,"From XD 1023, the job status deployed undeployed is available from JobInstance Repository and a job can be deployed undeployed correctly<span class='highlight-text severity-high'>. Implement Job deploy undeploy for a given job from JobDefinitions page and indicate status of the job definition deployed undeployed .</span>","minimal","punctuation","high",False
19207,"Job deployment currently goes through an overloaded version of deploy that takes 4 parameters. This prohibits job handling code from benefiting from common behavior and eg currently breaks deployAll Given that the 3 additional parameters are in fine handled as module parameters, let s push them to the job definition, known at creation time, rather than at deployment time as it does not really make sense to change those between deploys ",NULL,"Job deployment currently goes through an overloaded version of deploy that takes 4 parameters. This prohibits job handling code from benefiting from common behavior and eg currently breaks deployAll Given that the 3 additional parameters are in fine handled as module parameters, let s push them to the job definition, known at creation time, rather than at deployment time as it does not really make sense to change those between deploys ",NULL,"Add for who this story is","well_formed","no_role","high",False
19207,"Job deployment currently goes through an overloaded version of deploy that takes 4 parameters. This prohibits job handling code from benefiting from common behavior and eg currently breaks deployAll Given that the 3 additional parameters are in fine handled as module parameters, let s push them to the job definition, known at creation time, rather than at deployment time as it does not really make sense to change those between deploys ",NULL,"Job deployment currently goes through an overloaded version of deploy that takes 4 parameters. This prohibits job handling code from benefiting from common behavior and eg currently breaks deployAll Given that the 3 additional parameters are in fine handled as module parameters, let s push them to the job definition, known at creation time, rather than at deployment time as it does not really make sense to change those between deploys ",NULL,"Job deployment currently goes through an overloaded version of deploy that takes 4 parameters. This prohibits job handling code from benefiting from common behavior<span class='highlight-text severity-high'> and </span>eg currently breaks deployAll Given that the 3 additional parameters are in fine handled as module parameters, let s push them to the job definition, known at creation time, rather than at deployment time as it does not really make sense to change those between deploys ","atomic","conjunctions","high",False
19207,"Job deployment currently goes through an overloaded version of deploy that takes 4 parameters. This prohibits job handling code from benefiting from common behavior and eg currently breaks deployAll Given that the 3 additional parameters are in fine handled as module parameters, let s push them to the job definition, known at creation time, rather than at deployment time as it does not really make sense to change those between deploys ",NULL,"Job deployment currently goes through an overloaded version of deploy that takes 4 parameters. This prohibits job handling code from benefiting from common behavior and eg currently breaks deployAll Given that the 3 additional parameters are in fine handled as module parameters, let s push them to the job definition, known at creation time, rather than at deployment time as it does not really make sense to change those between deploys ",NULL,"Job deployment currently goes through an overloaded version of deploy that takes 4 parameters<span class='highlight-text severity-high'>. This prohibits job handling code from benefiting from common behavior and eg currently breaks deployAll Given that the 3 additional parameters are in fine handled as module parameters, let s push them to the job definition, known at creation time, rather than at deployment time as it does not really make sense to change those between deploys </span>","minimal","punctuation","high",False
19208,"The module list command currently has a very simplistic two column display of module name, module type . The is not very readable. Switch to a 4 column display Sources, Processors, Sinks, Jobs Additionally, mark composed module e.g. myhttp c ",NULL,"The module list command currently has a very simplistic two column display of module name, module type . The is not very readable. Switch to a 4 column display Sources, Processors, Sinks, Jobs Additionally, mark composed module e.g. myhttp c ",NULL,"Add for who this story is","well_formed","no_role","high",False
19208,"The module list command currently has a very simplistic two column display of module name, module type . The is not very readable. Switch to a 4 column display Sources, Processors, Sinks, Jobs Additionally, mark composed module e.g. myhttp c ",NULL,"The module list command currently has a very simplistic two column display of module name, module type . The is not very readable. Switch to a 4 column display Sources, Processors, Sinks, Jobs Additionally, mark composed module e.g. myhttp c ",NULL,"The module list command currently has a very simplistic two column display of module name, module type <span class='highlight-text severity-high'>. The is not very readable. Switch to a 4 column display Sources, Processors, Sinks, Jobs Additionally, mark composed module e.g. myhttp c </span>","minimal","punctuation","high",False
19209,"Create a composed module Create a stream that uses that module Try to undeploy the stream. Kaboom The dispatch is not correctly implemented in ModuleDeployer",NULL,"Create a composed module Create a stream that uses that module Try to undeploy the stream. Kaboom The dispatch is not correctly implemented in ModuleDeployer",NULL,"Add for who this story is","well_formed","no_role","high",False
19209,"Create a composed module Create a stream that uses that module Try to undeploy the stream. Kaboom The dispatch is not correctly implemented in ModuleDeployer",NULL,"Create a composed module Create a stream that uses that module Try to undeploy the stream. Kaboom The dispatch is not correctly implemented in ModuleDeployer",NULL,"Create a composed module Create a stream that uses that module Try to undeploy the stream<span class='highlight-text severity-high'>. Kaboom The dispatch is not correctly implemented in ModuleDeployer</span>","minimal","punctuation","high",False
19210,"ModuleDeployer has many methods with very similar names that are hard to understand. Moreover, there is a substantial amount of duplicated code that should be extracted in sub methods with descriptive names. One should even consider splitting the class ",NULL,"ModuleDeployer has many methods with very similar names that are hard to understand. Moreover, there is a substantial amount of duplicated code that should be extracted in sub methods with descriptive names. One should even consider splitting the class ",NULL,"Add for who this story is","well_formed","no_role","high",False
19212,"Create a reusable responsive UI layout to render the PagedResources returned from REST endpoint. As part of this, try upgrading the bootstrap to 3.0.0 and use the responsive styles offered in it.",NULL,"Create a reusable responsive UI layout to render the PagedResources returned from REST endpoint. As part of this, try upgrading the bootstrap to 3.0.0 and use the responsive styles offered in it.",NULL,"Create a reusable responsive UI layout to render the PagedResources returned from REST endpoint. As part of this, try upgrading the bootstrap to 3.0.0<span class='highlight-text severity-high'> and </span>use the responsive styles offered in it.","atomic","conjunctions","high",False
19212,"Create a reusable responsive UI layout to render the PagedResources returned from REST endpoint. As part of this, try upgrading the bootstrap to 3.0.0 and use the responsive styles offered in it.",NULL,"Create a reusable responsive UI layout to render the PagedResources returned from REST endpoint. As part of this, try upgrading the bootstrap to 3.0.0 and use the responsive styles offered in it.",NULL,"Create a reusable responsive UI layout to render the PagedResources returned from REST endpoint<span class='highlight-text severity-high'>. As part of this, try upgrading the bootstrap to 3.0.0 and use the responsive styles offered in it.</span>","minimal","punctuation","high",False
19228,"Support for using compression when writing Sequence Files Either block or record based compression. ",NULL,"Support for using compression when writing Sequence Files Either block or record based compression. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19228,"Support for using compression when writing Sequence Files Either block or record based compression. ",NULL,"Support for using compression when writing Sequence Files Either block or record based compression. ",NULL,"Support for using compression when writing Sequence Files Either block<span class='highlight-text severity-high'> or </span>record based compression. ","atomic","conjunctions","high",False
19230,"Same setup as XD 987 for ItemReader and ItemProcessor, but should write to HDFS. One can assume that the table structure has been created already external to the batch job execution.",NULL,"Same setup as XD 987 for ItemReader and ItemProcessor, but should write to HDFS. One can assume that the table structure has been created already external to the batch job execution.",NULL,"Add for who this story is","well_formed","no_role","high",False
19230,"Same setup as XD 987 for ItemReader and ItemProcessor, but should write to HDFS. One can assume that the table structure has been created already external to the batch job execution.",NULL,"Same setup as XD 987 for ItemReader and ItemProcessor, but should write to HDFS. One can assume that the table structure has been created already external to the batch job execution.",NULL,"Same setup as XD 987 for ItemReader and ItemProcessor, but should write to HDFS<span class='highlight-text severity-high'>. One can assume that the table structure has been created already external to the batch job execution.</span>","minimal","punctuation","high",False
19206,"The current XD uses an inadequate MessageRedisSerializer when extractPayload is false not currently being used . When porting this to SI, the serializer will be dropped. Suggest creation of Kryo serializer for Messages for when Redis source sinks are created.",NULL,"The current XD uses an inadequate MessageRedisSerializer when extractPayload is false not currently being used . When porting this to SI, the serializer will be dropped. Suggest creation of Kryo serializer for Messages for when Redis source sinks are created.",NULL,"Add for who this story is","well_formed","no_role","high",False
19206,"The current XD uses an inadequate MessageRedisSerializer when extractPayload is false not currently being used . When porting this to SI, the serializer will be dropped. Suggest creation of Kryo serializer for Messages for when Redis source sinks are created.",NULL,"The current XD uses an inadequate MessageRedisSerializer when extractPayload is false not currently being used . When porting this to SI, the serializer will be dropped. Suggest creation of Kryo serializer for Messages for when Redis source sinks are created.",NULL,"The current XD uses an inadequate MessageRedisSerializer when extractPayload is false not currently being used <span class='highlight-text severity-high'>. When porting this to SI, the serializer will be dropped. Suggest creation of Kryo serializer for Messages for when Redis source sinks are created.</span>","minimal","punctuation","high",False
19203,"Specify the default URL as code urlRoot location.protocol location.hostname location.port ? location.port code ",NULL,"Specify the default URL as code urlRoot location.protocol location.hostname location.port ? location.port code ",NULL,"Add for who this story is","well_formed","no_role","high",False
19203,"Specify the default URL as code urlRoot location.protocol location.hostname location.port ? location.port code ",NULL,"Specify the default URL as code urlRoot location.protocol location.hostname location.port ? location.port code ",NULL,"Specify the default URL as code urlRoot location.protocol location.hostname location.port <span class='highlight-text severity-high'>? location.port code </span>","minimal","punctuation","high",False
19204,"The current admin UI uses bootstrap 3.0.0 which provides responsive design. We need to expand our scope to support all supported user agents. This requires changes to use user agent specific layout for the UI views.",NULL,"The current admin UI uses bootstrap 3.0.0 which provides responsive design. We need to expand our scope to support all supported user agents. This requires changes to use user agent specific layout for the UI views.",NULL,"Add for who this story is","well_formed","no_role","high",False
19204,"The current admin UI uses bootstrap 3.0.0 which provides responsive design. We need to expand our scope to support all supported user agents. This requires changes to use user agent specific layout for the UI views.",NULL,"The current admin UI uses bootstrap 3.0.0 which provides responsive design. We need to expand our scope to support all supported user agents. This requires changes to use user agent specific layout for the UI views.",NULL,"The current admin UI uses bootstrap 3<span class='highlight-text severity-high'>.0.0 which provides responsive design. We need to expand our scope to support all supported user agents. This requires changes to use user agent specific layout for the UI views.</span>","minimal","punctuation","high",False
19217,"On clicking the Executions tab, user should see the list of all batch job executions. There should be options to filter job executions by few criteria such as by Job name , execution time etc., ",NULL,"On clicking the Executions tab, user should see the list of all batch job executions. There should be options to filter job executions by few criteria such as by Job name , execution time etc., ",NULL,"Add for who this story is","well_formed","no_role","high",False
19217,"On clicking the Executions tab, user should see the list of all batch job executions. There should be options to filter job executions by few criteria such as by Job name , execution time etc., ",NULL,"On clicking the Executions tab, user should see the list of all batch job executions. There should be options to filter job executions by few criteria such as by Job name , execution time etc., ",NULL,"On clicking the Executions tab, user should see the list of all batch job executions<span class='highlight-text severity-high'>. There should be options to filter job executions by few criteria such as by Job name , execution time etc., </span>","minimal","punctuation","high",False
19216,"On clicking details link on a job execution row, user should see the job details. Job detail page will show all the information about the job, where as the table listing of jobs on the Execution tab may have omitted some columns or aggregated values to convey information more easily.",NULL,"On clicking details link on a job execution row, user should see the job details. Job detail page will show all the information about the job, where as the table listing of jobs on the Execution tab may have omitted some columns or aggregated values to convey information more easily.",NULL,"Add for who this story is","well_formed","no_role","high",False
19216,"On clicking details link on a job execution row, user should see the job details. Job detail page will show all the information about the job, where as the table listing of jobs on the Execution tab may have omitted some columns or aggregated values to convey information more easily.",NULL,"On clicking details link on a job execution row, user should see the job details. Job detail page will show all the information about the job, where as the table listing of jobs on the Execution tab may have omitted some columns or aggregated values to convey information more easily.",NULL,"On clicking details link on a job execution row, user should see the job details. Job detail page will show all the information about the job, where as the table listing of jobs on the Execution tab may have omitted some columns<span class='highlight-text severity-high'> or </span>aggregated values to convey information more easily.","atomic","conjunctions","high",False
19216,"On clicking details link on a job execution row, user should see the job details. Job detail page will show all the information about the job, where as the table listing of jobs on the Execution tab may have omitted some columns or aggregated values to convey information more easily.",NULL,"On clicking details link on a job execution row, user should see the job details. Job detail page will show all the information about the job, where as the table listing of jobs on the Execution tab may have omitted some columns or aggregated values to convey information more easily.",NULL,"On clicking details link on a job execution row, user should see the job details<span class='highlight-text severity-high'>. Job detail page will show all the information about the job, where as the table listing of jobs on the Execution tab may have omitted some columns or aggregated values to convey information more easily.</span>","minimal","punctuation","high",False
19218,"From the Deployed jobs page, by clicking the Schedule button on a specific deployed job row, user should be able to schedule this job with 1 Cron trigger with cron expression as a source to job launching named channel 2 Fixed rate delay trigger as a source to job launching named channel ",NULL,"From the Deployed jobs page, by clicking the Schedule button on a specific deployed job row, user should be able to schedule this job with 1 Cron trigger with cron expression as a source to job launching named channel 2 Fixed rate delay trigger as a source to job launching named channel ",NULL,"Add for who this story is","well_formed","no_role","high",False
19219,"From the Deployed jobs page, user should be able to click on the Launch button on a specific job and specify the Job parameters as key value pairs in the text box and we will convert that into JSON string as JobParameters into JobLaunch request. ",NULL,"From the Deployed jobs page, user should be able to click on the Launch button on a specific job and specify the Job parameters as key value pairs in the text box and we will convert that into JSON string as JobParameters into JobLaunch request. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19219,"From the Deployed jobs page, user should be able to click on the Launch button on a specific job and specify the Job parameters as key value pairs in the text box and we will convert that into JSON string as JobParameters into JobLaunch request. ",NULL,"From the Deployed jobs page, user should be able to click on the Launch button on a specific job and specify the Job parameters as key value pairs in the text box and we will convert that into JSON string as JobParameters into JobLaunch request. ",NULL,"From the Deployed jobs page, user should be able to click on the Launch button on a specific job<span class='highlight-text severity-high'> and </span>specify the Job parameters as key value pairs in the text box and we will convert that into JSON string as JobParameters into JobLaunch request. ","atomic","conjunctions","high",False
19220,"On clicking a specific job name in the deployed jobs list, we need to redirect the user to show the list of all the job executions on that job. User should be able to navigate back to the deployed jobs list.",NULL,"On clicking a specific job name in the deployed jobs list, we need to redirect the user to show the list of all the job executions on that job. User should be able to navigate back to the deployed jobs list.",NULL,"Add for who this story is","well_formed","no_role","high",False
19220,"On clicking a specific job name in the deployed jobs list, we need to redirect the user to show the list of all the job executions on that job. User should be able to navigate back to the deployed jobs list.",NULL,"On clicking a specific job name in the deployed jobs list, we need to redirect the user to show the list of all the job executions on that job. User should be able to navigate back to the deployed jobs list.",NULL,"On clicking a specific job name in the deployed jobs list, we need to redirect the user to show the list of all the job executions on that job<span class='highlight-text severity-high'>. User should be able to navigate back to the deployed jobs list.</span>","minimal","punctuation","high",False
19221,"On clicking the Deployed Jobs , we can have a table view of all the deployed jobs. This is again a responsive table layout with all the job definitions with status deployed . The deployed XD job corresponds to a single batch Job Instance. This story addresses the UI layout changes to display existing JobInstance information.",NULL,"On clicking the Deployed Jobs , we can have a table view of all the deployed jobs. This is again a responsive table layout with all the job definitions with status deployed . The deployed XD job corresponds to a single batch Job Instance. This story addresses the UI layout changes to display existing JobInstance information.",NULL,"Add for who this story is","well_formed","no_role","high",False
19221,"On clicking the Deployed Jobs , we can have a table view of all the deployed jobs. This is again a responsive table layout with all the job definitions with status deployed . The deployed XD job corresponds to a single batch Job Instance. This story addresses the UI layout changes to display existing JobInstance information.",NULL,"On clicking the Deployed Jobs , we can have a table view of all the deployed jobs. This is again a responsive table layout with all the job definitions with status deployed . The deployed XD job corresponds to a single batch Job Instance. This story addresses the UI layout changes to display existing JobInstance information.",NULL,"On clicking the Deployed Jobs , we can have a table view of all the deployed jobs<span class='highlight-text severity-high'>. This is again a responsive table layout with all the job definitions with status deployed . The deployed XD job corresponds to a single batch Job Instance. This story addresses the UI layout changes to display existing JobInstance information.</span>","minimal","punctuation","high",False
19222,"Create a tab view with tabs Job Definitions , Runtime Jobs Deployed Jobs?, Job Instances Is Runtime Jobs a better name here? and Job Executions . On clicking Job Definitions tab, we can have a table view of job definitions. Since we bootstrap.js, we can have a responsive table layout to list all the available job definitions. At the REST layer, jobs provides the list of job definitions. We can expand the JobsController list s QueryOptions to add more criteria especially to list JobDefinition s status Deployed Undeployed . Also, this is the UI implementation for the shell command job list ",NULL,"Create a tab view with tabs Job Definitions , Runtime Jobs Deployed Jobs?, Job Instances Is Runtime Jobs a better name here? and Job Executions . On clicking Job Definitions tab, we can have a table view of job definitions. Since we bootstrap.js, we can have a responsive table layout to list all the available job definitions. At the REST layer, jobs provides the list of job definitions. We can expand the JobsController list s QueryOptions to add more criteria especially to list JobDefinition s status Deployed Undeployed . Also, this is the UI implementation for the shell command job list ",NULL,"Add for who this story is","well_formed","no_role","high",False
19222,"Create a tab view with tabs Job Definitions , Runtime Jobs Deployed Jobs?, Job Instances Is Runtime Jobs a better name here? and Job Executions . On clicking Job Definitions tab, we can have a table view of job definitions. Since we bootstrap.js, we can have a responsive table layout to list all the available job definitions. At the REST layer, jobs provides the list of job definitions. We can expand the JobsController list s QueryOptions to add more criteria especially to list JobDefinition s status Deployed Undeployed . Also, this is the UI implementation for the shell command job list ",NULL,"Create a tab view with tabs Job Definitions , Runtime Jobs Deployed Jobs?, Job Instances Is Runtime Jobs a better name here? and Job Executions . On clicking Job Definitions tab, we can have a table view of job definitions. Since we bootstrap.js, we can have a responsive table layout to list all the available job definitions. At the REST layer, jobs provides the list of job definitions. We can expand the JobsController list s QueryOptions to add more criteria especially to list JobDefinition s status Deployed Undeployed . Also, this is the UI implementation for the shell command job list ",NULL,"Create a tab view with tabs Job Definitions , Runtime Jobs Deployed Jobs<span class='highlight-text severity-high'>?, Job Instances Is Runtime Jobs a better name here? and Job Executions . On clicking Job Definitions tab, we can have a table view of job definitions. Since we bootstrap.js, we can have a responsive table layout to list all the available job definitions. At the REST layer, jobs provides the list of job definitions. We can expand the JobsController list s QueryOptions to add more criteria especially to list JobDefinition s status Deployed Undeployed . Also, this is the UI implementation for the shell command job list </span>","minimal","punctuation","high",False
19225,"Support for partitioning on a field, e.g. date.",NULL,"Support for partitioning on a field, e.g. date.",NULL,"Add for who this story is","well_formed","no_role","high",False
19225,"Support for partitioning on a field, e.g. date.",NULL,"Support for partitioning on a field, e.g. date.",NULL,"Support for partitioning on a field, e<span class='highlight-text severity-high'>.g. date.</span>","minimal","punctuation","high",False
19223,"Related to https jira.springsource.org browse BATCH 2109 DistributedJobService listJobExecutionsForJob overrides SimpleJobService listJobExecutionsForJob and does not include the StepExecution s. This is due to serializion issues with Jackson. In order to fix this, we need to add a Jackson MixIn. ",NULL,"Related to https jira.springsource.org browse BATCH 2109 DistributedJobService listJobExecutionsForJob overrides SimpleJobService listJobExecutionsForJob and does not include the StepExecution s. This is due to serializion issues with Jackson.","In order to fix this, we need to add a Jackson MixIn.","Add for who this story is","well_formed","no_role","high",False
19223,"Related to https jira.springsource.org browse BATCH 2109 DistributedJobService listJobExecutionsForJob overrides SimpleJobService listJobExecutionsForJob and does not include the StepExecution s. This is due to serializion issues with Jackson. In order to fix this, we need to add a Jackson MixIn. ",NULL,"Related to https jira.springsource.org browse BATCH 2109 DistributedJobService listJobExecutionsForJob overrides SimpleJobService listJobExecutionsForJob and does not include the StepExecution s. This is due to serializion issues with Jackson.","In order to fix this, we need to add a Jackson MixIn.","Related to https jira.springsource.org browse BATCH 2109 DistributedJobService listJobExecutionsForJob overrides SimpleJobService listJobExecutionsForJob<span class='highlight-text severity-high'> and </span>does not include the StepExecution s. This is due to serializion issues with Jackson. In order to fix this, we need to add a Jackson MixIn. ","atomic","conjunctions","high",False
19223,"Related to https jira.springsource.org browse BATCH 2109 DistributedJobService listJobExecutionsForJob overrides SimpleJobService listJobExecutionsForJob and does not include the StepExecution s. This is due to serializion issues with Jackson. In order to fix this, we need to add a Jackson MixIn. ",NULL,"Related to https jira.springsource.org browse BATCH 2109 DistributedJobService listJobExecutionsForJob overrides SimpleJobService listJobExecutionsForJob and does not include the StepExecution s. This is due to serializion issues with Jackson.","In order to fix this, we need to add a Jackson MixIn.","Related to https jira<span class='highlight-text severity-high'>.springsource.org browse BATCH 2109 DistributedJobService listJobExecutionsForJob overrides SimpleJobService listJobExecutionsForJob and does not include the StepExecution s. This is due to serializion issues with Jackson. In order to fix this, we need to add a Jackson MixIn. </span>","minimal","punctuation","high",False
19223,"Related to https jira.springsource.org browse BATCH 2109 DistributedJobService listJobExecutionsForJob overrides SimpleJobService listJobExecutionsForJob and does not include the StepExecution s. This is due to serializion issues with Jackson. In order to fix this, we need to add a Jackson MixIn. ",NULL,"Related to https jira.springsource.org browse BATCH 2109 DistributedJobService listJobExecutionsForJob overrides SimpleJobService listJobExecutionsForJob and does not include the StepExecution s. This is due to serializion issues with Jackson.","In order to fix this, we need to add a Jackson MixIn.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19224,"Need some sample usage, docs for https github.com spring projects spring xd tree master modules source gemfire ",NULL,"Need some sample usage, docs for https github.com spring projects spring xd tree master modules source gemfire ",NULL,"Add for who this story is","well_formed","no_role","high",False
19226,"Some tests esp. ModuleClasspathTests.testModuleWithClasspathAfterServerStarted seem to fail because of a race condition. Add a Hamcrest matcher that knows how to read the content of a FileSink Source and refactor those to read like e.g. assertThat fileSink, eventually hasContent foo ",NULL,NULL,NULL,"Some tests esp<span class='highlight-text severity-high'>. ModuleClasspathTests.testModuleWithClasspathAfterServerStarted seem to fail because of a race condition. Add a Hamcrest matcher that knows how to read the content of a FileSink Source and refactor those to read like e.g. assertThat fileSink, eventually hasContent foo </span>","minimal","punctuation","high",False
19214,"Currently, the jobs definition list REST endpoint doesn t include deployed undeployed status on a given job.",NULL,"Currently, the jobs definition list REST endpoint doesn t include deployed undeployed status on a given job.",NULL,"Add for who this story is","well_formed","no_role","high",False
19215,"On clicking the job detail page, we should display all the step executions associated with the specific job execution in a table view.",NULL,"On clicking the job detail page, we should display all the step executions associated with the specific job execution in a table view.",NULL,"Add for who this story is","well_formed","no_role","high",False
19233,"Create a sample batch job for inclusion in the distribution that will perform the following tasks. ItemReader Read a from a directory with multiple files configurable Support for CSV assume first line has header values Convert to tuple data structure ItemProcessor Provide groovy based no op ItemProcessor. configurable ItemWriter Write to JDBC. Provide Assume there is an DB instance running somewhere, specify connection info configurable The sample job should be documented ",NULL,"Create a sample batch job for inclusion in the distribution that will perform the following tasks. ItemReader Read a from a directory with multiple files configurable Support for CSV assume first line has header values Convert to tuple data structure ItemProcessor Provide groovy based no op ItemProcessor. configurable ItemWriter Write to JDBC. Provide Assume there is an DB instance running somewhere, specify connection info configurable The sample job should be documented ",NULL,"Add for who this story is","well_formed","no_role","high",False
19233,"Create a sample batch job for inclusion in the distribution that will perform the following tasks. ItemReader Read a from a directory with multiple files configurable Support for CSV assume first line has header values Convert to tuple data structure ItemProcessor Provide groovy based no op ItemProcessor. configurable ItemWriter Write to JDBC. Provide Assume there is an DB instance running somewhere, specify connection info configurable The sample job should be documented ",NULL,"Create a sample batch job for inclusion in the distribution that will perform the following tasks. ItemReader Read a from a directory with multiple files configurable Support for CSV assume first line has header values Convert to tuple data structure ItemProcessor Provide groovy based no op ItemProcessor. configurable ItemWriter Write to JDBC. Provide Assume there is an DB instance running somewhere, specify connection info configurable The sample job should be documented ",NULL,"Create a sample batch job for inclusion in the distribution that will perform the following tasks<span class='highlight-text severity-high'>. ItemReader Read a from a directory with multiple files configurable Support for CSV assume first line has header values Convert to tuple data structure ItemProcessor Provide groovy based no op ItemProcessor. configurable ItemWriter Write to JDBC. Provide Assume there is an DB instance running somewhere, specify connection info configurable The sample job should be documented </span>","minimal","punctuation","high",False
19234,"We used to have a shared guava 11.0.2.jar dependency in the lib dir. That s no longer there so hadoop distros that require this now fail at least any hadoop 2.0.x based ones We should also upgrade to current Hadoop versions Hadoop 2.2 stable ",NULL,"We used to have a shared guava 11.0.2.jar dependency in the lib dir. That s no longer there","so hadoop distros that require this now fail at least any hadoop 2.0.x based ones We should also upgrade to current Hadoop versions Hadoop 2.2 stable","Add for who this story is","well_formed","no_role","high",False
19234,"We used to have a shared guava 11.0.2.jar dependency in the lib dir. That s no longer there so hadoop distros that require this now fail at least any hadoop 2.0.x based ones We should also upgrade to current Hadoop versions Hadoop 2.2 stable ",NULL,"We used to have a shared guava 11.0.2.jar dependency in the lib dir. That s no longer there","so hadoop distros that require this now fail at least any hadoop 2.0.x based ones We should also upgrade to current Hadoop versions Hadoop 2.2 stable","We used to have a shared guava 11<span class='highlight-text severity-high'>.0.2.jar dependency in the lib dir. That s no longer there so hadoop distros that require this now fail at least any hadoop 2.0.x based ones We should also upgrade to current Hadoop versions Hadoop 2.2 stable </span>","minimal","punctuation","high",False
19234,"We used to have a shared guava 11.0.2.jar dependency in the lib dir. That s no longer there so hadoop distros that require this now fail at least any hadoop 2.0.x based ones We should also upgrade to current Hadoop versions Hadoop 2.2 stable ",NULL,"We used to have a shared guava 11.0.2.jar dependency in the lib dir. That s no longer there","so hadoop distros that require this now fail at least any hadoop 2.0.x based ones We should also upgrade to current Hadoop versions Hadoop 2.2 stable","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19261,"This is cause generally by someone having port 9100 hsqldb port in use. It is recommended that setup checks to see if port is in use. If it is throw an exception stating that hsqldb port 9100 is in use and suggest options like. Free up the port or change the hsqldb port.",NULL,"This is cause generally by someone having port 9100 hsqldb port in use. It is recommended that setup checks to see if port is in use. If it is throw an exception stating that hsqldb port 9100 is in use and suggest options like. Free up the port or change the hsqldb port.",NULL,"Add for who this story is","well_formed","no_role","high",False
19261,"This is cause generally by someone having port 9100 hsqldb port in use. It is recommended that setup checks to see if port is in use. If it is throw an exception stating that hsqldb port 9100 is in use and suggest options like. Free up the port or change the hsqldb port.",NULL,"This is cause generally by someone having port 9100 hsqldb port in use. It is recommended that setup checks to see if port is in use. If it is throw an exception stating that hsqldb port 9100 is in use and suggest options like. Free up the port or change the hsqldb port.",NULL,"This is cause generally by someone having port 9100 hsqldb port in use. It is recommended that setup checks to see if port is in use. If it is throw an exception stating that hsqldb port 9100 is in use<span class='highlight-text severity-high'> and </span>suggest options like. Free up the port<span class='highlight-text severity-high'> or </span>change the hsqldb port.","atomic","conjunctions","high",False
19261,"This is cause generally by someone having port 9100 hsqldb port in use. It is recommended that setup checks to see if port is in use. If it is throw an exception stating that hsqldb port 9100 is in use and suggest options like. Free up the port or change the hsqldb port.",NULL,"This is cause generally by someone having port 9100 hsqldb port in use. It is recommended that setup checks to see if port is in use. If it is throw an exception stating that hsqldb port 9100 is in use and suggest options like. Free up the port or change the hsqldb port.",NULL,"This is cause generally by someone having port 9100 hsqldb port in use<span class='highlight-text severity-high'>. It is recommended that setup checks to see if port is in use. If it is throw an exception stating that hsqldb port 9100 is in use and suggest options like. Free up the port or change the hsqldb port.</span>","minimal","punctuation","high",False
19231,"The ItemReader will read multiple files from HDFS and the data will be converted to a tuple data structure The ItemProcessor will be a no op groovy script. The ItemWriter will write the data to a MongoDB collection A TupleToDBObject converter will need to be developed. the sample job should be documented ",NULL,"The ItemReader will read multiple files from HDFS and the data will be converted to a tuple data structure The ItemProcessor will be a no op groovy script. The ItemWriter will write the data to a MongoDB collection A TupleToDBObject converter will need to be developed. the sample job should be documented ",NULL,"Add for who this story is","well_formed","no_role","high",False
19231,"The ItemReader will read multiple files from HDFS and the data will be converted to a tuple data structure The ItemProcessor will be a no op groovy script. The ItemWriter will write the data to a MongoDB collection A TupleToDBObject converter will need to be developed. the sample job should be documented ",NULL,"The ItemReader will read multiple files from HDFS and the data will be converted to a tuple data structure The ItemProcessor will be a no op groovy script. The ItemWriter will write the data to a MongoDB collection A TupleToDBObject converter will need to be developed. the sample job should be documented ",NULL,"The ItemReader will read multiple files from HDFS<span class='highlight-text severity-high'> and </span>the data will be converted to a tuple data structure The ItemProcessor will be a no op groovy script. The ItemWriter will write the data to a MongoDB collection A TupleToDBObject converter will need to be developed. the sample job should be documented ","atomic","conjunctions","high",False
19231,"The ItemReader will read multiple files from HDFS and the data will be converted to a tuple data structure The ItemProcessor will be a no op groovy script. The ItemWriter will write the data to a MongoDB collection A TupleToDBObject converter will need to be developed. the sample job should be documented ",NULL,"The ItemReader will read multiple files from HDFS and the data will be converted to a tuple data structure The ItemProcessor will be a no op groovy script. The ItemWriter will write the data to a MongoDB collection A TupleToDBObject converter will need to be developed. the sample job should be documented ",NULL,"The ItemReader will read multiple files from HDFS and the data will be converted to a tuple data structure The ItemProcessor will be a no op groovy script<span class='highlight-text severity-high'>. The ItemWriter will write the data to a MongoDB collection A TupleToDBObject converter will need to be developed. the sample job should be documented </span>","minimal","punctuation","high",False
19262,"The runtime module properties requires a format option when displayed in the Shell Based on the PR https github.com spring projects spring xd pull 340 , the module properties are stored as String and displayed as is. ",NULL,"The runtime module properties requires a format option when displayed in the Shell Based on the PR https github.com spring projects spring xd pull 340 , the module properties are stored as String and displayed as is. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19262,"The runtime module properties requires a format option when displayed in the Shell Based on the PR https github.com spring projects spring xd pull 340 , the module properties are stored as String and displayed as is. ",NULL,"The runtime module properties requires a format option when displayed in the Shell Based on the PR https github.com spring projects spring xd pull 340 , the module properties are stored as String and displayed as is. ",NULL,"The runtime module properties requires a format option when displayed in the Shell Based on the PR https github.com spring projects spring xd pull 340 , the module properties are stored as String<span class='highlight-text severity-high'> and </span>displayed as is. ","atomic","conjunctions","high",False
19672,"An aggregate counter rolls up counts into discrete time buckets. There is an existing POC implementation in Java based off the library https github.com thheller timed counter The README there has a good description of the desired feature set.",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19672,"An aggregate counter rolls up counts into discrete time buckets. There is an existing POC implementation in Java based off the library https github.com thheller timed counter The README there has a good description of the desired feature set.",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19672,"An aggregate counter rolls up counts into discrete time buckets. There is an existing POC implementation in Java based off the library https github.com thheller timed counter The README there has a good description of the desired feature set.",NULL,NULL,NULL,"An aggregate counter rolls up counts into discrete time buckets<span class='highlight-text severity-high'>. There is an existing POC implementation in Java based off the library https github.com thheller timed counter The README there has a good description of the desired feature set.</span>","minimal","punctuation","high",False
19240,"Automated test will use directly use the Deployer class asserts on basic info of RunningInstance check that EBS was mounted that application was unzipped redis and rabbit are running via port checks http requests on admin port for root path list of modules AfterClass that will look for the cluster name and terminate all instances Look at live tag in JClouds tests for some additional tactics ",NULL,"Automated test will use directly use the Deployer class asserts on basic info of RunningInstance check that EBS was mounted that application was unzipped redis and rabbit are running via port checks http requests on admin port for root path list of modules AfterClass that will look for the cluster name and terminate all instances Look at live tag in JClouds tests for some additional tactics ",NULL,"Add for who this story is","well_formed","no_role","high",False
19240,"Automated test will use directly use the Deployer class asserts on basic info of RunningInstance check that EBS was mounted that application was unzipped redis and rabbit are running via port checks http requests on admin port for root path list of modules AfterClass that will look for the cluster name and terminate all instances Look at live tag in JClouds tests for some additional tactics ",NULL,"Automated test will use directly use the Deployer class asserts on basic info of RunningInstance check that EBS was mounted that application was unzipped redis and rabbit are running via port checks http requests on admin port for root path list of modules AfterClass that will look for the cluster name and terminate all instances Look at live tag in JClouds tests for some additional tactics ",NULL,"Automated test will use directly use the Deployer class asserts on basic info of RunningInstance check that EBS was mounted that application was unzipped redis<span class='highlight-text severity-high'> and </span>rabbit are running via port checks http requests on admin port for root path list of modules AfterClass that will look for the cluster name and terminate all instances Look at live tag in JClouds tests for some additional tactics ","atomic","conjunctions","high",False
19242,"Logging into your XD Account For Example https 946513944028.signin.aws.amazon.com console Discuss how to terminate running instances. Users can terminate all instances using the UI on the EC2 admin page Usage monitoring via CloudWatch Investigate what metadata in each instance is required so that cloudwatch can track ",NULL,"Logging into your XD Account For Example https 946513944028.signin.aws.amazon.com console Discuss how to terminate running instances. Users can terminate all instances using the UI on the EC2 admin page Usage monitoring via CloudWatch Investigate what metadata in each instance is required","so that cloudwatch can track","Add for who this story is","well_formed","no_role","high",False
19242,"Logging into your XD Account For Example https 946513944028.signin.aws.amazon.com console Discuss how to terminate running instances. Users can terminate all instances using the UI on the EC2 admin page Usage monitoring via CloudWatch Investigate what metadata in each instance is required so that cloudwatch can track ",NULL,"Logging into your XD Account For Example https 946513944028.signin.aws.amazon.com console Discuss how to terminate running instances. Users can terminate all instances using the UI on the EC2 admin page Usage monitoring via CloudWatch Investigate what metadata in each instance is required","so that cloudwatch can track","Logging into your XD Account For Example https 946513944028<span class='highlight-text severity-high'>.signin.aws.amazon.com console Discuss how to terminate running instances. Users can terminate all instances using the UI on the EC2 admin page Usage monitoring via CloudWatch Investigate what metadata in each instance is required so that cloudwatch can track </span>","minimal","punctuation","high",False
19242,"Logging into your XD Account For Example https 946513944028.signin.aws.amazon.com console Discuss how to terminate running instances. Users can terminate all instances using the UI on the EC2 admin page Usage monitoring via CloudWatch Investigate what metadata in each instance is required so that cloudwatch can track ",NULL,"Logging into your XD Account For Example https 946513944028.signin.aws.amazon.com console Discuss how to terminate running instances. Users can terminate all instances using the UI on the EC2 admin page Usage monitoring via CloudWatch Investigate what metadata in each instance is required","so that cloudwatch can track","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18085,"As a developer, I'd like to optimize YARN deployer, so I can deploy stream and the modules part of the definition rapidly.","As a developer",", I'd like to optimize YARN deployer,","so I can deploy stream and the modules part of the definition rapidly.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19238,"Create a Spring application context. XML or Java, dealers choice Use XD eclipse code format policy Create Source Package structure Create Test Package Structure Gradle build JClouds and Spring deps ",NULL,"Create a Spring application context. XML or Java, dealers choice Use XD eclipse code format policy Create Source Package structure Create Test Package Structure Gradle build JClouds and Spring deps ",NULL,"Add for who this story is","well_formed","no_role","high",False
19238,"Create a Spring application context. XML or Java, dealers choice Use XD eclipse code format policy Create Source Package structure Create Test Package Structure Gradle build JClouds and Spring deps ",NULL,"Create a Spring application context. XML or Java, dealers choice Use XD eclipse code format policy Create Source Package structure Create Test Package Structure Gradle build JClouds and Spring deps ",NULL,"Create a Spring application context. XML<span class='highlight-text severity-high'> or </span>Java, dealers choice Use XD eclipse code format policy Create Source Package structure Create Test Package Structure Gradle build JClouds<span class='highlight-text severity-high'> and </span>Spring deps ","atomic","conjunctions","high",False
19238,"Create a Spring application context. XML or Java, dealers choice Use XD eclipse code format policy Create Source Package structure Create Test Package Structure Gradle build JClouds and Spring deps ",NULL,"Create a Spring application context. XML or Java, dealers choice Use XD eclipse code format policy Create Source Package structure Create Test Package Structure Gradle build JClouds and Spring deps ",NULL,"Create a Spring application context<span class='highlight-text severity-high'>. XML or Java, dealers choice Use XD eclipse code format policy Create Source Package structure Create Test Package Structure Gradle build JClouds and Spring deps </span>","minimal","punctuation","high",False
19243,"Add instructions to github wiki on the usage of the installer ",NULL,"Add instructions to github wiki on the usage of the installer ",NULL,"Add for who this story is","well_formed","no_role","high",False
19239,"Get a java.io.File and copy it into HDFS. Could be text or binary. Write compressed with Hadoop and third party codecs see XD 277, XD 279 should initially support bzip2 LZO ",NULL,"Get a java.io.File and copy it into HDFS. Could be text or binary. Write compressed with Hadoop and third party codecs see XD 277, XD 279 should initially support bzip2 LZO ",NULL,"Add for who this story is","well_formed","no_role","high",False
19239,"Get a java.io.File and copy it into HDFS. Could be text or binary. Write compressed with Hadoop and third party codecs see XD 277, XD 279 should initially support bzip2 LZO ",NULL,"Get a java.io.File and copy it into HDFS. Could be text or binary. Write compressed with Hadoop and third party codecs see XD 277, XD 279 should initially support bzip2 LZO ",NULL,"Get a java.io.File<span class='highlight-text severity-high'> and </span>copy it into HDFS. Could be text<span class='highlight-text severity-high'> or </span>binary. Write compressed with Hadoop and third party codecs see XD 277, XD 279 should initially support bzip2 LZO ","atomic","conjunctions","high",False
19384,"container and event . XDContainer references and is referenced by ContainerStartedEvent and stopped . https sonar.springsource.org drilldown measures 7173?metric package cycles rids 5B 5D 7717 ",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19474,"http static.springsource.org spring xd docs 1.0.0.BUILD SNAPSHOT reference html sources should have rabbit added to the list and also the corresponding section that shows some basic usage.",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19239,"Get a java.io.File and copy it into HDFS. Could be text or binary. Write compressed with Hadoop and third party codecs see XD 277, XD 279 should initially support bzip2 LZO ",NULL,"Get a java.io.File and copy it into HDFS. Could be text or binary. Write compressed with Hadoop and third party codecs see XD 277, XD 279 should initially support bzip2 LZO ",NULL,"Get a java<span class='highlight-text severity-high'>.io.File and copy it into HDFS. Could be text or binary. Write compressed with Hadoop and third party codecs see XD 277, XD 279 should initially support bzip2 LZO </span>","minimal","punctuation","high",False
19244,"Setup groups for xd user Setup privileges so users can only see their instances Setup user accounts Send created access key to users Send username and passwords to user ",NULL,NULL,"so users can only see their instances Setup user accounts Send created access key to users Send username and passwords to user","Add what you want to achieve","well_formed","no_means","high",False
19244,"Setup groups for xd user Setup privileges so users can only see their instances Setup user accounts Send created access key to users Send username and passwords to user ",NULL,NULL,"so users can only see their instances Setup user accounts Send created access key to users Send username and passwords to user","Add for who this story is","well_formed","no_role","high",False
19244,"Setup groups for xd user Setup privileges so users can only see their instances Setup user accounts Send created access key to users Send username and passwords to user ",NULL,NULL,"so users can only see their instances Setup user accounts Send created access key to users Send username and passwords to user","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19245,"Currently . gradlew clean test fails since the module dependencies are not packaged before the test task. ",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19245,"Currently . gradlew clean test fails since the module dependencies are not packaged before the test task. ",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19245,"Currently . gradlew clean test fails since the module dependencies are not packaged before the test task. ",NULL,NULL,NULL,"Currently <span class='highlight-text severity-high'>. gradlew clean test fails since the module dependencies are not packaged before the test task. </span>","minimal","punctuation","high",False
19300,"The admin Server s tomcat is not shutdown properly. There is an existing method shutdownCleanly on AdminServer but the spring xd shell tests hang when we use this method to shutdown the admin server. ",NULL,"The admin Server s tomcat is not shutdown properly. There is an existing method shutdownCleanly on AdminServer but the spring xd shell tests hang when we use this method to shutdown the admin server. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19300,"The admin Server s tomcat is not shutdown properly. There is an existing method shutdownCleanly on AdminServer but the spring xd shell tests hang when we use this method to shutdown the admin server. ",NULL,"The admin Server s tomcat is not shutdown properly. There is an existing method shutdownCleanly on AdminServer but the spring xd shell tests hang when we use this method to shutdown the admin server. ",NULL,"The admin Server s tomcat is not shutdown properly<span class='highlight-text severity-high'>. There is an existing method shutdownCleanly on AdminServer but the spring xd shell tests hang when we use this method to shutdown the admin server. </span>","minimal","punctuation","high",False
19249,"TBD",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19249,"TBD",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19250,"SPI for deployment on to YARN Local dirt cluster.",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19250,"SPI for deployment on to YARN Local dirt cluster.",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19253,"Now that XD 924 is merged, we can convert splunk, twitter and gemfire modules to rely on the newly created projects. The hadoop module will get its own story, as classpath handling is a bit more tricky for that one. From there on, no new dependency should be added to DIRT for the sole purpose of a module. Rather, it should directly be created as a CP aware module and project.",NULL,"Now that XD 924 is merged, we can convert splunk, twitter and gemfire modules to rely on the newly created projects. The hadoop module will get its own story, as classpath handling is a bit more tricky for that one. From there on, no new dependency should be added to DIRT for the sole purpose of a module. Rather, it should directly be created as a CP aware module and project.",NULL,"Add for who this story is","well_formed","no_role","high",False
19253,"Now that XD 924 is merged, we can convert splunk, twitter and gemfire modules to rely on the newly created projects. The hadoop module will get its own story, as classpath handling is a bit more tricky for that one. From there on, no new dependency should be added to DIRT for the sole purpose of a module. Rather, it should directly be created as a CP aware module and project.",NULL,"Now that XD 924 is merged, we can convert splunk, twitter and gemfire modules to rely on the newly created projects. The hadoop module will get its own story, as classpath handling is a bit more tricky for that one. From there on, no new dependency should be added to DIRT for the sole purpose of a module. Rather, it should directly be created as a CP aware module and project.",NULL,"Now that XD 924 is merged, we can convert splunk, twitter<span class='highlight-text severity-high'> and </span>gemfire modules to rely on the newly created projects. The hadoop module will get its own story, as classpath handling is a bit more tricky for that one. From there on, no new dependency should be added to DIRT for the sole purpose of a module. Rather, it should directly be created as a CP aware module and project.","atomic","conjunctions","high",False
19253,"Now that XD 924 is merged, we can convert splunk, twitter and gemfire modules to rely on the newly created projects. The hadoop module will get its own story, as classpath handling is a bit more tricky for that one. From there on, no new dependency should be added to DIRT for the sole purpose of a module. Rather, it should directly be created as a CP aware module and project.",NULL,"Now that XD 924 is merged, we can convert splunk, twitter and gemfire modules to rely on the newly created projects. The hadoop module will get its own story, as classpath handling is a bit more tricky for that one. From there on, no new dependency should be added to DIRT for the sole purpose of a module. Rather, it should directly be created as a CP aware module and project.",NULL,"Now that XD 924 is merged, we can convert splunk, twitter and gemfire modules to rely on the newly created projects<span class='highlight-text severity-high'>. The hadoop module will get its own story, as classpath handling is a bit more tricky for that one. From there on, no new dependency should be added to DIRT for the sole purpose of a module. Rather, it should directly be created as a CP aware module and project.</span>","minimal","punctuation","high",False
19254,"The RuntimeContainersController from PR 340 returns the list of runtime modules. Instead we need make it pageable.",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19254,"The RuntimeContainersController from PR 340 returns the list of runtime modules. Instead we need make it pageable.",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19254,"The RuntimeContainersController from PR 340 returns the list of runtime modules. Instead we need make it pageable.",NULL,NULL,NULL,"The RuntimeContainersController from PR 340 returns the list of runtime modules<span class='highlight-text severity-high'>. Instead we need make it pageable.</span>","minimal","punctuation","high",False
19257,"We need a way to find the runtime module info by module type source , sink , processor , job . ",NULL,"We need a way to find the runtime module info by module type source , sink , processor , job . ",NULL,"Add for who this story is","well_formed","no_role","high",False
19260,"See https github.com spring projects spring hateoas issues 89 Updating HATEOAS version and removing in a lot of controllers should be possible now. See eg https github.com spring projects spring xd blob 4919ea2498a13ef47aaa9437937308fb26a7a24f spring xd dirt src main java org springframework xd dirt rest XDController.java L219",NULL,"See https github.com spring projects spring hateoas issues 89 Updating HATEOAS version and removing in a lot of controllers should be possible now. See eg https github.com spring projects spring xd blob 4919ea2498a13ef47aaa9437937308fb26a7a24f spring xd dirt src main java org springframework xd dirt rest XDController.java L219",NULL,"Add for who this story is","well_formed","no_role","high",False
19260,"See https github.com spring projects spring hateoas issues 89 Updating HATEOAS version and removing in a lot of controllers should be possible now. See eg https github.com spring projects spring xd blob 4919ea2498a13ef47aaa9437937308fb26a7a24f spring xd dirt src main java org springframework xd dirt rest XDController.java L219",NULL,"See https github.com spring projects spring hateoas issues 89 Updating HATEOAS version and removing in a lot of controllers should be possible now. See eg https github.com spring projects spring xd blob 4919ea2498a13ef47aaa9437937308fb26a7a24f spring xd dirt src main java org springframework xd dirt rest XDController.java L219",NULL,"See https github.com spring projects spring hateoas issues 89 Updating HATEOAS version<span class='highlight-text severity-high'> and </span>removing in a lot of controllers should be possible now. See eg https github.com spring projects spring xd blob 4919ea2498a13ef47aaa9437937308fb26a7a24f spring xd dirt src main java org springframework xd dirt rest XDController.java L219","atomic","conjunctions","high",False
19260,"See https github.com spring projects spring hateoas issues 89 Updating HATEOAS version and removing in a lot of controllers should be possible now. See eg https github.com spring projects spring xd blob 4919ea2498a13ef47aaa9437937308fb26a7a24f spring xd dirt src main java org springframework xd dirt rest XDController.java L219",NULL,"See https github.com spring projects spring hateoas issues 89 Updating HATEOAS version and removing in a lot of controllers should be possible now. See eg https github.com spring projects spring xd blob 4919ea2498a13ef47aaa9437937308fb26a7a24f spring xd dirt src main java org springframework xd dirt rest XDController.java L219",NULL,"See https github<span class='highlight-text severity-high'>.com spring projects spring hateoas issues 89 Updating HATEOAS version and removing in a lot of controllers should be possible now. See eg https github.com spring projects spring xd blob 4919ea2498a13ef47aaa9437937308fb26a7a24f spring xd dirt src main java org springframework xd dirt rest XDController.java L219</span>","minimal","punctuation","high",False
19258,"Currently TupleCodec uses JSON for serialization deserialization. It should use Kryo. This will require some customization and potentially changes to Tuple to address the Tuple s conversionService field. ",NULL,"Currently TupleCodec uses JSON for serialization deserialization. It should use Kryo. This will require some customization and potentially changes to Tuple to address the Tuple s conversionService field. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19258,"Currently TupleCodec uses JSON for serialization deserialization. It should use Kryo. This will require some customization and potentially changes to Tuple to address the Tuple s conversionService field. ",NULL,"Currently TupleCodec uses JSON for serialization deserialization. It should use Kryo. This will require some customization and potentially changes to Tuple to address the Tuple s conversionService field. ",NULL,"Currently TupleCodec uses JSON for serialization deserialization. It should use Kryo. This will require some customization<span class='highlight-text severity-high'> and </span>potentially changes to Tuple to address the Tuple s conversionService field. ","atomic","conjunctions","high",False
19258,"Currently TupleCodec uses JSON for serialization deserialization. It should use Kryo. This will require some customization and potentially changes to Tuple to address the Tuple s conversionService field. ",NULL,"Currently TupleCodec uses JSON for serialization deserialization. It should use Kryo. This will require some customization and potentially changes to Tuple to address the Tuple s conversionService field. ",NULL,"Currently TupleCodec uses JSON for serialization deserialization<span class='highlight-text severity-high'>. It should use Kryo. This will require some customization and potentially changes to Tuple to address the Tuple s conversionService field. </span>","minimal","punctuation","high",False
19247,"This is currently missing and probably supersedes some of the stuff that s in there now.",NULL,"This is currently missing and probably supersedes some of the stuff that s in there now.",NULL,"Add for who this story is","well_formed","no_role","high",False
19247,"This is currently missing and probably supersedes some of the stuff that s in there now.",NULL,"This is currently missing and probably supersedes some of the stuff that s in there now.",NULL,"This is currently missing<span class='highlight-text severity-high'> and </span>probably supersedes some of the stuff that s in there now.","atomic","conjunctions","high",False
19255,"With PR 340, listing of runtime modules with a non existent containerId will display empty table. Instead, we can throw exception saying Container doesn t exist.",NULL,"With PR 340, listing of runtime modules with a non existent containerId will display empty table. Instead, we can throw exception saying Container doesn t exist.",NULL,"Add for who this story is","well_formed","no_role","high",False
19255,"With PR 340, listing of runtime modules with a non existent containerId will display empty table. Instead, we can throw exception saying Container doesn t exist.",NULL,"With PR 340, listing of runtime modules with a non existent containerId will display empty table. Instead, we can throw exception saying Container doesn t exist.",NULL,"With PR 340, listing of runtime modules with a non existent containerId will display empty table<span class='highlight-text severity-high'>. Instead, we can throw exception saying Container doesn t exist.</span>","minimal","punctuation","high",False
19248,"Provide the infrastructure for HTTP GET completions?start http file d that would return a list of possible completions in this case returning the file option names that start with d This story is about and only about Having that REST controller, delegating to some CompletionsEngine Implementing the Spring Shell Converter that talks to that It s an empty shell, useless but easy to do without the actual CompletionsEngine ",NULL,"Provide the infrastructure for HTTP GET completions?start http file d that would return a list of possible completions in this case returning the file option names that start with d This story is about and only about Having that REST controller, delegating to some CompletionsEngine Implementing the Spring Shell Converter that talks to that It s an empty shell, useless but easy to do without the actual CompletionsEngine ",NULL,"Add for who this story is","well_formed","no_role","high",False
19248,"Provide the infrastructure for HTTP GET completions?start http file d that would return a list of possible completions in this case returning the file option names that start with d This story is about and only about Having that REST controller, delegating to some CompletionsEngine Implementing the Spring Shell Converter that talks to that It s an empty shell, useless but easy to do without the actual CompletionsEngine ",NULL,"Provide the infrastructure for HTTP GET completions?start http file d that would return a list of possible completions in this case returning the file option names that start with d This story is about and only about Having that REST controller, delegating to some CompletionsEngine Implementing the Spring Shell Converter that talks to that It s an empty shell, useless but easy to do without the actual CompletionsEngine ",NULL,"Provide the infrastructure for HTTP GET completions?start http file d that would return a list of possible completions in this case returning the file option names that start with d This story is about<span class='highlight-text severity-high'> and </span>only about Having that REST controller, delegating to some CompletionsEngine Implementing the Spring Shell Converter that talks to that It s an empty shell, useless but easy to do without the actual CompletionsEngine ","atomic","conjunctions","high",False
19251,"See Epic https jira.springsource.org browse XD 234",NULL,"See Epic https jira.springsource.org browse XD 234",NULL,"Add for who this story is","well_formed","no_role","high",False
19252,"If you create a simple http log stream and list the modules, you ll see that JobPlugin adds its numberFormat, makeUnique, etc properties. Even though they do no harm, it s really strange for users. Plus, it could conflict with e.g. activation of profiles given present properties",NULL,"If you create a simple http log stream and list the modules, you ll see that JobPlugin adds its numberFormat, makeUnique, etc properties. Even though they do no harm, it s really strange for users. Plus, it could conflict with e.g. activation of profiles given present properties",NULL,"Add for who this story is","well_formed","no_role","high",False
19252,"If you create a simple http log stream and list the modules, you ll see that JobPlugin adds its numberFormat, makeUnique, etc properties. Even though they do no harm, it s really strange for users. Plus, it could conflict with e.g. activation of profiles given present properties",NULL,"If you create a simple http log stream and list the modules, you ll see that JobPlugin adds its numberFormat, makeUnique, etc properties. Even though they do no harm, it s really strange for users. Plus, it could conflict with e.g. activation of profiles given present properties",NULL,"If you create a simple http log stream<span class='highlight-text severity-high'> and </span>list the modules, you ll see that JobPlugin adds its numberFormat, makeUnique, etc properties. Even though they do no harm, it s really strange for users. Plus, it could conflict with e.g. activation of profiles given present properties","atomic","conjunctions","high",False
19252,"If you create a simple http log stream and list the modules, you ll see that JobPlugin adds its numberFormat, makeUnique, etc properties. Even though they do no harm, it s really strange for users. Plus, it could conflict with e.g. activation of profiles given present properties",NULL,"If you create a simple http log stream and list the modules, you ll see that JobPlugin adds its numberFormat, makeUnique, etc properties. Even though they do no harm, it s really strange for users. Plus, it could conflict with e.g. activation of profiles given present properties",NULL,"If you create a simple http log stream and list the modules, you ll see that JobPlugin adds its numberFormat, makeUnique, etc properties<span class='highlight-text severity-high'>. Even though they do no harm, it s really strange for users. Plus, it could conflict with e.g. activation of profiles given present properties</span>","minimal","punctuation","high",False
19256,"Currently, ModulesController creates the ModuleDefinitionRepository instance with ModuleRegistry. Instead, we should inject the moduleDefinitionRepository into ModulesController directly.",NULL,"Currently, ModulesController creates the ModuleDefinitionRepository instance with ModuleRegistry. Instead, we should inject the moduleDefinitionRepository into ModulesController directly.",NULL,"Currently, ModulesController creates the ModuleDefinitionRepository instance with ModuleRegistry<span class='highlight-text severity-high'>. Instead, we should inject the moduleDefinitionRepository into ModulesController directly.</span>","minimal","punctuation","high",False
19259,"Register a JSON to Object converter in the DefaultContentTypeAwareConverterRegistry. Currently we only support Object to JSON but not the other way. This may or may not work for an arbitrary object but is useful in many cases.",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19259,"Register a JSON to Object converter in the DefaultContentTypeAwareConverterRegistry. Currently we only support Object to JSON but not the other way. This may or may not work for an arbitrary object but is useful in many cases.",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19259,"Register a JSON to Object converter in the DefaultContentTypeAwareConverterRegistry. Currently we only support Object to JSON but not the other way. This may or may not work for an arbitrary object but is useful in many cases.",NULL,NULL,NULL,"Register a JSON to Object converter in the DefaultContentTypeAwareConverterRegistry<span class='highlight-text severity-high'>. Currently we only support Object to JSON but not the other way. This may or may not work for an arbitrary object but is useful in many cases.</span>","minimal","punctuation","high",False
19264,"In the testmodules.source Rename source config to packaged source Rename source config to packaged source no lib All xml files should be prefixed with test. i.e. testsource, testsink Make sure all tests pass with new configuration",NULL,"In the testmodules.source Rename source config to packaged source Rename source config to packaged source no lib All xml files should be prefixed with test. i.e. testsource, testsink Make sure all tests pass with new configuration",NULL,"Add for who this story is","well_formed","no_role","high",False
19264,"In the testmodules.source Rename source config to packaged source Rename source config to packaged source no lib All xml files should be prefixed with test. i.e. testsource, testsink Make sure all tests pass with new configuration",NULL,"In the testmodules.source Rename source config to packaged source Rename source config to packaged source no lib All xml files should be prefixed with test. i.e. testsource, testsink Make sure all tests pass with new configuration",NULL,"In the testmodules<span class='highlight-text severity-high'>.source Rename source config to packaged source Rename source config to packaged source no lib All xml files should be prefixed with test. i.e. testsource, testsink Make sure all tests pass with new configuration</span>","minimal","punctuation","high",False
19265,"Remove ModuleType.getModuleTypeByTypeName. All code should use the enum. ",NULL,"Remove ModuleType.getModuleTypeByTypeName. All code should use the enum. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19265,"Remove ModuleType.getModuleTypeByTypeName. All code should use the enum. ",NULL,"Remove ModuleType.getModuleTypeByTypeName. All code should use the enum. ",NULL,"Remove ModuleType<span class='highlight-text severity-high'>.getModuleTypeByTypeName. All code should use the enum. </span>","minimal","punctuation","high",False
19266,"Similar to what is done for e.g. hadoop, reactor, and http, some of the classes in the .x package namely gemfire, splunk, twitter should go in dedicated albeit small projects. This would enable further modularization see XD 915 ",NULL,"Similar to what is done for e.g. hadoop, reactor, and http, some of the classes in the .x package namely gemfire, splunk, twitter should go in dedicated albeit small projects. This would enable further modularization see XD 915 ",NULL,"Add for who this story is","well_formed","no_role","high",False
19266,"Similar to what is done for e.g. hadoop, reactor, and http, some of the classes in the .x package namely gemfire, splunk, twitter should go in dedicated albeit small projects. This would enable further modularization see XD 915 ",NULL,"Similar to what is done for e.g. hadoop, reactor, and http, some of the classes in the .x package namely gemfire, splunk, twitter should go in dedicated albeit small projects. This would enable further modularization see XD 915 ",NULL,"Similar to what is done for e<span class='highlight-text severity-high'>.g. hadoop, reactor, and http, some of the classes in the .x package namely gemfire, splunk, twitter should go in dedicated albeit small projects. This would enable further modularization see XD 915 </span>","minimal","punctuation","high",False
19268,"SingleNode server needs to stop cleanly with stopping both the admin server container server. Also, all the tests that require SingleNode main server needs to handle the server shutdown appropriately.",NULL,"SingleNode server needs to stop cleanly with stopping both the admin server container server. Also, all the tests that require SingleNode main server needs to handle the server shutdown appropriately.",NULL,"Add for who this story is","well_formed","no_role","high",False
19268,"SingleNode server needs to stop cleanly with stopping both the admin server container server. Also, all the tests that require SingleNode main server needs to handle the server shutdown appropriately.",NULL,"SingleNode server needs to stop cleanly with stopping both the admin server container server. Also, all the tests that require SingleNode main server needs to handle the server shutdown appropriately.",NULL,"SingleNode server needs to stop cleanly with stopping both the admin server container server<span class='highlight-text severity-high'>. Also, all the tests that require SingleNode main server needs to handle the server shutdown appropriately.</span>","minimal","punctuation","high",False
19267,"As a user, I'd like to be notified when a exception is thrown in a module so that I can tap into an error channel to receive the failures for each stream module. ","As a user",", I'd like to be notified when a exception is thrown in a module","so that I can tap into an error channel to receive the failures for each stream module.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19270,"json parameter is no longer required. Use outputType application json instead",NULL,"json parameter is no longer required. Use outputType application json instead",NULL,"Add for who this story is","well_formed","no_role","high",False
19270,"json parameter is no longer required. Use outputType application json instead",NULL,"json parameter is no longer required. Use outputType application json instead",NULL,"json parameter is no longer required<span class='highlight-text severity-high'>. Use outputType application json instead</span>","minimal","punctuation","high",False
19269,"Not everyone may be familiar with MQTT, or esp. with MQTT inside Rabbit",NULL,"Not everyone may be familiar with MQTT, or esp. with MQTT inside Rabbit",NULL,"Add for who this story is","well_formed","no_role","high",False
19269,"Not everyone may be familiar with MQTT, or esp. with MQTT inside Rabbit",NULL,"Not everyone may be familiar with MQTT, or esp. with MQTT inside Rabbit",NULL,"Not everyone may be familiar with MQTT, or esp<span class='highlight-text severity-high'>. with MQTT inside Rabbit</span>","minimal","punctuation","high",False
19272,"Enhance the stream parser to take message conversion into account in order to validate or automatically configure converters. For example noformat nopanel true source outputType my.Foo sink inputType some.other.Bar is likely invalid since XD doesn t know how to convert Foo Bar. noformat ",NULL,"Enhance the stream parser to take message conversion into account","in order to validate or automatically configure converters. For example noformat nopanel true source outputType my.Foo sink inputType some.other.Bar is likely invalid since XD doesn t know how to convert Foo Bar. noformat","Add for who this story is","well_formed","no_role","high",False
19272,"Enhance the stream parser to take message conversion into account in order to validate or automatically configure converters. For example noformat nopanel true source outputType my.Foo sink inputType some.other.Bar is likely invalid since XD doesn t know how to convert Foo Bar. noformat ",NULL,"Enhance the stream parser to take message conversion into account","in order to validate or automatically configure converters. For example noformat nopanel true source outputType my.Foo sink inputType some.other.Bar is likely invalid since XD doesn t know how to convert Foo Bar. noformat","Enhance the stream parser to take message conversion into account in order to validate or automatically configure converters<span class='highlight-text severity-high'>. For example noformat nopanel true source outputType my.Foo sink inputType some.other.Bar is likely invalid since XD doesn t know how to convert Foo Bar. noformat </span>","minimal","punctuation","high",False
19272,"Enhance the stream parser to take message conversion into account in order to validate or automatically configure converters. For example noformat nopanel true source outputType my.Foo sink inputType some.other.Bar is likely invalid since XD doesn t know how to convert Foo Bar. noformat ",NULL,"Enhance the stream parser to take message conversion into account","in order to validate or automatically configure converters. For example noformat nopanel true source outputType my.Foo sink inputType some.other.Bar is likely invalid since XD doesn t know how to convert Foo Bar. noformat","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19271,"See also XD 903, XD 915 A lot of dependencies have been added with the compile scope as an oversight over time. Some of them are only required at runtime, some may not be required anymore.",NULL,"See also XD 903, XD 915 A lot of dependencies have been added with the compile scope as an oversight over time. Some of them are only required at runtime, some may not be required anymore.",NULL,"Add for who this story is","well_formed","no_role","high",False
19271,"See also XD 903, XD 915 A lot of dependencies have been added with the compile scope as an oversight over time. Some of them are only required at runtime, some may not be required anymore.",NULL,"See also XD 903, XD 915 A lot of dependencies have been added with the compile scope as an oversight over time. Some of them are only required at runtime, some may not be required anymore.",NULL,"See also XD 903, XD 915 A lot of dependencies have been added with the compile scope as an oversight over time<span class='highlight-text severity-high'>. Some of them are only required at runtime, some may not be required anymore.</span>","minimal","punctuation","high",False
19276,"The XD build breaks with Gradle 1.8 due to some changes in dependency resolution.",NULL,"The XD build breaks with Gradle 1.8 due to some changes in dependency resolution.",NULL,"Add for who this story is","well_formed","no_role","high",False
19275,"See issue https jira.springsource.org browse XD 862 The docs should be updated to include examples that show how to use the standard SpEL based splitter, transformer, filters with jsonPath expressions.",NULL,"See issue https jira.springsource.org browse XD 862 The docs should be updated to include examples that show how to use the standard SpEL based splitter, transformer, filters with jsonPath expressions.",NULL,"Add for who this story is","well_formed","no_role","high",False
19331,"Now that taps are just channels, we need to update the docs. The preceding colon is no longer needed and will be removed altogether , so all examples should be like this code tap foo bar code ",NULL,"Now that taps are just channels, we need to update the docs. The preceding colon is no longer needed and will be removed altogether ,","so all examples should be like this code tap foo bar code","Add for who this story is","well_formed","no_role","high",False
19331,"Now that taps are just channels, we need to update the docs. The preceding colon is no longer needed and will be removed altogether , so all examples should be like this code tap foo bar code ",NULL,"Now that taps are just channels, we need to update the docs. The preceding colon is no longer needed and will be removed altogether ,","so all examples should be like this code tap foo bar code","Now that taps are just channels, we need to update the docs. The preceding colon is no longer needed<span class='highlight-text severity-high'> and </span>will be removed altogether , so all examples should be like this code tap foo bar code ","atomic","conjunctions","high",False
19331,"Now that taps are just channels, we need to update the docs. The preceding colon is no longer needed and will be removed altogether , so all examples should be like this code tap foo bar code ",NULL,"Now that taps are just channels, we need to update the docs. The preceding colon is no longer needed and will be removed altogether ,","so all examples should be like this code tap foo bar code","Now that taps are just channels, we need to update the docs<span class='highlight-text severity-high'>. The preceding colon is no longer needed and will be removed altogether , so all examples should be like this code tap foo bar code </span>","minimal","punctuation","high",False
19331,"Now that taps are just channels, we need to update the docs. The preceding colon is no longer needed and will be removed altogether , so all examples should be like this code tap foo bar code ",NULL,"Now that taps are just channels, we need to update the docs. The preceding colon is no longer needed and will be removed altogether ,","so all examples should be like this code tap foo bar code","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19277,"Users need to register custom message converters used by modules.",NULL,"Users need to register custom message converters used by modules.",NULL,"Add for who this story is","well_formed","no_role","high",False
19263,"The aggregate counter query result currently returns the interval that is passed in, whether it is aligned with the bucket resolution requested or not. It would be more intuitive if the time values returned are rounded down to the resolution of the query i.e. whole minutes, hours, days or whatever .",NULL,"The aggregate counter query result currently returns the interval that is passed in, whether it is aligned with the bucket resolution requested or not. It would be more intuitive if the time values returned are rounded down to the resolution of the query i.e. whole minutes, hours, days or whatever .",NULL,"Add for who this story is","well_formed","no_role","high",False
19263,"The aggregate counter query result currently returns the interval that is passed in, whether it is aligned with the bucket resolution requested or not. It would be more intuitive if the time values returned are rounded down to the resolution of the query i.e. whole minutes, hours, days or whatever .",NULL,"The aggregate counter query result currently returns the interval that is passed in, whether it is aligned with the bucket resolution requested or not. It would be more intuitive if the time values returned are rounded down to the resolution of the query i.e. whole minutes, hours, days or whatever .",NULL,"The aggregate counter query result currently returns the interval that is passed in, whether it is aligned with the bucket resolution requested or not<span class='highlight-text severity-high'>. It would be more intuitive if the time values returned are rounded down to the resolution of the query i.e. whole minutes, hours, days or whatever .</span>","minimal","punctuation","high",False
19273,"File source should output either the File itself serialized File object or the contents as a byte . This option is configured by a parameter contents true. The byte may be converted to a String using XD Message Conversion, e.g., output text plain;charset UTF 8",NULL,"File source should output either the File itself serialized File object or the contents as a byte . This option is configured by a parameter contents true. The byte may be converted to a String using XD Message Conversion, e.g., output text plain;charset UTF 8",NULL,"Add for who this story is","well_formed","no_role","high",False
19273,"File source should output either the File itself serialized File object or the contents as a byte . This option is configured by a parameter contents true. The byte may be converted to a String using XD Message Conversion, e.g., output text plain;charset UTF 8",NULL,"File source should output either the File itself serialized File object or the contents as a byte . This option is configured by a parameter contents true. The byte may be converted to a String using XD Message Conversion, e.g., output text plain;charset UTF 8",NULL,"File source should output either the File itself serialized File object or the contents as a byte <span class='highlight-text severity-high'>. This option is configured by a parameter contents true. The byte may be converted to a String using XD Message Conversion, e.g., output text plain;charset UTF 8</span>","minimal","punctuation","high",False
19278,"Should keep the critical error count as close to zero as possible.",NULL,"Should keep the critical error count as close to zero as possible.",NULL,"Add for who this story is","well_formed","no_role","high",False
19282,"The xd dirt project should be split in at least 3 parts Classes and resources pertaining to the admin server Container server Shared classes Additionally, we may consider splitting the first two in half as well, having a separate project for CLI handling and hence introduce 2 other projects for YARN, etc ",NULL,"The xd dirt project should be split in at least 3 parts Classes and resources pertaining to the admin server Container server Shared classes Additionally, we may consider splitting the first two in half as well, having a separate project for CLI handling and hence introduce 2 other projects for YARN, etc ",NULL,"Add for who this story is","well_formed","no_role","high",False
19282,"The xd dirt project should be split in at least 3 parts Classes and resources pertaining to the admin server Container server Shared classes Additionally, we may consider splitting the first two in half as well, having a separate project for CLI handling and hence introduce 2 other projects for YARN, etc ",NULL,"The xd dirt project should be split in at least 3 parts Classes and resources pertaining to the admin server Container server Shared classes Additionally, we may consider splitting the first two in half as well, having a separate project for CLI handling and hence introduce 2 other projects for YARN, etc ",NULL,"The xd dirt project should be split in at least 3 parts Classes<span class='highlight-text severity-high'> and </span>resources pertaining to the admin server Container server Shared classes Additionally, we may consider splitting the first two in half as well, having a separate project for CLI handling and hence introduce 2 other projects for YARN, etc ","atomic","conjunctions","high",False
19281,"It looks like Container s ContainerStartedEvent and ContainerStoppedEvent are published from ContainerLauncher s context whereas the ContainerEventListeners are running in XDContainer s context. This makes the container start stop events not getting processed. ",NULL,"It looks like Container s ContainerStartedEvent and ContainerStoppedEvent are published from ContainerLauncher s context whereas the ContainerEventListeners are running in XDContainer s context. This makes the container start stop events not getting processed. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19281,"It looks like Container s ContainerStartedEvent and ContainerStoppedEvent are published from ContainerLauncher s context whereas the ContainerEventListeners are running in XDContainer s context. This makes the container start stop events not getting processed. ",NULL,"It looks like Container s ContainerStartedEvent and ContainerStoppedEvent are published from ContainerLauncher s context whereas the ContainerEventListeners are running in XDContainer s context. This makes the container start stop events not getting processed. ",NULL,"It looks like Container s ContainerStartedEvent<span class='highlight-text severity-high'> and </span>ContainerStoppedEvent are published from ContainerLauncher s context whereas the ContainerEventListeners are running in XDContainer s context. This makes the container start stop events not getting processed. ","atomic","conjunctions","high",False
19436,"Example using name code filter expression payload.myfield.startsWith foo code Example using index code filter expression payload.2.startsWith foo code This should support nested keys as well code filter expression payload.myfield.subfield.startsWith foo code ",NULL,"Example using name code filter expression payload.myfield.startsWith foo code Example using index code filter expression payload.2.startsWith foo code This should support nested keys as well code filter expression payload.myfield.subfield.startsWith foo code ",NULL,"Add for who this story is","well_formed","no_role","high",False
19438,"the value for target is required there is no default , but the hint for that option states otherwise code xd http post target http post target required target the location to post to; default if option not present http localhost 9000 code ",NULL,"the value for target is required there is no default , but the hint for that option states otherwise code xd http post target http post target required target the location to post to; default if option not present http localhost 9000 code ",NULL,"Add for who this story is","well_formed","no_role","high",False
19281,"It looks like Container s ContainerStartedEvent and ContainerStoppedEvent are published from ContainerLauncher s context whereas the ContainerEventListeners are running in XDContainer s context. This makes the container start stop events not getting processed. ",NULL,"It looks like Container s ContainerStartedEvent and ContainerStoppedEvent are published from ContainerLauncher s context whereas the ContainerEventListeners are running in XDContainer s context. This makes the container start stop events not getting processed. ",NULL,"It looks like Container s ContainerStartedEvent and ContainerStoppedEvent are published from ContainerLauncher s context whereas the ContainerEventListeners are running in XDContainer s context<span class='highlight-text severity-high'>. This makes the container start stop events not getting processed. </span>","minimal","punctuation","high",False
19284,"We currently include jetty util 6.1.26.jar but we need to add correct jar for different distributions PHD uses jetty util 7.6.10.v20130312.jar Need to check hadoop hdfs dependencies for the distros and add jetty util to the jar copy for each distro ",NULL,"We currently include jetty util 6.1.26.jar but we need to add correct jar for different distributions PHD uses jetty util 7.6.10.v20130312.jar Need to check hadoop hdfs dependencies for the distros and add jetty util to the jar copy for each distro ",NULL,"Add for who this story is","well_formed","no_role","high",False
19284,"We currently include jetty util 6.1.26.jar but we need to add correct jar for different distributions PHD uses jetty util 7.6.10.v20130312.jar Need to check hadoop hdfs dependencies for the distros and add jetty util to the jar copy for each distro ",NULL,"We currently include jetty util 6.1.26.jar but we need to add correct jar for different distributions PHD uses jetty util 7.6.10.v20130312.jar Need to check hadoop hdfs dependencies for the distros and add jetty util to the jar copy for each distro ",NULL,"We currently include jetty util 6.1.26.jar but we need to add correct jar for different distributions PHD uses jetty util 7.6.10.v20130312.jar Need to check hadoop hdfs dependencies for the distros<span class='highlight-text severity-high'> and </span>add jetty util to the jar copy for each distro ","atomic","conjunctions","high",False
19283,"Tests that leverage Redis Rabbit AvailableRule often create another connection factory in the test body but fail to close it. Tests should properly close the resource. As an added benefit, the rule itself can expose the resource that it created for deciding whether to skip the test or not",NULL,"Tests that leverage Redis Rabbit AvailableRule often create another connection factory in the test body but fail to close it. Tests should properly close the resource. As an added benefit, the rule itself can expose the resource that it created for deciding whether to skip the test or not",NULL,"Add for who this story is","well_formed","no_role","high",False
19283,"Tests that leverage Redis Rabbit AvailableRule often create another connection factory in the test body but fail to close it. Tests should properly close the resource. As an added benefit, the rule itself can expose the resource that it created for deciding whether to skip the test or not",NULL,"Tests that leverage Redis Rabbit AvailableRule often create another connection factory in the test body but fail to close it. Tests should properly close the resource. As an added benefit, the rule itself can expose the resource that it created for deciding whether to skip the test or not",NULL,"Tests that leverage Redis Rabbit AvailableRule often create another connection factory in the test body but fail to close it<span class='highlight-text severity-high'>. Tests should properly close the resource. As an added benefit, the rule itself can expose the resource that it created for deciding whether to skip the test or not</span>","minimal","punctuation","high",False
19285,"When INT 3133 is resolved, SpEL PropertyAccessor s are inherited from parent contexts. Instead of adding the JsonPropertyAccessor to each module s context, add it to the parent instead.",NULL,"When INT 3133 is resolved, SpEL PropertyAccessor s are inherited from parent contexts. Instead of adding the JsonPropertyAccessor to each module s context, add it to the parent instead.",NULL,"Add for who this story is","well_formed","no_role","high",False
19285,"When INT 3133 is resolved, SpEL PropertyAccessor s are inherited from parent contexts. Instead of adding the JsonPropertyAccessor to each module s context, add it to the parent instead.",NULL,"When INT 3133 is resolved, SpEL PropertyAccessor s are inherited from parent contexts. Instead of adding the JsonPropertyAccessor to each module s context, add it to the parent instead.",NULL,"When INT 3133 is resolved, SpEL PropertyAccessor s are inherited from parent contexts<span class='highlight-text severity-high'>. Instead of adding the JsonPropertyAccessor to each module s context, add it to the parent instead.</span>","minimal","punctuation","high",False
19286,"currently xd container will not start due to a DB connection failure if the xd admin is not already running In fact, if someone is not using Batch jobs at all with XD, they should not even need a DB connection for either xd admin or xd container to run so... consider using LazyConnectionDataSourceProxy so a connection failure would only occur when the DataSource is actually invoked to retrieve a connection",NULL,"currently xd container will not start due to a DB connection failure if the xd admin is not already running In fact, if","someone is not using Batch jobs at all with XD, they should not even need a DB connection for either xd admin or xd container to run so... consider using LazyConnectionDataSourceProxy so a connection failure would only occur when the DataSource is actually invoked to retrieve a connection","Add for who this story is","well_formed","no_role","high",False
19286,"currently xd container will not start due to a DB connection failure if the xd admin is not already running In fact, if someone is not using Batch jobs at all with XD, they should not even need a DB connection for either xd admin or xd container to run so... consider using LazyConnectionDataSourceProxy so a connection failure would only occur when the DataSource is actually invoked to retrieve a connection",NULL,"currently xd container will not start due to a DB connection failure if the xd admin is not already running In fact, if","someone is not using Batch jobs at all with XD, they should not even need a DB connection for either xd admin or xd container to run so... consider using LazyConnectionDataSourceProxy so a connection failure would only occur when the DataSource is actually invoked to retrieve a connection","currently xd container will not start due to a DB connection failure if the xd admin is not already running In fact, if someone is not using Batch jobs at all with XD, they should not even need a DB connection for either xd admin or xd container to run so<span class='highlight-text severity-high'>... consider using LazyConnectionDataSourceProxy so a connection failure would only occur when the DataSource is actually invoked to retrieve a connection</span>","minimal","punctuation","high",False
19286,"currently xd container will not start due to a DB connection failure if the xd admin is not already running In fact, if someone is not using Batch jobs at all with XD, they should not even need a DB connection for either xd admin or xd container to run so... consider using LazyConnectionDataSourceProxy so a connection failure would only occur when the DataSource is actually invoked to retrieve a connection",NULL,"currently xd container will not start due to a DB connection failure if the xd admin is not already running In fact, if","someone is not using Batch jobs at all with XD, they should not even need a DB connection for either xd admin or xd container to run so... consider using LazyConnectionDataSourceProxy so a connection failure would only occur when the DataSource is actually invoked to retrieve a connection","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19673,"we have a prototype gardenhose adapter that was built directly upon RestTemplate streaming on a background thread , but Spring Social Twitter has an issue on its 1.1 roadmap that is relevant https jira.springsource.org browse SOCIALTW 2 ",NULL,"we have a prototype gardenhose adapter that was built directly upon RestTemplate streaming on a background thread , but Spring Social Twitter has an issue on its 1.1 roadmap that is relevant https jira.springsource.org browse SOCIALTW 2 ",NULL,"Add for who this story is","well_formed","no_role","high",False
19288,"The ability to configure message conversion via parameters. Consider programatic configuration of data type channels. Values can be media type, e.g., application json or a java class name.",NULL,"The ability to configure message conversion via parameters. Consider programatic configuration of data type channels. Values can be media type, e.g., application json or a java class name.",NULL,"Add for who this story is","well_formed","no_role","high",False
19288,"The ability to configure message conversion via parameters. Consider programatic configuration of data type channels. Values can be media type, e.g., application json or a java class name.",NULL,"The ability to configure message conversion via parameters. Consider programatic configuration of data type channels. Values can be media type, e.g., application json or a java class name.",NULL,"The ability to configure message conversion via parameters<span class='highlight-text severity-high'>. Consider programatic configuration of data type channels. Values can be media type, e.g., application json or a java class name.</span>","minimal","punctuation","high",False
19279,"Offers the functionality to make http request to a web service. i.e. outbound http gateway. Example implementations stream create name foo definition trigger rest reply timeout 1 url http earthquake.usgs.gov earthquakes feed geojson all day log stream create name foos definition trigger payload lat 34.0567006 lon 84.34368810000001 site all smap 1 searchresult Roswell 2C 20GA 2030076 2C 20USA .UktzaWSG1Dd rest url http forecast.weather.gov MapClick.php? log ",NULL,"Offers the functionality to make http request to a web service. i.e. outbound http gateway. Example implementations stream create name foo definition trigger rest reply timeout 1 url http earthquake.usgs.gov earthquakes feed geojson all day log stream create name foos definition trigger payload lat 34.0567006 lon 84.34368810000001 site all smap 1 searchresult Roswell 2C 20GA 2030076 2C 20USA .UktzaWSG1Dd rest url http forecast.weather.gov MapClick.php? log ",NULL,"Add for who this story is","well_formed","no_role","high",False
19279,"Offers the functionality to make http request to a web service. i.e. outbound http gateway. Example implementations stream create name foo definition trigger rest reply timeout 1 url http earthquake.usgs.gov earthquakes feed geojson all day log stream create name foos definition trigger payload lat 34.0567006 lon 84.34368810000001 site all smap 1 searchresult Roswell 2C 20GA 2030076 2C 20USA .UktzaWSG1Dd rest url http forecast.weather.gov MapClick.php? log ",NULL,"Offers the functionality to make http request to a web service. i.e. outbound http gateway. Example implementations stream create name foo definition trigger rest reply timeout 1 url http earthquake.usgs.gov earthquakes feed geojson all day log stream create name foos definition trigger payload lat 34.0567006 lon 84.34368810000001 site all smap 1 searchresult Roswell 2C 20GA 2030076 2C 20USA .UktzaWSG1Dd rest url http forecast.weather.gov MapClick.php? log ",NULL,"Offers the functionality to make http request to a web service<span class='highlight-text severity-high'>. i.e. outbound http gateway. Example implementations stream create name foo definition trigger rest reply timeout 1 url http earthquake.usgs.gov earthquakes feed geojson all day log stream create name foos definition trigger payload lat 34.0567006 lon 84.34368810000001 site all smap 1 searchresult Roswell 2C 20GA 2030076 2C 20USA .UktzaWSG1Dd rest url http forecast.weather.gov MapClick.php? log </span>","minimal","punctuation","high",False
19295,"If the spring batch database has already been initialized do not re initialize for each test run. ",NULL,"If the spring batch database has already been initialized do not re initialize for each test run. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19294,"Add Batch Job Listeners Automatically Each major listener category should send notifications to own channel StepExecution, Chunk, Item etc. Add attribute to disallow automatic adding of listeners",NULL,"Add Batch Job Listeners Automatically Each major listener category should send notifications to own channel StepExecution, Chunk, Item etc. Add attribute to disallow automatic adding of listeners",NULL,"Add for who this story is","well_formed","no_role","high",False
19294,"Add Batch Job Listeners Automatically Each major listener category should send notifications to own channel StepExecution, Chunk, Item etc. Add attribute to disallow automatic adding of listeners",NULL,"Add Batch Job Listeners Automatically Each major listener category should send notifications to own channel StepExecution, Chunk, Item etc. Add attribute to disallow automatic adding of listeners",NULL,"Add Batch Job Listeners Automatically Each major listener category should send notifications to own channel StepExecution, Chunk, Item etc<span class='highlight-text severity-high'>. Add attribute to disallow automatic adding of listeners</span>","minimal","punctuation","high",False
19297,"Set the enableJmx to false because contexts are not getting destroyed properly, and in some cases prevents testSystemPropertiesOverridesDefault from running successfully.",NULL,"Set the enableJmx to false because contexts are not getting destroyed properly, and in some cases prevents testSystemPropertiesOverridesDefault from running successfully.",NULL,"Add for who this story is","well_formed","no_role","high",False
19297,"Set the enableJmx to false because contexts are not getting destroyed properly, and in some cases prevents testSystemPropertiesOverridesDefault from running successfully.",NULL,"Set the enableJmx to false because contexts are not getting destroyed properly, and in some cases prevents testSystemPropertiesOverridesDefault from running successfully.",NULL,"Set the enableJmx to false because contexts are not getting destroyed properly,<span class='highlight-text severity-high'> and </span>in some cases prevents testSystemPropertiesOverridesDefault from running successfully.","atomic","conjunctions","high",False
19298,"We should consider moving to wire.js to encourage dependency injection in the UI Javascript code. See here https github.com cujojs wire blob master docs get.md",NULL,"We should consider moving to wire.js to encourage dependency injection in the UI Javascript code. See here https github.com cujojs wire blob master docs get.md",NULL,"Add for who this story is","well_formed","no_role","high",False
19298,"We should consider moving to wire.js to encourage dependency injection in the UI Javascript code. See here https github.com cujojs wire blob master docs get.md",NULL,"We should consider moving to wire.js to encourage dependency injection in the UI Javascript code. See here https github.com cujojs wire blob master docs get.md",NULL,"We should consider moving to wire<span class='highlight-text severity-high'>.js to encourage dependency injection in the UI Javascript code. See here https github.com cujojs wire blob master docs get.md</span>","minimal","punctuation","high",False
19291,"We probably need to look into some options to run our JavaScript tests Jasmine as part of the build process some possibilities Jasmine Gradle Plugin https github.com dzhaughnroth jasmine gradle plugin Saga http timurstrekalov.github.io saga Looks like Maven has slightly better support http searls.github.io jasmine maven plugin index.html See also XD 865",NULL,"We probably need to look into some options to run our JavaScript tests Jasmine as part of the build process some possibilities Jasmine Gradle Plugin https github.com dzhaughnroth jasmine gradle plugin Saga http timurstrekalov.github.io saga Looks like Maven has slightly better support http searls.github.io jasmine maven plugin index.html See also XD 865",NULL,"Add for who this story is","well_formed","no_role","high",False
19289,"Currently, for adhoc launching of Batch jobs you have to use code stream create name myTriggerStream definition trigger job helloSpringXD code For renewed triggering of the job you have to undeploy and then redeploy the job. It would be nice if there was possibly a slightly simpler way of doing this. Just FYI As a different approach you can also use the HTTP source source job create name myjob definition myjob stream create name myjobhttp definition http job http http post data source ",NULL,"Currently, for adhoc launching of Batch jobs you have to use code stream create name myTriggerStream definition trigger job helloSpringXD code For renewed triggering of the job you have to undeploy and then redeploy the job. It would be nice if there was possibly a slightly simpler way of doing this. Just FYI As a different approach you can also use the HTTP source source job create name myjob definition myjob stream create name myjobhttp definition http job http http post data source ",NULL,"Add for who this story is","well_formed","no_role","high",False
19289,"Currently, for adhoc launching of Batch jobs you have to use code stream create name myTriggerStream definition trigger job helloSpringXD code For renewed triggering of the job you have to undeploy and then redeploy the job. It would be nice if there was possibly a slightly simpler way of doing this. Just FYI As a different approach you can also use the HTTP source source job create name myjob definition myjob stream create name myjobhttp definition http job http http post data source ",NULL,"Currently, for adhoc launching of Batch jobs you have to use code stream create name myTriggerStream definition trigger job helloSpringXD code For renewed triggering of the job you have to undeploy and then redeploy the job. It would be nice if there was possibly a slightly simpler way of doing this. Just FYI As a different approach you can also use the HTTP source source job create name myjob definition myjob stream create name myjobhttp definition http job http http post data source ",NULL,"Currently, for adhoc launching of Batch jobs you have to use code stream create name myTriggerStream definition trigger job helloSpringXD code For renewed triggering of the job you have to undeploy<span class='highlight-text severity-high'> and </span>then redeploy the job. It would be nice if there was possibly a slightly simpler way of doing this. Just FYI As a different approach you can also use the HTTP source source job create name myjob definition myjob stream create name myjobhttp definition http job http http post data source ","atomic","conjunctions","high",False
19289,"Currently, for adhoc launching of Batch jobs you have to use code stream create name myTriggerStream definition trigger job helloSpringXD code For renewed triggering of the job you have to undeploy and then redeploy the job. It would be nice if there was possibly a slightly simpler way of doing this. Just FYI As a different approach you can also use the HTTP source source job create name myjob definition myjob stream create name myjobhttp definition http job http http post data source ",NULL,"Currently, for adhoc launching of Batch jobs you have to use code stream create name myTriggerStream definition trigger job helloSpringXD code For renewed triggering of the job you have to undeploy and then redeploy the job. It would be nice if there was possibly a slightly simpler way of doing this. Just FYI As a different approach you can also use the HTTP source source job create name myjob definition myjob stream create name myjobhttp definition http job http http post data source ",NULL,"Currently, for adhoc launching of Batch jobs you have to use code stream create name myTriggerStream definition trigger job helloSpringXD code For renewed triggering of the job you have to undeploy and then redeploy the job<span class='highlight-text severity-high'>. It would be nice if there was possibly a slightly simpler way of doing this. Just FYI As a different approach you can also use the HTTP source source job create name myjob definition myjob stream create name myjobhttp definition http job http http post data source </span>","minimal","punctuation","high",False
19292,"This conflicts with and out of the box hadoop installation that uses 8080 as the map reduce shuffle port . 8088 sound ok? ",NULL,"This conflicts with and out of the box hadoop installation that uses 8080 as the map reduce shuffle port . 8088 sound ok? ",NULL,"Add for who this story is","well_formed","no_role","high",False
19292,"This conflicts with and out of the box hadoop installation that uses 8080 as the map reduce shuffle port . 8088 sound ok? ",NULL,"This conflicts with and out of the box hadoop installation that uses 8080 as the map reduce shuffle port . 8088 sound ok? ",NULL,"This conflicts with and out of the box hadoop installation that uses 8080 as the map reduce shuffle port <span class='highlight-text severity-high'>. 8088 sound ok? </span>","minimal","punctuation","high",False
19290,"When clicking on a specific job execution from the job executions bar chart, the tool tips display isn t aligned with the job parameters. Please see the attachment.",NULL,"When clicking on a specific job execution from the job executions bar chart, the tool tips display isn t aligned with the job parameters. Please see the attachment.",NULL,"Add for who this story is","well_formed","no_role","high",False
19290,"When clicking on a specific job execution from the job executions bar chart, the tool tips display isn t aligned with the job parameters. Please see the attachment.",NULL,"When clicking on a specific job execution from the job executions bar chart, the tool tips display isn t aligned with the job parameters. Please see the attachment.",NULL,"When clicking on a specific job execution from the job executions bar chart, the tool tips display isn t aligned with the job parameters<span class='highlight-text severity-high'>. Please see the attachment.</span>","minimal","punctuation","high",False
19296,"In our tests the context is destroyed at the end of each module. It should be destroyed at the close of the container.",NULL,"In our tests the context is destroyed at the end of each module. It should be destroyed at the close of the container.",NULL,"Add for who this story is","well_formed","no_role","high",False
19296,"In our tests the context is destroyed at the end of each module. It should be destroyed at the close of the container.",NULL,"In our tests the context is destroyed at the end of each module. It should be destroyed at the close of the container.",NULL,"In our tests the context is destroyed at the end of each module<span class='highlight-text severity-high'>. It should be destroyed at the close of the container.</span>","minimal","punctuation","high",False
19299,"Currently, the data directory created by the HSQLDB process during the tests run is not cleaned up and may cause issues. We should delete the data directory after each test completion.",NULL,"Currently, the data directory created by the HSQLDB process during the tests run is not cleaned up and may cause issues. We should delete the data directory after each test completion.",NULL,"Add for who this story is","well_formed","no_role","high",False
19426,"String byte string.getBytes byte byte no serialization Pojo configured serialization ",NULL,"String byte string.getBytes byte byte no serialization Pojo configured serialization ",NULL,"Add for who this story is","well_formed","no_role","high",False
19299,"Currently, the data directory created by the HSQLDB process during the tests run is not cleaned up and may cause issues. We should delete the data directory after each test completion.",NULL,"Currently, the data directory created by the HSQLDB process during the tests run is not cleaned up and may cause issues. We should delete the data directory after each test completion.",NULL,"Currently, the data directory created by the HSQLDB process during the tests run is not cleaned up<span class='highlight-text severity-high'> and </span>may cause issues. We should delete the data directory after each test completion.","atomic","conjunctions","high",False
19299,"Currently, the data directory created by the HSQLDB process during the tests run is not cleaned up and may cause issues. We should delete the data directory after each test completion.",NULL,"Currently, the data directory created by the HSQLDB process during the tests run is not cleaned up and may cause issues. We should delete the data directory after each test completion.",NULL,"Currently, the data directory created by the HSQLDB process during the tests run is not cleaned up and may cause issues<span class='highlight-text severity-high'>. We should delete the data directory after each test completion.</span>","minimal","punctuation","high",False
19302,"This story may need to be broken into several stories Particularly for Batch scenarios one may not want to run a file to string transformer on the payload file in the file source but rather handle pass the file reference itself local SAN etc. e.g. in case somebody drops a 2GB or in scenarios where one wants to push those large files into HDFS and run hadoop jobs on the data. This is important for Batch Jobs as they need to access the file itself for the reader. We need to keep in mind the various transports we support . Not sure how Kryo handles file serialization. I would think we only need the File Meta Data to be persisted not the file data itself make that configurable?? . ",NULL,"This story may need to be broken into several stories Particularly for Batch scenarios one may not want to run a file to string transformer on the payload file in the file source but rather handle pass the file reference itself local SAN etc. e.g. in case somebody drops a 2GB or in scenarios where one wants to push those large files into HDFS and run hadoop jobs on the data. This is important for Batch Jobs as they need to access the file itself for the reader. We need to keep in mind the various transports we support . Not sure how Kryo handles file serialization. I would think we only need the File Meta Data to be persisted not the file data itself make that configurable?? . ",NULL,"Add for who this story is","well_formed","no_role","high",False
19302,"This story may need to be broken into several stories Particularly for Batch scenarios one may not want to run a file to string transformer on the payload file in the file source but rather handle pass the file reference itself local SAN etc. e.g. in case somebody drops a 2GB or in scenarios where one wants to push those large files into HDFS and run hadoop jobs on the data. This is important for Batch Jobs as they need to access the file itself for the reader. We need to keep in mind the various transports we support . Not sure how Kryo handles file serialization. I would think we only need the File Meta Data to be persisted not the file data itself make that configurable?? . ",NULL,"This story may need to be broken into several stories Particularly for Batch scenarios one may not want to run a file to string transformer on the payload file in the file source but rather handle pass the file reference itself local SAN etc. e.g. in case somebody drops a 2GB or in scenarios where one wants to push those large files into HDFS and run hadoop jobs on the data. This is important for Batch Jobs as they need to access the file itself for the reader. We need to keep in mind the various transports we support . Not sure how Kryo handles file serialization. I would think we only need the File Meta Data to be persisted not the file data itself make that configurable?? . ",NULL,"This story may need to be broken into several stories Particularly for Batch scenarios one may not want to run a file to string transformer on the payload file in the file source but rather handle pass the file reference itself local SAN etc. e.g. in case somebody drops a 2GB<span class='highlight-text severity-high'> or </span>in scenarios where one wants to push those large files into HDFS<span class='highlight-text severity-high'> and </span>run hadoop jobs on the data. This is important for Batch Jobs as they need to access the file itself for the reader. We need to keep in mind the various transports we support . Not sure how Kryo handles file serialization. I would think we only need the File Meta Data to be persisted not the file data itself make that configurable?? . ","atomic","conjunctions","high",False
19302,"This story may need to be broken into several stories Particularly for Batch scenarios one may not want to run a file to string transformer on the payload file in the file source but rather handle pass the file reference itself local SAN etc. e.g. in case somebody drops a 2GB or in scenarios where one wants to push those large files into HDFS and run hadoop jobs on the data. This is important for Batch Jobs as they need to access the file itself for the reader. We need to keep in mind the various transports we support . Not sure how Kryo handles file serialization. I would think we only need the File Meta Data to be persisted not the file data itself make that configurable?? . ",NULL,"This story may need to be broken into several stories Particularly for Batch scenarios one may not want to run a file to string transformer on the payload file in the file source but rather handle pass the file reference itself local SAN etc. e.g. in case somebody drops a 2GB or in scenarios where one wants to push those large files into HDFS and run hadoop jobs on the data. This is important for Batch Jobs as they need to access the file itself for the reader. We need to keep in mind the various transports we support . Not sure how Kryo handles file serialization. I would think we only need the File Meta Data to be persisted not the file data itself make that configurable?? . ",NULL,"This story may need to be broken into several stories Particularly for Batch scenarios one may not want to run a file to string transformer on the payload file in the file source but rather handle pass the file reference itself local SAN etc<span class='highlight-text severity-high'>. e.g. in case somebody drops a 2GB or in scenarios where one wants to push those large files into HDFS and run hadoop jobs on the data. This is important for Batch Jobs as they need to access the file itself for the reader. We need to keep in mind the various transports we support . Not sure how Kryo handles file serialization. I would think we only need the File Meta Data to be persisted not the file data itself make that configurable?? . </span>","minimal","punctuation","high",False
19308,"Get rid of all the thread.sleeps and code that supported them.",NULL,"Get rid of all the thread.sleeps and code that supported them.",NULL,"Add for who this story is","well_formed","no_role","high",False
19308,"Get rid of all the thread.sleeps and code that supported them.",NULL,"Get rid of all the thread.sleeps and code that supported them.",NULL,"Get rid of all the thread.sleeps<span class='highlight-text severity-high'> and </span>code that supported them.","atomic","conjunctions","high",False
19303,"Just wanted to create story for this so we can consider whether this should be addressed. In at least 2 modules we use non persisted states. We may want to consider making them persistent Twitter Search uses an in memory MetadataStore that keeps track of the twitter ids. There exists a corresponding issue for Spring Integration Create a Redis backed MetadataStore See https jira.springsource.org browse INT 3085 File Soure s File Inbound Channel Adapter uses a AcceptOnceFileListFilter, which uses an in memory Queue to keep track of duplicate files. ",NULL,"Just wanted to create story for this","so we can consider whether this should be addressed. In at least 2 modules we use non persisted states. We may want to consider making them persistent Twitter Search uses an in memory MetadataStore that keeps track of the twitter ids. There exists a corresponding issue for Spring Integration Create a Redis backed MetadataStore See https jira.springsource.org browse INT 3085 File Soure s File Inbound Channel Adapter uses a AcceptOnceFileListFilter, which uses an in memory Queue to keep track of duplicate files.","Add for who this story is","well_formed","no_role","high",False
19303,"Just wanted to create story for this so we can consider whether this should be addressed. In at least 2 modules we use non persisted states. We may want to consider making them persistent Twitter Search uses an in memory MetadataStore that keeps track of the twitter ids. There exists a corresponding issue for Spring Integration Create a Redis backed MetadataStore See https jira.springsource.org browse INT 3085 File Soure s File Inbound Channel Adapter uses a AcceptOnceFileListFilter, which uses an in memory Queue to keep track of duplicate files. ",NULL,"Just wanted to create story for this","so we can consider whether this should be addressed. In at least 2 modules we use non persisted states. We may want to consider making them persistent Twitter Search uses an in memory MetadataStore that keeps track of the twitter ids. There exists a corresponding issue for Spring Integration Create a Redis backed MetadataStore See https jira.springsource.org browse INT 3085 File Soure s File Inbound Channel Adapter uses a AcceptOnceFileListFilter, which uses an in memory Queue to keep track of duplicate files.","Just wanted to create story for this so we can consider whether this should be addressed<span class='highlight-text severity-high'>. In at least 2 modules we use non persisted states. We may want to consider making them persistent Twitter Search uses an in memory MetadataStore that keeps track of the twitter ids. There exists a corresponding issue for Spring Integration Create a Redis backed MetadataStore See https jira.springsource.org browse INT 3085 File Soure s File Inbound Channel Adapter uses a AcceptOnceFileListFilter, which uses an in memory Queue to keep track of duplicate files. </span>","minimal","punctuation","high",False
19303,"Just wanted to create story for this so we can consider whether this should be addressed. In at least 2 modules we use non persisted states. We may want to consider making them persistent Twitter Search uses an in memory MetadataStore that keeps track of the twitter ids. There exists a corresponding issue for Spring Integration Create a Redis backed MetadataStore See https jira.springsource.org browse INT 3085 File Soure s File Inbound Channel Adapter uses a AcceptOnceFileListFilter, which uses an in memory Queue to keep track of duplicate files. ",NULL,"Just wanted to create story for this","so we can consider whether this should be addressed. In at least 2 modules we use non persisted states. We may want to consider making them persistent Twitter Search uses an in memory MetadataStore that keeps track of the twitter ids. There exists a corresponding issue for Spring Integration Create a Redis backed MetadataStore See https jira.springsource.org browse INT 3085 File Soure s File Inbound Channel Adapter uses a AcceptOnceFileListFilter, which uses an in memory Queue to keep track of duplicate files.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19306,"We need to verify that we are seeing improved throughput when using the reactor based syslog adapter. A suggestion on a basic stream to perform a microbenchmark this would be using in memory counters, singlenode with the stream definition syslog counter . Based on the results of this microbenchmark, other stories may need to be created.",NULL,"We need to verify that we are seeing improved throughput when using the reactor based syslog adapter. A suggestion on a basic stream to perform a microbenchmark this would be using in memory counters, singlenode with the stream definition syslog counter . Based on the results of this microbenchmark, other stories may need to be created.",NULL,"Add for who this story is","well_formed","no_role","high",False
19306,"We need to verify that we are seeing improved throughput when using the reactor based syslog adapter. A suggestion on a basic stream to perform a microbenchmark this would be using in memory counters, singlenode with the stream definition syslog counter . Based on the results of this microbenchmark, other stories may need to be created.",NULL,"We need to verify that we are seeing improved throughput when using the reactor based syslog adapter. A suggestion on a basic stream to perform a microbenchmark this would be using in memory counters, singlenode with the stream definition syslog counter . Based on the results of this microbenchmark, other stories may need to be created.",NULL,"We need to verify that we are seeing improved throughput when using the reactor based syslog adapter<span class='highlight-text severity-high'>. A suggestion on a basic stream to perform a microbenchmark this would be using in memory counters, singlenode with the stream definition syslog counter . Based on the results of this microbenchmark, other stories may need to be created.</span>","minimal","punctuation","high",False
19548,"Support toString to emit JSON by default. Should be backed by a simple strategy to allow the possibility of other representations. Also provide toTuple String json . This supports seamless mapping JSON Tuple in XD",NULL,"Support toString to emit JSON by default. Should be backed by a simple strategy to allow the possibility of other representations. Also provide toTuple String json . This supports seamless mapping JSON Tuple in XD",NULL,"Add for who this story is","well_formed","no_role","high",False
19548,"Support toString to emit JSON by default. Should be backed by a simple strategy to allow the possibility of other representations. Also provide toTuple String json . This supports seamless mapping JSON Tuple in XD",NULL,"Support toString to emit JSON by default. Should be backed by a simple strategy to allow the possibility of other representations. Also provide toTuple String json . This supports seamless mapping JSON Tuple in XD",NULL,"Support toString to emit JSON by default<span class='highlight-text severity-high'>. Should be backed by a simple strategy to allow the possibility of other representations. Also provide toTuple String json . This supports seamless mapping JSON Tuple in XD</span>","minimal","punctuation","high",False
19310,"We need to move the BatchJobExecutionsByJobName method to BatchJobsController as that seems appropriate",NULL,"We need to move the BatchJobExecutionsByJobName method to BatchJobsController as that seems appropriate",NULL,"Add for who this story is","well_formed","no_role","high",False
19311,"Needs some investigation on how the update information from Spring Batch listeners can be sent as a message across modules in a single node configuration as well as across JVMs in a distributed node configuration.",NULL,"Needs some investigation on how the update information from Spring Batch listeners can be sent as a message across modules in a single node configuration as well as across JVMs in a distributed node configuration.",NULL,"Add for who this story is","well_formed","no_role","high",False
19549,"A pause means that a trigger will wait to fire its job until after the pause is removed. It does not apply the misfire behavior. ",NULL,"A pause means that a trigger will wait to fire its job until after the pause is removed. It does not apply the misfire behavior. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19549,"A pause means that a trigger will wait to fire its job until after the pause is removed. It does not apply the misfire behavior. ",NULL,"A pause means that a trigger will wait to fire its job until after the pause is removed. It does not apply the misfire behavior. ",NULL,"A pause means that a trigger will wait to fire its job until after the pause is removed<span class='highlight-text severity-high'>. It does not apply the misfire behavior. </span>","minimal","punctuation","high",False
19305,"i.e. http group1 filter group1 transform file then specifying anything labelled group1 goes to machineX",NULL,"i.e. http group1 filter group1 transform file then specifying anything labelled group1 goes to machineX",NULL,"Add for who this story is","well_formed","no_role","high",False
19305,"i.e. http group1 filter group1 transform file then specifying anything labelled group1 goes to machineX",NULL,"i.e. http group1 filter group1 transform file then specifying anything labelled group1 goes to machineX",NULL,"i<span class='highlight-text severity-high'>.e. http group1 filter group1 transform file then specifying anything labelled group1 goes to machineX</span>","minimal","punctuation","high",False
19307,"Parser creates a module compose command that allows a user to create a module of other modules. This composed module can accept parameters.",NULL,"Parser creates a module compose command that allows a user to create a module of other modules. This composed module can accept parameters.",NULL,"Add for who this story is","well_formed","no_role","high",False
19307,"Parser creates a module compose command that allows a user to create a module of other modules. This composed module can accept parameters.",NULL,"Parser creates a module compose command that allows a user to create a module of other modules. This composed module can accept parameters.",NULL,"Parser creates a module compose command that allows a user to create a module of other modules<span class='highlight-text severity-high'>. This composed module can accept parameters.</span>","minimal","punctuation","high",False
19550,"Commonly called Calendar support ",NULL,"Commonly called Calendar support ",NULL,"Add for who this story is","well_formed","no_role","high",False
19309,"The admin ui currently has no unit tests. Need to add a test suite and hook it up to the build so that tests are run on every build.",NULL,"The admin ui currently has no unit tests. Need to add a test suite and hook it up to the build","so that tests are run on every build.","Add for who this story is","well_formed","no_role","high",False
19309,"The admin ui currently has no unit tests. Need to add a test suite and hook it up to the build so that tests are run on every build.",NULL,"The admin ui currently has no unit tests. Need to add a test suite and hook it up to the build","so that tests are run on every build.","The admin ui currently has no unit tests. Need to add a test suite<span class='highlight-text severity-high'> and </span>hook it up to the build so that tests are run on every build.","atomic","conjunctions","high",False
19309,"The admin ui currently has no unit tests. Need to add a test suite and hook it up to the build so that tests are run on every build.",NULL,"The admin ui currently has no unit tests. Need to add a test suite and hook it up to the build","so that tests are run on every build.","The admin ui currently has no unit tests<span class='highlight-text severity-high'>. Need to add a test suite and hook it up to the build so that tests are run on every build.</span>","minimal","punctuation","high",False
19309,"The admin ui currently has no unit tests. Need to add a test suite and hook it up to the build so that tests are run on every build.",NULL,"The admin ui currently has no unit tests. Need to add a test suite and hook it up to the build","so that tests are run on every build.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19319,"Wildcard support for associating inbound and outbound channels with modules. The wild card will be represented by an asterisk . Example myEmailSource tap job send message to all jobs myEmailSource tap send message to all stream job taps myEmailSource foo send message to all channels that contain the channels that contains the word foo tap bar myEmailSource ",NULL,"Wildcard support for associating inbound and outbound channels with modules. The wild card will be represented by an asterisk . Example myEmailSource tap job send message to all jobs myEmailSource tap send message to all stream job taps myEmailSource foo send message to all channels that contain the channels that contains the word foo tap bar myEmailSource ",NULL,"Add for who this story is","well_formed","no_role","high",False
19319,"Wildcard support for associating inbound and outbound channels with modules. The wild card will be represented by an asterisk . Example myEmailSource tap job send message to all jobs myEmailSource tap send message to all stream job taps myEmailSource foo send message to all channels that contain the channels that contains the word foo tap bar myEmailSource ",NULL,"Wildcard support for associating inbound and outbound channels with modules. The wild card will be represented by an asterisk . Example myEmailSource tap job send message to all jobs myEmailSource tap send message to all stream job taps myEmailSource foo send message to all channels that contain the channels that contains the word foo tap bar myEmailSource ",NULL,"Wildcard support for associating inbound and outbound channels with modules<span class='highlight-text severity-high'>. The wild card will be represented by an asterisk . Example myEmailSource tap job send message to all jobs myEmailSource tap send message to all stream job taps myEmailSource foo send message to all channels that contain the channels that contains the word foo tap bar myEmailSource </span>","minimal","punctuation","high",False
19316,"Similar to what has been done for e.g. FileSink, refactor metrics related sinks to use smart Thread.sleep timings",NULL,"Similar to what has been done for e.g. FileSink, refactor metrics related sinks to use smart Thread.sleep timings",NULL,"Add for who this story is","well_formed","no_role","high",False
19316,"Similar to what has been done for e.g. FileSink, refactor metrics related sinks to use smart Thread.sleep timings",NULL,"Similar to what has been done for e.g. FileSink, refactor metrics related sinks to use smart Thread.sleep timings",NULL,"Similar to what has been done for e<span class='highlight-text severity-high'>.g. FileSink, refactor metrics related sinks to use smart Thread.sleep timings</span>","minimal","punctuation","high",False
19317,"The doc at http docs.spring.io spring xd docs 1.0.0.M3 reference html modules and spring refers to an old version of the counter sink, when it was still hardwired to use redis. The text next to it that explains placeholders is out of date with respect to the redis placeholders ",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19317,"The doc at http docs.spring.io spring xd docs 1.0.0.M3 reference html modules and spring refers to an old version of the counter sink, when it was still hardwired to use redis. The text next to it that explains placeholders is out of date with respect to the redis placeholders ",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19317,"The doc at http docs.spring.io spring xd docs 1.0.0.M3 reference html modules and spring refers to an old version of the counter sink, when it was still hardwired to use redis. The text next to it that explains placeholders is out of date with respect to the redis placeholders ",NULL,NULL,NULL,"The doc at http docs<span class='highlight-text severity-high'>.spring.io spring xd docs 1.0.0.M3 reference html modules and spring refers to an old version of the counter sink, when it was still hardwired to use redis. The text next to it that explains placeholders is out of date with respect to the redis placeholders </span>","minimal","punctuation","high",False
19400,"code filter expression payload.myfield.startsWith foo code Example using index code filter expression payload.2.startsWith foo code This should support nested keys as well code filter expression payload.myfield.subfield.startsWith foo code This is related to https jira.springsource.org browse XD 676 and that in turn depends on SI being able to configure SpEL",NULL,"code filter expression payload.myfield.startsWith foo code Example using index code filter expression payload.2.startsWith foo code This should support nested keys as well code filter expression payload.myfield.subfield.startsWith foo code This is related to https jira.springsource.org browse XD 676 and that in turn depends on SI being able to configure SpEL",NULL,"Add for who this story is","well_formed","no_role","high",False
19400,"code filter expression payload.myfield.startsWith foo code Example using index code filter expression payload.2.startsWith foo code This should support nested keys as well code filter expression payload.myfield.subfield.startsWith foo code This is related to https jira.springsource.org browse XD 676 and that in turn depends on SI being able to configure SpEL",NULL,"code filter expression payload.myfield.startsWith foo code Example using index code filter expression payload.2.startsWith foo code This should support nested keys as well code filter expression payload.myfield.subfield.startsWith foo code This is related to https jira.springsource.org browse XD 676 and that in turn depends on SI being able to configure SpEL",NULL,"code filter expression payload.myfield.startsWith foo code Example using index code filter expression payload.2.startsWith foo code This should support nested keys as well code filter expression payload.myfield.subfield.startsWith foo code This is related to https jira.springsource.org browse XD 676<span class='highlight-text severity-high'> and </span>that in turn depends on SI being able to configure SpEL","atomic","conjunctions","high",False
19314,"From the CLI, one should be able to get a listing of modules and be able to specifically ask for jobs, sources, sinks, and processors. A brief description of them would also be nice this might come from adding some metadata into the definition. Finer grained description implementation suggestion TBD",NULL,"From the CLI, one should be able to get a listing of modules and be able to specifically ask for jobs, sources, sinks, and processors. A brief description of them would also be nice this might come from adding some metadata into the definition. Finer grained description implementation suggestion TBD",NULL,"Add for who this story is","well_formed","no_role","high",False
19314,"From the CLI, one should be able to get a listing of modules and be able to specifically ask for jobs, sources, sinks, and processors. A brief description of them would also be nice this might come from adding some metadata into the definition. Finer grained description implementation suggestion TBD",NULL,"From the CLI, one should be able to get a listing of modules and be able to specifically ask for jobs, sources, sinks, and processors. A brief description of them would also be nice this might come from adding some metadata into the definition. Finer grained description implementation suggestion TBD",NULL,"From the CLI, one should be able to get a listing of modules<span class='highlight-text severity-high'> and </span>be able to specifically ask for jobs, sources, sinks, and processors. A brief description of them would also be nice this might come from adding some metadata into the definition. Finer grained description implementation suggestion TBD","atomic","conjunctions","high",False
19314,"From the CLI, one should be able to get a listing of modules and be able to specifically ask for jobs, sources, sinks, and processors. A brief description of them would also be nice this might come from adding some metadata into the definition. Finer grained description implementation suggestion TBD",NULL,"From the CLI, one should be able to get a listing of modules and be able to specifically ask for jobs, sources, sinks, and processors. A brief description of them would also be nice this might come from adding some metadata into the definition. Finer grained description implementation suggestion TBD",NULL,"From the CLI, one should be able to get a listing of modules and be able to specifically ask for jobs, sources, sinks, and processors<span class='highlight-text severity-high'>. A brief description of them would also be nice this might come from adding some metadata into the definition. Finer grained description implementation suggestion TBD</span>","minimal","punctuation","high",False
19312,"Similar to xpath with XML, there are now some initial support in SI that enable the use of filter routers based on JSON Path expressions. That support needs to be reviewed and then brought up to the level of exposure in Spring XD so that router filter modules could use JSON Path. json path filter router are components that need to be created, perhaps others. This story needs to be broken down further.",NULL,"Similar to xpath with XML, there are now some initial support in SI that enable the use of filter routers based on JSON Path expressions. That support needs to be reviewed and then brought up to the level of exposure in Spring XD","so that router filter modules could use JSON Path. json path filter router are components that need to be created, perhaps others. This story needs to be broken down further.","Add for who this story is","well_formed","no_role","high",False
19312,"Similar to xpath with XML, there are now some initial support in SI that enable the use of filter routers based on JSON Path expressions. That support needs to be reviewed and then brought up to the level of exposure in Spring XD so that router filter modules could use JSON Path. json path filter router are components that need to be created, perhaps others. This story needs to be broken down further.",NULL,"Similar to xpath with XML, there are now some initial support in SI that enable the use of filter routers based on JSON Path expressions. That support needs to be reviewed and then brought up to the level of exposure in Spring XD","so that router filter modules could use JSON Path. json path filter router are components that need to be created, perhaps others. This story needs to be broken down further.","Similar to xpath with XML, there are now some initial support in SI that enable the use of filter routers based on JSON Path expressions. That support needs to be reviewed<span class='highlight-text severity-high'> and </span>then brought up to the level of exposure in Spring XD so that router filter modules could use JSON Path. json path filter router are components that need to be created, perhaps others. This story needs to be broken down further.","atomic","conjunctions","high",False
19312,"Similar to xpath with XML, there are now some initial support in SI that enable the use of filter routers based on JSON Path expressions. That support needs to be reviewed and then brought up to the level of exposure in Spring XD so that router filter modules could use JSON Path. json path filter router are components that need to be created, perhaps others. This story needs to be broken down further.",NULL,"Similar to xpath with XML, there are now some initial support in SI that enable the use of filter routers based on JSON Path expressions. That support needs to be reviewed and then brought up to the level of exposure in Spring XD","so that router filter modules could use JSON Path. json path filter router are components that need to be created, perhaps others. This story needs to be broken down further.","Similar to xpath with XML, there are now some initial support in SI that enable the use of filter routers based on JSON Path expressions<span class='highlight-text severity-high'>. That support needs to be reviewed and then brought up to the level of exposure in Spring XD so that router filter modules could use JSON Path. json path filter router are components that need to be created, perhaps others. This story needs to be broken down further.</span>","minimal","punctuation","high",False
19312,"Similar to xpath with XML, there are now some initial support in SI that enable the use of filter routers based on JSON Path expressions. That support needs to be reviewed and then brought up to the level of exposure in Spring XD so that router filter modules could use JSON Path. json path filter router are components that need to be created, perhaps others. This story needs to be broken down further.",NULL,"Similar to xpath with XML, there are now some initial support in SI that enable the use of filter routers based on JSON Path expressions. That support needs to be reviewed and then brought up to the level of exposure in Spring XD","so that router filter modules could use JSON Path. json path filter router are components that need to be created, perhaps others. This story needs to be broken down further.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19315,"Apart from sanity checks, there is not much that ties FileModuleRegistry to actual java.io.Files. Using the Resource abstraction would work just the same, and would allow loading modules from the classpath in constrained environments or other file systems locations. HDFS HTTP ",NULL,"Apart from sanity checks, there is not much that ties FileModuleRegistry to actual java.io.Files. Using the Resource abstraction would work just the same, and would allow loading modules from the classpath in constrained environments or other file systems locations. HDFS HTTP ",NULL,"Add for who this story is","well_formed","no_role","high",False
19315,"Apart from sanity checks, there is not much that ties FileModuleRegistry to actual java.io.Files. Using the Resource abstraction would work just the same, and would allow loading modules from the classpath in constrained environments or other file systems locations. HDFS HTTP ",NULL,"Apart from sanity checks, there is not much that ties FileModuleRegistry to actual java.io.Files. Using the Resource abstraction would work just the same, and would allow loading modules from the classpath in constrained environments or other file systems locations. HDFS HTTP ",NULL,"Apart from sanity checks, there is not much that ties FileModuleRegistry to actual java.io.Files. Using the Resource abstraction would work just the same,<span class='highlight-text severity-high'> and </span>would allow loading modules from the classpath in constrained environments<span class='highlight-text severity-high'> or </span>other file systems locations. HDFS HTTP ","atomic","conjunctions","high",False
19315,"Apart from sanity checks, there is not much that ties FileModuleRegistry to actual java.io.Files. Using the Resource abstraction would work just the same, and would allow loading modules from the classpath in constrained environments or other file systems locations. HDFS HTTP ",NULL,"Apart from sanity checks, there is not much that ties FileModuleRegistry to actual java.io.Files. Using the Resource abstraction would work just the same, and would allow loading modules from the classpath in constrained environments or other file systems locations. HDFS HTTP ",NULL,"Apart from sanity checks, there is not much that ties FileModuleRegistry to actual java<span class='highlight-text severity-high'>.io.Files. Using the Resource abstraction would work just the same, and would allow loading modules from the classpath in constrained environments or other file systems locations. HDFS HTTP </span>","minimal","punctuation","high",False
19318,"Add serialVersionUID to the objects in package org.springframework.integration.x.twitter XDEntities XDUrlEntity XDHashTagEntity XDMentionEntity XDMediaEntity XDTickerSymbolEntity XDTweet The absence creates warnings during compile time.",NULL,"Add serialVersionUID to the objects in package org.springframework.integration.x.twitter XDEntities XDUrlEntity XDHashTagEntity XDMentionEntity XDMediaEntity XDTickerSymbolEntity XDTweet The absence creates warnings during compile time.",NULL,"Add for who this story is","well_formed","no_role","high",False
19322,"We should adjust our hadoopDistro options to the ones supported in the new spring data hadoop 1.0.1.RELEASE hadoop12 default , cdh4, hdp13, phd1, hadoop20 This includes updating the wiki pages",NULL,"We should adjust our hadoopDistro options to the ones supported in the new spring data hadoop 1.0.1.RELEASE hadoop12 default , cdh4, hdp13, phd1, hadoop20 This includes updating the wiki pages",NULL,"Add for who this story is","well_formed","no_role","high",False
19325,"As part of running in Cloud Foundry, one quick workaround for the lack of classpath support would be to use the Spring Boot Loader special ClassLoader and jar inside a jar support https github.com spring projects spring boot tree master spring boot tools spring boot loader","As part","of running in Cloud Foundry, one quick workaround for the lack of classpath support would be to use the Spring Boot Loader special ClassLoader and jar inside a jar support https github.com spring projects spring boot tree master spring boot tools spring boot loader",NULL,"As part of running in Cloud Foundry, one quick workaround for the lack of classpath support would be to use the Spring Boot Loader special ClassLoader<span class='highlight-text severity-high'> and </span>jar inside a jar support https github.com spring projects spring boot tree master spring boot tools spring boot loader","atomic","conjunctions","high",False
19325,"As part of running in Cloud Foundry, one quick workaround for the lack of classpath support would be to use the Spring Boot Loader special ClassLoader and jar inside a jar support https github.com spring projects spring boot tree master spring boot tools spring boot loader","As part","of running in Cloud Foundry, one quick workaround for the lack of classpath support would be to use the Spring Boot Loader special ClassLoader and jar inside a jar support https github.com spring projects spring boot tree master spring boot tools spring boot loader",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19323,"Tap and using numbers instead of module names.",NULL,"Tap and using numbers instead of module names.",NULL,"Add for who this story is","well_formed","no_role","high",False
19328,"In the XD UI, the list of jobs is refreshed from the server every 5 seconds. There are also tooltips that are activated when hovering over a job execution. These tooltips are no longer responsive ie they remain on the page and don t disappear after the list of jobs is refreshed.",NULL,"In the XD UI, the list of jobs is refreshed from the server every 5 seconds. There are also tooltips that are activated when hovering over a job execution. These tooltips are no longer responsive ie they remain on the page and don t disappear after the list of jobs is refreshed.",NULL,"Add for who this story is","well_formed","no_role","high",False
19328,"In the XD UI, the list of jobs is refreshed from the server every 5 seconds. There are also tooltips that are activated when hovering over a job execution. These tooltips are no longer responsive ie they remain on the page and don t disappear after the list of jobs is refreshed.",NULL,"In the XD UI, the list of jobs is refreshed from the server every 5 seconds. There are also tooltips that are activated when hovering over a job execution. These tooltips are no longer responsive ie they remain on the page and don t disappear after the list of jobs is refreshed.",NULL,"In the XD UI, the list of jobs is refreshed from the server every 5 seconds. There are also tooltips that are activated when hovering over a job execution. These tooltips are no longer responsive ie they remain on the page<span class='highlight-text severity-high'> and </span>don t disappear after the list of jobs is refreshed.","atomic","conjunctions","high",False
19328,"In the XD UI, the list of jobs is refreshed from the server every 5 seconds. There are also tooltips that are activated when hovering over a job execution. These tooltips are no longer responsive ie they remain on the page and don t disappear after the list of jobs is refreshed.",NULL,"In the XD UI, the list of jobs is refreshed from the server every 5 seconds. There are also tooltips that are activated when hovering over a job execution. These tooltips are no longer responsive ie they remain on the page and don t disappear after the list of jobs is refreshed.",NULL,"In the XD UI, the list of jobs is refreshed from the server every 5 seconds<span class='highlight-text severity-high'>. There are also tooltips that are activated when hovering over a job execution. These tooltips are no longer responsive ie they remain on the page and don t disappear after the list of jobs is refreshed.</span>","minimal","punctuation","high",False
19326,"First take on this involves being able to deploy the two separate applications xd admin xd container being able to CF service provided redis rabbit for internal needs of XD to some extent, make modules smart and CF aware e.g. http source uses correct port ",NULL,"First take on this involves being able to deploy the two separate applications xd admin xd container being able to CF service provided redis rabbit for internal needs of XD to some extent, make modules smart and CF aware e.g. http source uses correct port ",NULL,"Add for who this story is","well_formed","no_role","high",False
19326,"First take on this involves being able to deploy the two separate applications xd admin xd container being able to CF service provided redis rabbit for internal needs of XD to some extent, make modules smart and CF aware e.g. http source uses correct port ",NULL,"First take on this involves being able to deploy the two separate applications xd admin xd container being able to CF service provided redis rabbit for internal needs of XD to some extent, make modules smart and CF aware e.g. http source uses correct port ",NULL,"First take on this involves being able to deploy the two separate applications xd admin xd container being able to CF service provided redis rabbit for internal needs of XD to some extent, make modules smart<span class='highlight-text severity-high'> and </span>CF aware e.g. http source uses correct port ","atomic","conjunctions","high",False
19326,"First take on this involves being able to deploy the two separate applications xd admin xd container being able to CF service provided redis rabbit for internal needs of XD to some extent, make modules smart and CF aware e.g. http source uses correct port ",NULL,"First take on this involves being able to deploy the two separate applications xd admin xd container being able to CF service provided redis rabbit for internal needs of XD to some extent, make modules smart and CF aware e.g. http source uses correct port ",NULL,"First take on this involves being able to deploy the two separate applications xd admin xd container being able to CF service provided redis rabbit for internal needs of XD to some extent, make modules smart and CF aware e<span class='highlight-text severity-high'>.g. http source uses correct port </span>","minimal","punctuation","high",False
19329,"There are couple of issues here 1 The admin server destroy close event s onApplicationEvent ContextClosedEvent listener has stop to stop the admin server s tomcat instance. The stop also calls the applicationContext s destroy which loops again to stop. 2 With HSQLServer or any batch db server in future , the admin server stop also needs to handle the batch db server shutdown.",NULL,"There are couple of issues here 1 The admin server destroy close event s onApplicationEvent ContextClosedEvent listener has stop to stop the admin server s tomcat instance. The stop also calls the applicationContext s destroy which loops again to stop. 2 With HSQLServer or any batch db server in future , the admin server stop also needs to handle the batch db server shutdown.",NULL,"Add for who this story is","well_formed","no_role","high",False
19329,"There are couple of issues here 1 The admin server destroy close event s onApplicationEvent ContextClosedEvent listener has stop to stop the admin server s tomcat instance. The stop also calls the applicationContext s destroy which loops again to stop. 2 With HSQLServer or any batch db server in future , the admin server stop also needs to handle the batch db server shutdown.",NULL,"There are couple of issues here 1 The admin server destroy close event s onApplicationEvent ContextClosedEvent listener has stop to stop the admin server s tomcat instance. The stop also calls the applicationContext s destroy which loops again to stop. 2 With HSQLServer or any batch db server in future , the admin server stop also needs to handle the batch db server shutdown.",NULL,"There are couple of issues here 1 The admin server destroy close event s onApplicationEvent ContextClosedEvent listener has stop to stop the admin server s tomcat instance. The stop also calls the applicationContext s destroy which loops again to stop. 2 With HSQLServer<span class='highlight-text severity-high'> or </span>any batch db server in future , the admin server stop also needs to handle the batch db server shutdown.","atomic","conjunctions","high",False
19344,"Some recent changes caused this to be turned off. Basically the change was to police whether a stream is well formed at create time, rather than deploy time. By deferring that check we can create composed streams that are not deployable by themselves but that are when used as building blocks in proper streams.",NULL,"Some recent changes caused this to be turned off. Basically the change was to police whether a stream is well formed at create time, rather than deploy time. By deferring that check we can create composed streams that are not deployable by themselves but that are when used as building blocks in proper streams.",NULL,"Some recent changes caused this to be turned off<span class='highlight-text severity-high'>. Basically the change was to police whether a stream is well formed at create time, rather than deploy time. By deferring that check we can create composed streams that are not deployable by themselves but that are when used as building blocks in proper streams.</span>","minimal","punctuation","high",False
19329,"There are couple of issues here 1 The admin server destroy close event s onApplicationEvent ContextClosedEvent listener has stop to stop the admin server s tomcat instance. The stop also calls the applicationContext s destroy which loops again to stop. 2 With HSQLServer or any batch db server in future , the admin server stop also needs to handle the batch db server shutdown.",NULL,"There are couple of issues here 1 The admin server destroy close event s onApplicationEvent ContextClosedEvent listener has stop to stop the admin server s tomcat instance. The stop also calls the applicationContext s destroy which loops again to stop. 2 With HSQLServer or any batch db server in future , the admin server stop also needs to handle the batch db server shutdown.",NULL,"There are couple of issues here 1 The admin server destroy close event s onApplicationEvent ContextClosedEvent listener has stop to stop the admin server s tomcat instance<span class='highlight-text severity-high'>. The stop also calls the applicationContext s destroy which loops again to stop. 2 With HSQLServer or any batch db server in future , the admin server stop also needs to handle the batch db server shutdown.</span>","minimal","punctuation","high",False
19330,"Currently this works stream create foo definition tap baz.time log stream create baz definition time file But this doesn t stream create foo definition tap baz log stream create baz definition time file This is because the parser translates references to tap baz to named channel tap baz.time the name of the stream s first module . If the stream is not yet created, the parser cannot perform this translation. A fix for this will likely be related to the fix needed for XD 812.",NULL,"Currently this works stream create foo definition tap baz.time log stream create baz definition time file But this doesn t stream create foo definition tap baz log stream create baz definition time file This is because the parser translates references to tap baz to named channel tap baz.time the name of the stream s first module . If the stream is not yet created, the parser cannot perform this translation. A fix for this will likely be related to the fix needed for XD 812.",NULL,"Add for who this story is","well_formed","no_role","high",False
19330,"Currently this works stream create foo definition tap baz.time log stream create baz definition time file But this doesn t stream create foo definition tap baz log stream create baz definition time file This is because the parser translates references to tap baz to named channel tap baz.time the name of the stream s first module . If the stream is not yet created, the parser cannot perform this translation. A fix for this will likely be related to the fix needed for XD 812.",NULL,"Currently this works stream create foo definition tap baz.time log stream create baz definition time file But this doesn t stream create foo definition tap baz log stream create baz definition time file This is because the parser translates references to tap baz to named channel tap baz.time the name of the stream s first module . If the stream is not yet created, the parser cannot perform this translation. A fix for this will likely be related to the fix needed for XD 812.",NULL,"Currently this works stream create foo definition tap baz<span class='highlight-text severity-high'>.time log stream create baz definition time file But this doesn t stream create foo definition tap baz log stream create baz definition time file This is because the parser translates references to tap baz to named channel tap baz.time the name of the stream s first module . If the stream is not yet created, the parser cannot perform this translation. A fix for this will likely be related to the fix needed for XD 812.</span>","minimal","punctuation","high",False
19321,"Looks like there are some version mismatch issues with the build packaging of the XD components. Looking in xd lib I see the following which looks suspicious mqtt client 0.2.1.jar mqtt client 1.0.jar jackson core asl 1.9.13.jar jackson mapper asl 1.9.12.jar spring integration core 3.0.0.M3.jar spring integration http 2.2.5.RELEASE.jar spring data commons 1.6.0.M1.jar spring data commons core 1.4.0.RELEASE.jar ",NULL,"Looks like there are some version mismatch issues with the build packaging of the XD components. Looking in xd lib I see the following which looks suspicious mqtt client 0.2.1.jar mqtt client 1.0.jar jackson core asl 1.9.13.jar jackson mapper asl 1.9.12.jar spring integration core 3.0.0.M3.jar spring integration http 2.2.5.RELEASE.jar spring data commons 1.6.0.M1.jar spring data commons core 1.4.0.RELEASE.jar ",NULL,"Add for who this story is","well_formed","no_role","high",False
19321,"Looks like there are some version mismatch issues with the build packaging of the XD components. Looking in xd lib I see the following which looks suspicious mqtt client 0.2.1.jar mqtt client 1.0.jar jackson core asl 1.9.13.jar jackson mapper asl 1.9.12.jar spring integration core 3.0.0.M3.jar spring integration http 2.2.5.RELEASE.jar spring data commons 1.6.0.M1.jar spring data commons core 1.4.0.RELEASE.jar ",NULL,"Looks like there are some version mismatch issues with the build packaging of the XD components. Looking in xd lib I see the following which looks suspicious mqtt client 0.2.1.jar mqtt client 1.0.jar jackson core asl 1.9.13.jar jackson mapper asl 1.9.12.jar spring integration core 3.0.0.M3.jar spring integration http 2.2.5.RELEASE.jar spring data commons 1.6.0.M1.jar spring data commons core 1.4.0.RELEASE.jar ",NULL,"Looks like there are some version mismatch issues with the build packaging of the XD components<span class='highlight-text severity-high'>. Looking in xd lib I see the following which looks suspicious mqtt client 0.2.1.jar mqtt client 1.0.jar jackson core asl 1.9.13.jar jackson mapper asl 1.9.12.jar spring integration core 3.0.0.M3.jar spring integration http 2.2.5.RELEASE.jar spring data commons 1.6.0.M1.jar spring data commons core 1.4.0.RELEASE.jar </span>","minimal","punctuation","high",False
19324,"When using jobs, taps we no longer need to have the leading . i.e. tap foo. We should only support tap foo.",NULL,"When using jobs, taps we no longer need to have the leading . i.e. tap foo. We should only support tap foo.",NULL,"Add for who this story is","well_formed","no_role","high",False
19324,"When using jobs, taps we no longer need to have the leading . i.e. tap foo. We should only support tap foo.",NULL,"When using jobs, taps we no longer need to have the leading . i.e. tap foo. We should only support tap foo.",NULL,"When using jobs, taps we no longer need to have the leading <span class='highlight-text severity-high'>. i.e. tap foo. We should only support tap foo.</span>","minimal","punctuation","high",False
19327,"Add back classifier dist to distZip build target it was was accidentally removed.",NULL,"Add back classifier dist to distZip build target it was was accidentally removed.",NULL,"Add for who this story is","well_formed","no_role","high",False
19333,"We also would like to upgrade the hsqldb version on spring batch admin so that both are compatible.",NULL,"We also would like to upgrade the hsqldb version on spring batch admin","so that both are compatible.","Add for who this story is","well_formed","no_role","high",False
19333,"We also would like to upgrade the hsqldb version on spring batch admin so that both are compatible.",NULL,"We also would like to upgrade the hsqldb version on spring batch admin","so that both are compatible.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19335,"Twitter search source should produce JSON or Pojo. The Pojo requires a custom wrapper class that is JSON friendly e.g., zero arg constructor . The twittersearch module should have a parameter json true false default true to control the output type.",NULL,"Twitter search source should produce JSON or Pojo. The Pojo requires a custom wrapper class that is JSON friendly e.g., zero arg constructor . The twittersearch module should have a parameter json true false default true to control the output type.",NULL,"Add for who this story is","well_formed","no_role","high",False
19335,"Twitter search source should produce JSON or Pojo. The Pojo requires a custom wrapper class that is JSON friendly e.g., zero arg constructor . The twittersearch module should have a parameter json true false default true to control the output type.",NULL,"Twitter search source should produce JSON or Pojo. The Pojo requires a custom wrapper class that is JSON friendly e.g., zero arg constructor . The twittersearch module should have a parameter json true false default true to control the output type.",NULL,"Twitter search source should produce JSON or Pojo<span class='highlight-text severity-high'>. The Pojo requires a custom wrapper class that is JSON friendly e.g., zero arg constructor . The twittersearch module should have a parameter json true false default true to control the output type.</span>","minimal","punctuation","high",False
19334,"org.springframework.integration.x.bus.Bridge should now be called Binding we can also move the INBOUND OUTBOUND direction or possibly the CONSUMER PRODUCER role into this class; that should simplify its usage in conditional code within the MessageBus code and also reduce the use of in and out as Strings in that same code",NULL,"org.springframework.integration.x.bus.Bridge should now be called Binding we can also move the INBOUND OUTBOUND direction or possibly the CONSUMER PRODUCER role into this class; that should simplify its usage in conditional code within the MessageBus code and also reduce the use of in and out as Strings in that same code",NULL,"Add for who this story is","well_formed","no_role","high",False
19334,"org.springframework.integration.x.bus.Bridge should now be called Binding we can also move the INBOUND OUTBOUND direction or possibly the CONSUMER PRODUCER role into this class; that should simplify its usage in conditional code within the MessageBus code and also reduce the use of in and out as Strings in that same code",NULL,"org.springframework.integration.x.bus.Bridge should now be called Binding we can also move the INBOUND OUTBOUND direction or possibly the CONSUMER PRODUCER role into this class; that should simplify its usage in conditional code within the MessageBus code and also reduce the use of in and out as Strings in that same code",NULL,"org.springframework.integration.x.bus.Bridge should now be called Binding we can also move the INBOUND OUTBOUND direction<span class='highlight-text severity-high'> or </span>possibly the CONSUMER PRODUCER role into this class; that should simplify its usage in conditional code within the MessageBus code<span class='highlight-text severity-high'> and </span>also reduce the use of in and out as Strings in that same code","atomic","conjunctions","high",False
19334,"org.springframework.integration.x.bus.Bridge should now be called Binding we can also move the INBOUND OUTBOUND direction or possibly the CONSUMER PRODUCER role into this class; that should simplify its usage in conditional code within the MessageBus code and also reduce the use of in and out as Strings in that same code",NULL,"org.springframework.integration.x.bus.Bridge should now be called Binding we can also move the INBOUND OUTBOUND direction or possibly the CONSUMER PRODUCER role into this class; that should simplify its usage in conditional code within the MessageBus code and also reduce the use of in and out as Strings in that same code",NULL,"org.springframework.integration.x.bus.Bridge should now be called Binding we can also move the INBOUND OUTBOUND direction or possibly the CONSUMER PRODUCER role into this class<span class='highlight-text severity-high'>; that should simplify its usage in conditional code within the MessageBus code and also reduce the use of in and out as Strings in that same code</span>","minimal","punctuation","high",False
19336,"org.springframework.xd.shell.command.MailCommandTests in spring xd shell project. I get failures relating to invalid username password INFO Stream Name Stream Definition Status mailstream imap port 1044 protocol imap folder INBOX username johndoe password secret file dir tmp name FileSink1280066074228960855 suffix txt charset UTF 8 binary false deployed 13 09 04 11 32 11 WARN mail.ImapIdleChannelAdapter error occurred in idle task javax.mail.AuthenticationFailedException LOGIN failed. Invalid login password at com.sun.mail.imap.IMAPStore.protocolConnect IMAPStore.java 663 ",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19336,"org.springframework.xd.shell.command.MailCommandTests in spring xd shell project. I get failures relating to invalid username password INFO Stream Name Stream Definition Status mailstream imap port 1044 protocol imap folder INBOX username johndoe password secret file dir tmp name FileSink1280066074228960855 suffix txt charset UTF 8 binary false deployed 13 09 04 11 32 11 WARN mail.ImapIdleChannelAdapter error occurred in idle task javax.mail.AuthenticationFailedException LOGIN failed. Invalid login password at com.sun.mail.imap.IMAPStore.protocolConnect IMAPStore.java 663 ",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19336,"org.springframework.xd.shell.command.MailCommandTests in spring xd shell project. I get failures relating to invalid username password INFO Stream Name Stream Definition Status mailstream imap port 1044 protocol imap folder INBOX username johndoe password secret file dir tmp name FileSink1280066074228960855 suffix txt charset UTF 8 binary false deployed 13 09 04 11 32 11 WARN mail.ImapIdleChannelAdapter error occurred in idle task javax.mail.AuthenticationFailedException LOGIN failed. Invalid login password at com.sun.mail.imap.IMAPStore.protocolConnect IMAPStore.java 663 ",NULL,NULL,NULL,"org<span class='highlight-text severity-high'>.springframework.xd.shell.command.MailCommandTests in spring xd shell project. I get failures relating to invalid username password INFO Stream Name Stream Definition Status mailstream imap port 1044 protocol imap folder INBOX username johndoe password secret file dir tmp name FileSink1280066074228960855 suffix txt charset UTF 8 binary false deployed 13 09 04 11 32 11 WARN mail.ImapIdleChannelAdapter error occurred in idle task javax.mail.AuthenticationFailedException LOGIN failed. Invalid login password at com.sun.mail.imap.IMAPStore.protocolConnect IMAPStore.java 663 </span>","minimal","punctuation","high",False
19338,"My current thinking is... ChannelRegistry MessageBus RabbitChannelRegistry RabbitMessageBus ... Then, method names change like so createInbound registerConsumer createOutbound registerProducer ",NULL,"My current thinking is... ChannelRegistry MessageBus RabbitChannelRegistry RabbitMessageBus ... Then, method names change like","so createInbound registerConsumer createOutbound registerProducer","Add for who this story is","well_formed","no_role","high",False
19338,"My current thinking is... ChannelRegistry MessageBus RabbitChannelRegistry RabbitMessageBus ... Then, method names change like so createInbound registerConsumer createOutbound registerProducer ",NULL,"My current thinking is... ChannelRegistry MessageBus RabbitChannelRegistry RabbitMessageBus ... Then, method names change like","so createInbound registerConsumer createOutbound registerProducer","My current thinking is<span class='highlight-text severity-high'>... ChannelRegistry MessageBus RabbitChannelRegistry RabbitMessageBus ... Then, method names change like so createInbound registerConsumer createOutbound registerProducer </span>","minimal","punctuation","high",False
19338,"My current thinking is... ChannelRegistry MessageBus RabbitChannelRegistry RabbitMessageBus ... Then, method names change like so createInbound registerConsumer createOutbound registerProducer ",NULL,"My current thinking is... ChannelRegistry MessageBus RabbitChannelRegistry RabbitMessageBus ... Then, method names change like","so createInbound registerConsumer createOutbound registerProducer","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19339,"If true default filter for delete messages in the twitter stream and route to a discard channel. This creates a twitter stream including only new tweets and no references to deleted ones.",NULL,"If true default filter for delete messages in the twitter stream and route to a discard channel. This creates a twitter stream including only new tweets and no references to deleted ones.",NULL,"Add for who this story is","well_formed","no_role","high",False
19339,"If true default filter for delete messages in the twitter stream and route to a discard channel. This creates a twitter stream including only new tweets and no references to deleted ones.",NULL,"If true default filter for delete messages in the twitter stream and route to a discard channel. This creates a twitter stream including only new tweets and no references to deleted ones.",NULL,"If true default filter for delete messages in the twitter stream<span class='highlight-text severity-high'> and </span>route to a discard channel. This creates a twitter stream including only new tweets and no references to deleted ones.","atomic","conjunctions","high",False
19339,"If true default filter for delete messages in the twitter stream and route to a discard channel. This creates a twitter stream including only new tweets and no references to deleted ones.",NULL,"If true default filter for delete messages in the twitter stream and route to a discard channel. This creates a twitter stream including only new tweets and no references to deleted ones.",NULL,"If true default filter for delete messages in the twitter stream and route to a discard channel<span class='highlight-text severity-high'>. This creates a twitter stream including only new tweets and no references to deleted ones.</span>","minimal","punctuation","high",False
19343,"A user should be able to view some important details of the last execution of a job from a job list. The batch jobs REST endpoint should provide extra fields not currently available in the JobInfo class. At a minimum, I would like to see startTime startDate last job parameters duration last job status",NULL,"A user should be able to view some important details of the last execution of a job from a job list. The batch jobs REST endpoint should provide extra fields not currently available in the JobInfo class. At a minimum, I would like to see startTime startDate last job parameters duration last job status",NULL,"Add for who this story is","well_formed","no_role","high",False
19343,"A user should be able to view some important details of the last execution of a job from a job list. The batch jobs REST endpoint should provide extra fields not currently available in the JobInfo class. At a minimum, I would like to see startTime startDate last job parameters duration last job status",NULL,"A user should be able to view some important details of the last execution of a job from a job list. The batch jobs REST endpoint should provide extra fields not currently available in the JobInfo class. At a minimum, I would like to see startTime startDate last job parameters duration last job status",NULL,"A user should be able to view some important details of the last execution of a job from a job list<span class='highlight-text severity-high'>. The batch jobs REST endpoint should provide extra fields not currently available in the JobInfo class. At a minimum, I would like to see startTime startDate last job parameters duration last job status</span>","minimal","punctuation","high",False
19341,"Currently, the url for accessing the XD UI is http localhost 8080 admin ui index.html . This feels messy and dated. We should be able to access the ui without explicitly including the index.html, like this code http localhost 8080 admin ui code ",NULL,"Currently, the url for accessing the XD UI is http localhost 8080 admin ui index.html . This feels messy and dated. We should be able to access the ui without explicitly including the index.html, like this code http localhost 8080 admin ui code ",NULL,"Add for who this story is","well_formed","no_role","high",False
19341,"Currently, the url for accessing the XD UI is http localhost 8080 admin ui index.html . This feels messy and dated. We should be able to access the ui without explicitly including the index.html, like this code http localhost 8080 admin ui code ",NULL,"Currently, the url for accessing the XD UI is http localhost 8080 admin ui index.html . This feels messy and dated. We should be able to access the ui without explicitly including the index.html, like this code http localhost 8080 admin ui code ",NULL,"Currently, the url for accessing the XD UI is http localhost 8080 admin ui index.html . This feels messy<span class='highlight-text severity-high'> and </span>dated. We should be able to access the ui without explicitly including the index.html, like this code http localhost 8080 admin ui code ","atomic","conjunctions","high",False
19341,"Currently, the url for accessing the XD UI is http localhost 8080 admin ui index.html . This feels messy and dated. We should be able to access the ui without explicitly including the index.html, like this code http localhost 8080 admin ui code ",NULL,"Currently, the url for accessing the XD UI is http localhost 8080 admin ui index.html . This feels messy and dated. We should be able to access the ui without explicitly including the index.html, like this code http localhost 8080 admin ui code ",NULL,"Currently, the url for accessing the XD UI is http localhost 8080 admin ui index<span class='highlight-text severity-high'>.html . This feels messy and dated. We should be able to access the ui without explicitly including the index.html, like this code http localhost 8080 admin ui code </span>","minimal","punctuation","high",False
19347,"When an XD job is destroyed deleted, the batch jobRepository entries for the job associated JobInstances, JobExecutions etc., and the BatchJobLocator entries.",NULL,"When an XD job is destroyed deleted, the batch jobRepository entries for the job associated JobInstances, JobExecutions etc., and the BatchJobLocator entries.",NULL,"Add for who this story is","well_formed","no_role","high",False
19344,"Some recent changes caused this to be turned off. Basically the change was to police whether a stream is well formed at create time, rather than deploy time. By deferring that check we can create composed streams that are not deployable by themselves but that are when used as building blocks in proper streams.",NULL,"Some recent changes caused this to be turned off. Basically the change was to police whether a stream is well formed at create time, rather than deploy time. By deferring that check we can create composed streams that are not deployable by themselves but that are when used as building blocks in proper streams.",NULL,"Add for who this story is","well_formed","no_role","high",False
19346,"Now that the new batch admin api is taking shape, we need to rebase the XD web UI to use this. It s more than just changing the http urls sent to the xd server since the new API is not identical to the old one.",NULL,"Now that the new batch admin api is taking shape, we need to rebase the XD web UI to use this. It s more than just changing the http urls sent to the xd server since the new API is not identical to the old one.",NULL,"Now that the new batch admin api is taking shape, we need to rebase the XD web UI to use this<span class='highlight-text severity-high'>. It s more than just changing the http urls sent to the xd server since the new API is not identical to the old one.</span>","minimal","punctuation","high",False
19340,"When converting a JSON string to a tuple the JSON may contain id. This method should handle this. Same with timestamp",NULL,"When converting a JSON string to a tuple the JSON may contain id. This method should handle this. Same with timestamp",NULL,"Add for who this story is","well_formed","no_role","high",False
19340,"When converting a JSON string to a tuple the JSON may contain id. This method should handle this. Same with timestamp",NULL,"When converting a JSON string to a tuple the JSON may contain id. This method should handle this. Same with timestamp",NULL,"When converting a JSON string to a tuple the JSON may contain id<span class='highlight-text severity-high'>. This method should handle this. Same with timestamp</span>","minimal","punctuation","high",False
19350,"We should provide a better shell integration when XD is run on Yarn. 1. yarn kill id TAB completion 2. yarn submit, more options like app name 3. yarn list, filter by app states, etc 4. admin config server TAB completion for running xd apps on yarn",NULL,"We should provide a better shell integration when XD is run on Yarn. 1. yarn kill id TAB completion 2. yarn submit, more options like app name 3. yarn list, filter by app states, etc 4. admin config server TAB completion for running xd apps on yarn",NULL,"Add for who this story is","well_formed","no_role","high",False
19350,"We should provide a better shell integration when XD is run on Yarn. 1. yarn kill id TAB completion 2. yarn submit, more options like app name 3. yarn list, filter by app states, etc 4. admin config server TAB completion for running xd apps on yarn",NULL,"We should provide a better shell integration when XD is run on Yarn. 1. yarn kill id TAB completion 2. yarn submit, more options like app name 3. yarn list, filter by app states, etc 4. admin config server TAB completion for running xd apps on yarn",NULL,"We should provide a better shell integration when XD is run on Yarn<span class='highlight-text severity-high'>. 1. yarn kill id TAB completion 2. yarn submit, more options like app name 3. yarn list, filter by app states, etc 4. admin config server TAB completion for running xd apps on yarn</span>","minimal","punctuation","high",False
19352,"The parameters are not optimal for the counter name between Aggregate Counter Field Value Counter counterName versus name",NULL,"The parameters are not optimal for the counter name between Aggregate Counter Field Value Counter counterName versus name",NULL,"Add for who this story is","well_formed","no_role","high",False
19357,"Job channels need to have a namespace. i.e. job somejobname. Where the is the delimiter for the namespace. The preference is to use the instead of the . But XD 766 needs to be completed in order to support this.",NULL,"Job channels need to have a namespace. i.e. job somejobname. Where the is the delimiter for the namespace. The preference is to use the instead of the . But XD 766 needs to be completed","in order to support this.","Add for who this story is","well_formed","no_role","high",False
19357,"Job channels need to have a namespace. i.e. job somejobname. Where the is the delimiter for the namespace. The preference is to use the instead of the . But XD 766 needs to be completed in order to support this.",NULL,"Job channels need to have a namespace. i.e. job somejobname. Where the is the delimiter for the namespace. The preference is to use the instead of the . But XD 766 needs to be completed","in order to support this.","Job channels need to have a namespace<span class='highlight-text severity-high'>. i.e. job somejobname. Where the is the delimiter for the namespace. The preference is to use the instead of the . But XD 766 needs to be completed in order to support this.</span>","minimal","punctuation","high",False
19357,"Job channels need to have a namespace. i.e. job somejobname. Where the is the delimiter for the namespace. The preference is to use the instead of the . But XD 766 needs to be completed in order to support this.",NULL,"Job channels need to have a namespace. i.e. job somejobname. Where the is the delimiter for the namespace. The preference is to use the instead of the . But XD 766 needs to be completed","in order to support this.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19353,"Currently it s possible to do something like code http prot 8888 code It is possible to validate property names by parsing the module definition file s directly and matching property placeholders or profile declarations that may be mapped to properties, etc . This must account recursively for imports as well. I have some code in a branch that does this . ",NULL,"Currently it s possible to do something like code http prot 8888 code It is possible to validate property names by parsing the module definition file s directly and matching property placeholders or profile declarations that may be mapped to properties, etc . This must account recursively for imports as well. I have some code in a branch that does this . ",NULL,"Add for who this story is","well_formed","no_role","high",False
19353,"Currently it s possible to do something like code http prot 8888 code It is possible to validate property names by parsing the module definition file s directly and matching property placeholders or profile declarations that may be mapped to properties, etc . This must account recursively for imports as well. I have some code in a branch that does this . ",NULL,"Currently it s possible to do something like code http prot 8888 code It is possible to validate property names by parsing the module definition file s directly and matching property placeholders or profile declarations that may be mapped to properties, etc . This must account recursively for imports as well. I have some code in a branch that does this . ",NULL,"Currently it s possible to do something like code http prot 8888 code It is possible to validate property names by parsing the module definition file s directly<span class='highlight-text severity-high'> and </span>matching property placeholders<span class='highlight-text severity-high'> or </span>profile declarations that may be mapped to properties, etc . This must account recursively for imports as well. I have some code in a branch that does this . ","atomic","conjunctions","high",False
19353,"Currently it s possible to do something like code http prot 8888 code It is possible to validate property names by parsing the module definition file s directly and matching property placeholders or profile declarations that may be mapped to properties, etc . This must account recursively for imports as well. I have some code in a branch that does this . ",NULL,"Currently it s possible to do something like code http prot 8888 code It is possible to validate property names by parsing the module definition file s directly and matching property placeholders or profile declarations that may be mapped to properties, etc . This must account recursively for imports as well. I have some code in a branch that does this . ",NULL,"Currently it s possible to do something like code http prot 8888 code It is possible to validate property names by parsing the module definition file s directly and matching property placeholders or profile declarations that may be mapped to properties, etc <span class='highlight-text severity-high'>. This must account recursively for imports as well. I have some code in a branch that does this . </span>","minimal","punctuation","high",False
19358,"https sonar.springsource.org drilldown measures 7173?metric package tangle index rids 5B 5D 7717",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19358,"https sonar.springsource.org drilldown measures 7173?metric package tangle index rids 5B 5D 7717",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19355,"We need an abstraction in place to retrieve messages from a named channel programmatically. Right now there is no implementation agnostic way of doing this such as receiveMessage , queueSize . This could be quite useful for integration tests of streams. E.g. to do more focussed tests without resorting to temp files and non essential sinks or sources etc. e.g. code routeit router expression payload.contains a ? foo bar code ",NULL,"We need an abstraction in place to retrieve messages from a named channel programmatically. Right now there is no implementation agnostic way of doing this such as receiveMessage , queueSize . This could be quite useful for integration tests of streams. E.g. to do more focussed tests without resorting to temp files and non essential sinks or sources etc. e.g. code routeit router expression payload.contains a ? foo bar code ",NULL,"Add for who this story is","well_formed","no_role","high",False
19355,"We need an abstraction in place to retrieve messages from a named channel programmatically. Right now there is no implementation agnostic way of doing this such as receiveMessage , queueSize . This could be quite useful for integration tests of streams. E.g. to do more focussed tests without resorting to temp files and non essential sinks or sources etc. e.g. code routeit router expression payload.contains a ? foo bar code ",NULL,"We need an abstraction in place to retrieve messages from a named channel programmatically. Right now there is no implementation agnostic way of doing this such as receiveMessage , queueSize . This could be quite useful for integration tests of streams. E.g. to do more focussed tests without resorting to temp files and non essential sinks or sources etc. e.g. code routeit router expression payload.contains a ? foo bar code ",NULL,"We need an abstraction in place to retrieve messages from a named channel programmatically. Right now there is no implementation agnostic way of doing this such as receiveMessage , queueSize . This could be quite useful for integration tests of streams. E.g. to do more focussed tests without resorting to temp files<span class='highlight-text severity-high'> and </span>non essential sinks<span class='highlight-text severity-high'> or </span>sources etc. e.g. code routeit router expression payload.contains a ? foo bar code ","atomic","conjunctions","high",False
19355,"We need an abstraction in place to retrieve messages from a named channel programmatically. Right now there is no implementation agnostic way of doing this such as receiveMessage , queueSize . This could be quite useful for integration tests of streams. E.g. to do more focussed tests without resorting to temp files and non essential sinks or sources etc. e.g. code routeit router expression payload.contains a ? foo bar code ",NULL,"We need an abstraction in place to retrieve messages from a named channel programmatically. Right now there is no implementation agnostic way of doing this such as receiveMessage , queueSize . This could be quite useful for integration tests of streams. E.g. to do more focussed tests without resorting to temp files and non essential sinks or sources etc. e.g. code routeit router expression payload.contains a ? foo bar code ",NULL,"We need an abstraction in place to retrieve messages from a named channel programmatically<span class='highlight-text severity-high'>. Right now there is no implementation agnostic way of doing this such as receiveMessage , queueSize . This could be quite useful for integration tests of streams. E.g. to do more focussed tests without resorting to temp files and non essential sinks or sources etc. e.g. code routeit router expression payload.contains a ? foo bar code </span>","minimal","punctuation","high",False
19356,"This is to set the appropriate data source, so that the container will use admins batch repository.",NULL,"This is to set the appropriate data source,","so that the container will use admins batch repository.","Add for who this story is","well_formed","no_role","high",False
19356,"This is to set the appropriate data source, so that the container will use admins batch repository.",NULL,"This is to set the appropriate data source,","so that the container will use admins batch repository.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19401,"Enhance bean naming strategy or provide a value for the property that binds to this",NULL,"Enhance bean naming strategy or provide a value for the property that binds to this",NULL,"Add for who this story is","well_formed","no_role","high",False
19401,"Enhance bean naming strategy or provide a value for the property that binds to this",NULL,"Enhance bean naming strategy or provide a value for the property that binds to this",NULL,"Enhance bean naming strategy<span class='highlight-text severity-high'> or </span>provide a value for the property that binds to this","atomic","conjunctions","high",False
19731,"The TCP source supports inbound binary data. We currently have to go through unnecessary byte String byte conversion.",NULL,"The TCP source supports inbound binary data. We currently have to go through unnecessary byte String byte conversion.",NULL,"Add for who this story is","well_formed","no_role","high",False
19731,"The TCP source supports inbound binary data. We currently have to go through unnecessary byte String byte conversion.",NULL,"The TCP source supports inbound binary data. We currently have to go through unnecessary byte String byte conversion.",NULL,"The TCP source supports inbound binary data<span class='highlight-text severity-high'>. We currently have to go through unnecessary byte String byte conversion.</span>","minimal","punctuation","high",False
19732,"If, say, xd dirt is ahead of a local config on the classpath, it s log4j.properties is found first.",NULL,"If, say, xd dirt is ahead of a local config on the classpath, it s log4j.properties is found first.",NULL,"Add for who this story is","well_formed","no_role","high",False
19351,"This might mean we should adjust our hadoopDistro options to the ones supported in the new release hadoop12 default , cdh4, hdp13, phd1 and hadoop21",NULL,"This might mean we should adjust our hadoopDistro options to the ones supported in the new release hadoop12 default , cdh4, hdp13, phd1 and hadoop21",NULL,"Add for who this story is","well_formed","no_role","high",False
19349,"We need a REST endpoint to launch a job. Given the constraint on JobRegistry being not persistent and not available outside the container JVM, we can not use the batch job admin controller service to launch the job. One possible way is to use the trigger source to launch the job at XD.",NULL,"We need a REST endpoint to launch a job. Given the constraint on JobRegistry being not persistent and not available outside the container JVM, we can not use the batch job admin controller service to launch the job. One possible way is to use the trigger source to launch the job at XD.",NULL,"Add for who this story is","well_formed","no_role","high",False
19349,"We need a REST endpoint to launch a job. Given the constraint on JobRegistry being not persistent and not available outside the container JVM, we can not use the batch job admin controller service to launch the job. One possible way is to use the trigger source to launch the job at XD.",NULL,"We need a REST endpoint to launch a job. Given the constraint on JobRegistry being not persistent and not available outside the container JVM, we can not use the batch job admin controller service to launch the job. One possible way is to use the trigger source to launch the job at XD.",NULL,"We need a REST endpoint to launch a job. Given the constraint on JobRegistry being not persistent<span class='highlight-text severity-high'> and </span>not available outside the container JVM, we can not use the batch job admin controller service to launch the job. One possible way is to use the trigger source to launch the job at XD.","atomic","conjunctions","high",False
19349,"We need a REST endpoint to launch a job. Given the constraint on JobRegistry being not persistent and not available outside the container JVM, we can not use the batch job admin controller service to launch the job. One possible way is to use the trigger source to launch the job at XD.",NULL,"We need a REST endpoint to launch a job. Given the constraint on JobRegistry being not persistent and not available outside the container JVM, we can not use the batch job admin controller service to launch the job. One possible way is to use the trigger source to launch the job at XD.",NULL,"We need a REST endpoint to launch a job<span class='highlight-text severity-high'>. Given the constraint on JobRegistry being not persistent and not available outside the container JVM, we can not use the batch job admin controller service to launch the job. One possible way is to use the trigger source to launch the job at XD.</span>","minimal","punctuation","high",False
19354,"For testing purposes it would be super helpful if there be a hook to get notified when a named channel is up and running. In current tests one may have to resort to Thread.sleep .",NULL,"For testing purposes it would be super helpful if there be a hook to get notified when a named channel is up and running. In current tests one may have to resort to Thread.sleep .",NULL,"Add for who this story is","well_formed","no_role","high",False
19354,"For testing purposes it would be super helpful if there be a hook to get notified when a named channel is up and running. In current tests one may have to resort to Thread.sleep .",NULL,"For testing purposes it would be super helpful if there be a hook to get notified when a named channel is up and running. In current tests one may have to resort to Thread.sleep .",NULL,"For testing purposes it would be super helpful if there be a hook to get notified when a named channel is up<span class='highlight-text severity-high'> and </span>running. In current tests one may have to resort to Thread.sleep .","atomic","conjunctions","high",False
19354,"For testing purposes it would be super helpful if there be a hook to get notified when a named channel is up and running. In current tests one may have to resort to Thread.sleep .",NULL,"For testing purposes it would be super helpful if there be a hook to get notified when a named channel is up and running. In current tests one may have to resort to Thread.sleep .",NULL,"For testing purposes it would be super helpful if there be a hook to get notified when a named channel is up and running<span class='highlight-text severity-high'>. In current tests one may have to resort to Thread.sleep .</span>","minimal","punctuation","high",False
19551,"In the case that there are not enough resources to fire a trigger, the highest priority will be fired first.",NULL,"In the case that there are not enough resources to fire a trigger, the highest priority will be fired first.",NULL,"Add for who this story is","well_formed","no_role","high",False
19402,"Configure embedded servlet container needs to know where to load the UI code.",NULL,"Configure embedded servlet container needs to know where to load the UI code.",NULL,"Add for who this story is","well_formed","no_role","high",False
19362,"similar to ChannelRegistry AbstractChannelRegistryTests that has the real tests subclasses for each impl provide the registry to be tested Thus one test can run against multiple transports.",NULL,"similar to ChannelRegistry AbstractChannelRegistryTests that has the real tests subclasses for each impl provide the registry to be tested Thus one test can run against multiple transports.",NULL,"Add for who this story is","well_formed","no_role","high",False
19363,"related to XD 779. We need the ability to provide JSON serializable JobExecution information. Change from using JavaSerialization back to returning objects ",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19363,"related to XD 779. We need the ability to provide JSON serializable JobExecution information. Change from using JavaSerialization back to returning objects ",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19428,"xd stream create name testgemfire definition http port 8887 gemfire 16 20 28,503 WARN Spring Shell client.RestTemplate 524 POST request for http localhost 8080 streams resulted in 500 Internal Server Error ; invoking error handler Command failed org.springframework.xd.rest.client.impl.SpringXDException org.springframework.beans.factory.BeanDefinitionStoreException Invalid bean definition with name region defined in null Could not resolve placeholder regionName in string value regionName ",NULL,"xd stream create name testgemfire definition http port 8887 gemfire 16 20 28,503 WARN Spring Shell client.RestTemplate 524 POST request for http localhost 8080 streams resulted in 500 Internal Server Error ; invoking error handler Command failed org.springframework.xd.rest.client.impl.SpringXDException org.springframework.beans.factory.BeanDefinitionStoreException Invalid bean definition with name region defined in null Could not resolve placeholder regionName in string value regionName ",NULL,"Add for who this story is","well_formed","no_role","high",False
19365,"This came up when working on email source. There is int mail imap idle channel adapter and int mail inbound channel adapter It would be nice to be able to put those in two profiles and have one of the profile being activated from module options e.g. email polling true false Don t know the runtime cost of activating profiles, but we could blindly activate profiles from all options passed explicitly beans profile profile optionname optionvalue Not sure if this is the same as XD 132",NULL,"This came up when working on email source. There is int mail imap idle channel adapter and int mail inbound channel adapter It would be nice to be able to put those in two profiles and have one of the profile being activated from module options e.g. email polling true false Don t know the runtime cost of activating profiles, but we could blindly activate profiles from all options passed explicitly beans profile profile optionname optionvalue Not sure if this is the same as XD 132",NULL,"This came up when working on email source. There is int mail imap idle channel adapter<span class='highlight-text severity-high'> and </span>int mail inbound channel adapter It would be nice to be able to put those in two profiles and have one of the profile being activated from module options e.g. email polling true false Don t know the runtime cost of activating profiles, but we could blindly activate profiles from all options passed explicitly beans profile profile optionname optionvalue Not sure if this is the same as XD 132","atomic","conjunctions","high",False
19365,"This came up when working on email source. There is int mail imap idle channel adapter and int mail inbound channel adapter It would be nice to be able to put those in two profiles and have one of the profile being activated from module options e.g. email polling true false Don t know the runtime cost of activating profiles, but we could blindly activate profiles from all options passed explicitly beans profile profile optionname optionvalue Not sure if this is the same as XD 132",NULL,"This came up when working on email source. There is int mail imap idle channel adapter and int mail inbound channel adapter It would be nice to be able to put those in two profiles and have one of the profile being activated from module options e.g. email polling true false Don t know the runtime cost of activating profiles, but we could blindly activate profiles from all options passed explicitly beans profile profile optionname optionvalue Not sure if this is the same as XD 132",NULL,"This came up when working on email source<span class='highlight-text severity-high'>. There is int mail imap idle channel adapter and int mail inbound channel adapter It would be nice to be able to put those in two profiles and have one of the profile being activated from module options e.g. email polling true false Don t know the runtime cost of activating profiles, but we could blindly activate profiles from all options passed explicitly beans profile profile optionname optionvalue Not sure if this is the same as XD 132</span>","minimal","punctuation","high",False
19364,"Should it be fatal vs. warning?",NULL,"Should it be fatal vs. warning?",NULL,"Add for who this story is","well_formed","no_role","high",False
19364,"Should it be fatal vs. warning?",NULL,"Should it be fatal vs. warning?",NULL,"Should it be fatal vs<span class='highlight-text severity-high'>. warning?</span>","minimal","punctuation","high",False
19367,"See discussion at https github.com SpringSource spring xd pull 240 discussion r6045724",NULL,"See discussion at https github.com SpringSource spring xd pull 240 discussion r6045724",NULL,"Add for who this story is","well_formed","no_role","high",False
19366,"export XD HOME foo gradle clean test build fails. Need to detected environment variables and override for the build",NULL,"export XD HOME foo gradle clean test build fails. Need to detected environment variables and override for the build",NULL,"Add for who this story is","well_formed","no_role","high",False
19366,"export XD HOME foo gradle clean test build fails. Need to detected environment variables and override for the build",NULL,"export XD HOME foo gradle clean test build fails. Need to detected environment variables and override for the build",NULL,"export XD HOME foo gradle clean test build fails. Need to detected environment variables<span class='highlight-text severity-high'> and </span>override for the build","atomic","conjunctions","high",False
19366,"export XD HOME foo gradle clean test build fails. Need to detected environment variables and override for the build",NULL,"export XD HOME foo gradle clean test build fails. Need to detected environment variables and override for the build",NULL,"export XD HOME foo gradle clean test build fails<span class='highlight-text severity-high'>. Need to detected environment variables and override for the build</span>","minimal","punctuation","high",False
19376,"Currently we have 2 trigger sources trigger cron trigger. The preference is to have a user to just use a single trigger source. for example trigger myjob trigger cron ... myjob trigger fixedDelay ... myjob One option to handle this is to use spel to reference a bean and then have different trigger beans defined. i.e. trigger cronTriggerBean . Each trigger bean would setup the channel with the correct poller. ",NULL,"Currently we have 2 trigger sources trigger cron trigger. The preference is to have a user to just use a single trigger source. for example trigger myjob trigger cron ... myjob trigger fixedDelay ... myjob One option to handle this is to use spel to reference a bean and then have different trigger beans defined. i.e. trigger cronTriggerBean . Each trigger bean would setup the channel with the correct poller. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19376,"Currently we have 2 trigger sources trigger cron trigger. The preference is to have a user to just use a single trigger source. for example trigger myjob trigger cron ... myjob trigger fixedDelay ... myjob One option to handle this is to use spel to reference a bean and then have different trigger beans defined. i.e. trigger cronTriggerBean . Each trigger bean would setup the channel with the correct poller. ",NULL,"Currently we have 2 trigger sources trigger cron trigger. The preference is to have a user to just use a single trigger source. for example trigger myjob trigger cron ... myjob trigger fixedDelay ... myjob One option to handle this is to use spel to reference a bean and then have different trigger beans defined. i.e. trigger cronTriggerBean . Each trigger bean would setup the channel with the correct poller. ",NULL,"Currently we have 2 trigger sources trigger cron trigger. The preference is to have a user to just use a single trigger source. for example trigger myjob trigger cron ... myjob trigger fixedDelay ... myjob One option to handle this is to use spel to reference a bean<span class='highlight-text severity-high'> and </span>then have different trigger beans defined. i.e. trigger cronTriggerBean . Each trigger bean would setup the channel with the correct poller. ","atomic","conjunctions","high",False
19376,"Currently we have 2 trigger sources trigger cron trigger. The preference is to have a user to just use a single trigger source. for example trigger myjob trigger cron ... myjob trigger fixedDelay ... myjob One option to handle this is to use spel to reference a bean and then have different trigger beans defined. i.e. trigger cronTriggerBean . Each trigger bean would setup the channel with the correct poller. ",NULL,"Currently we have 2 trigger sources trigger cron trigger. The preference is to have a user to just use a single trigger source. for example trigger myjob trigger cron ... myjob trigger fixedDelay ... myjob One option to handle this is to use spel to reference a bean and then have different trigger beans defined. i.e. trigger cronTriggerBean . Each trigger bean would setup the channel with the correct poller. ",NULL,"Currently we have 2 trigger sources trigger cron trigger<span class='highlight-text severity-high'>. The preference is to have a user to just use a single trigger source. for example trigger myjob trigger cron ... myjob trigger fixedDelay ... myjob One option to handle this is to use spel to reference a bean and then have different trigger beans defined. i.e. trigger cronTriggerBean . Each trigger bean would setup the channel with the correct poller. </span>","minimal","punctuation","high",False
19368,"It would be nice to be able to have modules that are simply made of Their context xml file Some kind of manifest that expresses dependencies and have the runtime take care of the deps",NULL,"It would be nice to be able to have modules that are simply made of Their context xml file Some kind of manifest that expresses dependencies and have the runtime take care of the deps",NULL,"Add for who this story is","well_formed","no_role","high",False
19368,"It would be nice to be able to have modules that are simply made of Their context xml file Some kind of manifest that expresses dependencies and have the runtime take care of the deps",NULL,"It would be nice to be able to have modules that are simply made of Their context xml file Some kind of manifest that expresses dependencies and have the runtime take care of the deps",NULL,"It would be nice to be able to have modules that are simply made of Their context xml file Some kind of manifest that expresses dependencies<span class='highlight-text severity-high'> and </span>have the runtime take care of the deps","atomic","conjunctions","high",False
19370,"for an example, see comments here https jira.springsource.org browse XD 671 ",NULL,"for an example, see comments here https jira.springsource.org browse XD 671 ",NULL,"Add for who this story is","well_formed","no_role","high",False
19374,"Also drop the enhanced portion of the EnhancedStreamParser.",NULL,"Also drop the enhanced portion of the EnhancedStreamParser.",NULL,"Add for who this story is","well_formed","no_role","high",False
19372,"See discussion at https github.com SpringSource spring xd pull 250 files r6034885",NULL,"See discussion at https github.com SpringSource spring xd pull 250 files r6034885",NULL,"Add for who this story is","well_formed","no_role","high",False
19375,"Jobs will be started via trigger. So we won t need the JobTriggerBean.",NULL,"Jobs will be started via trigger.","So we won t need the JobTriggerBean.","Add for who this story is","well_formed","no_role","high",False
19375,"Jobs will be started via trigger. So we won t need the JobTriggerBean.",NULL,"Jobs will be started via trigger.","So we won t need the JobTriggerBean.","Jobs will be started via trigger<span class='highlight-text severity-high'>. So we won t need the JobTriggerBean.</span>","minimal","punctuation","high",False
19375,"Jobs will be started via trigger. So we won t need the JobTriggerBean.",NULL,"Jobs will be started via trigger.","So we won t need the JobTriggerBean.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19360,"Once XD 785 is merged",NULL,"Once XD 785 is merged",NULL,"Add for who this story is","well_formed","no_role","high",False
19373,"User shall have the ability to get a listing of available named channels order by name ascending from the shell Add support to controllers Add tests",NULL,"User shall have the ability to get a listing of available named channels order by name ascending from the shell Add support to controllers Add tests",NULL,"Add for who this story is","well_formed","no_role","high",False
19361,"SingleNodeMain.launchSingleNodeServer options calls System.exit causing a gradle buffer underflow. This is called from SingleNodeMainIntegrationTests. System.exit should be called from the main method instead. ",NULL,"SingleNodeMain.launchSingleNodeServer options calls System.exit causing a gradle buffer underflow. This is called from SingleNodeMainIntegrationTests. System.exit should be called from the main method instead. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19361,"SingleNodeMain.launchSingleNodeServer options calls System.exit causing a gradle buffer underflow. This is called from SingleNodeMainIntegrationTests. System.exit should be called from the main method instead. ",NULL,"SingleNodeMain.launchSingleNodeServer options calls System.exit causing a gradle buffer underflow. This is called from SingleNodeMainIntegrationTests. System.exit should be called from the main method instead. ",NULL,"SingleNodeMain<span class='highlight-text severity-high'>.launchSingleNodeServer options calls System.exit causing a gradle buffer underflow. This is called from SingleNodeMainIntegrationTests. System.exit should be called from the main method instead. </span>","minimal","punctuation","high",False
19371,"In order to hook up the to get access to all the jobs available the job registry has to be shared. currently the only implmentation is is the MapJobRegistry. Testability. The admin will need to be see all jobs created by its containers.",NULL,"In order to hook up the to get access to all the jobs available the job registry has to be shared. currently the only implmentation is is the MapJobRegistry. Testability. The admin will need to be see all jobs created by its containers.",NULL,"Add for who this story is","well_formed","no_role","high",False
19371,"In order to hook up the to get access to all the jobs available the job registry has to be shared. currently the only implmentation is is the MapJobRegistry. Testability. The admin will need to be see all jobs created by its containers.",NULL,"In order to hook up the to get access to all the jobs available the job registry has to be shared. currently the only implmentation is is the MapJobRegistry. Testability. The admin will need to be see all jobs created by its containers.",NULL,"In order to hook up the to get access to all the jobs available the job registry has to be shared<span class='highlight-text severity-high'>. currently the only implmentation is is the MapJobRegistry. Testability. The admin will need to be see all jobs created by its containers.</span>","minimal","punctuation","high",False
19369,"See https github.com SpringSource spring xd pull 240 discussion r6045724",NULL,"See https github.com SpringSource spring xd pull 240 discussion r6045724",NULL,"Add for who this story is","well_formed","no_role","high",False
19383,"Use a profile or similar to only include the Environment conditionally currently in module common.xml. Also Jon Brisbin one thing to keep in mind we talked about having a properties file for XD that configured the RingBuffer et al in a non default way Jon Brisbin e.g. no event loop Dispatchers a ThreadPoolDispatcher with a large thread pool size 50 threads? 100? and maybe even two RingBufferDispatchers input and output Jon Brisbin so we might want to change from strictly a default Environment bean to an EnvironmentFactoryBean with a specific configuration thinking about it now I maybe should add a namespace element for the Environment",NULL,"Use a profile or similar to only include the Environment conditionally currently in module common.xml. Al","so Jon Brisbin one thing to keep in mind we talked about having a properties file for XD that configured the RingBuffer et al in a non default way Jon Brisbin e.g. no event loop Dispatchers a ThreadPoolDispatcher with a large thread pool size 50 threads? 100? and maybe even two RingBufferDispatchers input and output Jon Brisbin so we might want to change from strictly a default Environment bean to an EnvironmentFactoryBean with a specific configuration thinking about it now I maybe should add a namespace element for the Environment","Add for who this story is","well_formed","no_role","high",False
19383,"Use a profile or similar to only include the Environment conditionally currently in module common.xml. Also Jon Brisbin one thing to keep in mind we talked about having a properties file for XD that configured the RingBuffer et al in a non default way Jon Brisbin e.g. no event loop Dispatchers a ThreadPoolDispatcher with a large thread pool size 50 threads? 100? and maybe even two RingBufferDispatchers input and output Jon Brisbin so we might want to change from strictly a default Environment bean to an EnvironmentFactoryBean with a specific configuration thinking about it now I maybe should add a namespace element for the Environment",NULL,"Use a profile or similar to only include the Environment conditionally currently in module common.xml. Al","so Jon Brisbin one thing to keep in mind we talked about having a properties file for XD that configured the RingBuffer et al in a non default way Jon Brisbin e.g. no event loop Dispatchers a ThreadPoolDispatcher with a large thread pool size 50 threads? 100? and maybe even two RingBufferDispatchers input and output Jon Brisbin so we might want to change from strictly a default Environment bean to an EnvironmentFactoryBean with a specific configuration thinking about it now I maybe should add a namespace element for the Environment","Use a profile<span class='highlight-text severity-high'> or </span>similar to only include the Environment conditionally currently in module common.xml. Also Jon Brisbin one thing to keep in mind we talked about having a properties file for XD that configured the RingBuffer et al in a non default way Jon Brisbin e.g. no event loop Dispatchers a ThreadPoolDispatcher with a large thread pool size 50 threads? 100?<span class='highlight-text severity-high'> and </span>maybe even two RingBufferDispatchers input and output Jon Brisbin so we might want to change from strictly a default Environment bean to an EnvironmentFactoryBean with a specific configuration thinking about it now I maybe should add a namespace element for the Environment","atomic","conjunctions","high",False
19383,"Use a profile or similar to only include the Environment conditionally currently in module common.xml. Also Jon Brisbin one thing to keep in mind we talked about having a properties file for XD that configured the RingBuffer et al in a non default way Jon Brisbin e.g. no event loop Dispatchers a ThreadPoolDispatcher with a large thread pool size 50 threads? 100? and maybe even two RingBufferDispatchers input and output Jon Brisbin so we might want to change from strictly a default Environment bean to an EnvironmentFactoryBean with a specific configuration thinking about it now I maybe should add a namespace element for the Environment",NULL,"Use a profile or similar to only include the Environment conditionally currently in module common.xml. Al","so Jon Brisbin one thing to keep in mind we talked about having a properties file for XD that configured the RingBuffer et al in a non default way Jon Brisbin e.g. no event loop Dispatchers a ThreadPoolDispatcher with a large thread pool size 50 threads? 100? and maybe even two RingBufferDispatchers input and output Jon Brisbin so we might want to change from strictly a default Environment bean to an EnvironmentFactoryBean with a specific configuration thinking about it now I maybe should add a namespace element for the Environment","Use a profile or similar to only include the Environment conditionally currently in module common<span class='highlight-text severity-high'>.xml. Also Jon Brisbin one thing to keep in mind we talked about having a properties file for XD that configured the RingBuffer et al in a non default way Jon Brisbin e.g. no event loop Dispatchers a ThreadPoolDispatcher with a large thread pool size 50 threads? 100? and maybe even two RingBufferDispatchers input and output Jon Brisbin so we might want to change from strictly a default Environment bean to an EnvironmentFactoryBean with a specific configuration thinking about it now I maybe should add a namespace element for the Environment</span>","minimal","punctuation","high",False
19383,"Use a profile or similar to only include the Environment conditionally currently in module common.xml. Also Jon Brisbin one thing to keep in mind we talked about having a properties file for XD that configured the RingBuffer et al in a non default way Jon Brisbin e.g. no event loop Dispatchers a ThreadPoolDispatcher with a large thread pool size 50 threads? 100? and maybe even two RingBufferDispatchers input and output Jon Brisbin so we might want to change from strictly a default Environment bean to an EnvironmentFactoryBean with a specific configuration thinking about it now I maybe should add a namespace element for the Environment",NULL,"Use a profile or similar to only include the Environment conditionally currently in module common.xml. Al","so Jon Brisbin one thing to keep in mind we talked about having a properties file for XD that configured the RingBuffer et al in a non default way Jon Brisbin e.g. no event loop Dispatchers a ThreadPoolDispatcher with a large thread pool size 50 threads? 100? and maybe even two RingBufferDispatchers input and output Jon Brisbin so we might want to change from strictly a default Environment bean to an EnvironmentFactoryBean with a specific configuration thinking about it now I maybe should add a namespace element for the Environment","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19382,"UDP and Legacy syslog sources emit a Map ; reactor emits a POJO. Make them consistent and emit Tuple s.",NULL,"UDP and Legacy syslog sources emit a Map ; reactor emits a POJO. Make them consistent and emit Tuple s.",NULL,"Add for who this story is","well_formed","no_role","high",False
19382,"UDP and Legacy syslog sources emit a Map ; reactor emits a POJO. Make them consistent and emit Tuple s.",NULL,"UDP and Legacy syslog sources emit a Map ; reactor emits a POJO. Make them consistent and emit Tuple s.",NULL,"UDP<span class='highlight-text severity-high'> and </span>Legacy syslog sources emit a Map ; reactor emits a POJO. Make them consistent and emit Tuple s.","atomic","conjunctions","high",False
19384,"container and event . XDContainer references and is referenced by ContainerStartedEvent and stopped . https sonar.springsource.org drilldown measures 7173?metric package cycles rids 5B 5D 7717 ",NULL,NULL,NULL,"container and event <span class='highlight-text severity-high'>. XDContainer references and is referenced by ContainerStartedEvent and stopped . https sonar.springsource.org drilldown measures 7173?metric package cycles rids 5B 5D 7717 </span>","minimal","punctuation","high",False
19388,"1. We ll need a system which give better control of what yarn xd containers are out there and what is a status of those containers. 2. We also need grouping of containers order to choose, prioritize and scale tasks. 3. We need heartbeating of the grid nodes. Hadoop Yarn itself doesn t give enough tools to know if container is alive .",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19388,"1. We ll need a system which give better control of what yarn xd containers are out there and what is a status of those containers. 2. We also need grouping of containers order to choose, prioritize and scale tasks. 3. We need heartbeating of the grid nodes. Hadoop Yarn itself doesn t give enough tools to know if container is alive .",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19388,"1. We ll need a system which give better control of what yarn xd containers are out there and what is a status of those containers. 2. We also need grouping of containers order to choose, prioritize and scale tasks. 3. We need heartbeating of the grid nodes. Hadoop Yarn itself doesn t give enough tools to know if container is alive .",NULL,NULL,NULL,"1<span class='highlight-text severity-high'>. We ll need a system which give better control of what yarn xd containers are out there and what is a status of those containers. 2. We also need grouping of containers order to choose, prioritize and scale tasks. 3. We need heartbeating of the grid nodes. Hadoop Yarn itself doesn t give enough tools to know if container is alive .</span>","minimal","punctuation","high",False
19390,"1. How we talk to the XD instance s on Yarn 2. There is a rest interface which location can be exposed either via resource manager or appmaster 3. Technically appmaster could also expose interface which could eihter be proxy for xd rest or dedicated interface implementation i.e. thrift or spring int ",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19390,"1. How we talk to the XD instance s on Yarn 2. There is a rest interface which location can be exposed either via resource manager or appmaster 3. Technically appmaster could also expose interface which could eihter be proxy for xd rest or dedicated interface implementation i.e. thrift or spring int ",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19390,"1. How we talk to the XD instance s on Yarn 2. There is a rest interface which location can be exposed either via resource manager or appmaster 3. Technically appmaster could also expose interface which could eihter be proxy for xd rest or dedicated interface implementation i.e. thrift or spring int ",NULL,NULL,NULL,"1<span class='highlight-text severity-high'>. How we talk to the XD instance s on Yarn 2. There is a rest interface which location can be exposed either via resource manager or appmaster 3. Technically appmaster could also expose interface which could eihter be proxy for xd rest or dedicated interface implementation i.e. thrift or spring int </span>","minimal","punctuation","high",False
19386,"Currently the Job launcher launches all the batch jobs configured in the job module. Please refer, ModuleJobLauncher s executeBatchJob . This makes the JobRegistry registers with multiple batch jobs under the same Spring XD job name group name . Also, it is understood that having multiple jobs configuration under the same config xml is uncommon.",NULL,"Currently the Job launcher launches all the batch jobs configured in the job module. Please refer, ModuleJobLauncher s executeBatchJob . This makes the JobRegistry registers with multiple batch jobs under the same Spring XD job name group name . Also, it is understood that having multiple jobs configuration under the same config xml is uncommon.",NULL,"Add for who this story is","well_formed","no_role","high",False
19386,"Currently the Job launcher launches all the batch jobs configured in the job module. Please refer, ModuleJobLauncher s executeBatchJob . This makes the JobRegistry registers with multiple batch jobs under the same Spring XD job name group name . Also, it is understood that having multiple jobs configuration under the same config xml is uncommon.",NULL,"Currently the Job launcher launches all the batch jobs configured in the job module. Please refer, ModuleJobLauncher s executeBatchJob . This makes the JobRegistry registers with multiple batch jobs under the same Spring XD job name group name . Also, it is understood that having multiple jobs configuration under the same config xml is uncommon.",NULL,"Currently the Job launcher launches all the batch jobs configured in the job module<span class='highlight-text severity-high'>. Please refer, ModuleJobLauncher s executeBatchJob . This makes the JobRegistry registers with multiple batch jobs under the same Spring XD job name group name . Also, it is understood that having multiple jobs configuration under the same config xml is uncommon.</span>","minimal","punctuation","high",False
19387,"Technically speaking of we want to integrate XD UI on Hadoop tools we should do it so that the proxy on resource manager works with XD UI. From Hadoop Yarn resource manager point of view this proxied url is the applications tracking url which is registered when application is deployed .",NULL,"Technically speaking of we want to integrate XD UI on Hadoop tools we should do it","so that the proxy on resource manager works with XD UI. From Hadoop Yarn resource manager point of view this proxied url is the applications tracking url which is registered when application is deployed .","Add for who this story is","well_formed","no_role","high",False
19387,"Technically speaking of we want to integrate XD UI on Hadoop tools we should do it so that the proxy on resource manager works with XD UI. From Hadoop Yarn resource manager point of view this proxied url is the applications tracking url which is registered when application is deployed .",NULL,"Technically speaking of we want to integrate XD UI on Hadoop tools we should do it","so that the proxy on resource manager works with XD UI. From Hadoop Yarn resource manager point of view this proxied url is the applications tracking url which is registered when application is deployed .","Technically speaking of we want to integrate XD UI on Hadoop tools we should do it so that the proxy on resource manager works with XD UI<span class='highlight-text severity-high'>. From Hadoop Yarn resource manager point of view this proxied url is the applications tracking url which is registered when application is deployed .</span>","minimal","punctuation","high",False
19387,"Technically speaking of we want to integrate XD UI on Hadoop tools we should do it so that the proxy on resource manager works with XD UI. From Hadoop Yarn resource manager point of view this proxied url is the applications tracking url which is registered when application is deployed .",NULL,"Technically speaking of we want to integrate XD UI on Hadoop tools we should do it","so that the proxy on resource manager works with XD UI. From Hadoop Yarn resource manager point of view this proxied url is the applications tracking url which is registered when application is deployed .","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19378,"This is issue depends on XD 761 https build.springsource.org browse XD JDK8",NULL,"This is issue depends on XD 761 https build.springsource.org browse XD JDK8",NULL,"Add for who this story is","well_formed","no_role","high",False
19389,"We need to be able to talk to appmaster which will control the whole xd yarn app. 1. Choose the implementation? Thrift? Spring Int? Something else? ",NULL,"We need to be able to talk to appmaster which will control the whole xd yarn app. 1. Choose the implementation? Thrift? Spring Int? Something else? ",NULL,"Add for who this story is","well_formed","no_role","high",False
19389,"We need to be able to talk to appmaster which will control the whole xd yarn app. 1. Choose the implementation? Thrift? Spring Int? Something else? ",NULL,"We need to be able to talk to appmaster which will control the whole xd yarn app. 1. Choose the implementation? Thrift? Spring Int? Something else? ",NULL,"We need to be able to talk to appmaster which will control the whole xd yarn app<span class='highlight-text severity-high'>. 1. Choose the implementation? Thrift? Spring Int? Something else? </span>","minimal","punctuation","high",False
19379,"JavaDoc issues are causing the build to fail with Java 8",NULL,"JavaDoc issues are causing the build to fail with Java 8",NULL,"Add for who this story is","well_formed","no_role","high",False
19380,"https sonar.springsource.org drilldown measures 7173?metric package cycles rids 5B 5D 7717",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19380,"https sonar.springsource.org drilldown measures 7173?metric package cycles rids 5B 5D 7717",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19381,"The xd singlenode script currently has 644 permissions unlike xd admin and xd container which have 755 code rwxr xr x 1 mark staff 5899 Aug 26 16 19 xd admin rwxr xr x 1 mark staff 5955 Aug 26 16 19 xd container rw r r 1 mark staff 5919 Aug 26 16 19 xd singlenode code ",NULL,"The xd singlenode script currently has 644 permissions unlike xd admin and xd container which have 755 code rwxr xr x 1 mark staff 5899 Aug 26 16 19 xd admin rwxr xr x 1 mark staff 5955 Aug 26 16 19 xd container rw r r 1 mark staff 5919 Aug 26 16 19 xd singlenode code ",NULL,"Add for who this story is","well_formed","no_role","high",False
19381,"The xd singlenode script currently has 644 permissions unlike xd admin and xd container which have 755 code rwxr xr x 1 mark staff 5899 Aug 26 16 19 xd admin rwxr xr x 1 mark staff 5955 Aug 26 16 19 xd container rw r r 1 mark staff 5919 Aug 26 16 19 xd singlenode code ",NULL,"The xd singlenode script currently has 644 permissions unlike xd admin and xd container which have 755 code rwxr xr x 1 mark staff 5899 Aug 26 16 19 xd admin rwxr xr x 1 mark staff 5955 Aug 26 16 19 xd container rw r r 1 mark staff 5919 Aug 26 16 19 xd singlenode code ",NULL,"The xd singlenode script currently has 644 permissions unlike xd admin<span class='highlight-text severity-high'> and </span>xd container which have 755 code rwxr xr x 1 mark staff 5899 Aug 26 16 19 xd admin rwxr xr x 1 mark staff 5955 Aug 26 16 19 xd container rw r r 1 mark staff 5919 Aug 26 16 19 xd singlenode code ","atomic","conjunctions","high",False
19392,"Currently Main class provide alternate static methods for parsing CLI options. One used for testing does not call System.exit just throws an exception. This code should be moved to spring xd test to support integration testing.",NULL,"Currently Main class provide alternate static methods for parsing CLI options. One used for testing does not call System.exit just throws an exception. This code should be moved to spring xd test to support integration testing.",NULL,"Add for who this story is","well_formed","no_role","high",False
19392,"Currently Main class provide alternate static methods for parsing CLI options. One used for testing does not call System.exit just throws an exception. This code should be moved to spring xd test to support integration testing.",NULL,"Currently Main class provide alternate static methods for parsing CLI options. One used for testing does not call System.exit just throws an exception. This code should be moved to spring xd test to support integration testing.",NULL,"Currently Main class provide alternate static methods for parsing CLI options<span class='highlight-text severity-high'>. One used for testing does not call System.exit just throws an exception. This code should be moved to spring xd test to support integration testing.</span>","minimal","punctuation","high",False
19393,"Remove System.setProperty or System.getProperty for internal xd properties. Use spring Environment abstraction instead. Also, replace . in property names with XDPropertyKeys . This is compatible with environment variable names. As a result, XD should accept System properties or environment variables or command line options. Command line options should have highest precedence. Retain StandardEnvironment order wrt to System properties and environment variable. ",NULL,"Remove System.setProperty or System.getProperty for internal xd properties. Use spring Environment abstraction instead. Also, replace . in property names with XDPropertyKeys . This is compatible with environment variable names. As a result, XD should accept System properties or environment variables or command line options. Command line options should have highest precedence. Retain StandardEnvironment order wrt to System properties and environment variable. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19393,"Remove System.setProperty or System.getProperty for internal xd properties. Use spring Environment abstraction instead. Also, replace . in property names with XDPropertyKeys . This is compatible with environment variable names. As a result, XD should accept System properties or environment variables or command line options. Command line options should have highest precedence. Retain StandardEnvironment order wrt to System properties and environment variable. ",NULL,"Remove System.setProperty or System.getProperty for internal xd properties. Use spring Environment abstraction instead. Also, replace . in property names with XDPropertyKeys . This is compatible with environment variable names. As a result, XD should accept System properties or environment variables or command line options. Command line options should have highest precedence. Retain StandardEnvironment order wrt to System properties and environment variable. ",NULL,"Remove System.setProperty<span class='highlight-text severity-high'> or </span>System.getProperty for internal xd properties. Use spring Environment abstraction instead. Also, replace . in property names with XDPropertyKeys . This is compatible with environment variable names. As a result, XD should accept System properties or environment variables or command line options. Command line options should have highest precedence. Retain StandardEnvironment order wrt to System properties<span class='highlight-text severity-high'> and </span>environment variable. ","atomic","conjunctions","high",False
19393,"Remove System.setProperty or System.getProperty for internal xd properties. Use spring Environment abstraction instead. Also, replace . in property names with XDPropertyKeys . This is compatible with environment variable names. As a result, XD should accept System properties or environment variables or command line options. Command line options should have highest precedence. Retain StandardEnvironment order wrt to System properties and environment variable. ",NULL,"Remove System.setProperty or System.getProperty for internal xd properties. Use spring Environment abstraction instead. Also, replace . in property names with XDPropertyKeys . This is compatible with environment variable names. As a result, XD should accept System properties or environment variables or command line options. Command line options should have highest precedence. Retain StandardEnvironment order wrt to System properties and environment variable. ",NULL,"Remove System<span class='highlight-text severity-high'>.setProperty or System.getProperty for internal xd properties. Use spring Environment abstraction instead. Also, replace . in property names with XDPropertyKeys . This is compatible with environment variable names. As a result, XD should accept System properties or environment variables or command line options. Command line options should have highest precedence. Retain StandardEnvironment order wrt to System properties and environment variable. </span>","minimal","punctuation","high",False
19394,"Rename XDContainer, ContainerMain, ContainerLauncher, ContainerLauncherFactory and any variable or methodNames, bean names, etc. that refer to container in favor of the term Node . Eliminate the dirt.container package, and move Node into .server",NULL,"Rename XDContainer, ContainerMain, ContainerLauncher, ContainerLauncherFactory and any variable or methodNames, bean names, etc. that refer to container in favor of the term Node . Eliminate the dirt.container package, and move Node into .server",NULL,"Add for who this story is","well_formed","no_role","high",False
19394,"Rename XDContainer, ContainerMain, ContainerLauncher, ContainerLauncherFactory and any variable or methodNames, bean names, etc. that refer to container in favor of the term Node . Eliminate the dirt.container package, and move Node into .server",NULL,"Rename XDContainer, ContainerMain, ContainerLauncher, ContainerLauncherFactory and any variable or methodNames, bean names, etc. that refer to container in favor of the term Node . Eliminate the dirt.container package, and move Node into .server",NULL,"Rename XDContainer, ContainerMain, ContainerLauncher, ContainerLauncherFactory<span class='highlight-text severity-high'> and </span>any variable<span class='highlight-text severity-high'> or </span>methodNames, bean names, etc. that refer to container in favor of the term Node . Eliminate the dirt.container package, and move Node into .server","atomic","conjunctions","high",False
19394,"Rename XDContainer, ContainerMain, ContainerLauncher, ContainerLauncherFactory and any variable or methodNames, bean names, etc. that refer to container in favor of the term Node . Eliminate the dirt.container package, and move Node into .server",NULL,"Rename XDContainer, ContainerMain, ContainerLauncher, ContainerLauncherFactory and any variable or methodNames, bean names, etc. that refer to container in favor of the term Node . Eliminate the dirt.container package, and move Node into .server",NULL,"Rename XDContainer, ContainerMain, ContainerLauncher, ContainerLauncherFactory and any variable or methodNames, bean names, etc<span class='highlight-text severity-high'>. that refer to container in favor of the term Node . Eliminate the dirt.container package, and move Node into .server</span>","minimal","punctuation","high",False
19395,"Trigger can send a message to a named channel. For example trigger create name mytrigger definition trigger cron 10 message Good Luck, we are all counting on you channel foo Where the message contains the message that will be sent to foo job component.",NULL,"Trigger can send a message to a named channel. For example trigger create name mytrigger definition trigger cron 10 message Good Luck, we are all counting on you channel foo Where the message contains the message that will be sent to foo job component.",NULL,"Add for who this story is","well_formed","no_role","high",False
19395,"Trigger can send a message to a named channel. For example trigger create name mytrigger definition trigger cron 10 message Good Luck, we are all counting on you channel foo Where the message contains the message that will be sent to foo job component.",NULL,"Trigger can send a message to a named channel. For example trigger create name mytrigger definition trigger cron 10 message Good Luck, we are all counting on you channel foo Where the message contains the message that will be sent to foo job component.",NULL,"Trigger can send a message to a named channel<span class='highlight-text severity-high'>. For example trigger create name mytrigger definition trigger cron 10 message Good Luck, we are all counting on you channel foo Where the message contains the message that will be sent to foo job component.</span>","minimal","punctuation","high",False
19396,"This story utilizes BatchAdmin and its restful interfaces to show the state of jobs in Spring XD. Steps Create a Branch in the BatchAdmin we don t want to lose history Update the restful API s to XD standards. Create bamboo task to push jars to artifactory Update gradle.build to pull in the Batchadmin jars. Expose the restful calls.",NULL,"This story utilizes BatchAdmin and its restful interfaces to show the state of jobs in Spring XD. Steps Create a Branch in the BatchAdmin we don t want to lose history Update the restful API s to XD standards. Create bamboo task to push jars to artifactory Update gradle.build to pull in the Batchadmin jars. Expose the restful calls.",NULL,"Add for who this story is","well_formed","no_role","high",False
19396,"This story utilizes BatchAdmin and its restful interfaces to show the state of jobs in Spring XD. Steps Create a Branch in the BatchAdmin we don t want to lose history Update the restful API s to XD standards. Create bamboo task to push jars to artifactory Update gradle.build to pull in the Batchadmin jars. Expose the restful calls.",NULL,"This story utilizes BatchAdmin and its restful interfaces to show the state of jobs in Spring XD. Steps Create a Branch in the BatchAdmin we don t want to lose history Update the restful API s to XD standards. Create bamboo task to push jars to artifactory Update gradle.build to pull in the Batchadmin jars. Expose the restful calls.",NULL,"This story utilizes BatchAdmin<span class='highlight-text severity-high'> and </span>its restful interfaces to show the state of jobs in Spring XD. Steps Create a Branch in the BatchAdmin we don t want to lose history Update the restful API s to XD standards. Create bamboo task to push jars to artifactory Update gradle.build to pull in the Batchadmin jars. Expose the restful calls.","atomic","conjunctions","high",False
19396,"This story utilizes BatchAdmin and its restful interfaces to show the state of jobs in Spring XD. Steps Create a Branch in the BatchAdmin we don t want to lose history Update the restful API s to XD standards. Create bamboo task to push jars to artifactory Update gradle.build to pull in the Batchadmin jars. Expose the restful calls.",NULL,"This story utilizes BatchAdmin and its restful interfaces to show the state of jobs in Spring XD. Steps Create a Branch in the BatchAdmin we don t want to lose history Update the restful API s to XD standards. Create bamboo task to push jars to artifactory Update gradle.build to pull in the Batchadmin jars. Expose the restful calls.",NULL,"This story utilizes BatchAdmin and its restful interfaces to show the state of jobs in Spring XD<span class='highlight-text severity-high'>. Steps Create a Branch in the BatchAdmin we don t want to lose history Update the restful API s to XD standards. Create bamboo task to push jars to artifactory Update gradle.build to pull in the Batchadmin jars. Expose the restful calls.</span>","minimal","punctuation","high",False
19399,"The JobLaunchRequest Transformer shall accept the following payloads File JSON String Properties Map Tuple Use Migrate some of the logic from JobParametersBean , e.g. using the DefaultJobParametersConverter . Special Case File When handling a File , add special JobParameter absoluteFilePath populating it with message.getPayload .getAbsolutePath Add unit tests ",NULL,"The JobLaunchRequest Transformer shall accept the following payloads File JSON String Properties Map Tuple Use Migrate some of the logic from JobParametersBean , e.g. using the DefaultJobParametersConverter . Special Case File When handling a File , add special JobParameter absoluteFilePath populating it with message.getPayload .getAbsolutePath Add unit tests ",NULL,"Add for who this story is","well_formed","no_role","high",False
19399,"The JobLaunchRequest Transformer shall accept the following payloads File JSON String Properties Map Tuple Use Migrate some of the logic from JobParametersBean , e.g. using the DefaultJobParametersConverter . Special Case File When handling a File , add special JobParameter absoluteFilePath populating it with message.getPayload .getAbsolutePath Add unit tests ",NULL,"The JobLaunchRequest Transformer shall accept the following payloads File JSON String Properties Map Tuple Use Migrate some of the logic from JobParametersBean , e.g. using the DefaultJobParametersConverter . Special Case File When handling a File , add special JobParameter absoluteFilePath populating it with message.getPayload .getAbsolutePath Add unit tests ",NULL,"The JobLaunchRequest Transformer shall accept the following payloads File JSON String Properties Map Tuple Use Migrate some of the logic from JobParametersBean , e<span class='highlight-text severity-high'>.g. using the DefaultJobParametersConverter . Special Case File When handling a File , add special JobParameter absoluteFilePath populating it with message.getPayload .getAbsolutePath Add unit tests </span>","minimal","punctuation","high",False
19397,"The HSQLDB that stores the JobRepository needs to have its content exposed via TCP network service so that container has access to update the status of a job run. Needs to have property that enumerates the host and port for the admin that is accessible by the user. If one is not specified it should default to localhost 9500. Testing Unit Tests Bring up module and admin. ^Verify that default host and port work ^Verify that container on different machine has access to admin ",NULL,"The HSQLDB that stores the JobRepository needs to have its content exposed via TCP network service","so that container has access to update the status of a job run. Needs to have property that enumerates the host and port for the admin that is accessible by the user. If one is not specified it should default to localhost 9500. Testing Unit Tests Bring up module and admin. ^Verify that default host and port work ^Verify that container on different machine has access to admin","Add for who this story is","well_formed","no_role","high",False
19397,"The HSQLDB that stores the JobRepository needs to have its content exposed via TCP network service so that container has access to update the status of a job run. Needs to have property that enumerates the host and port for the admin that is accessible by the user. If one is not specified it should default to localhost 9500. Testing Unit Tests Bring up module and admin. ^Verify that default host and port work ^Verify that container on different machine has access to admin ",NULL,"The HSQLDB that stores the JobRepository needs to have its content exposed via TCP network service","so that container has access to update the status of a job run. Needs to have property that enumerates the host and port for the admin that is accessible by the user. If one is not specified it should default to localhost 9500. Testing Unit Tests Bring up module and admin. ^Verify that default host and port work ^Verify that container on different machine has access to admin","The HSQLDB that stores the JobRepository needs to have its content exposed via TCP network service so that container has access to update the status of a job run<span class='highlight-text severity-high'>. Needs to have property that enumerates the host and port for the admin that is accessible by the user. If one is not specified it should default to localhost 9500. Testing Unit Tests Bring up module and admin. ^Verify that default host and port work ^Verify that container on different machine has access to admin </span>","minimal","punctuation","high",False
19397,"The HSQLDB that stores the JobRepository needs to have its content exposed via TCP network service so that container has access to update the status of a job run. Needs to have property that enumerates the host and port for the admin that is accessible by the user. If one is not specified it should default to localhost 9500. Testing Unit Tests Bring up module and admin. ^Verify that default host and port work ^Verify that container on different machine has access to admin ",NULL,"The HSQLDB that stores the JobRepository needs to have its content exposed via TCP network service","so that container has access to update the status of a job run. Needs to have property that enumerates the host and port for the admin that is accessible by the user. If one is not specified it should default to localhost 9500. Testing Unit Tests Bring up module and admin. ^Verify that default host and port work ^Verify that container on different machine has access to admin","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19391,"We should depend on dependency groupId org.springframework.batch groupId artifactId spring batch admin manager artifactId version project.parent.version version dependency dependency groupId org.springframework.batch groupId artifactId spring batch admin resources artifactId version project.parent.version version dependency we are using spring batch 2.2.0.RELEASE. We need to depend on spring batch admin version 1.3.0.BUILD SNAPSHOT ",NULL,"We should depend on dependency groupId org.springframework.batch groupId artifactId spring batch admin manager artifactId version project.parent.version version dependency dependency groupId org.springframework.batch groupId artifactId spring batch admin resources artifactId version project.parent.version version dependency we are using spring batch 2.2.0.RELEASE. We need to depend on spring batch admin version 1.3.0.BUILD SNAPSHOT ",NULL,"Add for who this story is","well_formed","no_role","high",False
19391,"We should depend on dependency groupId org.springframework.batch groupId artifactId spring batch admin manager artifactId version project.parent.version version dependency dependency groupId org.springframework.batch groupId artifactId spring batch admin resources artifactId version project.parent.version version dependency we are using spring batch 2.2.0.RELEASE. We need to depend on spring batch admin version 1.3.0.BUILD SNAPSHOT ",NULL,"We should depend on dependency groupId org.springframework.batch groupId artifactId spring batch admin manager artifactId version project.parent.version version dependency dependency groupId org.springframework.batch groupId artifactId spring batch admin resources artifactId version project.parent.version version dependency we are using spring batch 2.2.0.RELEASE. We need to depend on spring batch admin version 1.3.0.BUILD SNAPSHOT ",NULL,"We should depend on dependency groupId org<span class='highlight-text severity-high'>.springframework.batch groupId artifactId spring batch admin manager artifactId version project.parent.version version dependency dependency groupId org.springframework.batch groupId artifactId spring batch admin resources artifactId version project.parent.version version dependency we are using spring batch 2.2.0.RELEASE. We need to depend on spring batch admin version 1.3.0.BUILD SNAPSHOT </span>","minimal","punctuation","high",False
19406,"Register the module under test Send a message to the sink using a test source and verify the sink contents this requires checking an external resource depends on the sink ",NULL,"Register the module under test Send a message to the sink using a test source and verify the sink contents this requires checking an external resource depends on the sink ",NULL,"Add for who this story is","well_formed","no_role","high",False
19406,"Register the module under test Send a message to the sink using a test source and verify the sink contents this requires checking an external resource depends on the sink ",NULL,"Register the module under test Send a message to the sink using a test source and verify the sink contents this requires checking an external resource depends on the sink ",NULL,"Register the module under test Send a message to the sink using a test source<span class='highlight-text severity-high'> and </span>verify the sink contents this requires checking an external resource depends on the sink ","atomic","conjunctions","high",False
19405,"Create a directory structure that best benefits UI'development. The copying of the UI files and other gradle build tasks so that the UI can be run inside the embedded servlet container of XD will be a seperate story",NULL,"Create a directory structure that best benefits UI'development. The copying of the UI files and other gradle build tasks","so that the UI can be run inside the embedded servlet container of XD will be a seperate story","Add for who this story is","well_formed","no_role","high",False
19405,"Create a directory structure that best benefits UI'development. The copying of the UI files and other gradle build tasks so that the UI can be run inside the embedded servlet container of XD will be a seperate story",NULL,"Create a directory structure that best benefits UI'development. The copying of the UI files and other gradle build tasks","so that the UI can be run inside the embedded servlet container of XD will be a seperate story","Create a directory structure that best benefits UI'development. The copying of the UI files<span class='highlight-text severity-high'> and </span>other gradle build tasks so that the UI can be run inside the embedded servlet container of XD will be a seperate story","atomic","conjunctions","high",False
19405,"Create a directory structure that best benefits UI'development. The copying of the UI files and other gradle build tasks so that the UI can be run inside the embedded servlet container of XD will be a seperate story",NULL,"Create a directory structure that best benefits UI'development. The copying of the UI files and other gradle build tasks","so that the UI can be run inside the embedded servlet container of XD will be a seperate story","Create a directory structure that best benefits UI'development<span class='highlight-text severity-high'>. The copying of the UI files and other gradle build tasks so that the UI can be run inside the embedded servlet container of XD will be a seperate story</span>","minimal","punctuation","high",False
19405,"Create a directory structure that best benefits UI'development. The copying of the UI files and other gradle build tasks so that the UI can be run inside the embedded servlet container of XD will be a seperate story",NULL,"Create a directory structure that best benefits UI'development. The copying of the UI files and other gradle build tasks","so that the UI can be run inside the embedded servlet container of XD will be a seperate story","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19410,"This is the other side of launching a job by sending a message. The Job plugin should add listeners at the job step level so that the job step context information can be sent out on a channel. myjob.notifications is a suggested channel name that would be created automatically.",NULL,"This is the other side of launching a job by sending a message. The Job plugin should add listeners at the job step level","so that the job step context information can be sent out on a channel. myjob.notifications is a suggested channel name that would be created automatically.","Add for who this story is","well_formed","no_role","high",False
19410,"This is the other side of launching a job by sending a message. The Job plugin should add listeners at the job step level so that the job step context information can be sent out on a channel. myjob.notifications is a suggested channel name that would be created automatically.",NULL,"This is the other side of launching a job by sending a message. The Job plugin should add listeners at the job step level","so that the job step context information can be sent out on a channel. myjob.notifications is a suggested channel name that would be created automatically.","This is the other side of launching a job by sending a message<span class='highlight-text severity-high'>. The Job plugin should add listeners at the job step level so that the job step context information can be sent out on a channel. myjob.notifications is a suggested channel name that would be created automatically.</span>","minimal","punctuation","high",False
19410,"This is the other side of launching a job by sending a message. The Job plugin should add listeners at the job step level so that the job step context information can be sent out on a channel. myjob.notifications is a suggested channel name that would be created automatically.",NULL,"This is the other side of launching a job by sending a message. The Job plugin should add listeners at the job step level","so that the job step context information can be sent out on a channel. myjob.notifications is a suggested channel name that would be created automatically.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19407,"Register the module under test and have access to a source channel that drives messages into the processor and a output channel where output messages are sent. Examples Built in Message conversion send JSON to a processor module that accepts Tuples. ",NULL,"Register the module under test and have access to a source channel that drives messages into the processor and a output channel where output messages are sent. Examples Built in Message conversion send JSON to a processor module that accepts Tuples. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19407,"Register the module under test and have access to a source channel that drives messages into the processor and a output channel where output messages are sent. Examples Built in Message conversion send JSON to a processor module that accepts Tuples. ",NULL,"Register the module under test and have access to a source channel that drives messages into the processor and a output channel where output messages are sent. Examples Built in Message conversion send JSON to a processor module that accepts Tuples. ",NULL,"Register the module under test<span class='highlight-text severity-high'> and </span>have access to a source channel that drives messages into the processor and a output channel where output messages are sent. Examples Built in Message conversion send JSON to a processor module that accepts Tuples. ","atomic","conjunctions","high",False
19407,"Register the module under test and have access to a source channel that drives messages into the processor and a output channel where output messages are sent. Examples Built in Message conversion send JSON to a processor module that accepts Tuples. ",NULL,"Register the module under test and have access to a source channel that drives messages into the processor and a output channel where output messages are sent. Examples Built in Message conversion send JSON to a processor module that accepts Tuples. ",NULL,"Register the module under test and have access to a source channel that drives messages into the processor and a output channel where output messages are sent<span class='highlight-text severity-high'>. Examples Built in Message conversion send JSON to a processor module that accepts Tuples. </span>","minimal","punctuation","high",False
19408,"Register the module under test and deploy the module Verify output across all transports Examples Be able to start the rabbitmq source just by pointing to modules source rabbit.xml, pass in some property file for parameters to be replaced, and outgoing message is placed in a in memory queue backed channel for use with assertions to verify functionality. Test that sending json, results in media type header is set to json Test that sending POJO POJO Test that sending Tuple Tuple Test that sending a JSON String String Test that sending raw bytes raw bytes ",NULL,"Register the module under test and deploy the module Verify output across all transports Examples Be able to start the rabbitmq source just by pointing to modules source rabbit.xml, pass in some property file for parameters to be replaced, and outgoing message is placed in a in memory queue backed channel for use with assertions to verify functionality. Test that sending json, results in media type header is set to json Test that sending POJO POJO Test that sending Tuple Tuple Test that sending a JSON String String Test that sending raw bytes raw bytes ",NULL,"Add for who this story is","well_formed","no_role","high",False
19408,"Register the module under test and deploy the module Verify output across all transports Examples Be able to start the rabbitmq source just by pointing to modules source rabbit.xml, pass in some property file for parameters to be replaced, and outgoing message is placed in a in memory queue backed channel for use with assertions to verify functionality. Test that sending json, results in media type header is set to json Test that sending POJO POJO Test that sending Tuple Tuple Test that sending a JSON String String Test that sending raw bytes raw bytes ",NULL,"Register the module under test and deploy the module Verify output across all transports Examples Be able to start the rabbitmq source just by pointing to modules source rabbit.xml, pass in some property file for parameters to be replaced, and outgoing message is placed in a in memory queue backed channel for use with assertions to verify functionality. Test that sending json, results in media type header is set to json Test that sending POJO POJO Test that sending Tuple Tuple Test that sending a JSON String String Test that sending raw bytes raw bytes ",NULL,"Register the module under test<span class='highlight-text severity-high'> and </span>deploy the module Verify output across all transports Examples Be able to start the rabbitmq source just by pointing to modules source rabbit.xml, pass in some property file for parameters to be replaced, and outgoing message is placed in a in memory queue backed channel for use with assertions to verify functionality. Test that sending json, results in media type header is set to json Test that sending POJO POJO Test that sending Tuple Tuple Test that sending a JSON String String Test that sending raw bytes raw bytes ","atomic","conjunctions","high",False
19408,"Register the module under test and deploy the module Verify output across all transports Examples Be able to start the rabbitmq source just by pointing to modules source rabbit.xml, pass in some property file for parameters to be replaced, and outgoing message is placed in a in memory queue backed channel for use with assertions to verify functionality. Test that sending json, results in media type header is set to json Test that sending POJO POJO Test that sending Tuple Tuple Test that sending a JSON String String Test that sending raw bytes raw bytes ",NULL,"Register the module under test and deploy the module Verify output across all transports Examples Be able to start the rabbitmq source just by pointing to modules source rabbit.xml, pass in some property file for parameters to be replaced, and outgoing message is placed in a in memory queue backed channel for use with assertions to verify functionality. Test that sending json, results in media type header is set to json Test that sending POJO POJO Test that sending Tuple Tuple Test that sending a JSON String String Test that sending raw bytes raw bytes ",NULL,"Register the module under test and deploy the module Verify output across all transports Examples Be able to start the rabbitmq source just by pointing to modules source rabbit<span class='highlight-text severity-high'>.xml, pass in some property file for parameters to be replaced, and outgoing message is placed in a in memory queue backed channel for use with assertions to verify functionality. Test that sending json, results in media type header is set to json Test that sending POJO POJO Test that sending Tuple Tuple Test that sending a JSON String String Test that sending raw bytes raw bytes </span>","minimal","punctuation","high",False
19409,"There are a lot of Thread.sleep calls with delays chosen in the 1 2 seconds range. Change to a while loop with smaller pauses until a timeout is reached and give up. This applies to verification code e.g. verifying that a counter has expected value as well as File setup, or http being ready to accept requests etc",NULL,"There are a lot of Thread.sleep calls with delays chosen in the 1 2 seconds range. Change to a while loop with smaller pauses until a timeout is reached and give up. This applies to verification code e.g. verifying that a counter has expected value as well as File setup, or http being ready to accept requests etc",NULL,"Add for who this story is","well_formed","no_role","high",False
19409,"There are a lot of Thread.sleep calls with delays chosen in the 1 2 seconds range. Change to a while loop with smaller pauses until a timeout is reached and give up. This applies to verification code e.g. verifying that a counter has expected value as well as File setup, or http being ready to accept requests etc",NULL,"There are a lot of Thread.sleep calls with delays chosen in the 1 2 seconds range. Change to a while loop with smaller pauses until a timeout is reached and give up. This applies to verification code e.g. verifying that a counter has expected value as well as File setup, or http being ready to accept requests etc",NULL,"There are a lot of Thread.sleep calls with delays chosen in the 1 2 seconds range. Change to a while loop with smaller pauses until a timeout is reached<span class='highlight-text severity-high'> and </span>give up. This applies to verification code e.g. verifying that a counter has expected value as well as File setup,<span class='highlight-text severity-high'> or </span>http being ready to accept requests etc","atomic","conjunctions","high",False
19409,"There are a lot of Thread.sleep calls with delays chosen in the 1 2 seconds range. Change to a while loop with smaller pauses until a timeout is reached and give up. This applies to verification code e.g. verifying that a counter has expected value as well as File setup, or http being ready to accept requests etc",NULL,"There are a lot of Thread.sleep calls with delays chosen in the 1 2 seconds range. Change to a while loop with smaller pauses until a timeout is reached and give up. This applies to verification code e.g. verifying that a counter has expected value as well as File setup, or http being ready to accept requests etc",NULL,"There are a lot of Thread<span class='highlight-text severity-high'>.sleep calls with delays chosen in the 1 2 seconds range. Change to a while loop with smaller pauses until a timeout is reached and give up. This applies to verification code e.g. verifying that a counter has expected value as well as File setup, or http being ready to accept requests etc</span>","minimal","punctuation","high",False
19438,"the value for target is required there is no default , but the hint for that option states otherwise code xd http post target http post target required target the location to post to; default if option not present http localhost 9000 code ",NULL,"the value for target is required there is no default , but the hint for that option states otherwise code xd http post target http post target required target the location to post to; default if option not present http localhost 9000 code ",NULL,"the value for target is required there is no default , but the hint for that option states otherwise code xd http post target http post target required target the location to post to<span class='highlight-text severity-high'>; default if option not present http localhost 9000 code </span>","minimal","punctuation","high",False
19404,"The UI code will be sitting in one or more top level directories in the repository This story will address the need to 1 copy over the UI code into a location so that it can be picked up by the embedded servlet container when running inside eclipse",NULL,"The UI code will be sitting in one or more top level directories in the repository This story will address the need to 1 copy over the UI code into a location","so that it can be picked up by the embedded servlet container when running inside eclipse","Add for who this story is","well_formed","no_role","high",False
19404,"The UI code will be sitting in one or more top level directories in the repository This story will address the need to 1 copy over the UI code into a location so that it can be picked up by the embedded servlet container when running inside eclipse",NULL,"The UI code will be sitting in one or more top level directories in the repository This story will address the need to 1 copy over the UI code into a location","so that it can be picked up by the embedded servlet container when running inside eclipse","The UI code will be sitting in one<span class='highlight-text severity-high'> or </span>more top level directories in the repository This story will address the need to 1 copy over the UI code into a location so that it can be picked up by the embedded servlet container when running inside eclipse","atomic","conjunctions","high",False
19404,"The UI code will be sitting in one or more top level directories in the repository This story will address the need to 1 copy over the UI code into a location so that it can be picked up by the embedded servlet container when running inside eclipse",NULL,"The UI code will be sitting in one or more top level directories in the repository This story will address the need to 1 copy over the UI code into a location","so that it can be picked up by the embedded servlet container when running inside eclipse","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19403,"The UI code will be sitting in one or more top level directories in the repository This story will address the need to 1 copy over the UI code into a location so that it can be picked up by the embedded servlet container when the distribution zip is unzipped . After running . xd admin or . xd singlenode one should be able to hit the UI at http localhost 8999 xd just an example ",NULL,"The UI code will be sitting in one or more top level directories in the repository This story will address the need to 1 copy over the UI code into a location","so that it can be picked up by the embedded servlet container when the distribution zip is unzipped . After running . xd admin or . xd singlenode one should be able to hit the UI at http localhost 8999 xd just an example","Add for who this story is","well_formed","no_role","high",False
19403,"The UI code will be sitting in one or more top level directories in the repository This story will address the need to 1 copy over the UI code into a location so that it can be picked up by the embedded servlet container when the distribution zip is unzipped . After running . xd admin or . xd singlenode one should be able to hit the UI at http localhost 8999 xd just an example ",NULL,"The UI code will be sitting in one or more top level directories in the repository This story will address the need to 1 copy over the UI code into a location","so that it can be picked up by the embedded servlet container when the distribution zip is unzipped . After running . xd admin or . xd singlenode one should be able to hit the UI at http localhost 8999 xd just an example","The UI code will be sitting in one<span class='highlight-text severity-high'> or </span>more top level directories in the repository This story will address the need to 1 copy over the UI code into a location so that it can be picked up by the embedded servlet container when the distribution zip is unzipped . After running . xd admin or . xd singlenode one should be able to hit the UI at http localhost 8999 xd just an example ","atomic","conjunctions","high",False
19403,"The UI code will be sitting in one or more top level directories in the repository This story will address the need to 1 copy over the UI code into a location so that it can be picked up by the embedded servlet container when the distribution zip is unzipped . After running . xd admin or . xd singlenode one should be able to hit the UI at http localhost 8999 xd just an example ",NULL,"The UI code will be sitting in one or more top level directories in the repository This story will address the need to 1 copy over the UI code into a location","so that it can be picked up by the embedded servlet container when the distribution zip is unzipped . After running . xd admin or . xd singlenode one should be able to hit the UI at http localhost 8999 xd just an example","The UI code will be sitting in one or more top level directories in the repository This story will address the need to 1 copy over the UI code into a location so that it can be picked up by the embedded servlet container when the distribution zip is unzipped <span class='highlight-text severity-high'>. After running . xd admin or . xd singlenode one should be able to hit the UI at http localhost 8999 xd just an example </span>","minimal","punctuation","high",False
19403,"The UI code will be sitting in one or more top level directories in the repository This story will address the need to 1 copy over the UI code into a location so that it can be picked up by the embedded servlet container when the distribution zip is unzipped . After running . xd admin or . xd singlenode one should be able to hit the UI at http localhost 8999 xd just an example ",NULL,"The UI code will be sitting in one or more top level directories in the repository This story will address the need to 1 copy over the UI code into a location","so that it can be picked up by the embedded servlet container when the distribution zip is unzipped . After running . xd admin or . xd singlenode one should be able to hit the UI at http localhost 8999 xd just an example","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19733,"We need to have an externalized property file under xd conf for the xd container admin scripts to use as options. ",NULL,"We need to have an externalized property file under xd conf for the xd container admin scripts to use as options. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19419,"Need to support Rabbit virtual host property in properties file and as args to Rabbit source and sink",NULL,"Need to support Rabbit virtual host property in properties file and as args to Rabbit source and sink",NULL,"Add for who this story is","well_formed","no_role","high",False
19420,"Remove bean id idGenerator class org.springframework.xd.dirt.container.UUIDGenerator container.xml Delete org.springframework.xd.dirt.container.UUIDGenerator remove compile dependency on eaio from build.gradle",NULL,"Remove bean id idGenerator class org.springframework.xd.dirt.container.UUIDGenerator container.xml Delete org.springframework.xd.dirt.container.UUIDGenerator remove compile dependency on eaio from build.gradle",NULL,"Add for who this story is","well_formed","no_role","high",False
19414,"The above method is invoked before the shared context is refreshed. preProcess... is more accurate",NULL,"The above method is invoked before the shared context is refreshed. preProcess... is more accurate",NULL,"Add for who this story is","well_formed","no_role","high",False
19414,"The above method is invoked before the shared context is refreshed. preProcess... is more accurate",NULL,"The above method is invoked before the shared context is refreshed. preProcess... is more accurate",NULL,"The above method is invoked before the shared context is refreshed<span class='highlight-text severity-high'>. preProcess... is more accurate</span>","minimal","punctuation","high",False
19413,"Convert a simple module, such as file, to further test that what was done in the previous story, Use ParentLastClassLoader to create the Modules ApplicationContext. works as expected. Implementation Suggestions Remove from build.gradle the dependency on spring integration file and place that jar inside a directory . modules source file lib place the current file.xml inside . modules source file config How to verify it works. 1. Running tests that currently use the file source, e.g. in spring shell, should work as before 2. When deploying a stream file log, we should be able to interrogate the channel registry and make sure it found the dependencies for the module, ModuleDescriptor should have a not null URL property.",NULL,"Convert a simple module, such as file, to further test that what was done in the previous story, Use ParentLastClassLoader to create the Modules ApplicationContext. works as expected. Implementation Suggestions Remove from build.gradle the dependency on spring integration file and place that jar inside a directory . modules source file lib place the current file.xml inside . modules source file config How to verify it works. 1. Running tests that currently use the file source, e.g. in spring shell, should work as before 2. When deploying a stream file log, we should be able to interrogate the channel registry and make sure it found the dependencies for the module, ModuleDescriptor should have a not null URL property.",NULL,"Add for who this story is","well_formed","no_role","high",False
19413,"Convert a simple module, such as file, to further test that what was done in the previous story, Use ParentLastClassLoader to create the Modules ApplicationContext. works as expected. Implementation Suggestions Remove from build.gradle the dependency on spring integration file and place that jar inside a directory . modules source file lib place the current file.xml inside . modules source file config How to verify it works. 1. Running tests that currently use the file source, e.g. in spring shell, should work as before 2. When deploying a stream file log, we should be able to interrogate the channel registry and make sure it found the dependencies for the module, ModuleDescriptor should have a not null URL property.",NULL,"Convert a simple module, such as file, to further test that what was done in the previous story, Use ParentLastClassLoader to create the Modules ApplicationContext. works as expected. Implementation Suggestions Remove from build.gradle the dependency on spring integration file and place that jar inside a directory . modules source file lib place the current file.xml inside . modules source file config How to verify it works. 1. Running tests that currently use the file source, e.g. in spring shell, should work as before 2. When deploying a stream file log, we should be able to interrogate the channel registry and make sure it found the dependencies for the module, ModuleDescriptor should have a not null URL property.",NULL,"Convert a simple module, such as file, to further test that what was done in the previous story, Use ParentLastClassLoader to create the Modules ApplicationContext. works as expected. Implementation Suggestions Remove from build.gradle the dependency on spring integration file<span class='highlight-text severity-high'> and </span>place that jar inside a directory . modules source file lib place the current file.xml inside . modules source file config How to verify it works. 1. Running tests that currently use the file source, e.g. in spring shell, should work as before 2. When deploying a stream file log, we should be able to interrogate the channel registry and make sure it found the dependencies for the module, ModuleDescriptor should have a not null URL property.","atomic","conjunctions","high",False
19413,"Convert a simple module, such as file, to further test that what was done in the previous story, Use ParentLastClassLoader to create the Modules ApplicationContext. works as expected. Implementation Suggestions Remove from build.gradle the dependency on spring integration file and place that jar inside a directory . modules source file lib place the current file.xml inside . modules source file config How to verify it works. 1. Running tests that currently use the file source, e.g. in spring shell, should work as before 2. When deploying a stream file log, we should be able to interrogate the channel registry and make sure it found the dependencies for the module, ModuleDescriptor should have a not null URL property.",NULL,"Convert a simple module, such as file, to further test that what was done in the previous story, Use ParentLastClassLoader to create the Modules ApplicationContext. works as expected. Implementation Suggestions Remove from build.gradle the dependency on spring integration file and place that jar inside a directory . modules source file lib place the current file.xml inside . modules source file config How to verify it works. 1. Running tests that currently use the file source, e.g. in spring shell, should work as before 2. When deploying a stream file log, we should be able to interrogate the channel registry and make sure it found the dependencies for the module, ModuleDescriptor should have a not null URL property.",NULL,"Convert a simple module, such as file, to further test that what was done in the previous story, Use ParentLastClassLoader to create the Modules ApplicationContext<span class='highlight-text severity-high'>. works as expected. Implementation Suggestions Remove from build.gradle the dependency on spring integration file and place that jar inside a directory . modules source file lib place the current file.xml inside . modules source file config How to verify it works. 1. Running tests that currently use the file source, e.g. in spring shell, should work as before 2. When deploying a stream file log, we should be able to interrogate the channel registry and make sure it found the dependencies for the module, ModuleDescriptor should have a not null URL property.</span>","minimal","punctuation","high",False
19415,"Register in MBeanExportingPlugin. ",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19415,"Register in MBeanExportingPlugin. ",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19466,"The classes under test are pluralized. Therefore, the test classes themselves should reflect that. E.g. rename JobCommandTests to JobCommandsTests as it tests class JobCommands . Please check all tests in that package for correct naming.",NULL,"The classes under test are pluralized. Therefore, the test classes themselves should reflect that. E.g. rename JobCommandTests to JobCommandsTests as it tests class JobCommands . Please check all tests in that package for correct naming.",NULL,"Add for who this story is","well_formed","no_role","high",False
19466,"The classes under test are pluralized. Therefore, the test classes themselves should reflect that. E.g. rename JobCommandTests to JobCommandsTests as it tests class JobCommands . Please check all tests in that package for correct naming.",NULL,"The classes under test are pluralized. Therefore, the test classes themselves should reflect that. E.g. rename JobCommandTests to JobCommandsTests as it tests class JobCommands . Please check all tests in that package for correct naming.",NULL,"The classes under test are pluralized<span class='highlight-text severity-high'>. Therefore, the test classes themselves should reflect that. E.g. rename JobCommandTests to JobCommandsTests as it tests class JobCommands . Please check all tests in that package for correct naming.</span>","minimal","punctuation","high",False
19416,"Update JavaDocs in Plugin interface to clearly describe the Plugin contract, i.e., where in the lifecycle each method is invoked",NULL,"Update JavaDocs in Plugin interface to clearly describe the Plugin contract, i.e., where in the lifecycle each method is invoked",NULL,"Add for who this story is","well_formed","no_role","high",False
19417,"Remove unnecessary code that adds beans to shared context after refresh Scheduler should not be in common.xml register it in TriggerPlugin.postProcessSharedContext ",NULL,"Remove unnecessary code that adds beans to shared context after refresh Scheduler should not be in common.xml register it in TriggerPlugin.postProcessSharedContext ",NULL,"Add for who this story is","well_formed","no_role","high",False
19418,"SingleNodeMain parent new AC .. AdminMain.launch parent ; ContainerMain.launch parent ; This should make startup processing more consistent and symmetrical ",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19418,"SingleNodeMain parent new AC .. AdminMain.launch parent ; ContainerMain.launch parent ; This should make startup processing more consistent and symmetrical ",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19418,"SingleNodeMain parent new AC .. AdminMain.launch parent ; ContainerMain.launch parent ; This should make startup processing more consistent and symmetrical ",NULL,NULL,NULL,"SingleNodeMain parent new AC <span class='highlight-text severity-high'>.. AdminMain.launch parent ; ContainerMain.launch parent ; This should make startup processing more consistent and symmetrical </span>","minimal","punctuation","high",False
19742,"To allow for groups of beans to be defined or not in the container that runs a module. When deploying a stream e.g. via the REST API , it should be possible to also provide profile names. Then those would apply to any modules within that particular stream deployment.",NULL,"To allow for groups of beans to be defined or not in the container that runs a module. When deploying a stream e.g. via the REST API , it should be possible to also provide profile names. Then those would apply to any modules within that particular stream deployment.",NULL,"Add for who this story is","well_formed","no_role","high",False
19742,"To allow for groups of beans to be defined or not in the container that runs a module. When deploying a stream e.g. via the REST API , it should be possible to also provide profile names. Then those would apply to any modules within that particular stream deployment.",NULL,"To allow for groups of beans to be defined or not in the container that runs a module. When deploying a stream e.g. via the REST API , it should be possible to also provide profile names. Then those would apply to any modules within that particular stream deployment.",NULL,"To allow for groups of beans to be defined<span class='highlight-text severity-high'> or </span>not in the container that runs a module. When deploying a stream e.g. via the REST API , it should be possible to also provide profile names. Then those would apply to any modules within that particular stream deployment.","atomic","conjunctions","high",False
19742,"To allow for groups of beans to be defined or not in the container that runs a module. When deploying a stream e.g. via the REST API , it should be possible to also provide profile names. Then those would apply to any modules within that particular stream deployment.",NULL,"To allow for groups of beans to be defined or not in the container that runs a module. When deploying a stream e.g. via the REST API , it should be possible to also provide profile names. Then those would apply to any modules within that particular stream deployment.",NULL,"To allow for groups of beans to be defined or not in the container that runs a module<span class='highlight-text severity-high'>. When deploying a stream e.g. via the REST API , it should be possible to also provide profile names. Then those would apply to any modules within that particular stream deployment.</span>","minimal","punctuation","high",False
19425,"AbstractDeployer has 4 subclasses, 3 of which override e.g. deploy making the boilerplate factorization ineffective. Introduce an intermediate class for those deployers that support the concept of an instance Stream, Tap, Job to some extent ",NULL,"AbstractDeployer has 4 subclasses, 3 of which override e.g. deploy making the boilerplate factorization ineffective. Introduce an intermediate class for those deployers that support the concept of an instance Stream, Tap, Job to some extent ",NULL,"Add for who this story is","well_formed","no_role","high",False
19425,"AbstractDeployer has 4 subclasses, 3 of which override e.g. deploy making the boilerplate factorization ineffective. Introduce an intermediate class for those deployers that support the concept of an instance Stream, Tap, Job to some extent ",NULL,"AbstractDeployer has 4 subclasses, 3 of which override e.g. deploy making the boilerplate factorization ineffective. Introduce an intermediate class for those deployers that support the concept of an instance Stream, Tap, Job to some extent ",NULL,"AbstractDeployer has 4 subclasses, 3 of which override e<span class='highlight-text severity-high'>.g. deploy making the boilerplate factorization ineffective. Introduce an intermediate class for those deployers that support the concept of an instance Stream, Tap, Job to some extent </span>","minimal","punctuation","high",False
19424,"Make the default value of enableJmx false until we have tested documented JMX functionality",NULL,"Make the default value of enableJmx false until we have tested documented JMX functionality",NULL,"Add for who this story is","well_formed","no_role","high",False
19429,"Provide some syntax allowing multiple tap points to be directed to a named channel. e.g. tap foo.4 namedTap tap bar.2 namedTap or tap.foo counter",NULL,"Provide some syntax allowing multiple tap points to be directed to a named channel. e.g. tap foo.4 namedTap tap bar.2 namedTap or tap.foo counter",NULL,"Add for who this story is","well_formed","no_role","high",False
19429,"Provide some syntax allowing multiple tap points to be directed to a named channel. e.g. tap foo.4 namedTap tap bar.2 namedTap or tap.foo counter",NULL,"Provide some syntax allowing multiple tap points to be directed to a named channel. e.g. tap foo.4 namedTap tap bar.2 namedTap or tap.foo counter",NULL,"Provide some syntax allowing multiple tap points to be directed to a named channel<span class='highlight-text severity-high'>. e.g. tap foo.4 namedTap tap bar.2 namedTap or tap.foo counter</span>","minimal","punctuation","high",False
19428,"xd stream create name testgemfire definition http port 8887 gemfire 16 20 28,503 WARN Spring Shell client.RestTemplate 524 POST request for http localhost 8080 streams resulted in 500 Internal Server Error ; invoking error handler Command failed org.springframework.xd.rest.client.impl.SpringXDException org.springframework.beans.factory.BeanDefinitionStoreException Invalid bean definition with name region defined in null Could not resolve placeholder regionName in string value regionName ",NULL,"xd stream create name testgemfire definition http port 8887 gemfire 16 20 28,503 WARN Spring Shell client.RestTemplate 524 POST request for http localhost 8080 streams resulted in 500 Internal Server Error ; invoking error handler Command failed org.springframework.xd.rest.client.impl.SpringXDException org.springframework.beans.factory.BeanDefinitionStoreException Invalid bean definition with name region defined in null Could not resolve placeholder regionName in string value regionName ",NULL,"xd stream create name testgemfire definition http port 8887 gemfire 16 20 28,503 WARN Spring Shell client.RestTemplate 524 POST request for http localhost 8080 streams resulted in 500 Internal Server Error <span class='highlight-text severity-high'>; invoking error handler Command failed org.springframework.xd.rest.client.impl.SpringXDException org.springframework.beans.factory.BeanDefinitionStoreException Invalid bean definition with name region defined in null Could not resolve placeholder regionName in string value regionName </span>","minimal","punctuation","high",False
19433,"See https github.com dturanski spring xd commit 18302ec62a85d5fd8918beb26cf19968d4a63a2d",NULL,"See https github.com dturanski spring xd commit 18302ec62a85d5fd8918beb26cf19968d4a63a2d",NULL,"Add for who this story is","well_formed","no_role","high",False
19743,"Based off SI tcp inbound adapter. This will allow for event fowarding.",NULL,"Based off SI tcp inbound adapter. This will allow for event fowarding.",NULL,"Add for who this story is","well_formed","no_role","high",False
19743,"Based off SI tcp inbound adapter. This will allow for event fowarding.",NULL,"Based off SI tcp inbound adapter. This will allow for event fowarding.",NULL,"Based off SI tcp inbound adapter<span class='highlight-text severity-high'>. This will allow for event fowarding.</span>","minimal","punctuation","high",False
19430,"Taps are currently source modules. They could be refactored to simply bridge the tapped module s tap pub sub topic directly with conversion to the first tap module s input channel. Note ensure destroy works. Currently the tap is destroyed by the simple fact it is a module; if it s no longer a module we ll need special handling to stop remove the tap adapter.",NULL,"Taps are currently source modules. They could be refactored to simply bridge the tapped module s tap pub sub topic directly with conversion to the first tap module s input channel. Note ensure destroy works. Currently the tap is destroyed by the simple fact it is a module; if it s no longer a module we ll need special handling to stop remove the tap adapter.",NULL,"Add for who this story is","well_formed","no_role","high",False
19430,"Taps are currently source modules. They could be refactored to simply bridge the tapped module s tap pub sub topic directly with conversion to the first tap module s input channel. Note ensure destroy works. Currently the tap is destroyed by the simple fact it is a module; if it s no longer a module we ll need special handling to stop remove the tap adapter.",NULL,"Taps are currently source modules. They could be refactored to simply bridge the tapped module s tap pub sub topic directly with conversion to the first tap module s input channel. Note ensure destroy works. Currently the tap is destroyed by the simple fact it is a module; if it s no longer a module we ll need special handling to stop remove the tap adapter.",NULL,"Taps are currently source modules<span class='highlight-text severity-high'>. They could be refactored to simply bridge the tapped module s tap pub sub topic directly with conversion to the first tap module s input channel. Note ensure destroy works. Currently the tap is destroyed by the simple fact it is a module; if it s no longer a module we ll need special handling to stop remove the tap adapter.</span>","minimal","punctuation","high",False
19431,"also, the current behavior is broken; it checks if the property is set but does not actually check whether it s true or false",NULL,"also, the current behavior is broken; it checks if the property is set but does not actually check whether it s true or false",NULL,"Add for who this story is","well_formed","no_role","high",False
19431,"also, the current behavior is broken; it checks if the property is set but does not actually check whether it s true or false",NULL,"also, the current behavior is broken; it checks if the property is set but does not actually check whether it s true or false",NULL,"also, the current behavior is broken<span class='highlight-text severity-high'>; it checks if the property is set but does not actually check whether it s true or false</span>","minimal","punctuation","high",False
19432,"The expression currently appends . suffix where the default suffix is out . If the suffix value were an empty String, this would lead to the file name ending with a dot. We should update the expression so that it only appends the dot if the suffix is not empty. This might be possible with a ternary expression.",NULL,"The expression currently appends . suffix where the default suffix is out . If the suffix value were an empty String, this would lead to the file name ending with a dot. We should update the expression","so that it only appends the dot if the suffix is not empty. This might be possible with a ternary expression.","Add for who this story is","well_formed","no_role","high",False
19432,"The expression currently appends . suffix where the default suffix is out . If the suffix value were an empty String, this would lead to the file name ending with a dot. We should update the expression so that it only appends the dot if the suffix is not empty. This might be possible with a ternary expression.",NULL,"The expression currently appends . suffix where the default suffix is out . If the suffix value were an empty String, this would lead to the file name ending with a dot. We should update the expression","so that it only appends the dot if the suffix is not empty. This might be possible with a ternary expression.","The expression currently appends <span class='highlight-text severity-high'>. suffix where the default suffix is out . If the suffix value were an empty String, this would lead to the file name ending with a dot. We should update the expression so that it only appends the dot if the suffix is not empty. This might be possible with a ternary expression.</span>","minimal","punctuation","high",False
19432,"The expression currently appends . suffix where the default suffix is out . If the suffix value were an empty String, this would lead to the file name ending with a dot. We should update the expression so that it only appends the dot if the suffix is not empty. This might be possible with a ternary expression.",NULL,"The expression currently appends . suffix where the default suffix is out . If the suffix value were an empty String, this would lead to the file name ending with a dot. We should update the expression","so that it only appends the dot if the suffix is not empty. This might be possible with a ternary expression.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19434,"This example should require no code. Just the basic XML.",NULL,"This example should require no code. Just the basic XML.",NULL,"Add for who this story is","well_formed","no_role","high",False
19434,"This example should require no code. Just the basic XML.",NULL,"This example should require no code. Just the basic XML.",NULL,"This example should require no code<span class='highlight-text severity-high'>. Just the basic XML.</span>","minimal","punctuation","high",False
19435,"Please put in suggestions for the current .settings file. Maybe one suggestion is to not format on save?",NULL,"Please put in suggestions for the current .settings file. Maybe one suggestion is to not format on save?",NULL,"Add for who this story is","well_formed","no_role","high",False
19435,"Please put in suggestions for the current .settings file. Maybe one suggestion is to not format on save?",NULL,"Please put in suggestions for the current .settings file. Maybe one suggestion is to not format on save?",NULL,"Please put in suggestions for the current <span class='highlight-text severity-high'>.settings file. Maybe one suggestion is to not format on save?</span>","minimal","punctuation","high",False
19427,"stream create name test1 definition http port 8827 gemfire server Created new stream test1 stream create name test2 definition http port 8828 gemfire json server Created new stream test2 even if gemfire server is not started, streams are successfully created. This behavior is inconsistent with hdfs where if hdfs connection is not available, creating stream using http hdfs will fail. ",NULL,"stream create name test1 definition http port 8827 gemfire server Created new stream test1 stream create name test2 definition http port 8828 gemfire json server Created new stream test2 even if gemfire server is not started, streams are successfully created. This behavior is inconsistent with hdfs where if hdfs connection is not available, creating stream using http hdfs will fail. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19427,"stream create name test1 definition http port 8827 gemfire server Created new stream test1 stream create name test2 definition http port 8828 gemfire json server Created new stream test2 even if gemfire server is not started, streams are successfully created. This behavior is inconsistent with hdfs where if hdfs connection is not available, creating stream using http hdfs will fail. ",NULL,"stream create name test1 definition http port 8827 gemfire server Created new stream test1 stream create name test2 definition http port 8828 gemfire json server Created new stream test2 even if gemfire server is not started, streams are successfully created. This behavior is inconsistent with hdfs where if hdfs connection is not available, creating stream using http hdfs will fail. ",NULL,"stream create name test1 definition http port 8827 gemfire server Created new stream test1 stream create name test2 definition http port 8828 gemfire json server Created new stream test2 even if gemfire server is not started, streams are successfully created<span class='highlight-text severity-high'>. This behavior is inconsistent with hdfs where if hdfs connection is not available, creating stream using http hdfs will fail. </span>","minimal","punctuation","high",False
19422,"Automate running integration tests on all supported transports",NULL,"Automate running integration tests on all supported transports",NULL,"Add for who this story is","well_formed","no_role","high",False
19423,"we should check the actual deployment requests were built correctly for each module in the testCreateUndeployAndDeleteOfStream test. Currently we just use the anyListOf check.",NULL,"we should check the actual deployment requests were built correctly for each module in the testCreateUndeployAndDeleteOfStream test. Currently we just use the anyListOf check.",NULL,"Add for who this story is","well_formed","no_role","high",False
19423,"we should check the actual deployment requests were built correctly for each module in the testCreateUndeployAndDeleteOfStream test. Currently we just use the anyListOf check.",NULL,"we should check the actual deployment requests were built correctly for each module in the testCreateUndeployAndDeleteOfStream test. Currently we just use the anyListOf check.",NULL,"we should check the actual deployment requests were built correctly for each module in the testCreateUndeployAndDeleteOfStream test<span class='highlight-text severity-high'>. Currently we just use the anyListOf check.</span>","minimal","punctuation","high",False
19441,"Not only should it not use Joda see XD 668 but the passing of dates currently relies on default formatting",NULL,"Not only should it not use Joda see XD 668 but the passing of dates currently relies on default formatting",NULL,"Add for who this story is","well_formed","no_role","high",False
19446,"Keep getting the following warning WARN Spring Shell conf.Configuration 817 fs.default.name is deprecated. Instead, use fs.defaultFS Should switch to use the runtime value of the FS DEFAULT NAME KEY constant based on Hadoop version used.",NULL,"Keep getting the following warning WARN Spring Shell conf.Configuration 817 fs.default.name is deprecated. Instead, use fs.defaultFS Should switch to use the runtime value of the FS DEFAULT NAME KEY constant based on Hadoop version used.",NULL,"Add for who this story is","well_formed","no_role","high",False
19446,"Keep getting the following warning WARN Spring Shell conf.Configuration 817 fs.default.name is deprecated. Instead, use fs.defaultFS Should switch to use the runtime value of the FS DEFAULT NAME KEY constant based on Hadoop version used.",NULL,"Keep getting the following warning WARN Spring Shell conf.Configuration 817 fs.default.name is deprecated. Instead, use fs.defaultFS Should switch to use the runtime value of the FS DEFAULT NAME KEY constant based on Hadoop version used.",NULL,"Keep getting the following warning WARN Spring Shell conf<span class='highlight-text severity-high'>.Configuration 817 fs.default.name is deprecated. Instead, use fs.defaultFS Should switch to use the runtime value of the FS DEFAULT NAME KEY constant based on Hadoop version used.</span>","minimal","punctuation","high",False
19447,"Remove the NoOpRedisSerializer and use the non serialization feature of M2.",NULL,"Remove the NoOpRedisSerializer and use the non serialization feature of M2.",NULL,"Add for who this story is","well_formed","no_role","high",False
19447,"Remove the NoOpRedisSerializer and use the non serialization feature of M2.",NULL,"Remove the NoOpRedisSerializer and use the non serialization feature of M2.",NULL,"Remove the NoOpRedisSerializer<span class='highlight-text severity-high'> and </span>use the non serialization feature of M2.","atomic","conjunctions","high",False
19467,"Currently, you have to set the default name node every time your start the shell. We should do 2 things Provide a default Name node Set Default Hadoop Name Node for Shell hdfs localhost 8020 Should we provide some form of persistence? It kind of sucks that you have to re specify the name node every time the shell starts up code xd hadoop fs ls You must set fs URL before run fs commands code ",NULL,"Currently, you have to set the default name node every time your start the shell. We should do 2 things Provide a default Name node Set Default Hadoop Name Node for Shell hdfs localhost 8020 Should we provide some form of persistence? It kind of sucks that you have to re specify the name node every time the shell starts up code xd hadoop fs ls You must set fs URL before run fs commands code ",NULL,"Add for who this story is","well_formed","no_role","high",False
19467,"Currently, you have to set the default name node every time your start the shell. We should do 2 things Provide a default Name node Set Default Hadoop Name Node for Shell hdfs localhost 8020 Should we provide some form of persistence? It kind of sucks that you have to re specify the name node every time the shell starts up code xd hadoop fs ls You must set fs URL before run fs commands code ",NULL,"Currently, you have to set the default name node every time your start the shell. We should do 2 things Provide a default Name node Set Default Hadoop Name Node for Shell hdfs localhost 8020 Should we provide some form of persistence? It kind of sucks that you have to re specify the name node every time the shell starts up code xd hadoop fs ls You must set fs URL before run fs commands code ",NULL,"Currently, you have to set the default name node every time your start the shell<span class='highlight-text severity-high'>. We should do 2 things Provide a default Name node Set Default Hadoop Name Node for Shell hdfs localhost 8020 Should we provide some form of persistence? It kind of sucks that you have to re specify the name node every time the shell starts up code xd hadoop fs ls You must set fs URL before run fs commands code </span>","minimal","punctuation","high",False
19442,"The Rest Client project should not impose Joda to the user.",NULL,"The Rest Client project should not impose Joda to the user.",NULL,"Add for who this story is","well_formed","no_role","high",False
19439,"should go thru the list of all commands available and make sure that a simple not connected message is returned instead of something like this code org.springframework.web.client.ResourceAccessException I O error on GET request for http localhost 8080 Unexpected end of file from server; nested exception is java.net.SocketException Unexpected end of file from server at org.springframework.web.client.RestTemplate.doExecute RestTemplate.java 498 .... code ",NULL,"should go thru the list of all commands available and make sure that a simple not connected message is returned instead of something like this code org.springframework.web.client.ResourceAccessException I O error on GET request for http localhost 8080 Unexpected end of file from server; nested exception is java.net.SocketException Unexpected end of file from server at org.springframework.web.client.RestTemplate.doExecute RestTemplate.java 498 .... code ",NULL,"Add for who this story is","well_formed","no_role","high",False
19439,"should go thru the list of all commands available and make sure that a simple not connected message is returned instead of something like this code org.springframework.web.client.ResourceAccessException I O error on GET request for http localhost 8080 Unexpected end of file from server; nested exception is java.net.SocketException Unexpected end of file from server at org.springframework.web.client.RestTemplate.doExecute RestTemplate.java 498 .... code ",NULL,"should go thru the list of all commands available and make sure that a simple not connected message is returned instead of something like this code org.springframework.web.client.ResourceAccessException I O error on GET request for http localhost 8080 Unexpected end of file from server; nested exception is java.net.SocketException Unexpected end of file from server at org.springframework.web.client.RestTemplate.doExecute RestTemplate.java 498 .... code ",NULL,"should go thru the list of all commands available<span class='highlight-text severity-high'> and </span>make sure that a simple not connected message is returned instead of something like this code org.springframework.web.client.ResourceAccessException I O error on GET request for http localhost 8080 Unexpected end of file from server; nested exception is java.net.SocketException Unexpected end of file from server at org.springframework.web.client.RestTemplate.doExecute RestTemplate.java 498 .... code ","atomic","conjunctions","high",False
19439,"should go thru the list of all commands available and make sure that a simple not connected message is returned instead of something like this code org.springframework.web.client.ResourceAccessException I O error on GET request for http localhost 8080 Unexpected end of file from server; nested exception is java.net.SocketException Unexpected end of file from server at org.springframework.web.client.RestTemplate.doExecute RestTemplate.java 498 .... code ",NULL,"should go thru the list of all commands available and make sure that a simple not connected message is returned instead of something like this code org.springframework.web.client.ResourceAccessException I O error on GET request for http localhost 8080 Unexpected end of file from server; nested exception is java.net.SocketException Unexpected end of file from server at org.springframework.web.client.RestTemplate.doExecute RestTemplate.java 498 .... code ",NULL,"should go thru the list of all commands available and make sure that a simple not connected message is returned instead of something like this code org<span class='highlight-text severity-high'>.springframework.web.client.ResourceAccessException I O error on GET request for http localhost 8080 Unexpected end of file from server; nested exception is java.net.SocketException Unexpected end of file from server at org.springframework.web.client.RestTemplate.doExecute RestTemplate.java 498 .... code </span>","minimal","punctuation","high",False
19440,"Provide Shell TAB completion when referencing an existing entity",NULL,"Provide Shell TAB completion when referencing an existing entity",NULL,"Add for who this story is","well_formed","no_role","high",False
19443,"Currently, the initial tap module accepted media types are not retrieved from the module when creating the tap.",NULL,"Currently, the initial tap module accepted media types are not retrieved from the module when creating the tap.",NULL,"Add for who this story is","well_formed","no_role","high",False
19444,"Redis transport headers are not removed in taps.",NULL,"Redis transport headers are not removed in taps.",NULL,"Add for who this story is","well_formed","no_role","high",False
19445,"It would be nice to have lastHours and lastDays options for aggregatecounter display command.",NULL,"It would be nice to have lastHours and lastDays options for aggregatecounter display command.",NULL,"Add for who this story is","well_formed","no_role","high",False
19452,"Any kind of sophisticated artifact retrieval mechanism in XD will need to grab more than one kind of artifact at once. For example, if I want to see all taps, streams, triggers, and jobs ie everything , I need to make 4 http requests. I can imagine dashboards that need to display information on artifacts of multiple kinds. There will also need to be a way to pass a query to return a sub set of artifacts, but that should be designed separately.",NULL,"Any kind of sophisticated artifact retrieval mechanism in XD will need to grab more than one kind of artifact at once. For example, if I want to see all taps, streams, triggers, and jobs ie everything , I need to make 4 http requests. I can imagine dashboards that need to display information on artifacts of multiple kinds. There will also need to be a way to pass a query to return a sub set of artifacts, but that should be designed separately.",NULL,"Add for who this story is","well_formed","no_role","high",False
19452,"Any kind of sophisticated artifact retrieval mechanism in XD will need to grab more than one kind of artifact at once. For example, if I want to see all taps, streams, triggers, and jobs ie everything , I need to make 4 http requests. I can imagine dashboards that need to display information on artifacts of multiple kinds. There will also need to be a way to pass a query to return a sub set of artifacts, but that should be designed separately.",NULL,"Any kind of sophisticated artifact retrieval mechanism in XD will need to grab more than one kind of artifact at once. For example, if I want to see all taps, streams, triggers, and jobs ie everything , I need to make 4 http requests. I can imagine dashboards that need to display information on artifacts of multiple kinds. There will also need to be a way to pass a query to return a sub set of artifacts, but that should be designed separately.",NULL,"Any kind of sophisticated artifact retrieval mechanism in XD will need to grab more than one kind of artifact at once. For example, if I want to see all taps, streams, triggers,<span class='highlight-text severity-high'> and </span>jobs ie everything , I need to make 4 http requests. I can imagine dashboards that need to display information on artifacts of multiple kinds. There will also need to be a way to pass a query to return a sub set of artifacts, but that should be designed separately.","atomic","conjunctions","high",False
19452,"Any kind of sophisticated artifact retrieval mechanism in XD will need to grab more than one kind of artifact at once. For example, if I want to see all taps, streams, triggers, and jobs ie everything , I need to make 4 http requests. I can imagine dashboards that need to display information on artifacts of multiple kinds. There will also need to be a way to pass a query to return a sub set of artifacts, but that should be designed separately.",NULL,"Any kind of sophisticated artifact retrieval mechanism in XD will need to grab more than one kind of artifact at once. For example, if I want to see all taps, streams, triggers, and jobs ie everything , I need to make 4 http requests. I can imagine dashboards that need to display information on artifacts of multiple kinds. There will also need to be a way to pass a query to return a sub set of artifacts, but that should be designed separately.",NULL,"Any kind of sophisticated artifact retrieval mechanism in XD will need to grab more than one kind of artifact at once<span class='highlight-text severity-high'>. For example, if I want to see all taps, streams, triggers, and jobs ie everything , I need to make 4 http requests. I can imagine dashboards that need to display information on artifacts of multiple kinds. There will also need to be a way to pass a query to return a sub set of artifacts, but that should be designed separately.</span>","minimal","punctuation","high",False
19452,"Any kind of sophisticated artifact retrieval mechanism in XD will need to grab more than one kind of artifact at once. For example, if I want to see all taps, streams, triggers, and jobs ie everything , I need to make 4 http requests. I can imagine dashboards that need to display information on artifacts of multiple kinds. There will also need to be a way to pass a query to return a sub set of artifacts, but that should be designed separately.",NULL,"Any kind of sophisticated artifact retrieval mechanism in XD will need to grab more than one kind of artifact at once. For example, if I want to see all taps, streams, triggers, and jobs ie everything , I need to make 4 http requests. I can imagine dashboards that need to display information on artifacts of multiple kinds. There will also need to be a way to pass a query to return a sub set of artifacts, but that should be designed separately.",NULL,"Any kind of sophisticated artifact retrieval mechanism in XD will need to grab more than one kind of artifact at once. For example, if<span class='highlight-text severity-high'> I want to </span>see all taps, streams, triggers, and jobs ie everything , I need to make 4 http requests.<span class='highlight-text severity-high'> I can </span>imagine dashboards that need to display information on artifacts of multiple kinds. There will also need to be a way to pass a query to return a sub set of artifacts, but that should be designed separately.","minimal","indicator_repetition","high",False
19452,"Any kind of sophisticated artifact retrieval mechanism in XD will need to grab more than one kind of artifact at once. For example, if I want to see all taps, streams, triggers, and jobs ie everything , I need to make 4 http requests. I can imagine dashboards that need to display information on artifacts of multiple kinds. There will also need to be a way to pass a query to return a sub set of artifacts, but that should be designed separately.",NULL,"Any kind of sophisticated artifact retrieval mechanism in XD will need to grab more than one kind of artifact at once. For example, if I want to see all taps, streams, triggers, and jobs ie everything , I need to make 4 http requests. I can imagine dashboards that need to display information on artifacts of multiple kinds. There will also need to be a way to pass a query to return a sub set of artifacts, but that should be designed separately.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19453,"After running gradle refresh source folders on the spring xd module project in Eclipse, there is an error because the src test java folder is missing. Solution is to add a placeholder file.",NULL,"After running gradle refresh source folders on the spring xd module project in Eclipse, there is an error because the src test java folder is missing. Solution is to add a placeholder file.",NULL,"Add for who this story is","well_formed","no_role","high",False
19453,"After running gradle refresh source folders on the spring xd module project in Eclipse, there is an error because the src test java folder is missing. Solution is to add a placeholder file.",NULL,"After running gradle refresh source folders on the spring xd module project in Eclipse, there is an error because the src test java folder is missing. Solution is to add a placeholder file.",NULL,"After running gradle refresh source folders on the spring xd module project in Eclipse, there is an error because the src test java folder is missing<span class='highlight-text severity-high'>. Solution is to add a placeholder file.</span>","minimal","punctuation","high",False
19455,"As we overwrote changes to file source by mistake, let s add some regression tests, esp. to the file location. Plan on extending the utility source and sink functionality",NULL,"As we overwrote changes to file source by mistake, let s add some regression tests, esp. to the file location. Plan on extending the utility source and sink functionality",NULL,"Add for who this story is","well_formed","no_role","high",False
19455,"As we overwrote changes to file source by mistake, let s add some regression tests, esp. to the file location. Plan on extending the utility source and sink functionality",NULL,"As we overwrote changes to file source by mistake, let s add some regression tests, esp. to the file location. Plan on extending the utility source and sink functionality",NULL,"As we overwrote changes to file source by mistake, let s add some regression tests, esp<span class='highlight-text severity-high'>. to the file location. Plan on extending the utility source and sink functionality</span>","minimal","punctuation","high",False
19457,"support of the alpha parameter is awkward and can confuse people who are expecting a simple average mean. Consider splitting RichGauge in two flavors arithmetic and exponential. Involves quite some work at the repository, handler and REST level though... ",NULL,"support of the alpha parameter is awkward and can confuse people who are expecting a simple average mean. Consider splitting RichGauge in two flavors arithmetic and exponential. Involves quite some work at the repository, handler and REST level though... ",NULL,"Add for who this story is","well_formed","no_role","high",False
19457,"support of the alpha parameter is awkward and can confuse people who are expecting a simple average mean. Consider splitting RichGauge in two flavors arithmetic and exponential. Involves quite some work at the repository, handler and REST level though... ",NULL,"support of the alpha parameter is awkward and can confuse people who are expecting a simple average mean. Consider splitting RichGauge in two flavors arithmetic and exponential. Involves quite some work at the repository, handler and REST level though... ",NULL,"support of the alpha parameter is awkward<span class='highlight-text severity-high'> and </span>can confuse people who are expecting a simple average mean. Consider splitting RichGauge in two flavors arithmetic and exponential. Involves quite some work at the repository, handler and REST level though... ","atomic","conjunctions","high",False
19457,"support of the alpha parameter is awkward and can confuse people who are expecting a simple average mean. Consider splitting RichGauge in two flavors arithmetic and exponential. Involves quite some work at the repository, handler and REST level though... ",NULL,"support of the alpha parameter is awkward and can confuse people who are expecting a simple average mean. Consider splitting RichGauge in two flavors arithmetic and exponential. Involves quite some work at the repository, handler and REST level though... ",NULL,"support of the alpha parameter is awkward and can confuse people who are expecting a simple average mean<span class='highlight-text severity-high'>. Consider splitting RichGauge in two flavors arithmetic and exponential. Involves quite some work at the repository, handler and REST level though... </span>","minimal","punctuation","high",False
19456,"Current implementation converts to a String. See if we can emit raw payload given that we also emit content type header Setting to 8 points, as this may have lots of implications down the line though",NULL,"Current implementation converts to a String. See if we can emit raw payload given that we also emit content type header Setting to 8 points, as this may have lots of implications down the line though",NULL,"Add for who this story is","well_formed","no_role","high",False
19456,"Current implementation converts to a String. See if we can emit raw payload given that we also emit content type header Setting to 8 points, as this may have lots of implications down the line though",NULL,"Current implementation converts to a String. See if we can emit raw payload given that we also emit content type header Setting to 8 points, as this may have lots of implications down the line though",NULL,"Current implementation converts to a String<span class='highlight-text severity-high'>. See if we can emit raw payload given that we also emit content type header Setting to 8 points, as this may have lots of implications down the line though</span>","minimal","punctuation","high",False
19458,"We need to add support for matching column names with underscores like user name and map them to camel case style keys like userName in the JdbcMessagePayloadTransformer.",NULL,"We need to add support for matching column names with underscores like user name and map them to camel case style keys like userName in the JdbcMessagePayloadTransformer.",NULL,"Add for who this story is","well_formed","no_role","high",False
19458,"We need to add support for matching column names with underscores like user name and map them to camel case style keys like userName in the JdbcMessagePayloadTransformer.",NULL,"We need to add support for matching column names with underscores like user name and map them to camel case style keys like userName in the JdbcMessagePayloadTransformer.",NULL,"We need to add support for matching column names with underscores like user name<span class='highlight-text severity-high'> and </span>map them to camel case style keys like userName in the JdbcMessagePayloadTransformer.","atomic","conjunctions","high",False
19460,"Factor out common Redis Rabbit ChannelRegistry code. Also, factor out common inbound tap code very similar . Change transport nternals to Use AbstractTransformer instead of ARPMH and BridgeHandler .",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19460,"Factor out common Redis Rabbit ChannelRegistry code. Also, factor out common inbound tap code very similar . Change transport nternals to Use AbstractTransformer instead of ARPMH and BridgeHandler .",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19460,"Factor out common Redis Rabbit ChannelRegistry code. Also, factor out common inbound tap code very similar . Change transport nternals to Use AbstractTransformer instead of ARPMH and BridgeHandler .",NULL,NULL,NULL,"Factor out common Redis Rabbit ChannelRegistry code<span class='highlight-text severity-high'>. Also, factor out common inbound tap code very similar . Change transport nternals to Use AbstractTransformer instead of ARPMH and BridgeHandler .</span>","minimal","punctuation","high",False
19461,"The AbstractReplyProducingMessageHandler in the Rabbit transport exposes the internal transport content type, if none existed on the original transported message.",NULL,"The AbstractReplyProducingMessageHandler in the Rabbit transport exposes the internal transport content type, if none existed on the original transported message.",NULL,"Add for who this story is","well_formed","no_role","high",False
19459,"Figure 1 here https github.com SpringSource spring xd wiki Architecture should also show Rabbit as an option, since otherwise people will think we are tied to redis.",NULL,"Figure 1 here https github.com SpringSource spring xd wiki Architecture should also show Rabbit as an option, since otherwise people will think we are tied to redis.",NULL,"Add for who this story is","well_formed","no_role","high",False
19468,"Model the API more akin to SpringXDOperations api.",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19468,"Model the API more akin to SpringXDOperations api.",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19464,"Consistency with JMS Source.",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19464,"Consistency with JMS Source.",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19463,"Spring Xd currently ships with Guava 12.0 while Hadoop 2.0.5 and Pivotal HD 1.0 depends on 11.0.2 this could lead to classpath problems if we include both.",NULL,"Spring Xd currently ships with Guava 12.0 while Hadoop 2.0.5 and Pivotal HD 1.0 depends on 11.0.2 this could lead to classpath problems if we include both.",NULL,"Add for who this story is","well_formed","no_role","high",False
19454,"This will come back in M3 once we iron out the issues.",NULL,"This will come back in M3 once we iron out the issues.",NULL,"Add for who this story is","well_formed","no_role","high",False
19450,"Support pubsub named channels the story could be a bit more general though enable channel creation with configurable settings via the REST API and shell namedchannel create foo domain PUBSUB",NULL,"Support pubsub named channels the story could be a bit more general though enable channel creation with configurable settings via the REST API and shell namedchannel create foo domain PUBSUB",NULL,"Add for who this story is","well_formed","no_role","high",False
19450,"Support pubsub named channels the story could be a bit more general though enable channel creation with configurable settings via the REST API and shell namedchannel create foo domain PUBSUB",NULL,"Support pubsub named channels the story could be a bit more general though enable channel creation with configurable settings via the REST API and shell namedchannel create foo domain PUBSUB",NULL,"Support pubsub named channels the story could be a bit more general though enable channel creation with configurable settings via the REST API<span class='highlight-text severity-high'> and </span>shell namedchannel create foo domain PUBSUB","atomic","conjunctions","high",False
19451,"Allow Json payloads from external sources, e.g., http post to contain single quoted field names and values. This is required where XD uses Jackson to convert payloads from Json to object or Tuples.",NULL,"Allow Json payloads from external sources, e.g., http post to contain single quoted field names and values. This is required where XD uses Jackson to convert payloads from Json to object or Tuples.",NULL,"Add for who this story is","well_formed","no_role","high",False
19451,"Allow Json payloads from external sources, e.g., http post to contain single quoted field names and values. This is required where XD uses Jackson to convert payloads from Json to object or Tuples.",NULL,"Allow Json payloads from external sources, e.g., http post to contain single quoted field names and values. This is required where XD uses Jackson to convert payloads from Json to object or Tuples.",NULL,"Allow Json payloads from external sources, e.g., http post to contain single quoted field names<span class='highlight-text severity-high'> and </span>values. This is required where XD uses Jackson to convert payloads from Json to object<span class='highlight-text severity-high'> or </span>Tuples.","atomic","conjunctions","high",False
19451,"Allow Json payloads from external sources, e.g., http post to contain single quoted field names and values. This is required where XD uses Jackson to convert payloads from Json to object or Tuples.",NULL,"Allow Json payloads from external sources, e.g., http post to contain single quoted field names and values. This is required where XD uses Jackson to convert payloads from Json to object or Tuples.",NULL,"Allow Json payloads from external sources, e<span class='highlight-text severity-high'>.g., http post to contain single quoted field names and values. This is required where XD uses Jackson to convert payloads from Json to object or Tuples.</span>","minimal","punctuation","high",False
19462,"pollers should standardize on fixed delay vs fixed rate. The value should accept a property with a standard name like interval ",NULL,"pollers should standardize on fixed delay vs fixed rate. The value should accept a property with a standard name like interval ",NULL,"Add for who this story is","well_formed","no_role","high",False
19462,"pollers should standardize on fixed delay vs fixed rate. The value should accept a property with a standard name like interval ",NULL,"pollers should standardize on fixed delay vs fixed rate. The value should accept a property with a standard name like interval ",NULL,"pollers should standardize on fixed delay vs fixed rate<span class='highlight-text severity-high'>. The value should accept a property with a standard name like interval </span>","minimal","punctuation","high",False
19465,"E.g. allow for posting of JSON data stored in local files. Allow users to specify the content type . Ensure that Unicode data UTF posts correctly.",NULL,"E.g. allow for posting of JSON data stored in local files. Allow users to specify the content type . Ensure that Unicode data UTF posts correctly.",NULL,"Add for who this story is","well_formed","no_role","high",False
19465,"E.g. allow for posting of JSON data stored in local files. Allow users to specify the content type . Ensure that Unicode data UTF posts correctly.",NULL,"E.g. allow for posting of JSON data stored in local files. Allow users to specify the content type . Ensure that Unicode data UTF posts correctly.",NULL,"E<span class='highlight-text severity-high'>.g. allow for posting of JSON data stored in local files. Allow users to specify the content type . Ensure that Unicode data UTF posts correctly.</span>","minimal","punctuation","high",False
19477,"Need to investigate why this is happening, normally setting code xml gfe client cache close false code prevents the singleton cache from closing when the application context is closed. ",NULL,"Need to investigate why this is happening, normally setting code xml gfe client cache close false code prevents the singleton cache from closing when the application context is closed. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19483,"The current flow of gradle tasks is confusing. Suggest the following changes to simplify the flow. 1. Move the current task logic in zipXD to distZip 2. Have distZip depend on dist 3. Update the how to build docs on the wiki 4. Make sure that the distZip task only shows up once in the list of gradle target. ",NULL,"The current flow of gradle tasks is confusing. Suggest the following changes to simplify the flow. 1. Move the current task logic in zipXD to distZip 2. Have distZip depend on dist 3. Update the how to build docs on the wiki 4. Make sure that the distZip task only shows up once in the list of gradle target. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19483,"The current flow of gradle tasks is confusing. Suggest the following changes to simplify the flow. 1. Move the current task logic in zipXD to distZip 2. Have distZip depend on dist 3. Update the how to build docs on the wiki 4. Make sure that the distZip task only shows up once in the list of gradle target. ",NULL,"The current flow of gradle tasks is confusing. Suggest the following changes to simplify the flow. 1. Move the current task logic in zipXD to distZip 2. Have distZip depend on dist 3. Update the how to build docs on the wiki 4. Make sure that the distZip task only shows up once in the list of gradle target. ",NULL,"The current flow of gradle tasks is confusing<span class='highlight-text severity-high'>. Suggest the following changes to simplify the flow. 1. Move the current task logic in zipXD to distZip 2. Have distZip depend on dist 3. Update the how to build docs on the wiki 4. Make sure that the distZip task only shows up once in the list of gradle target. </span>","minimal","punctuation","high",False
19478,"When running an ad hoc job without the use of a trigger adhoc or named . The user has to wait for job to complete before receiving a success. We need to launch a job and get a success back to the user letting them know the job has been launched. for example immediate",NULL,"When running an ad hoc job without the use of a trigger adhoc or named . The user has to wait for job to complete before receiving a success. We need to launch a job and get a success back to the user letting them know the job has been launched. for example immediate",NULL,"Add for who this story is","well_formed","no_role","high",False
19478,"When running an ad hoc job without the use of a trigger adhoc or named . The user has to wait for job to complete before receiving a success. We need to launch a job and get a success back to the user letting them know the job has been launched. for example immediate",NULL,"When running an ad hoc job without the use of a trigger adhoc or named . The user has to wait for job to complete before receiving a success. We need to launch a job and get a success back to the user letting them know the job has been launched. for example immediate",NULL,"When running an ad hoc job without the use of a trigger adhoc<span class='highlight-text severity-high'> or </span>named . The user has to wait for job to complete before receiving a success. We need to launch a job<span class='highlight-text severity-high'> and </span>get a success back to the user letting them know the job has been launched. for example immediate","atomic","conjunctions","high",False
19478,"When running an ad hoc job without the use of a trigger adhoc or named . The user has to wait for job to complete before receiving a success. We need to launch a job and get a success back to the user letting them know the job has been launched. for example immediate",NULL,"When running an ad hoc job without the use of a trigger adhoc or named . The user has to wait for job to complete before receiving a success. We need to launch a job and get a success back to the user letting them know the job has been launched. for example immediate",NULL,"When running an ad hoc job without the use of a trigger adhoc or named <span class='highlight-text severity-high'>. The user has to wait for job to complete before receiving a success. We need to launch a job and get a success back to the user letting them know the job has been launched. for example immediate</span>","minimal","punctuation","high",False
19481,"We need to add list delete commands for the metrics InMemoryAggregateCounter FieldValueCounter Gauge RichGauge Currently, the AbstractMetricsController class has the delete method to delete the metric from the repository. We can probably use the same for all the metrics.",NULL,"We need to add list delete commands for the metrics InMemoryAggregateCounter FieldValueCounter Gauge RichGauge Currently, the AbstractMetricsController class has the delete method to delete the metric from the repository. We can probably use the same for all the metrics.",NULL,"Add for who this story is","well_formed","no_role","high",False
19481,"We need to add list delete commands for the metrics InMemoryAggregateCounter FieldValueCounter Gauge RichGauge Currently, the AbstractMetricsController class has the delete method to delete the metric from the repository. We can probably use the same for all the metrics.",NULL,"We need to add list delete commands for the metrics InMemoryAggregateCounter FieldValueCounter Gauge RichGauge Currently, the AbstractMetricsController class has the delete method to delete the metric from the repository. We can probably use the same for all the metrics.",NULL,"We need to add list delete commands for the metrics InMemoryAggregateCounter FieldValueCounter Gauge RichGauge Currently, the AbstractMetricsController class has the delete method to delete the metric from the repository<span class='highlight-text severity-high'>. We can probably use the same for all the metrics.</span>","minimal","punctuation","high",False
19479,"Add CONTRIBUTING.md file, use the Spring Integration file as the basis.",NULL,"Add CONTRIBUTING.md file, use the Spring Integration file as the basis.",NULL,"Add for who this story is","well_formed","no_role","high",False
19482,"Add counter delete shell command. This also requires implementation of DELETE rest end point at CountersController.",NULL,"Add counter delete shell command. This also requires implementation of DELETE rest end point at CountersController.",NULL,"Add for who this story is","well_formed","no_role","high",False
19482,"Add counter delete shell command. This also requires implementation of DELETE rest end point at CountersController.",NULL,"Add counter delete shell command. This also requires implementation of DELETE rest end point at CountersController.",NULL,"Add counter delete shell command<span class='highlight-text severity-high'>. This also requires implementation of DELETE rest end point at CountersController.</span>","minimal","punctuation","high",False
19745,"This will eventually be supplied by the admin server, but for now write it up by hand in the documentation",NULL,"This will eventually be supplied by the admin server, but for now write it up by hand in the documentation",NULL,"Add for who this story is","well_formed","no_role","high",False
19484,"Keep track of named streams that were create and use After to destroy them.",NULL,"Keep track of named streams that were create and use After to destroy them.",NULL,"Add for who this story is","well_formed","no_role","high",False
19484,"Keep track of named streams that were create and use After to destroy them.",NULL,"Keep track of named streams that were create and use After to destroy them.",NULL,"Keep track of named streams that were create<span class='highlight-text severity-high'> and </span>use After to destroy them.","atomic","conjunctions","high",False
19470,"When using Redis store, stored deployed streams should be deployed on container restart.",NULL,"When using Redis store, stored deployed streams should be deployed on container restart.",NULL,"Add for who this story is","well_formed","no_role","high",False
19475,"This may be an issue following the search replace from curl to Shell, but for example, this documentation line does not work http post target http localhost 9000 data symbol VMW , price 72.04 The backslash prior to quote is left in the payload and hence Jackson chokes on it We need clear rules about quoting at the shell level",NULL,"This may be an issue following the search replace from curl to Shell, but for example, this documentation line does not work http post target http localhost 9000 data symbol VMW , price 72.04 The backslash prior to quote is left in the payload and hence Jackson chokes on it We need clear rules about quoting at the shell level",NULL,"Add for who this story is","well_formed","no_role","high",False
19475,"This may be an issue following the search replace from curl to Shell, but for example, this documentation line does not work http post target http localhost 9000 data symbol VMW , price 72.04 The backslash prior to quote is left in the payload and hence Jackson chokes on it We need clear rules about quoting at the shell level",NULL,"This may be an issue following the search replace from curl to Shell, but for example, this documentation line does not work http post target http localhost 9000 data symbol VMW , price 72.04 The backslash prior to quote is left in the payload and hence Jackson chokes on it We need clear rules about quoting at the shell level",NULL,"This may be an issue following the search replace from curl to Shell, but for example, this documentation line does not work http post target http localhost 9000 data symbol VMW , price 72.04 The backslash prior to quote is left in the payload<span class='highlight-text severity-high'> and </span>hence Jackson chokes on it We need clear rules about quoting at the shell level","atomic","conjunctions","high",False
19471,"https github.com springsource spring xd wiki Sinks should have rabbit added to the list and also the corresponding section that shows some basic usage.",NULL,"https github.com springsource spring xd wiki Sinks should have rabbit added to the list and also the corresponding section that shows some basic usage.",NULL,"Add for who this story is","well_formed","no_role","high",False
19471,"https github.com springsource spring xd wiki Sinks should have rabbit added to the list and also the corresponding section that shows some basic usage.",NULL,"https github.com springsource spring xd wiki Sinks should have rabbit added to the list and also the corresponding section that shows some basic usage.",NULL,"https github.com springsource spring xd wiki Sinks should have rabbit added to the list<span class='highlight-text severity-high'> and </span>also the corresponding section that shows some basic usage.","atomic","conjunctions","high",False
19472,"http static.springsource.org spring xd docs 1.0.0.BUILD SNAPSHOT reference html sources should have file added to the list and also the corresponding section that shows some basic usage.",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19472,"http static.springsource.org spring xd docs 1.0.0.BUILD SNAPSHOT reference html sources should have file added to the list and also the corresponding section that shows some basic usage.",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19476,"The RichGauge section does not mention the alpha parameter in redis output, nor does it explain its meaning.",NULL,"The RichGauge section does not mention the alpha parameter in redis output, nor does it explain its meaning.",NULL,"Add for who this story is","well_formed","no_role","high",False
19744,"Based off SI tcp inbound adapter. This will allow for event forwarding that can select among the existing SI serialized deserializer options. ",NULL,"Based off SI tcp inbound adapter. This will allow for event forwarding that can select among the existing SI serialized deserializer options. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19744,"Based off SI tcp inbound adapter. This will allow for event forwarding that can select among the existing SI serialized deserializer options. ",NULL,"Based off SI tcp inbound adapter. This will allow for event forwarding that can select among the existing SI serialized deserializer options. ",NULL,"Based off SI tcp inbound adapter<span class='highlight-text severity-high'>. This will allow for event forwarding that can select among the existing SI serialized deserializer options. </span>","minimal","punctuation","high",False
19480,"We need to fix the github wiki to use the xd shell command prompt xd .",NULL,"We need to fix the github wiki to use the xd shell command prompt xd .",NULL,"Add for who this story is","well_formed","no_role","high",False
19489,"http localhost 8080 hadoop config fs namenode webhdfs localhost 50070 http localhost 8080 hadoop fs ls Hadoop configuration changed, re initializing shell... run HDFS shell failed. Message is org mortbay util ajax JSON This was on a hadoop 1.0.1 install The hdfs http interface was available curl i http localhost 50070 webhdfs v1 tmp?op GETFILESTATUS HTTP 1.1 200 OK Content Type application json Transfer Encoding chunked Server Jetty 6.1.26 FileStatus accessTime 0, blockSize 0, group supergroup , length 0, modificationTime 1365015846724, owner mpollack , pathSuffix , permission 777 , replication 0, type DIRECTORY ",NULL,"http localhost 8080 hadoop config fs namenode webhdfs localhost 50070 http localhost 8080 hadoop fs ls Hadoop configuration changed, re initializing shell... run HDFS shell failed. Message is org mortbay util ajax JSON This was on a hadoop 1.0.1 install The hdfs http interface was available curl i http localhost 50070 webhdfs v1 tmp?op GETFILESTATUS HTTP 1.1 200 OK Content Type application json Transfer Encoding chunked Server Jetty 6.1.26 FileStatus accessTime 0, blockSize 0, group supergroup , length 0, modificationTime 1365015846724, owner mpollack , pathSuffix , permission 777 , replication 0, type DIRECTORY ",NULL,"Add for who this story is","well_formed","no_role","high",False
19489,"http localhost 8080 hadoop config fs namenode webhdfs localhost 50070 http localhost 8080 hadoop fs ls Hadoop configuration changed, re initializing shell... run HDFS shell failed. Message is org mortbay util ajax JSON This was on a hadoop 1.0.1 install The hdfs http interface was available curl i http localhost 50070 webhdfs v1 tmp?op GETFILESTATUS HTTP 1.1 200 OK Content Type application json Transfer Encoding chunked Server Jetty 6.1.26 FileStatus accessTime 0, blockSize 0, group supergroup , length 0, modificationTime 1365015846724, owner mpollack , pathSuffix , permission 777 , replication 0, type DIRECTORY ",NULL,"http localhost 8080 hadoop config fs namenode webhdfs localhost 50070 http localhost 8080 hadoop fs ls Hadoop configuration changed, re initializing shell... run HDFS shell failed. Message is org mortbay util ajax JSON This was on a hadoop 1.0.1 install The hdfs http interface was available curl i http localhost 50070 webhdfs v1 tmp?op GETFILESTATUS HTTP 1.1 200 OK Content Type application json Transfer Encoding chunked Server Jetty 6.1.26 FileStatus accessTime 0, blockSize 0, group supergroup , length 0, modificationTime 1365015846724, owner mpollack , pathSuffix , permission 777 , replication 0, type DIRECTORY ",NULL,"http localhost 8080 hadoop config fs namenode webhdfs localhost 50070 http localhost 8080 hadoop fs ls Hadoop configuration changed, re initializing shell<span class='highlight-text severity-high'>... run HDFS shell failed. Message is org mortbay util ajax JSON This was on a hadoop 1.0.1 install The hdfs http interface was available curl i http localhost 50070 webhdfs v1 tmp?op GETFILESTATUS HTTP 1.1 200 OK Content Type application json Transfer Encoding chunked Server Jetty 6.1.26 FileStatus accessTime 0, blockSize 0, group supergroup , length 0, modificationTime 1365015846724, owner mpollack , pathSuffix , permission 777 , replication 0, type DIRECTORY </span>","minimal","punctuation","high",False
19490,"The current http command is of the form http localhost 8080 post httpsource target http localhost 9090 data 10 It isn t intuitive to think post , rather the command can be http post target http localhost 9090 data 10 which will allow us to have support for other http verbs and cleanly separate the namespace from hadoop etc. The RestShell from which this came was only concerned with http actions, so the leading command classification probably seemed superfluous. ",NULL,"The current http command is of the form http localhost 8080 post http","source target http localhost 9090 data 10 It isn t intuitive to think post , rather the command can be http post target http localhost 9090 data 10 which will allow us to have support for other http verbs and cleanly separate the namespace from hadoop etc. The RestShell from which this came was only concerned with http actions, so the leading command classification probably seemed superfluous.","Add for who this story is","well_formed","no_role","high",False
19490,"The current http command is of the form http localhost 8080 post httpsource target http localhost 9090 data 10 It isn t intuitive to think post , rather the command can be http post target http localhost 9090 data 10 which will allow us to have support for other http verbs and cleanly separate the namespace from hadoop etc. The RestShell from which this came was only concerned with http actions, so the leading command classification probably seemed superfluous. ",NULL,"The current http command is of the form http localhost 8080 post http","source target http localhost 9090 data 10 It isn t intuitive to think post , rather the command can be http post target http localhost 9090 data 10 which will allow us to have support for other http verbs and cleanly separate the namespace from hadoop etc. The RestShell from which this came was only concerned with http actions, so the leading command classification probably seemed superfluous.","The current http command is of the form http localhost 8080 post httpsource target http localhost 9090 data 10 It isn t intuitive to think post , rather the command can be http post target http localhost 9090 data 10 which will allow us to have support for other http verbs and cleanly separate the namespace from hadoop etc<span class='highlight-text severity-high'>. The RestShell from which this came was only concerned with http actions, so the leading command classification probably seemed superfluous. </span>","minimal","punctuation","high",False
19490,"The current http command is of the form http localhost 8080 post httpsource target http localhost 9090 data 10 It isn t intuitive to think post , rather the command can be http post target http localhost 9090 data 10 which will allow us to have support for other http verbs and cleanly separate the namespace from hadoop etc. The RestShell from which this came was only concerned with http actions, so the leading command classification probably seemed superfluous. ",NULL,"The current http command is of the form http localhost 8080 post http","source target http localhost 9090 data 10 It isn t intuitive to think post , rather the command can be http post target http localhost 9090 data 10 which will allow us to have support for other http verbs and cleanly separate the namespace from hadoop etc. The RestShell from which this came was only concerned with http actions, so the leading command classification probably seemed superfluous.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19487,"results in both in memory and redis based definitions of RichGaugeService can t satisfy autowiring because there are two candidates. Had to change analytics memory to get the application context to load.",NULL,"results in both in memory and redis based definitions of RichGaugeService can t satisfy autowiring because there are two candidates. Had to change analytics memory to get the application context to load.",NULL,"Add for who this story is","well_formed","no_role","high",False
19487,"results in both in memory and redis based definitions of RichGaugeService can t satisfy autowiring because there are two candidates. Had to change analytics memory to get the application context to load.",NULL,"results in both in memory and redis based definitions of RichGaugeService can t satisfy autowiring because there are two candidates. Had to change analytics memory to get the application context to load.",NULL,"results in both in memory and redis based definitions of RichGaugeService can t satisfy autowiring because there are two candidates<span class='highlight-text severity-high'>. Had to change analytics memory to get the application context to load.</span>","minimal","punctuation","high",False
19485,"Both Luke s original code and my refactored PR 1 which uses same code snippet seem to behave strangely. Stored values seem fine, but the getCounts method seems phony. To test 1 stream create foo definition time log 2 tap create bar definition tap foo aggregatecounter 3 curl H application json http localhost 8080 metrics aggregate counters bar this gives default bucketing hourly but chances are that they are empty. ",NULL,"Both Luke s original code and my refactored PR 1 which uses same code snippet seem to behave strangely. Stored values seem fine, but the getCounts method seems phony. To test 1 stream create foo definition time log 2 tap create bar definition tap foo aggregatecounter 3 curl H application json http localhost 8080 metrics aggregate counters bar this gives default bucketing hourly but chances are that they are empty. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19485,"Both Luke s original code and my refactored PR 1 which uses same code snippet seem to behave strangely. Stored values seem fine, but the getCounts method seems phony. To test 1 stream create foo definition time log 2 tap create bar definition tap foo aggregatecounter 3 curl H application json http localhost 8080 metrics aggregate counters bar this gives default bucketing hourly but chances are that they are empty. ",NULL,"Both Luke s original code and my refactored PR 1 which uses same code snippet seem to behave strangely. Stored values seem fine, but the getCounts method seems phony. To test 1 stream create foo definition time log 2 tap create bar definition tap foo aggregatecounter 3 curl H application json http localhost 8080 metrics aggregate counters bar this gives default bucketing hourly but chances are that they are empty. ",NULL,"Both Luke s original code<span class='highlight-text severity-high'> and </span>my refactored PR 1 which uses same code snippet seem to behave strangely. Stored values seem fine, but the getCounts method seems phony. To test 1 stream create foo definition time log 2 tap create bar definition tap foo aggregatecounter 3 curl H application json http localhost 8080 metrics aggregate counters bar this gives default bucketing hourly but chances are that they are empty. ","atomic","conjunctions","high",False
19504,"Currently many methods take module, group, index defining a module instance; group and index can be encapsulated in Module so one arg can be passed around.",NULL,"Currently many methods take module, group, index defining a module instance; group and index can be encapsulated in Module","so one arg can be passed around.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19505,"Use an undeploy topic to broadcast undeploy requests to all containers. Applies to Redis and Rabbit transports, not local. Also, rename ModuleDeploymentRequest to ModuleOperationRequest with an enum DEPLOY , UNDEPLOY .",NULL,"Use an undeploy topic to broadcast undeploy requests to all containers. Applies to Redis and Rabbit transports, not local. Also, rename ModuleDeploymentRequest to ModuleOperationRequest with an enum DEPLOY , UNDEPLOY .",NULL,"Add for who this story is","well_formed","no_role","high",False
19760,"Provide optional command line arg to embed the container launcher, aka xd admin server. XDContainer.sh embeddAdmin",NULL,"Provide optional command line arg to embed the container launcher, aka xd admin server. XDContainer.sh embeddAdmin",NULL,"Add for who this story is","well_formed","no_role","high",False
19485,"Both Luke s original code and my refactored PR 1 which uses same code snippet seem to behave strangely. Stored values seem fine, but the getCounts method seems phony. To test 1 stream create foo definition time log 2 tap create bar definition tap foo aggregatecounter 3 curl H application json http localhost 8080 metrics aggregate counters bar this gives default bucketing hourly but chances are that they are empty. ",NULL,"Both Luke s original code and my refactored PR 1 which uses same code snippet seem to behave strangely. Stored values seem fine, but the getCounts method seems phony. To test 1 stream create foo definition time log 2 tap create bar definition tap foo aggregatecounter 3 curl H application json http localhost 8080 metrics aggregate counters bar this gives default bucketing hourly but chances are that they are empty. ",NULL,"Both Luke s original code and my refactored PR 1 which uses same code snippet seem to behave strangely<span class='highlight-text severity-high'>. Stored values seem fine, but the getCounts method seems phony. To test 1 stream create foo definition time log 2 tap create bar definition tap foo aggregatecounter 3 curl H application json http localhost 8080 metrics aggregate counters bar this gives default bucketing hourly but chances are that they are empty. </span>","minimal","punctuation","high",False
19486,"Sending data to an incomplete stream which is created using a named sink channel only works when using Redis or Rabbit?, not tested . Since the in memory version doesn t use a queue, it will fail if you are using xd singlenode. We should use a queue channel with unlimited capacity to allow messages to be sent before the full stream is created.",NULL,"Sending data to an incomplete stream which is created using a named sink channel only works when using Redis or Rabbit?, not tested . Since the in memory version doesn t use a queue, it will fail if you are using xd singlenode. We should use a queue channel with unlimited capacity to allow messages to be sent before the full stream is created.",NULL,"Add for who this story is","well_formed","no_role","high",False
19486,"Sending data to an incomplete stream which is created using a named sink channel only works when using Redis or Rabbit?, not tested . Since the in memory version doesn t use a queue, it will fail if you are using xd singlenode. We should use a queue channel with unlimited capacity to allow messages to be sent before the full stream is created.",NULL,"Sending data to an incomplete stream which is created using a named sink channel only works when using Redis or Rabbit?, not tested . Since the in memory version doesn t use a queue, it will fail if you are using xd singlenode. We should use a queue channel with unlimited capacity to allow messages to be sent before the full stream is created.",NULL,"Sending data to an incomplete stream which is created using a named sink channel only works when using Redis or Rabbit?, not tested <span class='highlight-text severity-high'>. Since the in memory version doesn t use a queue, it will fail if you are using xd singlenode. We should use a queue channel with unlimited capacity to allow messages to be sent before the full stream is created.</span>","minimal","punctuation","high",False
19488,"From https github.com SpringSource spring xd pull 161 The command shell needs to also support different hadoop distribution options. Perhaps the shell just uses a relative path to the location of xd lib ",NULL,"From https github.com SpringSource spring xd pull 161 The command shell needs to also support different hadoop distribution options. Perhaps the shell just uses a relative path to the location of xd lib ",NULL,"Add for who this story is","well_formed","no_role","high",False
19488,"From https github.com SpringSource spring xd pull 161 The command shell needs to also support different hadoop distribution options. Perhaps the shell just uses a relative path to the location of xd lib ",NULL,"From https github.com SpringSource spring xd pull 161 The command shell needs to also support different hadoop distribution options. Perhaps the shell just uses a relative path to the location of xd lib ",NULL,"From https github<span class='highlight-text severity-high'>.com SpringSource spring xd pull 161 The command shell needs to also support different hadoop distribution options. Perhaps the shell just uses a relative path to the location of xd lib </span>","minimal","punctuation","high",False
19498,"There is some overlap between the gauge and counter repository types and also between the domain resources used by the REST controllers CounterResource, GaugeResource . The aggregate counter may wish to return a simple count value for the counter too, which would also be a simple integral value.",NULL,"There is some overlap between the gauge and counter repository types and also between the domain resources used by the REST controllers CounterResource, GaugeResource . The aggregate counter may wish to return a simple count value for the counter too, which would also be a simple integral value.",NULL,"Add for who this story is","well_formed","no_role","high",False
19498,"There is some overlap between the gauge and counter repository types and also between the domain resources used by the REST controllers CounterResource, GaugeResource . The aggregate counter may wish to return a simple count value for the counter too, which would also be a simple integral value.",NULL,"There is some overlap between the gauge and counter repository types and also between the domain resources used by the REST controllers CounterResource, GaugeResource . The aggregate counter may wish to return a simple count value for the counter too, which would also be a simple integral value.",NULL,"There is some overlap between the gauge<span class='highlight-text severity-high'> and </span>counter repository types and also between the domain resources used by the REST controllers CounterResource, GaugeResource . The aggregate counter may wish to return a simple count value for the counter too, which would also be a simple integral value.","atomic","conjunctions","high",False
19498,"There is some overlap between the gauge and counter repository types and also between the domain resources used by the REST controllers CounterResource, GaugeResource . The aggregate counter may wish to return a simple count value for the counter too, which would also be a simple integral value.",NULL,"There is some overlap between the gauge and counter repository types and also between the domain resources used by the REST controllers CounterResource, GaugeResource . The aggregate counter may wish to return a simple count value for the counter too, which would also be a simple integral value.",NULL,"There is some overlap between the gauge and counter repository types and also between the domain resources used by the REST controllers CounterResource, GaugeResource <span class='highlight-text severity-high'>. The aggregate counter may wish to return a simple count value for the counter too, which would also be a simple integral value.</span>","minimal","punctuation","high",False
19497,"reproduce 1 Create a bad stream definition name bad Try to recreate with the same name, but correct stream definitions. The system will report that the stream already exists. ",NULL,"reproduce 1 Create a bad stream definition name bad Try to recreate with the same name, but correct stream definitions. The system will report that the stream already exists. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19497,"reproduce 1 Create a bad stream definition name bad Try to recreate with the same name, but correct stream definitions. The system will report that the stream already exists. ",NULL,"reproduce 1 Create a bad stream definition name bad Try to recreate with the same name, but correct stream definitions. The system will report that the stream already exists. ",NULL,"reproduce 1 Create a bad stream definition name bad Try to recreate with the same name, but correct stream definitions<span class='highlight-text severity-high'>. The system will report that the stream already exists. </span>","minimal","punctuation","high",False
19499,"It seems the html rendering of documentation is using a variable width font for some of the code esp. source,sh apparently rendering. Weird thing though is that when mouse hovering over some of them, they went back to fixed width. See screenshot. http static.springsource.org spring xd docs current SNAPSHOT reference html start the runtime and the xd shell ",NULL,"It seems the html rendering of documentation is using a variable width font for some of the code esp. source,sh apparently rendering. Weird thing though is that when mouse hovering over some of them, they went back to fixed width. See screenshot. http static.springsource.org spring xd docs current SNAPSHOT reference html start the runtime and the xd shell ",NULL,"Add for who this story is","well_formed","no_role","high",False
19499,"It seems the html rendering of documentation is using a variable width font for some of the code esp. source,sh apparently rendering. Weird thing though is that when mouse hovering over some of them, they went back to fixed width. See screenshot. http static.springsource.org spring xd docs current SNAPSHOT reference html start the runtime and the xd shell ",NULL,"It seems the html rendering of documentation is using a variable width font for some of the code esp. source,sh apparently rendering. Weird thing though is that when mouse hovering over some of them, they went back to fixed width. See screenshot. http static.springsource.org spring xd docs current SNAPSHOT reference html start the runtime and the xd shell ",NULL,"It seems the html rendering of documentation is using a variable width font for some of the code esp<span class='highlight-text severity-high'>. source,sh apparently rendering. Weird thing though is that when mouse hovering over some of them, they went back to fixed width. See screenshot. http static.springsource.org spring xd docs current SNAPSHOT reference html start the runtime and the xd shell </span>","minimal","punctuation","high",False
19500,"Split plugin module processing into pre and post processing where preprocessing is done before the context is refreshed and post processing is done after the refresh, but before the start. In the Stream plugin, wire the module into the ChannelRegistry during post processing, instead of using the ChannelRegistrar .",NULL,"Split plugin module processing into pre and post processing where preprocessing is done before the context is refreshed and post processing is done after the refresh, but before the start. In the Stream plugin, wire the module into the ChannelRegistry during post processing, instead of using the ChannelRegistrar .",NULL,"Add for who this story is","well_formed","no_role","high",False
19500,"Split plugin module processing into pre and post processing where preprocessing is done before the context is refreshed and post processing is done after the refresh, but before the start. In the Stream plugin, wire the module into the ChannelRegistry during post processing, instead of using the ChannelRegistrar .",NULL,"Split plugin module processing into pre and post processing where preprocessing is done before the context is refreshed and post processing is done after the refresh, but before the start. In the Stream plugin, wire the module into the ChannelRegistry during post processing, instead of using the ChannelRegistrar .",NULL,"Split plugin module processing into pre<span class='highlight-text severity-high'> and </span>post processing where preprocessing is done before the context is refreshed and post processing is done after the refresh, but before the start. In the Stream plugin, wire the module into the ChannelRegistry during post processing, instead of using the ChannelRegistrar .","atomic","conjunctions","high",False
19500,"Split plugin module processing into pre and post processing where preprocessing is done before the context is refreshed and post processing is done after the refresh, but before the start. In the Stream plugin, wire the module into the ChannelRegistry during post processing, instead of using the ChannelRegistrar .",NULL,"Split plugin module processing into pre and post processing where preprocessing is done before the context is refreshed and post processing is done after the refresh, but before the start. In the Stream plugin, wire the module into the ChannelRegistry during post processing, instead of using the ChannelRegistrar .",NULL,"Split plugin module processing into pre and post processing where preprocessing is done before the context is refreshed and post processing is done after the refresh, but before the start<span class='highlight-text severity-high'>. In the Stream plugin, wire the module into the ChannelRegistry during post processing, instead of using the ChannelRegistrar .</span>","minimal","punctuation","high",False
19501,"Seems like the current file source results from an initial POC. Very few things can be parameterized, including the polled directory that needs to be in tmp xxx To be useful in production, we might want to revisit",NULL,"Seems like the current file source results from an initial POC. Very few things can be parameterized, including the polled directory that needs to be in tmp xxx To be useful in production, we might want to revisit",NULL,"Add for who this story is","well_formed","no_role","high",False
19501,"Seems like the current file source results from an initial POC. Very few things can be parameterized, including the polled directory that needs to be in tmp xxx To be useful in production, we might want to revisit",NULL,"Seems like the current file source results from an initial POC. Very few things can be parameterized, including the polled directory that needs to be in tmp xxx To be useful in production, we might want to revisit",NULL,"Seems like the current file source results from an initial POC<span class='highlight-text severity-high'>. Very few things can be parameterized, including the polled directory that needs to be in tmp xxx To be useful in production, we might want to revisit</span>","minimal","punctuation","high",False
19502,"FieldValueCounterHandler was first written to support setting several fields at once, but the current constructor field value counter.xml does not use it. Either leverage that feature or simplify code",NULL,"FieldValueCounterHandler was first written to support setting several fields at once, but the current constructor field value counter.xml does not use it. Either leverage that feature or simplify code",NULL,"Add for who this story is","well_formed","no_role","high",False
19502,"FieldValueCounterHandler was first written to support setting several fields at once, but the current constructor field value counter.xml does not use it. Either leverage that feature or simplify code",NULL,"FieldValueCounterHandler was first written to support setting several fields at once, but the current constructor field value counter.xml does not use it. Either leverage that feature or simplify code",NULL,"FieldValueCounterHandler was first written to support setting several fields at once, but the current constructor field value counter.xml does not use it. Either leverage that feature<span class='highlight-text severity-high'> or </span>simplify code","atomic","conjunctions","high",False
19502,"FieldValueCounterHandler was first written to support setting several fields at once, but the current constructor field value counter.xml does not use it. Either leverage that feature or simplify code",NULL,"FieldValueCounterHandler was first written to support setting several fields at once, but the current constructor field value counter.xml does not use it. Either leverage that feature or simplify code",NULL,"FieldValueCounterHandler was first written to support setting several fields at once, but the current constructor field value counter<span class='highlight-text severity-high'>.xml does not use it. Either leverage that feature or simplify code</span>","minimal","punctuation","high",False
19503,"Most of the infrastructure and code cleanup has been done for In Memory Analytics. The only remaining issue is that, by including memory analytics.xml from common.xml, we re actually creating e.g. a new InMemoryCounterRepository that is different from the one present in the Admin process space. This story involves fixing that. It may actually be done as part of XD 353, handling the local transport as a special case context inheritance rather than import based on xd.transport",NULL,"Most of the infrastructure and code cleanup has been done for In Memory Analytics. The only remaining issue is that, by including memory analytics.xml from common.xml, we re actually creating e.g. a new InMemoryCounterRepository that is different from the one present in the Admin process space. This story involves fixing that. It may actually be done as part of XD 353, handling the local transport as a special case context inheritance rather than import based on xd.transport",NULL,"Add for who this story is","well_formed","no_role","high",False
19503,"Most of the infrastructure and code cleanup has been done for In Memory Analytics. The only remaining issue is that, by including memory analytics.xml from common.xml, we re actually creating e.g. a new InMemoryCounterRepository that is different from the one present in the Admin process space. This story involves fixing that. It may actually be done as part of XD 353, handling the local transport as a special case context inheritance rather than import based on xd.transport",NULL,"Most of the infrastructure and code cleanup has been done for In Memory Analytics. The only remaining issue is that, by including memory analytics.xml from common.xml, we re actually creating e.g. a new InMemoryCounterRepository that is different from the one present in the Admin process space. This story involves fixing that. It may actually be done as part of XD 353, handling the local transport as a special case context inheritance rather than import based on xd.transport",NULL,"Most of the infrastructure and code cleanup has been done for In Memory Analytics<span class='highlight-text severity-high'>. The only remaining issue is that, by including memory analytics.xml from common.xml, we re actually creating e.g. a new InMemoryCounterRepository that is different from the one present in the Admin process space. This story involves fixing that. It may actually be done as part of XD 353, handling the local transport as a special case context inheritance rather than import based on xd.transport</span>","minimal","punctuation","high",False
19506,"Investigate using transactions and pipelining to improve performance of both the inbound and outbound RedisQueue channel adapters. Involves testing against a pre release of SDR 1.1 M2.",NULL,"Investigate using transactions and pipelining to improve performance of both the inbound and outbound RedisQueue channel adapters. Involves testing against a pre release of SDR 1.1 M2.",NULL,"Add for who this story is","well_formed","no_role","high",False
19506,"Investigate using transactions and pipelining to improve performance of both the inbound and outbound RedisQueue channel adapters. Involves testing against a pre release of SDR 1.1 M2.",NULL,"Investigate using transactions and pipelining to improve performance of both the inbound and outbound RedisQueue channel adapters. Involves testing against a pre release of SDR 1.1 M2.",NULL,"Investigate using transactions<span class='highlight-text severity-high'> and </span>pipelining to improve performance of both the inbound and outbound RedisQueue channel adapters. Involves testing against a pre release of SDR 1.1 M2.","atomic","conjunctions","high",False
19506,"Investigate using transactions and pipelining to improve performance of both the inbound and outbound RedisQueue channel adapters. Involves testing against a pre release of SDR 1.1 M2.",NULL,"Investigate using transactions and pipelining to improve performance of both the inbound and outbound RedisQueue channel adapters. Involves testing against a pre release of SDR 1.1 M2.",NULL,"Investigate using transactions and pipelining to improve performance of both the inbound and outbound RedisQueue channel adapters<span class='highlight-text severity-high'>. Involves testing against a pre release of SDR 1.1 M2.</span>","minimal","punctuation","high",False
19504,"Currently many methods take module, group, index defining a module instance; group and index can be encapsulated in Module so one arg can be passed around.",NULL,"Currently many methods take module, group, index defining a module instance; group and index can be encapsulated in Module","so one arg can be passed around.","Add for who this story is","well_formed","no_role","high",False
19504,"Currently many methods take module, group, index defining a module instance; group and index can be encapsulated in Module so one arg can be passed around.",NULL,"Currently many methods take module, group, index defining a module instance; group and index can be encapsulated in Module","so one arg can be passed around.","Currently many methods take module, group, index defining a module instance; group<span class='highlight-text severity-high'> and </span>index can be encapsulated in Module so one arg can be passed around.","atomic","conjunctions","high",False
19504,"Currently many methods take module, group, index defining a module instance; group and index can be encapsulated in Module so one arg can be passed around.",NULL,"Currently many methods take module, group, index defining a module instance; group and index can be encapsulated in Module","so one arg can be passed around.","Currently many methods take module, group, index defining a module instance<span class='highlight-text severity-high'>; group and index can be encapsulated in Module so one arg can be passed around.</span>","minimal","punctuation","high",False
19505,"Use an undeploy topic to broadcast undeploy requests to all containers. Applies to Redis and Rabbit transports, not local. Also, rename ModuleDeploymentRequest to ModuleOperationRequest with an enum DEPLOY , UNDEPLOY .",NULL,"Use an undeploy topic to broadcast undeploy requests to all containers. Applies to Redis and Rabbit transports, not local. Also, rename ModuleDeploymentRequest to ModuleOperationRequest with an enum DEPLOY , UNDEPLOY .",NULL,"Use an undeploy topic to broadcast undeploy requests to all containers<span class='highlight-text severity-high'>. Applies to Redis and Rabbit transports, not local. Also, rename ModuleDeploymentRequest to ModuleOperationRequest with an enum DEPLOY , UNDEPLOY .</span>","minimal","punctuation","high",False
19507,"This requires to boostrap the singlenode admin server in process, submit commands to the shell programmatically, and assert on the results of executing the command.",NULL,"This requires to boostrap the singlenode admin server in process, submit commands to the shell programmatically, and assert on the results of executing the command.",NULL,"Add for who this story is","well_formed","no_role","high",False
19507,"This requires to boostrap the singlenode admin server in process, submit commands to the shell programmatically, and assert on the results of executing the command.",NULL,"This requires to boostrap the singlenode admin server in process, submit commands to the shell programmatically, and assert on the results of executing the command.",NULL,"This requires to boostrap the singlenode admin server in process, submit commands to the shell programmatically,<span class='highlight-text severity-high'> and </span>assert on the results of executing the command.","atomic","conjunctions","high",False
19496,"make sure nothing is broken spot check using. 1 ticktock 2 twitter 3 gemfire ",NULL,"make sure nothing is broken spot check using. 1 ticktock 2 twitter 3 gemfire ",NULL,"Add for who this story is","well_formed","no_role","high",False
19496,"make sure nothing is broken spot check using. 1 ticktock 2 twitter 3 gemfire ",NULL,"make sure nothing is broken spot check using. 1 ticktock 2 twitter 3 gemfire ",NULL,"make sure nothing is broken spot check using<span class='highlight-text severity-high'>. 1 ticktock 2 twitter 3 gemfire </span>","minimal","punctuation","high",False
19494,"Create a reproducible series of steps or shell integration test. ",NULL,"Create a reproducible series of steps or shell integration test. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19495,"Show how to select a specific hadoop distribution when starting embedded standalone XDContainer.",NULL,"Show how to select a specific hadoop distribution when starting embedded standalone XDContainer.",NULL,"Add for who this story is","well_formed","no_role","high",False
19514,"Still keep existing one.",NULL,"Still keep existing one.",NULL,"Add for who this story is","well_formed","no_role","high",False
19512,"create, delete, deploy streams...",NULL,"create, delete, deploy streams...",NULL,"Add for who this story is","well_formed","no_role","high",False
19515,"Currently, living at the root of the project, those files don t benefit from IDE SI awareness. Make it so that they belong to a java project which sees the correct version of the SI jars used. Has impact on the build.gradle file",NULL,"Currently, living at the root of the project, those files don t benefit from IDE SI awareness. Make it","so that they belong to a java project which sees the correct version of the SI jars used. Has impact on the build.gradle file","Add for who this story is","well_formed","no_role","high",False
19515,"Currently, living at the root of the project, those files don t benefit from IDE SI awareness. Make it so that they belong to a java project which sees the correct version of the SI jars used. Has impact on the build.gradle file",NULL,"Currently, living at the root of the project, those files don t benefit from IDE SI awareness. Make it","so that they belong to a java project which sees the correct version of the SI jars used. Has impact on the build.gradle file","Currently, living at the root of the project, those files don t benefit from IDE SI awareness<span class='highlight-text severity-high'>. Make it so that they belong to a java project which sees the correct version of the SI jars used. Has impact on the build.gradle file</span>","minimal","punctuation","high",False
19515,"Currently, living at the root of the project, those files don t benefit from IDE SI awareness. Make it so that they belong to a java project which sees the correct version of the SI jars used. Has impact on the build.gradle file",NULL,"Currently, living at the root of the project, those files don t benefit from IDE SI awareness. Make it","so that they belong to a java project which sees the correct version of the SI jars used. Has impact on the build.gradle file","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19513,"Existing code https github.com ghillert springone2012",NULL,"Existing code https github.com ghillert springone2012",NULL,"Add for who this story is","well_formed","no_role","high",False
19519,"CI job will run integration tests that are tagged for CI build.",NULL,"CI job will run integration tests that are tagged for CI build.",NULL,"Add for who this story is","well_formed","no_role","high",False
19521,"Add top level utility methods to manage XD runtime deploy, start and stop . These methods will be used by underlying integration tests to control runtime test environment. ",NULL,"Add top level utility methods to manage XD runtime deploy, start and stop . These methods will be used by underlying integration tests to control runtime test environment. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19521,"Add top level utility methods to manage XD runtime deploy, start and stop . These methods will be used by underlying integration tests to control runtime test environment. ",NULL,"Add top level utility methods to manage XD runtime deploy, start and stop . These methods will be used by underlying integration tests to control runtime test environment. ",NULL,"Add top level utility methods to manage XD runtime deploy, start and stop <span class='highlight-text severity-high'>. These methods will be used by underlying integration tests to control runtime test environment. </span>","minimal","punctuation","high",False
19522,"See XD 477",NULL,"See XD 477",NULL,"Add for who this story is","well_formed","no_role","high",False
19516,"Thinking about using the official SpringSource rules as a template",NULL,"Thinking about using the official SpringSource rules as a template",NULL,"Add for who this story is","well_formed","no_role","high",False
19517,"We need to fail fast.",NULL,"We need to fail fast.",NULL,"Add for who this story is","well_formed","no_role","high",False
19520,"Since most of the xd.shell.itests will do more then one thing deploy a stream, start it, add a tap, add a job, stop, etc we decided to decouple writing testcases with running it. Test cases will be written in spring shell scriptlets. Scriptlets are json files with command and expectedResult as tokens. Here s an example code testscript command stream create definition http file name http2file , result Created new stream http2file , command stream list , result ... code A parser will parse scriptlets, executes it and asserts on expected results. ",NULL,"Since most of the xd.shell.itests will do more then one thing deploy a stream, start it, add a tap, add a job, stop, etc we decided to decouple writing testcases with running it. Test cases will be written in spring shell scriptlets. Scriptlets are json files with command and expectedResult as tokens. Here s an example code testscript command stream create definition http file name http2file , result Created new stream http2file , command stream list , result ... code A parser will parse scriptlets, executes it and asserts on expected results. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19520,"Since most of the xd.shell.itests will do more then one thing deploy a stream, start it, add a tap, add a job, stop, etc we decided to decouple writing testcases with running it. Test cases will be written in spring shell scriptlets. Scriptlets are json files with command and expectedResult as tokens. Here s an example code testscript command stream create definition http file name http2file , result Created new stream http2file , command stream list , result ... code A parser will parse scriptlets, executes it and asserts on expected results. ",NULL,"Since most of the xd.shell.itests will do more then one thing deploy a stream, start it, add a tap, add a job, stop, etc we decided to decouple writing testcases with running it. Test cases will be written in spring shell scriptlets. Scriptlets are json files with command and expectedResult as tokens. Here s an example code testscript command stream create definition http file name http2file , result Created new stream http2file , command stream list , result ... code A parser will parse scriptlets, executes it and asserts on expected results. ",NULL,"Since most of the xd.shell.itests will do more then one thing deploy a stream, start it, add a tap, add a job, stop, etc we decided to decouple writing testcases with running it. Test cases will be written in spring shell scriptlets. Scriptlets are json files with command<span class='highlight-text severity-high'> and </span>expectedResult as tokens. Here s an example code testscript command stream create definition http file name http2file , result Created new stream http2file , command stream list , result ... code A parser will parse scriptlets, executes it and asserts on expected results. ","atomic","conjunctions","high",False
19520,"Since most of the xd.shell.itests will do more then one thing deploy a stream, start it, add a tap, add a job, stop, etc we decided to decouple writing testcases with running it. Test cases will be written in spring shell scriptlets. Scriptlets are json files with command and expectedResult as tokens. Here s an example code testscript command stream create definition http file name http2file , result Created new stream http2file , command stream list , result ... code A parser will parse scriptlets, executes it and asserts on expected results. ",NULL,"Since most of the xd.shell.itests will do more then one thing deploy a stream, start it, add a tap, add a job, stop, etc we decided to decouple writing testcases with running it. Test cases will be written in spring shell scriptlets. Scriptlets are json files with command and expectedResult as tokens. Here s an example code testscript command stream create definition http file name http2file , result Created new stream http2file , command stream list , result ... code A parser will parse scriptlets, executes it and asserts on expected results. ",NULL,"Since most of the xd<span class='highlight-text severity-high'>.shell.itests will do more then one thing deploy a stream, start it, add a tap, add a job, stop, etc we decided to decouple writing testcases with running it. Test cases will be written in spring shell scriptlets. Scriptlets are json files with command and expectedResult as tokens. Here s an example code testscript command stream create definition http file name http2file , result Created new stream http2file , command stream list , result ... code A parser will parse scriptlets, executes it and asserts on expected results. </span>","minimal","punctuation","high",False
19584,"See http static.springsource.org spring xd docs 1.0.0.M1 reference html twittersearch",NULL,"See http static.springsource.org spring xd docs 1.0.0.M1 reference html twittersearch",NULL,"Add for who this story is","well_formed","no_role","high",False
19585,"http static.springsource.org spring xd docs 1.0.0.M1 reference html tail",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19585,"http static.springsource.org spring xd docs 1.0.0.M1 reference html tail",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19523,"XD 482 addresses the use of camel case in fixed delay job module parameter name. and, we need to fix the same for other module parameters wherever is being used. ",NULL,"XD 482 addresses the use of camel case in fixed delay job module parameter name. and, we need to fix the same for other module parameters wherever is being used. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19523,"XD 482 addresses the use of camel case in fixed delay job module parameter name. and, we need to fix the same for other module parameters wherever is being used. ",NULL,"XD 482 addresses the use of camel case in fixed delay job module parameter name. and, we need to fix the same for other module parameters wherever is being used. ",NULL,"XD 482 addresses the use of camel case in fixed delay job module parameter name<span class='highlight-text severity-high'>. and, we need to fix the same for other module parameters wherever is being used. </span>","minimal","punctuation","high",False
19524,"We need to determine where this information could fit in. It can be either in README at the project home page or Getting started wiki page.",NULL,"We need to determine where this information could fit in. It can be either in README at the project home page or Getting started wiki page.",NULL,"Add for who this story is","well_formed","no_role","high",False
19524,"We need to determine where this information could fit in. It can be either in README at the project home page or Getting started wiki page.",NULL,"We need to determine where this information could fit in. It can be either in README at the project home page or Getting started wiki page.",NULL,"We need to determine where this information could fit in. It can be either in README at the project home page<span class='highlight-text severity-high'> or </span>Getting started wiki page.","atomic","conjunctions","high",False
19524,"We need to determine where this information could fit in. It can be either in README at the project home page or Getting started wiki page.",NULL,"We need to determine where this information could fit in. It can be either in README at the project home page or Getting started wiki page.",NULL,"We need to determine where this information could fit in<span class='highlight-text severity-high'>. It can be either in README at the project home page or Getting started wiki page.</span>","minimal","punctuation","high",False
19525,"Shell command to delete a trigger. Note this command will only remove the trigger definition not modifying the jobs that use the trigger.",NULL,"Shell command to delete a trigger. Note this command will only remove the trigger definition not modifying the jobs that use the trigger.",NULL,"Add for who this story is","well_formed","no_role","high",False
19525,"Shell command to delete a trigger. Note this command will only remove the trigger definition not modifying the jobs that use the trigger.",NULL,"Shell command to delete a trigger. Note this command will only remove the trigger definition not modifying the jobs that use the trigger.",NULL,"Shell command to delete a trigger<span class='highlight-text severity-high'>. Note this command will only remove the trigger definition not modifying the jobs that use the trigger.</span>","minimal","punctuation","high",False
19526,"public StreamPlugin postProcessContextPath CHANNEL REGISTRY; Subclasses should not directly update superclass fields.",NULL,"public StreamPlugin postProcessContextPath CHANNEL REGISTRY; Subclasses should not directly update superclass fields.",NULL,"Add for who this story is","well_formed","no_role","high",False
19526,"public StreamPlugin postProcessContextPath CHANNEL REGISTRY; Subclasses should not directly update superclass fields.",NULL,"public StreamPlugin postProcessContextPath CHANNEL REGISTRY; Subclasses should not directly update superclass fields.",NULL,"public StreamPlugin postProcessContextPath CHANNEL REGISTRY<span class='highlight-text severity-high'>; Subclasses should not directly update superclass fields.</span>","minimal","punctuation","high",False
19527,"Current behavior is to just have a prompt of unknown I think any return value of a CliCommand method is not shown b c the whole infrastructure is not up at that time",NULL,"Current behavior is to just have a prompt of unknown I think any return value of a CliCommand method is not shown b c the whole infrastructure is not up at that time",NULL,"Add for who this story is","well_formed","no_role","high",False
19528,"Caused by that weird annotation dependency problem that I worked around for compile. But Sonar complains. One solution would be to add Jackson 2 to the Sonar classpath , but I'did not manage to do that. ",NULL,"Caused by that weird annotation dependency problem that I worked around for compile. But Sonar complains. One solution would be to add Jackson 2 to the Sonar classpath , but I'did not manage to do that. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19528,"Caused by that weird annotation dependency problem that I worked around for compile. But Sonar complains. One solution would be to add Jackson 2 to the Sonar classpath , but I'did not manage to do that. ",NULL,"Caused by that weird annotation dependency problem that I worked around for compile. But Sonar complains. One solution would be to add Jackson 2 to the Sonar classpath , but I'did not manage to do that. ",NULL,"Caused by that weird annotation dependency problem that I worked around for compile<span class='highlight-text severity-high'>. But Sonar complains. One solution would be to add Jackson 2 to the Sonar classpath , but I'did not manage to do that. </span>","minimal","punctuation","high",False
19583,"See http static.springsource.org spring xd docs 1.0.0.M1 reference html gemfire cq",NULL,"See http static.springsource.org spring xd docs 1.0.0.M1 reference html gemfire cq",NULL,"Add for who this story is","well_formed","no_role","high",False
19509,"creating job defs, deploying jobs, undeploying jobs, deleting job defs",NULL,"creating job defs, deploying jobs, undeploying jobs, deleting job defs",NULL,"Add for who this story is","well_formed","no_role","high",False
19518,"Create proper test coverage for Controllers",NULL,"Create proper test coverage for Controllers",NULL,"Add for who this story is","well_formed","no_role","high",False
19510,"creating triggers, deleting triggers",NULL,"creating triggers, deleting triggers",NULL,"Add for who this story is","well_formed","no_role","high",False
19511,"creating taps, deleting",NULL,"creating taps, deleting",NULL,"Add for who this story is","well_formed","no_role","high",False
19530,"Save Save a XYZDefinition method used to be create Delete Delete a XYZDefinition method used to be called destroy Deploy Deploy a XYZDefinition Undeploy Undeploy a XYZDefinition List List a XYZDefinition Returns PagedResources XYZDefinitionResource Display Get specific information about a XYZDefinition Create other stories for each Controller and include in this weeks sprint ",NULL,"Save Save a XYZDefinition method used to be create Delete Delete a XYZDefinition method used to be called destroy Deploy Deploy a XYZDefinition Undeploy Undeploy a XYZDefinition List List a XYZDefinition Returns PagedResources XYZDefinitionResource Display Get specific information about a XYZDefinition Create other stories for each Controller and include in this weeks sprint ",NULL,"Add for who this story is","well_formed","no_role","high",False
19557,"Once issues like XD 438 have been completed the wiki doc will need updates to reflect the current behaviour and syntax options.",NULL,"Once issues like XD 438 have been completed the wiki doc will need updates to reflect the current behaviour and syntax options.",NULL,"Add for who this story is","well_formed","no_role","high",False
19746,"Need to shutdown cleanly, no exception messages are shown. Order of components in the stream should be shut down from first to last opposite of creation ",NULL,"Need to shutdown cleanly, no exception messages are shown. Order of components in the stream should be shut down from first to last opposite of creation ",NULL,"Add for who this story is","well_formed","no_role","high",False
19530,"Save Save a XYZDefinition method used to be create Delete Delete a XYZDefinition method used to be called destroy Deploy Deploy a XYZDefinition Undeploy Undeploy a XYZDefinition List List a XYZDefinition Returns PagedResources XYZDefinitionResource Display Get specific information about a XYZDefinition Create other stories for each Controller and include in this weeks sprint ",NULL,"Save Save a XYZDefinition method used to be create Delete Delete a XYZDefinition method used to be called destroy Deploy Deploy a XYZDefinition Undeploy Undeploy a XYZDefinition List List a XYZDefinition Returns PagedResources XYZDefinitionResource Display Get specific information about a XYZDefinition Create other stories for each Controller and include in this weeks sprint ",NULL,"Save Save a XYZDefinition method used to be create Delete Delete a XYZDefinition method used to be called destroy Deploy Deploy a XYZDefinition Undeploy Undeploy a XYZDefinition List List a XYZDefinition Returns PagedResources XYZDefinitionResource Display Get specific information about a XYZDefinition Create other stories for each Controller<span class='highlight-text severity-high'> and </span>include in this weeks sprint ","atomic","conjunctions","high",False
19531,"Only create in TapDeployer has some additional code to check if the stream exists, could take place in another location.",NULL,"Only create in TapDeployer has some additional code to check if the stream exists, could take place in another location.",NULL,"Add for who this story is","well_formed","no_role","high",False
19532,"To be consistent with Spring Data Repository method names.",NULL,"To be consistent with Spring Data Repository method names.",NULL,"Add for who this story is","well_formed","no_role","high",False
19533,"Favor using custom exceptions instead of using Assert.notNull, review usage and make changes. Eg. if a stream can t be found or another definition a XYZNotFoundException instead of Assert.Null on the return value of a findOne method ",NULL,"Favor using custom exceptions instead of using Assert.notNull, review usage and make changes. Eg. if a stream can t be found or another definition a XYZNotFoundException instead of Assert.Null on the return value of a findOne method ",NULL,"Add for who this story is","well_formed","no_role","high",False
19533,"Favor using custom exceptions instead of using Assert.notNull, review usage and make changes. Eg. if a stream can t be found or another definition a XYZNotFoundException instead of Assert.Null on the return value of a findOne method ",NULL,"Favor using custom exceptions instead of using Assert.notNull, review usage and make changes. Eg. if a stream can t be found or another definition a XYZNotFoundException instead of Assert.Null on the return value of a findOne method ",NULL,"Favor using custom exceptions instead of using Assert.notNull, review usage<span class='highlight-text severity-high'> and </span>make changes. Eg. if a stream can t be found<span class='highlight-text severity-high'> or </span>another definition a XYZNotFoundException instead of Assert.Null on the return value of a findOne method ","atomic","conjunctions","high",False
19533,"Favor using custom exceptions instead of using Assert.notNull, review usage and make changes. Eg. if a stream can t be found or another definition a XYZNotFoundException instead of Assert.Null on the return value of a findOne method ",NULL,"Favor using custom exceptions instead of using Assert.notNull, review usage and make changes. Eg. if a stream can t be found or another definition a XYZNotFoundException instead of Assert.Null on the return value of a findOne method ",NULL,"Favor using custom exceptions instead of using Assert<span class='highlight-text severity-high'>.notNull, review usage and make changes. Eg. if a stream can t be found or another definition a XYZNotFoundException instead of Assert.Null on the return value of a findOne method </span>","minimal","punctuation","high",False
19534,"StreamController to not access the repository instance directly, all access to go through StreamDeployer ",NULL,"StreamController to not access the repository instance directly, all access to go through StreamDeployer ",NULL,"Add for who this story is","well_formed","no_role","high",False
19535,"Rename existing Tap class to something else.",NULL,"Rename existing Tap class to something else.",NULL,"Add for who this story is","well_formed","no_role","high",False
19536,"See implementation used for Steams and apply to jobs, taps, triggers.",NULL,"See implementation used for Steams and apply to jobs, taps, triggers.",NULL,"Add for who this story is","well_formed","no_role","high",False
19536,"See implementation used for Steams and apply to jobs, taps, triggers.",NULL,"See implementation used for Steams and apply to jobs, taps, triggers.",NULL,"See implementation used for Steams<span class='highlight-text severity-high'> and </span>apply to jobs, taps, triggers.","atomic","conjunctions","high",False
19537,"Resource objects should be returned from all controller methods. MVC Tests should be added to check returned values.",NULL,"Resource objects should be returned from all controller methods. MVC Tests should be added to check returned values.",NULL,"Add for who this story is","well_formed","no_role","high",False
19537,"Resource objects should be returned from all controller methods. MVC Tests should be added to check returned values.",NULL,"Resource objects should be returned from all controller methods. MVC Tests should be added to check returned values.",NULL,"Resource objects should be returned from all controller methods<span class='highlight-text severity-high'>. MVC Tests should be added to check returned values.</span>","minimal","punctuation","high",False
19538,"We need a few steps 1. Investigate if we need to move off Spring Shell 1.0 dependency, e.g. need to use code in Spring Shell 2.0 branch 2. If we need to use code in Spring Shell 2.0 branch, we need to release a Spring Shell 1.1 M1 release with appropriate code changes. Create stories related to Shell release. 3. Determine and document the basic recipe for doing integration tests. 4. Create stories to provide integration tests for each existing command",NULL,"We need a few steps 1. Investigate if we need to move off Spring Shell 1.0 dependency, e.g. need to use code in Spring Shell 2.0 branch 2. If we need to use code in Spring Shell 2.0 branch, we need to release a Spring Shell 1.1 M1 release with appropriate code changes. Create stories related to Shell release. 3. Determine and document the basic recipe for doing integration tests. 4. Create stories to provide integration tests for each existing command",NULL,"Add for who this story is","well_formed","no_role","high",False
19538,"We need a few steps 1. Investigate if we need to move off Spring Shell 1.0 dependency, e.g. need to use code in Spring Shell 2.0 branch 2. If we need to use code in Spring Shell 2.0 branch, we need to release a Spring Shell 1.1 M1 release with appropriate code changes. Create stories related to Shell release. 3. Determine and document the basic recipe for doing integration tests. 4. Create stories to provide integration tests for each existing command",NULL,"We need a few steps 1. Investigate if we need to move off Spring Shell 1.0 dependency, e.g. need to use code in Spring Shell 2.0 branch 2. If we need to use code in Spring Shell 2.0 branch, we need to release a Spring Shell 1.1 M1 release with appropriate code changes. Create stories related to Shell release. 3. Determine and document the basic recipe for doing integration tests. 4. Create stories to provide integration tests for each existing command",NULL,"We need a few steps 1. Investigate if we need to move off Spring Shell 1.0 dependency, e.g. need to use code in Spring Shell 2.0 branch 2. If we need to use code in Spring Shell 2.0 branch, we need to release a Spring Shell 1.1 M1 release with appropriate code changes. Create stories related to Shell release. 3. Determine<span class='highlight-text severity-high'> and </span>document the basic recipe for doing integration tests. 4. Create stories to provide integration tests for each existing command","atomic","conjunctions","high",False
19538,"We need a few steps 1. Investigate if we need to move off Spring Shell 1.0 dependency, e.g. need to use code in Spring Shell 2.0 branch 2. If we need to use code in Spring Shell 2.0 branch, we need to release a Spring Shell 1.1 M1 release with appropriate code changes. Create stories related to Shell release. 3. Determine and document the basic recipe for doing integration tests. 4. Create stories to provide integration tests for each existing command",NULL,"We need a few steps 1. Investigate if we need to move off Spring Shell 1.0 dependency, e.g. need to use code in Spring Shell 2.0 branch 2. If we need to use code in Spring Shell 2.0 branch, we need to release a Spring Shell 1.1 M1 release with appropriate code changes. Create stories related to Shell release. 3. Determine and document the basic recipe for doing integration tests. 4. Create stories to provide integration tests for each existing command",NULL,"We need a few steps 1<span class='highlight-text severity-high'>. Investigate if we need to move off Spring Shell 1.0 dependency, e.g. need to use code in Spring Shell 2.0 branch 2. If we need to use code in Spring Shell 2.0 branch, we need to release a Spring Shell 1.1 M1 release with appropriate code changes. Create stories related to Shell release. 3. Determine and document the basic recipe for doing integration tests. 4. Create stories to provide integration tests for each existing command</span>","minimal","punctuation","high",False
19540,"A module can declare one ore more payload types it will accept. This will inform the runtime re. automatic payload conversion. This can be done in the module XML configuration and processed by StreamPlugin",NULL,"A module can declare one ore more payload types it will accept. This will inform the runtime re. automatic payload conversion. This can be done in the module XML configuration and processed by StreamPlugin",NULL,"Add for who this story is","well_formed","no_role","high",False
19540,"A module can declare one ore more payload types it will accept. This will inform the runtime re. automatic payload conversion. This can be done in the module XML configuration and processed by StreamPlugin",NULL,"A module can declare one ore more payload types it will accept. This will inform the runtime re. automatic payload conversion. This can be done in the module XML configuration and processed by StreamPlugin",NULL,"A module can declare one ore more payload types it will accept. This will inform the runtime re. automatic payload conversion. This can be done in the module XML configuration<span class='highlight-text severity-high'> and </span>processed by StreamPlugin","atomic","conjunctions","high",False
19540,"A module can declare one ore more payload types it will accept. This will inform the runtime re. automatic payload conversion. This can be done in the module XML configuration and processed by StreamPlugin",NULL,"A module can declare one ore more payload types it will accept. This will inform the runtime re. automatic payload conversion. This can be done in the module XML configuration and processed by StreamPlugin",NULL,"A module can declare one ore more payload types it will accept<span class='highlight-text severity-high'>. This will inform the runtime re. automatic payload conversion. This can be done in the module XML configuration and processed by StreamPlugin</span>","minimal","punctuation","high",False
19541,"Spring HATEOAS is here to help. Nonetheless, there are currently a number of outstanding issues, namely https github.com SpringSource spring hateoas pull 98 https jira.springsource.org browse SPR 10262 comment 91685 https github.com SpringSource spring hateoas pull 94 Creating a issue here for future reference",NULL,"Spring HATEOAS is here to help. Nonetheless, there are currently a number of outstanding issues, namely https github.com SpringSource spring hateoas pull 98 https jira.springsource.org browse SPR 10262 comment 91685 https github.com SpringSource spring hateoas pull 94 Creating a issue here for future reference",NULL,"Add for who this story is","well_formed","no_role","high",False
19541,"Spring HATEOAS is here to help. Nonetheless, there are currently a number of outstanding issues, namely https github.com SpringSource spring hateoas pull 98 https jira.springsource.org browse SPR 10262 comment 91685 https github.com SpringSource spring hateoas pull 94 Creating a issue here for future reference",NULL,"Spring HATEOAS is here to help. Nonetheless, there are currently a number of outstanding issues, namely https github.com SpringSource spring hateoas pull 98 https jira.springsource.org browse SPR 10262 comment 91685 https github.com SpringSource spring hateoas pull 94 Creating a issue here for future reference",NULL,"Spring HATEOAS is here to help<span class='highlight-text severity-high'>. Nonetheless, there are currently a number of outstanding issues, namely https github.com SpringSource spring hateoas pull 98 https jira.springsource.org browse SPR 10262 comment 91685 https github.com SpringSource spring hateoas pull 94 Creating a issue here for future reference</span>","minimal","punctuation","high",False
19542,"We need a generic script that can do JSON to tab delimited text transformation for data written to HDFS HAWQ external tables. Users should be able to specify columns fields to be included.",NULL,"We need a generic script that can do JSON to tab delimited text transformation for data written to HDFS HAWQ external tables. Users should be able to specify columns fields to be included.",NULL,"Add for who this story is","well_formed","no_role","high",False
19542,"We need a generic script that can do JSON to tab delimited text transformation for data written to HDFS HAWQ external tables. Users should be able to specify columns fields to be included.",NULL,"We need a generic script that can do JSON to tab delimited text transformation for data written to HDFS HAWQ external tables. Users should be able to specify columns fields to be included.",NULL,"We need a generic script that can do JSON to tab delimited text transformation for data written to HDFS HAWQ external tables<span class='highlight-text severity-high'>. Users should be able to specify columns fields to be included.</span>","minimal","punctuation","high",False
19543,"we need to modify startup script to use hadoop 1.1.2 as default or phd1 when specified with hadoopDistro phd1",NULL,"we need to modify startup script to use hadoop 1.1.2 as default or phd1 when specified with hadoopDistro phd1",NULL,"Add for who this story is","well_formed","no_role","high",False
19543,"we need to modify startup script to use hadoop 1.1.2 as default or phd1 when specified with hadoopDistro phd1",NULL,"we need to modify startup script to use hadoop 1.1.2 as default or phd1 when specified with hadoopDistro phd1",NULL,"we need to modify startup script to use hadoop 1.1.2 as default<span class='highlight-text severity-high'> or </span>phd1 when specified with hadoopDistro phd1","atomic","conjunctions","high",False
19544,"we need to modify build adding two sub projects for spring xd hadoop one for hadoop 1.1.2 and one for phd1 Pivotal HD to pull in transitive dependencies for correct Hadoop distro",NULL,"we need to modify build adding two sub projects for spring xd hadoop one for hadoop 1.1.2 and one for phd1 Pivotal HD to pull in transitive dependencies for correct Hadoop distro",NULL,"Add for who this story is","well_formed","no_role","high",False
19544,"we need to modify build adding two sub projects for spring xd hadoop one for hadoop 1.1.2 and one for phd1 Pivotal HD to pull in transitive dependencies for correct Hadoop distro",NULL,"we need to modify build adding two sub projects for spring xd hadoop one for hadoop 1.1.2 and one for phd1 Pivotal HD to pull in transitive dependencies for correct Hadoop distro",NULL,"we need to modify build adding two sub projects for spring xd hadoop one for hadoop 1.1.2<span class='highlight-text severity-high'> and </span>one for phd1 Pivotal HD to pull in transitive dependencies for correct Hadoop distro","atomic","conjunctions","high",False
19545,"we need a batching JDBC channel adapter int jdbc outbound channel adapter is not batching statements AFAICT ",NULL,"we need a batching JDBC channel adapter int jdbc outbound channel adapter is not batching statements AFAICT ",NULL,"Add for who this story is","well_formed","no_role","high",False
19546,"we need a JDBC sink for writing to HAWQ using int jdbc outbound channel adapter and postgresql JDBC driver ",NULL,"we need a JDBC sink for writing to HAWQ using int jdbc outbound channel adapter and postgresql JDBC driver ",NULL,"Add for who this story is","well_formed","no_role","high",False
19547,"spring data hadoop 1.0.1.RC1 provides flavors for commonly used Hadoop distros versions and we should make use of that.",NULL,"spring data hadoop 1.0.1.RC1 provides flavors for commonly used Hadoop distros versions and we should make use of that.",NULL,"Add for who this story is","well_formed","no_role","high",False
19547,"spring data hadoop 1.0.1.RC1 provides flavors for commonly used Hadoop distros versions and we should make use of that.",NULL,"spring data hadoop 1.0.1.RC1 provides flavors for commonly used Hadoop distros versions and we should make use of that.",NULL,"spring data hadoop 1.0.1.RC1 provides flavors for commonly used Hadoop distros versions<span class='highlight-text severity-high'> and </span>we should make use of that.","atomic","conjunctions","high",False
19539,"Implements automatic conversion. Provide APIs to channel registry to register Payload conversion. Includes Redis and Rabbit transport.",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19539,"Implements automatic conversion. Provide APIs to channel registry to register Payload conversion. Includes Redis and Rabbit transport.",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19539,"Implements automatic conversion. Provide APIs to channel registry to register Payload conversion. Includes Redis and Rabbit transport.",NULL,NULL,NULL,"Implements automatic conversion<span class='highlight-text severity-high'>. Provide APIs to channel registry to register Payload conversion. Includes Redis and Rabbit transport.</span>","minimal","punctuation","high",False
19554,"2 options are 1 Fire the trigger immediate Launch the job when trigger can gather the resources necessary start the job 2 Do nothing Ignore this job fire time and catch this scenario can occur if XD is down or resources threads are not available at the time a job is to be launched. ",NULL,"2 options are 1 Fire the trigger immediate Launch the job when trigger can gather the resources necessary start the job 2 Do nothing Ignore this job fire time and catch this scenario can occur if XD is down or resources threads are not available at the time a job is to be launched. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19554,"2 options are 1 Fire the trigger immediate Launch the job when trigger can gather the resources necessary start the job 2 Do nothing Ignore this job fire time and catch this scenario can occur if XD is down or resources threads are not available at the time a job is to be launched. ",NULL,"2 options are 1 Fire the trigger immediate Launch the job when trigger can gather the resources necessary start the job 2 Do nothing Ignore this job fire time and catch this scenario can occur if XD is down or resources threads are not available at the time a job is to be launched. ",NULL,"2 options are 1 Fire the trigger immediate Launch the job when trigger can gather the resources necessary start the job 2 Do nothing Ignore this job fire time<span class='highlight-text severity-high'> and </span>catch this scenario can occur if XD is down<span class='highlight-text severity-high'> or </span>resources threads are not available at the time a job is to be launched. ","atomic","conjunctions","high",False
19746,"Need to shutdown cleanly, no exception messages are shown. Order of components in the stream should be shut down from first to last opposite of creation ",NULL,"Need to shutdown cleanly, no exception messages are shown. Order of components in the stream should be shut down from first to last opposite of creation ",NULL,"Need to shutdown cleanly, no exception messages are shown<span class='highlight-text severity-high'>. Order of components in the stream should be shut down from first to last opposite of creation </span>","minimal","punctuation","high",False
19556,"Dependent servers should be required on the CI server, but optional on developer systems.",NULL,"Dependent servers should be required on the CI server, but optional on developer systems.",NULL,"Add for who this story is","well_formed","no_role","high",False
19559,"The new parser supports for connecting regular modules and for connecting job steps. The modules in the ast that were connected with are tagged but nothing is currently using that information it doesnt get into the module deployment request . We need to think about using this data policing the modules that are being deployed to ensure they are job steps, for example.",NULL,"The new parser supports for connecting regular modules and for connecting job steps. The modules in the ast that were connected with are tagged but nothing is currently using that information it doesnt get into the module deployment request . We need to think about using this data policing the modules that are being deployed to ensure they are job steps, for example.",NULL,"Add for who this story is","well_formed","no_role","high",False
19559,"The new parser supports for connecting regular modules and for connecting job steps. The modules in the ast that were connected with are tagged but nothing is currently using that information it doesnt get into the module deployment request . We need to think about using this data policing the modules that are being deployed to ensure they are job steps, for example.",NULL,"The new parser supports for connecting regular modules and for connecting job steps. The modules in the ast that were connected with are tagged but nothing is currently using that information it doesnt get into the module deployment request . We need to think about using this data policing the modules that are being deployed to ensure they are job steps, for example.",NULL,"The new parser supports for connecting regular modules<span class='highlight-text severity-high'> and </span>for connecting job steps. The modules in the ast that were connected with are tagged but nothing is currently using that information it doesnt get into the module deployment request . We need to think about using this data policing the modules that are being deployed to ensure they are job steps, for example.","atomic","conjunctions","high",False
19559,"The new parser supports for connecting regular modules and for connecting job steps. The modules in the ast that were connected with are tagged but nothing is currently using that information it doesnt get into the module deployment request . We need to think about using this data policing the modules that are being deployed to ensure they are job steps, for example.",NULL,"The new parser supports for connecting regular modules and for connecting job steps. The modules in the ast that were connected with are tagged but nothing is currently using that information it doesnt get into the module deployment request . We need to think about using this data policing the modules that are being deployed to ensure they are job steps, for example.",NULL,"The new parser supports for connecting regular modules and for connecting job steps<span class='highlight-text severity-high'>. The modules in the ast that were connected with are tagged but nothing is currently using that information it doesnt get into the module deployment request . We need to think about using this data policing the modules that are being deployed to ensure they are job steps, for example.</span>","minimal","punctuation","high",False
19586,"The documentation in the Running in Distributed Mode chapter should discuss that the distributed runtime can use essentially any middleware to communicate between nodes. This functionality is provided by the core ChannelRegistry abstraction. A new intro paragraph shoul convey that it isn t a redis only or rabbitmq only system. There should be Installing RabbitMQ and Starting RabbitMQ sections to match those for Redis. Starting Spring XD in Distributed Mode should cover how to configure the system to select to use Redis or Rabbit.",NULL,"The documentation in the Running in Distributed Mode chapter should discuss that the distributed runtime can use essentially any middleware to communicate between nodes. This functionality is provided by the core ChannelRegistry abstraction. A new intro paragraph shoul convey that it isn t a redis only or rabbitmq only system. There should be Installing RabbitMQ and Starting RabbitMQ sections to match those for Redis. Starting Spring XD in Distributed Mode should cover how to configure the system to select to use Redis or Rabbit.",NULL,"Add for who this story is","well_formed","no_role","high",False
19586,"The documentation in the Running in Distributed Mode chapter should discuss that the distributed runtime can use essentially any middleware to communicate between nodes. This functionality is provided by the core ChannelRegistry abstraction. A new intro paragraph shoul convey that it isn t a redis only or rabbitmq only system. There should be Installing RabbitMQ and Starting RabbitMQ sections to match those for Redis. Starting Spring XD in Distributed Mode should cover how to configure the system to select to use Redis or Rabbit.",NULL,"The documentation in the Running in Distributed Mode chapter should discuss that the distributed runtime can use essentially any middleware to communicate between nodes. This functionality is provided by the core ChannelRegistry abstraction. A new intro paragraph shoul convey that it isn t a redis only or rabbitmq only system. There should be Installing RabbitMQ and Starting RabbitMQ sections to match those for Redis. Starting Spring XD in Distributed Mode should cover how to configure the system to select to use Redis or Rabbit.",NULL,"The documentation in the Running in Distributed Mode chapter should discuss that the distributed runtime can use essentially any middleware to communicate between nodes. This functionality is provided by the core ChannelRegistry abstraction. A new intro paragraph shoul convey that it isn t a redis only<span class='highlight-text severity-high'> or </span>rabbitmq only system. There should be Installing RabbitMQ<span class='highlight-text severity-high'> and </span>Starting RabbitMQ sections to match those for Redis. Starting Spring XD in Distributed Mode should cover how to configure the system to select to use Redis or Rabbit.","atomic","conjunctions","high",False
19586,"The documentation in the Running in Distributed Mode chapter should discuss that the distributed runtime can use essentially any middleware to communicate between nodes. This functionality is provided by the core ChannelRegistry abstraction. A new intro paragraph shoul convey that it isn t a redis only or rabbitmq only system. There should be Installing RabbitMQ and Starting RabbitMQ sections to match those for Redis. Starting Spring XD in Distributed Mode should cover how to configure the system to select to use Redis or Rabbit.",NULL,"The documentation in the Running in Distributed Mode chapter should discuss that the distributed runtime can use essentially any middleware to communicate between nodes. This functionality is provided by the core ChannelRegistry abstraction. A new intro paragraph shoul convey that it isn t a redis only or rabbitmq only system. There should be Installing RabbitMQ and Starting RabbitMQ sections to match those for Redis. Starting Spring XD in Distributed Mode should cover how to configure the system to select to use Redis or Rabbit.",NULL,"The documentation in the Running in Distributed Mode chapter should discuss that the distributed runtime can use essentially any middleware to communicate between nodes<span class='highlight-text severity-high'>. This functionality is provided by the core ChannelRegistry abstraction. A new intro paragraph shoul convey that it isn t a redis only or rabbitmq only system. There should be Installing RabbitMQ and Starting RabbitMQ sections to match those for Redis. Starting Spring XD in Distributed Mode should cover how to configure the system to select to use Redis or Rabbit.</span>","minimal","punctuation","high",False
19587,"See http static.springsource.org spring xd docs 1.0.0.M1 reference html http ",NULL,"See http static.springsource.org spring xd docs 1.0.0.M1 reference html http ",NULL,"Add for who this story is","well_formed","no_role","high",False
19747,"The current default is hdfs localhost 9000 but most new distributions installs use 8020",NULL,"The current default is hdfs localhost 9000 but most new distributions installs use 8020",NULL,"Add for who this story is","well_formed","no_role","high",False
19553,"The following will retrieve the names of all module types eg sources, sinks, jobs, processors, triggers . code GET module types code I m expecting that the plural would be used, but singular would work as well. The following gets modules of a given type code GET module types type code This would be similar to the modules call in XD 265, but it would only return modules of the specified type. ",NULL,"The following will retrieve the names of all module types eg sources, sinks, jobs, processors, triggers . code GET module types code I m expecting that the plural would be used, but singular would work as well. The following gets modules of a given type code GET module types type code This would be similar to the modules call in XD 265, but it would only return modules of the specified type. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19553,"The following will retrieve the names of all module types eg sources, sinks, jobs, processors, triggers . code GET module types code I m expecting that the plural would be used, but singular would work as well. The following gets modules of a given type code GET module types type code This would be similar to the modules call in XD 265, but it would only return modules of the specified type. ",NULL,"The following will retrieve the names of all module types eg sources, sinks, jobs, processors, triggers . code GET module types code I m expecting that the plural would be used, but singular would work as well. The following gets modules of a given type code GET module types type code This would be similar to the modules call in XD 265, but it would only return modules of the specified type. ",NULL,"The following will retrieve the names of all module types eg sources, sinks, jobs, processors, triggers <span class='highlight-text severity-high'>. code GET module types code I m expecting that the plural would be used, but singular would work as well. The following gets modules of a given type code GET module types type code This would be similar to the modules call in XD 265, but it would only return modules of the specified type. </span>","minimal","punctuation","high",False
19555,"We need to have the ability to set read timeout for http request. This is already implemented here https github.com SpringSource rest shell ",NULL,"We need to have the ability to set read timeout for http request. This is already implemented here https github.com SpringSource rest shell ",NULL,"Add for who this story is","well_formed","no_role","high",False
19555,"We need to have the ability to set read timeout for http request. This is already implemented here https github.com SpringSource rest shell ",NULL,"We need to have the ability to set read timeout for http request. This is already implemented here https github.com SpringSource rest shell ",NULL,"We need to have the ability to set read timeout for http request<span class='highlight-text severity-high'>. This is already implemented here https github.com SpringSource rest shell </span>","minimal","punctuation","high",False
19588,"There are existing commands that can be taken from https github.com SpringSource spring hadoop samples or https github.com SpringSource impala that can be used for this",NULL,"There are existing commands that can be taken from https github.com SpringSource spring hadoop samples or https github.com SpringSource impala that can be used for this",NULL,"Add for who this story is","well_formed","no_role","high",False
19588,"There are existing commands that can be taken from https github.com SpringSource spring hadoop samples or https github.com SpringSource impala that can be used for this",NULL,"There are existing commands that can be taken from https github.com SpringSource spring hadoop samples or https github.com SpringSource impala that can be used for this",NULL,"There are existing commands that can be taken from https github.com SpringSource spring hadoop samples<span class='highlight-text severity-high'> or </span>https github.com SpringSource impala that can be used for this","atomic","conjunctions","high",False
19563,"The basic launch configurations should be tested automatically to ensure that startup scripts and launch aren t broken by changes.",NULL,"The basic launch configurations should be tested automatically to ensure that startup scripts and launch aren t broken by changes.",NULL,"Add for who this story is","well_formed","no_role","high",False
19563,"The basic launch configurations should be tested automatically to ensure that startup scripts and launch aren t broken by changes.",NULL,"The basic launch configurations should be tested automatically to ensure that startup scripts and launch aren t broken by changes.",NULL,"The basic launch configurations should be tested automatically to ensure that startup scripts<span class='highlight-text severity-high'> and </span>launch aren t broken by changes.","atomic","conjunctions","high",False
19562,"Currently spring xd dirt has direct dependencies on Redis and Rabbit. Consider moving transport dependent classes to separate jars with runtime dependencies",NULL,"Currently spring xd dirt has direct dependencies on Redis and Rabbit. Consider moving transport dependent classes to separate jars with runtime dependencies",NULL,"Add for who this story is","well_formed","no_role","high",False
19562,"Currently spring xd dirt has direct dependencies on Redis and Rabbit. Consider moving transport dependent classes to separate jars with runtime dependencies",NULL,"Currently spring xd dirt has direct dependencies on Redis and Rabbit. Consider moving transport dependent classes to separate jars with runtime dependencies",NULL,"Currently spring xd dirt has direct dependencies on Redis and Rabbit<span class='highlight-text severity-high'>. Consider moving transport dependent classes to separate jars with runtime dependencies</span>","minimal","punctuation","high",False
19565,"XD 162 requires registering message converters with the ChannelRegistry. End user needs to configure this statically as the Spring configuration is not exposed.",NULL,"XD 162 requires registering message converters with the ChannelRegistry. End user needs to configure this statically as the Spring configuration is not exposed.",NULL,"Add for who this story is","well_formed","no_role","high",False
19565,"XD 162 requires registering message converters with the ChannelRegistry. End user needs to configure this statically as the Spring configuration is not exposed.",NULL,"XD 162 requires registering message converters with the ChannelRegistry. End user needs to configure this statically as the Spring configuration is not exposed.",NULL,"XD 162 requires registering message converters with the ChannelRegistry<span class='highlight-text severity-high'>. End user needs to configure this statically as the Spring configuration is not exposed.</span>","minimal","punctuation","high",False
19564,"If Redis is not running, the container fails to initialize in ContainerMain.launch because the connection factory attempts to eagerly connect. If RabbitMQ is not running, the container fails to initialize in AbstractContainerLauncher.launch . Make the failure behavior consistent from a user perspective and add a spring retry RetryTemplate to retry container startup.",NULL,"If Redis is not running, the container fails to initialize in ContainerMain.launch because the connection factory attempts to eagerly connect. If RabbitMQ is not running, the container fails to initialize in AbstractContainerLauncher.launch . Make the failure behavior consistent from a user perspective and add a spring retry RetryTemplate to retry container startup.",NULL,"Add for who this story is","well_formed","no_role","high",False
19564,"If Redis is not running, the container fails to initialize in ContainerMain.launch because the connection factory attempts to eagerly connect. If RabbitMQ is not running, the container fails to initialize in AbstractContainerLauncher.launch . Make the failure behavior consistent from a user perspective and add a spring retry RetryTemplate to retry container startup.",NULL,"If Redis is not running, the container fails to initialize in ContainerMain.launch because the connection factory attempts to eagerly connect. If RabbitMQ is not running, the container fails to initialize in AbstractContainerLauncher.launch . Make the failure behavior consistent from a user perspective and add a spring retry RetryTemplate to retry container startup.",NULL,"If Redis is not running, the container fails to initialize in ContainerMain.launch because the connection factory attempts to eagerly connect. If RabbitMQ is not running, the container fails to initialize in AbstractContainerLauncher.launch . Make the failure behavior consistent from a user perspective<span class='highlight-text severity-high'> and </span>add a spring retry RetryTemplate to retry container startup.","atomic","conjunctions","high",False
19564,"If Redis is not running, the container fails to initialize in ContainerMain.launch because the connection factory attempts to eagerly connect. If RabbitMQ is not running, the container fails to initialize in AbstractContainerLauncher.launch . Make the failure behavior consistent from a user perspective and add a spring retry RetryTemplate to retry container startup.",NULL,"If Redis is not running, the container fails to initialize in ContainerMain.launch because the connection factory attempts to eagerly connect. If RabbitMQ is not running, the container fails to initialize in AbstractContainerLauncher.launch . Make the failure behavior consistent from a user perspective and add a spring retry RetryTemplate to retry container startup.",NULL,"If Redis is not running, the container fails to initialize in ContainerMain<span class='highlight-text severity-high'>.launch because the connection factory attempts to eagerly connect. If RabbitMQ is not running, the container fails to initialize in AbstractContainerLauncher.launch . Make the failure behavior consistent from a user perspective and add a spring retry RetryTemplate to retry container startup.</span>","minimal","punctuation","high",False
19567,"See http static.springsource.org spring xd docs 1.0.x SNAPSHOT reference html test the deployed module 3 ",NULL,"See http static.springsource.org spring xd docs 1.0.x SNAPSHOT reference html test the deployed module 3 ",NULL,"Add for who this story is","well_formed","no_role","high",False
19566,"Avoid the need for jmxPort xxxx when running both a Container and Admin on the same server",NULL,"Avoid the need for jmxPort xxxx when running both a Container and Admin on the same server",NULL,"Add for who this story is","well_formed","no_role","high",False
19571,"See http static.springsource.org spring xd docs 1.0.0.M1 reference html taps The existing docs should be made to show a real stream being created with filter and or transformer and then a tap that goes to logging. The shell syntax to also stop undeploy a tap should be shown here as well since the lifecycle is discussed.",NULL,"See http static.springsource.org spring xd docs 1.0.0.M1 reference html taps The existing docs should be made to show a real stream being created with filter and or transformer and then a tap that goes to logging. The shell syntax to also stop undeploy a tap should be shown here as well since the lifecycle is discussed.",NULL,"Add for who this story is","well_formed","no_role","high",False
19571,"See http static.springsource.org spring xd docs 1.0.0.M1 reference html taps The existing docs should be made to show a real stream being created with filter and or transformer and then a tap that goes to logging. The shell syntax to also stop undeploy a tap should be shown here as well since the lifecycle is discussed.",NULL,"See http static.springsource.org spring xd docs 1.0.0.M1 reference html taps The existing docs should be made to show a real stream being created with filter and or transformer and then a tap that goes to logging. The shell syntax to also stop undeploy a tap should be shown here as well since the lifecycle is discussed.",NULL,"See http static.springsource.org spring xd docs 1.0.0.M1 reference html taps The existing docs should be made to show a real stream being created with filter<span class='highlight-text severity-high'> and </span>span class='highlight-text severity-high'> or </span>transformer and then a tap that goes to logging. The shell syntax to also stop undeploy a tap should be shown here as well since the lifecycle is discussed.","atomic","conjunctions","high",False
19620,"The object naming is still not ideal for XD since SI conventions add some noise. Likely need to design and implement a custom naming strategy",NULL,"The object naming is still not ideal for XD since SI conventions add some noise. Likely need to design and implement a custom naming strategy",NULL,"The object naming is still not ideal for XD since SI conventions add some noise. Likely need to design<span class='highlight-text severity-high'> and </span>implement a custom naming strategy","atomic","conjunctions","high",False
19571,"See http static.springsource.org spring xd docs 1.0.0.M1 reference html taps The existing docs should be made to show a real stream being created with filter and or transformer and then a tap that goes to logging. The shell syntax to also stop undeploy a tap should be shown here as well since the lifecycle is discussed.",NULL,"See http static.springsource.org spring xd docs 1.0.0.M1 reference html taps The existing docs should be made to show a real stream being created with filter and or transformer and then a tap that goes to logging. The shell syntax to also stop undeploy a tap should be shown here as well since the lifecycle is discussed.",NULL,"See http static<span class='highlight-text severity-high'>.springsource.org spring xd docs 1.0.0.M1 reference html taps The existing docs should be made to show a real stream being created with filter and or transformer and then a tap that goes to logging. The shell syntax to also stop undeploy a tap should be shown here as well since the lifecycle is discussed.</span>","minimal","punctuation","high",False
19568,"See http static.springsource.org spring xd docs 1.0.x SNAPSHOT reference html test the deployed module",NULL,"See http static.springsource.org spring xd docs 1.0.x SNAPSHOT reference html test the deployed module",NULL,"Add for who this story is","well_formed","no_role","high",False
19570,"Add section to Analytics chapter on use of AggregateCounter. The example should show the use of the shell to create the tap that uses the AggregateCounter.",NULL,"Add section to Analytics chapter on use of AggregateCounter. The example should show the use of the shell to create the tap that uses the AggregateCounter.",NULL,"Add for who this story is","well_formed","no_role","high",False
19570,"Add section to Analytics chapter on use of AggregateCounter. The example should show the use of the shell to create the tap that uses the AggregateCounter.",NULL,"Add section to Analytics chapter on use of AggregateCounter. The example should show the use of the shell to create the tap that uses the AggregateCounter.",NULL,"Add section to Analytics chapter on use of AggregateCounter<span class='highlight-text severity-high'>. The example should show the use of the shell to create the tap that uses the AggregateCounter.</span>","minimal","punctuation","high",False
19572,"See http static.springsource.org spring xd docs 1.0.0.M1 reference html gemfire ",NULL,"See http static.springsource.org spring xd docs 1.0.0.M1 reference html gemfire ",NULL,"Add for who this story is","well_formed","no_role","high",False
19574,"See http static.springsource.org spring xd docs 1.0.0.M1 reference html hdfs",NULL,"See http static.springsource.org spring xd docs 1.0.0.M1 reference html hdfs",NULL,"Add for who this story is","well_formed","no_role","high",False
19580,"See http static.springsource.org spring xd docs 1.0.0.M1 reference html filter http static.springsource.org spring xd docs 1.0.0.M1 reference html json value filter",NULL,"See http static.springsource.org spring xd docs 1.0.0.M1 reference html filter http static.springsource.org spring xd docs 1.0.0.M1 reference html json value filter",NULL,"Add for who this story is","well_formed","no_role","high",False
19575,"See http static.springsource.org spring xd docs 1.0.0.M1 reference html file sinks",NULL,"See http static.springsource.org spring xd docs 1.0.0.M1 reference html file sinks",NULL,"Add for who this story is","well_formed","no_role","high",False
19579,"See http static.springsource.org spring xd docs 1.0.0.M1 reference html transform",NULL,"See http static.springsource.org spring xd docs 1.0.0.M1 reference html transform",NULL,"Add for who this story is","well_formed","no_role","high",False
19576,"See http static.springsource.org spring xd docs 1.0.0.M1 reference html log sinks",NULL,"See http static.springsource.org spring xd docs 1.0.0.M1 reference html log sinks",NULL,"Add for who this story is","well_formed","no_role","high",False
19577,"See http static.springsource.org spring xd docs 1.0.0.M1 reference html script",NULL,"See http static.springsource.org spring xd docs 1.0.0.M1 reference html script",NULL,"Add for who this story is","well_formed","no_role","high",False
19578,"See http static.springsource.org spring xd docs 1.0.0.M1 reference html json field extractor",NULL,"See http static.springsource.org spring xd docs 1.0.0.M1 reference html json field extractor",NULL,"Add for who this story is","well_formed","no_role","high",False
19581,"See http static.springsource.org spring xd docs 1.0.0.M1 reference html tcp",NULL,"See http static.springsource.org spring xd docs 1.0.0.M1 reference html tcp",NULL,"Add for who this story is","well_formed","no_role","high",False
19582,"See http static.springsource.org spring xd docs 1.0.0.M1 reference html syslog",NULL,"See http static.springsource.org spring xd docs 1.0.0.M1 reference html syslog",NULL,"Add for who this story is","well_formed","no_role","high",False
19573,"See http static.springsource.org spring xd docs 1.0.0.M1 reference html tcp sinks",NULL,"See http static.springsource.org spring xd docs 1.0.0.M1 reference html tcp sinks",NULL,"Add for who this story is","well_formed","no_role","high",False
19561,"Looking at the latest Sonar run we have 3 package tangles in Spring XD https sonar.springsource.org drilldown measures 7717?metric 87",NULL,"Looking at the latest Sonar run we have 3 package tangles in Spring XD https sonar.springsource.org drilldown measures 7717?metric 87",NULL,"Add for who this story is","well_formed","no_role","high",False
19569,"test the deployed module sub section uses curl.",NULL,"test the deployed module sub section uses curl.",NULL,"Add for who this story is","well_formed","no_role","high",False
19590,"the current streams chapter http static.springsource.org spring xd docs 1.0.0.M1 reference html streams shows creation and deleting streams using CURL switch to use shell. Also add listing of a stream. there is also an example of creating a stream, this should be replaced as well. ",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19590,"the current streams chapter http static.springsource.org spring xd docs 1.0.0.M1 reference html streams shows creation and deleting streams using CURL switch to use shell. Also add listing of a stream. there is also an example of creating a stream, this should be replaced as well. ",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19590,"the current streams chapter http static.springsource.org spring xd docs 1.0.0.M1 reference html streams shows creation and deleting streams using CURL switch to use shell. Also add listing of a stream. there is also an example of creating a stream, this should be replaced as well. ",NULL,NULL,NULL,"the current streams chapter http static<span class='highlight-text severity-high'>.springsource.org spring xd docs 1.0.0.M1 reference html streams shows creation and deleting streams using CURL switch to use shell. Also add listing of a stream. there is also an example of creating a stream, this should be replaced as well. </span>","minimal","punctuation","high",False
19593,"This section should discuss what is exposed via JMX, how you can view it in JConsole, and how you can view it over http via Jolikia. in particular showing how some existing metrics for inbound message channel adapters or the inbound channel of the stream, that indicate the number of messages processed per section. ",NULL,"This section should discuss what is exposed via JMX, how you can view it in JConsole, and how you can view it over http via Jolikia. in particular showing how some existing metrics for inbound message channel adapters or the inbound channel of the stream, that indicate the number of messages processed per section. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19593,"This section should discuss what is exposed via JMX, how you can view it in JConsole, and how you can view it over http via Jolikia. in particular showing how some existing metrics for inbound message channel adapters or the inbound channel of the stream, that indicate the number of messages processed per section. ",NULL,"This section should discuss what is exposed via JMX, how you can view it in JConsole, and how you can view it over http via Jolikia. in particular showing how some existing metrics for inbound message channel adapters or the inbound channel of the stream, that indicate the number of messages processed per section. ",NULL,"This section should discuss what is exposed via JMX, how you can view it in JConsole,<span class='highlight-text severity-high'> and </span>how you can view it over http via Jolikia. in particular showing how some existing metrics for inbound message channel adapters<span class='highlight-text severity-high'> or </span>the inbound channel of the stream, that indicate the number of messages processed per section. ","atomic","conjunctions","high",False
19593,"This section should discuss what is exposed via JMX, how you can view it in JConsole, and how you can view it over http via Jolikia. in particular showing how some existing metrics for inbound message channel adapters or the inbound channel of the stream, that indicate the number of messages processed per section. ",NULL,"This section should discuss what is exposed via JMX, how you can view it in JConsole, and how you can view it over http via Jolikia. in particular showing how some existing metrics for inbound message channel adapters or the inbound channel of the stream, that indicate the number of messages processed per section. ",NULL,"This section should discuss what is exposed via JMX, how you can view it in JConsole, and how you can view it over http via Jolikia<span class='highlight-text severity-high'>. in particular showing how some existing metrics for inbound message channel adapters or the inbound channel of the stream, that indicate the number of messages processed per section. </span>","minimal","punctuation","high",False
19619,"Trigger Add support for fixed delay interval",NULL,"Trigger Add support for fixed delay interval",NULL,"Add for who this story is","well_formed","no_role","high",False
19594,"This should likely be in the start the runtime section of Getting Started section.",NULL,"This should likely be in the start the runtime section of Getting Started section.",NULL,"Add for who this story is","well_formed","no_role","high",False
19622,"We should be able to write a script that can examine the table structure for a given HAWQ table and then extract the data from JSON without the custom script we are using now.",NULL,"We should be able to write a script that can examine the table structure for a given HAWQ table and then extract the data from JSON without the custom script we are using now.",NULL,"Add for who this story is","well_formed","no_role","high",False
19622,"We should be able to write a script that can examine the table structure for a given HAWQ table and then extract the data from JSON without the custom script we are using now.",NULL,"We should be able to write a script that can examine the table structure for a given HAWQ table and then extract the data from JSON without the custom script we are using now.",NULL,"We should be able to write a script that can examine the table structure for a given HAWQ table<span class='highlight-text severity-high'> and </span>then extract the data from JSON without the custom script we are using now.","atomic","conjunctions","high",False
19595,"As part of the Hadoop World demonstration work, the flow of data using XD from twitter to be analyzed by HAWQ as done. Part of this work had the data going into HDFS that HAWQ was able to query using external tables. The work for this story is to identify the concrete technical tasks stories to be created do deliver and document this functionality in XD. ","As part of the Hadoop World demonstrat","ion work, the flow of data using XD from twitter to be analyzed by HAWQ as done. Part of this work had the data going into HDFS that HAWQ was able to query using external tables. The work for this story is to identify the concrete technical tasks stories to be created do deliver and document this functionality in XD.",NULL,"As part of the Hadoop World demonstration work, the flow of data using XD from twitter to be analyzed by HAWQ as done<span class='highlight-text severity-high'>. Part of this work had the data going into HDFS that HAWQ was able to query using external tables. The work for this story is to identify the concrete technical tasks stories to be created do deliver and document this functionality in XD. </span>","minimal","punctuation","high",False
19595,"As part of the Hadoop World demonstration work, the flow of data using XD from twitter to be analyzed by HAWQ as done. Part of this work had the data going into HDFS that HAWQ was able to query using external tables. The work for this story is to identify the concrete technical tasks stories to be created do deliver and document this functionality in XD. ","As part of the Hadoop World demonstrat","ion work, the flow of data using XD from twitter to be analyzed by HAWQ as done. Part of this work had the data going into HDFS that HAWQ was able to query using external tables. The work for this story is to identify the concrete technical tasks stories to be created do deliver and document this functionality in XD.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19618,"Currently Jobs can be either executed using cron expression or immediately at once. We should also support the one time scheduling of jobs in the future. Would this possibly require us to implement schedule persistence? That could severely impact story points.",NULL,"Currently Jobs can be either executed using cron expression or immediately at once. We should also support the one time scheduling of jobs in the future. Would this possibly require us to implement schedule persistence? That could severely impact story points.",NULL,"Add for who this story is","well_formed","no_role","high",False
19618,"Currently Jobs can be either executed using cron expression or immediately at once. We should also support the one time scheduling of jobs in the future. Would this possibly require us to implement schedule persistence? That could severely impact story points.",NULL,"Currently Jobs can be either executed using cron expression or immediately at once. We should also support the one time scheduling of jobs in the future. Would this possibly require us to implement schedule persistence? That could severely impact story points.",NULL,"Currently Jobs can be either executed using cron expression or immediately at once<span class='highlight-text severity-high'>. We should also support the one time scheduling of jobs in the future. Would this possibly require us to implement schedule persistence? That could severely impact story points.</span>","minimal","punctuation","high",False
19621,"Document jmx command line options and refer to jolokia",NULL,"Document jmx command line options and refer to jolokia",NULL,"Add for who this story is","well_formed","no_role","high",False
19627,"For a sink source processor this would be among other things message rate, number of messages",NULL,"For a sink source processor this would be among other things message rate, number of messages",NULL,"Add for who this story is","well_formed","no_role","high",False
19623,"Investigate how efficiently we can integrate profiler into the performance test.",NULL,"Investigate how efficiently we can integrate profiler into the performance test.",NULL,"Add for who this story is","well_formed","no_role","high",False
19626,"TODO as part of this see XD 537 Get rid of so called Service layer in analytics project doesn t do much right now, and logic would better live in the Handler IMO Have REST controllers depend on XRepository in all cases",NULL,"TODO as part of this see XD 537 Get rid of","so called Service layer in analytics project doesn t do much right now, and logic would better live in the Handler IMO Have REST controllers depend on XRepository in all cases","Add for who this story is","well_formed","no_role","high",False
19626,"TODO as part of this see XD 537 Get rid of so called Service layer in analytics project doesn t do much right now, and logic would better live in the Handler IMO Have REST controllers depend on XRepository in all cases",NULL,"TODO as part of this see XD 537 Get rid of","so called Service layer in analytics project doesn t do much right now, and logic would better live in the Handler IMO Have REST controllers depend on XRepository in all cases","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19625,"updated story points to 14 since 5 of us just participated in a 2 hour call, and we still need to discuss topology support after some dev spikes later this week",NULL,"updated story points to 14 since 5 of us just participated in a 2 hour call, and we still need to discuss topology support after some dev spikes later this week",NULL,"Add for who this story is","well_formed","no_role","high",False
19625,"updated story points to 14 since 5 of us just participated in a 2 hour call, and we still need to discuss topology support after some dev spikes later this week",NULL,"updated story points to 14 since 5 of us just participated in a 2 hour call, and we still need to discuss topology support after some dev spikes later this week",NULL,"updated story points to 14 since 5 of us just participated in a 2 hour call,<span class='highlight-text severity-high'> and </span>we still need to discuss topology support after some dev spikes later this week","atomic","conjunctions","high",False
19628,"POST?",NULL,"POST?",NULL,"Add for who this story is","well_formed","no_role","high",False
19629,"This would be based off the spring integration extenstions splunk project. The use of this adapter for storing tweet data is in https github.com markpollack springone We should be able to reproduce the use case as done in that demo",NULL,"This would be based off the spring integration extenstions splunk project. The use of this adapter for storing tweet data is in https github.com markpollack springone We should be able to reproduce the use case as done in that demo",NULL,"Add for who this story is","well_formed","no_role","high",False
19629,"This would be based off the spring integration extenstions splunk project. The use of this adapter for storing tweet data is in https github.com markpollack springone We should be able to reproduce the use case as done in that demo",NULL,"This would be based off the spring integration extenstions splunk project. The use of this adapter for storing tweet data is in https github.com markpollack springone We should be able to reproduce the use case as done in that demo",NULL,"This would be based off the spring integration extenstions splunk project<span class='highlight-text severity-high'>. The use of this adapter for storing tweet data is in https github.com markpollack springone We should be able to reproduce the use case as done in that demo</span>","minimal","punctuation","high",False
19624,"Create a load generator script which can generate messages at specific 1 Rate 2 Payload 3 Concurrency to a specific tcp udp port where a syslog adapter is listening.",NULL,"Create a load generator script which can generate messages at specific 1 Rate 2 Payload 3 Concurrency to a specific tcp udp port where a syslog adapter is listening.",NULL,"Add for who this story is","well_formed","no_role","high",False
19591,"The chapter on how to start up the shell should ocme right after start the runtime and before create the stream ",NULL,"The chapter on how to start up the shell should ocme right after start the runtime and before create the stream ",NULL,"Add for who this story is","well_formed","no_role","high",False
19591,"The chapter on how to start up the shell should ocme right after start the runtime and before create the stream ",NULL,"The chapter on how to start up the shell should ocme right after start the runtime and before create the stream ",NULL,"The chapter on how to start up the shell should ocme right after start the runtime<span class='highlight-text severity-high'> and </span>before create the stream ","atomic","conjunctions","high",False
19592,"See http static.springsource.org spring xd docs 1.0.0.M1 reference html getting started ",NULL,"See http static.springsource.org spring xd docs 1.0.0.M1 reference html getting started ",NULL,"Add for who this story is","well_formed","no_role","high",False
19620,"The object naming is still not ideal for XD since SI conventions add some noise. Likely need to design and implement a custom naming strategy",NULL,"The object naming is still not ideal for XD since SI conventions add some noise. Likely need to design and implement a custom naming strategy",NULL,"Add for who this story is","well_formed","no_role","high",False
19620,"The object naming is still not ideal for XD since SI conventions add some noise. Likely need to design and implement a custom naming strategy",NULL,"The object naming is still not ideal for XD since SI conventions add some noise. Likely need to design and implement a custom naming strategy",NULL,"The object naming is still not ideal for XD since SI conventions add some noise<span class='highlight-text severity-high'>. Likely need to design and implement a custom naming strategy</span>","minimal","punctuation","high",False
19609,"This is currently too chatty. It should be possible to use a single connection for each increment operation.",NULL,"This is currently too chatty. It should be possible to use a single connection for each increment operation.",NULL,"Add for who this story is","well_formed","no_role","high",False
19609,"This is currently too chatty. It should be possible to use a single connection for each increment operation.",NULL,"This is currently too chatty. It should be possible to use a single connection for each increment operation.",NULL,"This is currently too chatty<span class='highlight-text severity-high'>. It should be possible to use a single connection for each increment operation.</span>","minimal","punctuation","high",False
19600,"see https docs.google.com a gopivotal.com drawings d 1kCNbVspRBjGc10itF9cSkwST8WgCjgg3wGzFKqLCAzU edit create TapsController if necessary",NULL,"see https docs.google.com a gopivotal.com drawings d 1kCNbVspRBjGc10itF9cSkwST8WgCjgg3wGzFKqLCAzU edit create TapsController if necessary",NULL,"Add for who this story is","well_formed","no_role","high",False
19604,"see https docs.google.com a gopivotal.com drawings d 1kCNbVspRBjGc10itF9cSkwST8WgCjgg3wGzFKqLCAzU edit Create the controller if it doesn t exist. Test with MvcTest",NULL,"see https docs.google.com a gopivotal.com drawings d 1kCNbVspRBjGc10itF9cSkwST8WgCjgg3wGzFKqLCAzU edit Create the controller if it doesn t exist. Test with MvcTest",NULL,"Add for who this story is","well_formed","no_role","high",False
19604,"see https docs.google.com a gopivotal.com drawings d 1kCNbVspRBjGc10itF9cSkwST8WgCjgg3wGzFKqLCAzU edit Create the controller if it doesn t exist. Test with MvcTest",NULL,"see https docs.google.com a gopivotal.com drawings d 1kCNbVspRBjGc10itF9cSkwST8WgCjgg3wGzFKqLCAzU edit Create the controller if it doesn t exist. Test with MvcTest",NULL,"see https docs<span class='highlight-text severity-high'>.google.com a gopivotal.com drawings d 1kCNbVspRBjGc10itF9cSkwST8WgCjgg3wGzFKqLCAzU edit Create the controller if it doesn t exist. Test with MvcTest</span>","minimal","punctuation","high",False
19602,"see https docs.google.com a gopivotal.com drawings d 1kCNbVspRBjGc10itF9cSkwST8WgCjgg3wGzFKqLCAzU edit create optionally deploys",NULL,"see https docs.google.com a gopivotal.com drawings d 1kCNbVspRBjGc10itF9cSkwST8WgCjgg3wGzFKqLCAzU edit create optionally deploys",NULL,"Add for who this story is","well_formed","no_role","high",False
19606,"Deploy an existing job. Must exist in the JobsRepository",NULL,"Deploy an existing job. Must exist in the JobsRepository",NULL,"Add for who this story is","well_formed","no_role","high",False
19606,"Deploy an existing job. Must exist in the JobsRepository",NULL,"Deploy an existing job. Must exist in the JobsRepository",NULL,"Deploy an existing job<span class='highlight-text severity-high'>. Must exist in the JobsRepository</span>","minimal","punctuation","high",False
19615,"It might be worth creating a base class for Plugins that combines common concerns across plugins. E.g. That would allow us to hide the commonApplicationContext and BeanDefinitionAddingPostProcessor for common cases, instead exposing a simple addBeanDefinition method to sub classes. ",NULL,"It might be worth creating a base class for Plugins that combines common concerns across plugins. E.g. That would allow us to hide the commonApplicationContext and BeanDefinitionAddingPostProcessor for common cases, instead exposing a simple addBeanDefinition method to sub classes. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19615,"It might be worth creating a base class for Plugins that combines common concerns across plugins. E.g. That would allow us to hide the commonApplicationContext and BeanDefinitionAddingPostProcessor for common cases, instead exposing a simple addBeanDefinition method to sub classes. ",NULL,"It might be worth creating a base class for Plugins that combines common concerns across plugins. E.g. That would allow us to hide the commonApplicationContext and BeanDefinitionAddingPostProcessor for common cases, instead exposing a simple addBeanDefinition method to sub classes. ",NULL,"It might be worth creating a base class for Plugins that combines common concerns across plugins. E.g. That would allow us to hide the commonApplicationContext<span class='highlight-text severity-high'> and </span>BeanDefinitionAddingPostProcessor for common cases, instead exposing a simple addBeanDefinition method to sub classes. ","atomic","conjunctions","high",False
19615,"It might be worth creating a base class for Plugins that combines common concerns across plugins. E.g. That would allow us to hide the commonApplicationContext and BeanDefinitionAddingPostProcessor for common cases, instead exposing a simple addBeanDefinition method to sub classes. ",NULL,"It might be worth creating a base class for Plugins that combines common concerns across plugins. E.g. That would allow us to hide the commonApplicationContext and BeanDefinitionAddingPostProcessor for common cases, instead exposing a simple addBeanDefinition method to sub classes. ",NULL,"It might be worth creating a base class for Plugins that combines common concerns across plugins<span class='highlight-text severity-high'>. E.g. That would allow us to hide the commonApplicationContext and BeanDefinitionAddingPostProcessor for common cases, instead exposing a simple addBeanDefinition method to sub classes. </span>","minimal","punctuation","high",False
19605,"To store it s definition and optionally deploy with autostart flag",NULL,"To store it s definition and optionally deploy with autostart flag",NULL,"Add for who this story is","well_formed","no_role","high",False
19605,"To store it s definition and optionally deploy with autostart flag",NULL,"To store it s definition and optionally deploy with autostart flag",NULL,"To store it s definition<span class='highlight-text severity-high'> and </span>optionally deploy with autostart flag","atomic","conjunctions","high",False
19610,"Get closure on open discussion points for REST API wrt to streams, taps and jobs. ",NULL,"Get closure on open discussion points for REST API wrt to streams, taps and jobs. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19607,"optional autostart switch to also deploy the job",NULL,"optional autostart switch to also deploy the job",NULL,"Add for who this story is","well_formed","no_role","high",False
19612,"Right now the PPC for jolokia mgmt is conflicting with the PPC used to resolve redis properties. Need to determine a strategy such that multiple PPCs can be used.",NULL,"Right now the PPC for jolokia mgmt is conflicting with the PPC used to resolve redis properties. Need to determine a strategy such that multiple PPCs can be used.",NULL,"Add for who this story is","well_formed","no_role","high",False
19612,"Right now the PPC for jolokia mgmt is conflicting with the PPC used to resolve redis properties. Need to determine a strategy such that multiple PPCs can be used.",NULL,"Right now the PPC for jolokia mgmt is conflicting with the PPC used to resolve redis properties. Need to determine a strategy such that multiple PPCs can be used.",NULL,"Right now the PPC for jolokia mgmt is conflicting with the PPC used to resolve redis properties<span class='highlight-text severity-high'>. Need to determine a strategy such that multiple PPCs can be used.</span>","minimal","punctuation","high",False
19617,"Need to understand how individual modules may or may not share Dispatchers that are part of the parent context. If modules have their own dispatchers, those also need to be configurable.",NULL,"Need to understand how individual modules may or may not share Dispatchers that are part of the parent context. If modules have their own dispatchers, those also need to be configurable.",NULL,"Add for who this story is","well_formed","no_role","high",False
19617,"Need to understand how individual modules may or may not share Dispatchers that are part of the parent context. If modules have their own dispatchers, those also need to be configurable.",NULL,"Need to understand how individual modules may or may not share Dispatchers that are part of the parent context. If modules have their own dispatchers, those also need to be configurable.",NULL,"Need to understand how individual modules may<span class='highlight-text severity-high'> or </span>may not share Dispatchers that are part of the parent context. If modules have their own dispatchers, those also need to be configurable.","atomic","conjunctions","high",False
19617,"Need to understand how individual modules may or may not share Dispatchers that are part of the parent context. If modules have their own dispatchers, those also need to be configurable.",NULL,"Need to understand how individual modules may or may not share Dispatchers that are part of the parent context. If modules have their own dispatchers, those also need to be configurable.",NULL,"Need to understand how individual modules may or may not share Dispatchers that are part of the parent context<span class='highlight-text severity-high'>. If modules have their own dispatchers, those also need to be configurable.</span>","minimal","punctuation","high",False
19611,"when posting the DSL to create a spring batch job e.g. trigger job.xml option1 foo it should be stored in redis so that a listing of XD job definitions can be retrieved.",NULL,"when posting the DSL to create a spring batch job e.g. trigger job.xml option1 foo it should be stored in redis","so that a listing of XD job definitions can be retrieved.","Add for who this story is","well_formed","no_role","high",False
19611,"when posting the DSL to create a spring batch job e.g. trigger job.xml option1 foo it should be stored in redis so that a listing of XD job definitions can be retrieved.",NULL,"when posting the DSL to create a spring batch job e.g. trigger job.xml option1 foo it should be stored in redis","so that a listing of XD job definitions can be retrieved.","when posting the DSL to create a spring batch job e<span class='highlight-text severity-high'>.g. trigger job.xml option1 foo it should be stored in redis so that a listing of XD job definitions can be retrieved.</span>","minimal","punctuation","high",False
19611,"when posting the DSL to create a spring batch job e.g. trigger job.xml option1 foo it should be stored in redis so that a listing of XD job definitions can be retrieved.",NULL,"when posting the DSL to create a spring batch job e.g. trigger job.xml option1 foo it should be stored in redis","so that a listing of XD job definitions can be retrieved.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19613,"Redis based.",NULL,"Redis based.",NULL,"Add for who this story is","well_formed","no_role","high",False
19614,"Simple cron based triggers",NULL,"Simple cron based triggers",NULL,"Add for who this story is","well_formed","no_role","high",False
19601,"see https docs.google.com a gopivotal.com drawings d 1kCNbVspRBjGc10itF9cSkwST8WgCjgg3wGzFKqLCAzU edit Refactor current DefaultStreamDeployer",NULL,"see https docs.google.com a gopivotal.com drawings d 1kCNbVspRBjGc10itF9cSkwST8WgCjgg3wGzFKqLCAzU edit Refactor current DefaultStreamDeployer",NULL,"Add for who this story is","well_formed","no_role","high",False
19597,"Have proper exceptions for common error cases on Stream creation deployment and propagate those to clients correctly.",NULL,"Have proper exceptions for common error cases on Stream creation deployment and propagate those to clients correctly.",NULL,"Add for who this story is","well_formed","no_role","high",False
19597,"Have proper exceptions for common error cases on Stream creation deployment and propagate those to clients correctly.",NULL,"Have proper exceptions for common error cases on Stream creation deployment and propagate those to clients correctly.",NULL,"Have proper exceptions for common error cases on Stream creation deployment<span class='highlight-text severity-high'> and </span>propagate those to clients correctly.","atomic","conjunctions","high",False
19603,"see https docs.google.com a gopivotal.com drawings d 1kCNbVspRBjGc10itF9cSkwST8WgCjgg3wGzFKqLCAzU edit Create the deployer if it doesn t exist.",NULL,"see https docs.google.com a gopivotal.com drawings d 1kCNbVspRBjGc10itF9cSkwST8WgCjgg3wGzFKqLCAzU edit Create the deployer if it doesn t exist.",NULL,"Add for who this story is","well_formed","no_role","high",False
19598,"see StreamsRepository as an example. This includes in memory and Redis implementations",NULL,"see StreamsRepository as an example. This includes in memory and Redis implementations",NULL,"Add for who this story is","well_formed","no_role","high",False
19598,"see StreamsRepository as an example. This includes in memory and Redis implementations",NULL,"see StreamsRepository as an example. This includes in memory and Redis implementations",NULL,"see StreamsRepository as an example<span class='highlight-text severity-high'>. This includes in memory and Redis implementations</span>","minimal","punctuation","high",False
19616,"startup scripts on windows should be tested, xd admin, xd container, xd shell.",NULL,"startup scripts on windows should be tested, xd admin, xd container, xd shell.",NULL,"Add for who this story is","well_formed","no_role","high",False
19599,"see https docs.google.com a gopivotal.com drawings d 1kCNbVspRBjGc10itF9cSkwST8WgCjgg3wGzFKqLCAzU edit",NULL,"see https docs.google.com a gopivotal.com drawings d 1kCNbVspRBjGc10itF9cSkwST8WgCjgg3wGzFKqLCAzU edit",NULL,"Add for who this story is","well_formed","no_role","high",False
19608,"Deploy a named stream. The stream must exist in the StreamRepository",NULL,"Deploy a named stream. The stream must exist in the StreamRepository",NULL,"Add for who this story is","well_formed","no_role","high",False
19608,"Deploy a named stream. The stream must exist in the StreamRepository",NULL,"Deploy a named stream. The stream must exist in the StreamRepository",NULL,"Deploy a named stream<span class='highlight-text severity-high'>. The stream must exist in the StreamRepository</span>","minimal","punctuation","high",False
19631,"The shell should be an executable delivered out of the box in much the same way that xd container and xd admin are right now. If we follow how redis mongo distribut the shell, it sits side by side with the other binaries",NULL,"The shell should be an executable delivered out of the box in much the same way that xd container and xd admin are right now. If we follow how redis mongo distribut the shell, it sits side by side with the other binaries",NULL,"Add for who this story is","well_formed","no_role","high",False
19631,"The shell should be an executable delivered out of the box in much the same way that xd container and xd admin are right now. If we follow how redis mongo distribut the shell, it sits side by side with the other binaries",NULL,"The shell should be an executable delivered out of the box in much the same way that xd container and xd admin are right now. If we follow how redis mongo distribut the shell, it sits side by side with the other binaries",NULL,"The shell should be an executable delivered out of the box in much the same way that xd container<span class='highlight-text severity-high'> and </span>xd admin are right now. If we follow how redis mongo distribut the shell, it sits side by side with the other binaries","atomic","conjunctions","high",False
19631,"The shell should be an executable delivered out of the box in much the same way that xd container and xd admin are right now. If we follow how redis mongo distribut the shell, it sits side by side with the other binaries",NULL,"The shell should be an executable delivered out of the box in much the same way that xd container and xd admin are right now. If we follow how redis mongo distribut the shell, it sits side by side with the other binaries",NULL,"The shell should be an executable delivered out of the box in much the same way that xd container and xd admin are right now<span class='highlight-text severity-high'>. If we follow how redis mongo distribut the shell, it sits side by side with the other binaries</span>","minimal","punctuation","high",False
19632,"Expose runtime stats for core components.",NULL,"Expose runtime stats for core components.",NULL,"Add for who this story is","well_formed","no_role","high",False
19634,"WAR Vs. JVM Jolokia Agent Jolokia Vs. JVM MBeanServer Probably needs support for Spring Profiles.",NULL,"WAR Vs. JVM Jolokia Agent Jolokia Vs. JVM MBeanServer Probably needs support for Spring Profiles.",NULL,"Add for who this story is","well_formed","no_role","high",False
19634,"WAR Vs. JVM Jolokia Agent Jolokia Vs. JVM MBeanServer Probably needs support for Spring Profiles.",NULL,"WAR Vs. JVM Jolokia Agent Jolokia Vs. JVM MBeanServer Probably needs support for Spring Profiles.",NULL,"WAR Vs<span class='highlight-text severity-high'>. JVM Jolokia Agent Jolokia Vs. JVM MBeanServer Probably needs support for Spring Profiles.</span>","minimal","punctuation","high",False
19639,"Be able to point to the processor xml file, e.g. modules processors transformer.xml, and have access to a source channel that drives messages into the processor and a output channel where output messages are send. The outbound channel is queue backed. Test sending JSON to a processor module that uses Tuples. ",NULL,"Be able to point to the processor xml file, e.g. modules processors transformer.xml, and have access to a source channel that drives messages into the processor and a output channel where output messages are send. The outbound channel is queue backed. Test sending JSON to a processor module that uses Tuples. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19679,"curl X DELETE http localhost 8080 streams ticktock",NULL,"curl X DELETE http localhost 8080 streams ticktock",NULL,"Add for who this story is","well_formed","no_role","high",False
19681,"The transform, filter, and script processor modules support passing in a properties location for script variables. We need a default location on the classpath for users to provide custom properties files.",NULL,"The transform, filter, and script processor modules support passing in a properties location for script variables. We need a default location on the classpath for users to provide custom properties files.",NULL,"Add for who this story is","well_formed","no_role","high",False
19801,"Base integration of core HDFS writer functionality with Spring Batch.",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19639,"Be able to point to the processor xml file, e.g. modules processors transformer.xml, and have access to a source channel that drives messages into the processor and a output channel where output messages are send. The outbound channel is queue backed. Test sending JSON to a processor module that uses Tuples. ",NULL,"Be able to point to the processor xml file, e.g. modules processors transformer.xml, and have access to a source channel that drives messages into the processor and a output channel where output messages are send. The outbound channel is queue backed. Test sending JSON to a processor module that uses Tuples. ",NULL,"Be able to point to the processor xml file, e.g. modules processors transformer.xml,<span class='highlight-text severity-high'> and </span>have access to a source channel that drives messages into the processor and a output channel where output messages are send. The outbound channel is queue backed. Test sending JSON to a processor module that uses Tuples. ","atomic","conjunctions","high",False
19639,"Be able to point to the processor xml file, e.g. modules processors transformer.xml, and have access to a source channel that drives messages into the processor and a output channel where output messages are send. The outbound channel is queue backed. Test sending JSON to a processor module that uses Tuples. ",NULL,"Be able to point to the processor xml file, e.g. modules processors transformer.xml, and have access to a source channel that drives messages into the processor and a output channel where output messages are send. The outbound channel is queue backed. Test sending JSON to a processor module that uses Tuples. ",NULL,"Be able to point to the processor xml file, e<span class='highlight-text severity-high'>.g. modules processors transformer.xml, and have access to a source channel that drives messages into the processor and a output channel where output messages are send. The outbound channel is queue backed. Test sending JSON to a processor module that uses Tuples. </span>","minimal","punctuation","high",False
19636,"See XD 194 for additional considerations. Zip support should be similar to uber jar, or possibly replace uber jar support.",NULL,"See XD 194 for additional considerations. Zip support should be similar to uber jar, or possibly replace uber jar support.",NULL,"Add for who this story is","well_formed","no_role","high",False
19636,"See XD 194 for additional considerations. Zip support should be similar to uber jar, or possibly replace uber jar support.",NULL,"See XD 194 for additional considerations. Zip support should be similar to uber jar, or possibly replace uber jar support.",NULL,"See XD 194 for additional considerations. Zip support should be similar to uber jar,<span class='highlight-text severity-high'> or </span>possibly replace uber jar support.","atomic","conjunctions","high",False
19636,"See XD 194 for additional considerations. Zip support should be similar to uber jar, or possibly replace uber jar support.",NULL,"See XD 194 for additional considerations. Zip support should be similar to uber jar, or possibly replace uber jar support.",NULL,"See XD 194 for additional considerations<span class='highlight-text severity-high'>. Zip support should be similar to uber jar, or possibly replace uber jar support.</span>","minimal","punctuation","high",False
19643,"Twitter s streaming APIs have more capabilities than just the plain statuses sample.json. In particular we should support the filter.json option and the use of track https dev.twitter.com docs streaming apis parameters track as well as other request parameters delimited, language etc .",NULL,"Twitter s streaming APIs have more capabilities than just the plain statuses sample.json. In particular we should support the filter.json option and the use of track https dev.twitter.com docs streaming apis parameters track as well as other request parameters delimited, language etc .",NULL,"Add for who this story is","well_formed","no_role","high",False
19643,"Twitter s streaming APIs have more capabilities than just the plain statuses sample.json. In particular we should support the filter.json option and the use of track https dev.twitter.com docs streaming apis parameters track as well as other request parameters delimited, language etc .",NULL,"Twitter s streaming APIs have more capabilities than just the plain statuses sample.json. In particular we should support the filter.json option and the use of track https dev.twitter.com docs streaming apis parameters track as well as other request parameters delimited, language etc .",NULL,"Twitter s streaming APIs have more capabilities than just the plain statuses sample.json. In particular we should support the filter.json option<span class='highlight-text severity-high'> and </span>the use of track https dev.twitter.com docs streaming apis parameters track as well as other request parameters delimited, language etc .","atomic","conjunctions","high",False
19643,"Twitter s streaming APIs have more capabilities than just the plain statuses sample.json. In particular we should support the filter.json option and the use of track https dev.twitter.com docs streaming apis parameters track as well as other request parameters delimited, language etc .",NULL,"Twitter s streaming APIs have more capabilities than just the plain statuses sample.json. In particular we should support the filter.json option and the use of track https dev.twitter.com docs streaming apis parameters track as well as other request parameters delimited, language etc .",NULL,"Twitter s streaming APIs have more capabilities than just the plain statuses sample<span class='highlight-text severity-high'>.json. In particular we should support the filter.json option and the use of track https dev.twitter.com docs streaming apis parameters track as well as other request parameters delimited, language etc .</span>","minimal","punctuation","high",False
19640,"So that we can validate the message content in the stream",NULL,"So that we can validate the message content in the stream",NULL,"Add for who this story is","well_formed","no_role","high",False
19642,"It would be nice if we have a git repo for Spring XD performance testing. This would enable us to have a common repository rather than inside spring xd as a subproject for all performance related code specific to any module, message middleware etc., ",NULL,"It would be nice if we have a git repo for Spring XD performance testing. This would enable us to have a common repository rather than inside spring xd as a subproject for all performance related code specific to any module, message middleware etc., ",NULL,"Add for who this story is","well_formed","no_role","high",False
19642,"It would be nice if we have a git repo for Spring XD performance testing. This would enable us to have a common repository rather than inside spring xd as a subproject for all performance related code specific to any module, message middleware etc., ",NULL,"It would be nice if we have a git repo for Spring XD performance testing. This would enable us to have a common repository rather than inside spring xd as a subproject for all performance related code specific to any module, message middleware etc., ",NULL,"It would be nice if we have a git repo for Spring XD performance testing<span class='highlight-text severity-high'>. This would enable us to have a common repository rather than inside spring xd as a subproject for all performance related code specific to any module, message middleware etc., </span>","minimal","punctuation","high",False
19644,"I wanted to have a rollover feature when I was streaming tweets to a file overnight, just to avoid dealing with a single enormous file in case I collected more data than my demo could handle and needed to split it up .",NULL,"I wanted to have a rollover feature when I was streaming tweets to a file overnight, just to avoid dealing with a single enormous file in case I collected more data than my demo could handle and needed to split it up .",NULL,"Add for who this story is","well_formed","no_role","high",False
19644,"I wanted to have a rollover feature when I was streaming tweets to a file overnight, just to avoid dealing with a single enormous file in case I collected more data than my demo could handle and needed to split it up .",NULL,"I wanted to have a rollover feature when I was streaming tweets to a file overnight, just to avoid dealing with a single enormous file in case I collected more data than my demo could handle and needed to split it up .",NULL,"I wanted to have a rollover feature when I was streaming tweets to a file overnight, just to avoid dealing with a single enormous file in case I collected more data than my demo could handle<span class='highlight-text severity-high'> and </span>needed to split it up .","atomic","conjunctions","high",False
19645,"The changes for XD 144 mean that log4j files are no longer in the library jars. The admin server already has a logging configuration which should be activated by the startup scripts, but the separate gemfire app doesn t.",NULL,"The changes for XD 144 mean that log4j files are no longer in the library jars. The admin server already has a logging configuration which should be activated by the startup scripts, but the separate gemfire app doesn t.",NULL,"Add for who this story is","well_formed","no_role","high",False
19645,"The changes for XD 144 mean that log4j files are no longer in the library jars. The admin server already has a logging configuration which should be activated by the startup scripts, but the separate gemfire app doesn t.",NULL,"The changes for XD 144 mean that log4j files are no longer in the library jars. The admin server already has a logging configuration which should be activated by the startup scripts, but the separate gemfire app doesn t.",NULL,"The changes for XD 144 mean that log4j files are no longer in the library jars<span class='highlight-text severity-high'>. The admin server already has a logging configuration which should be activated by the startup scripts, but the separate gemfire app doesn t.</span>","minimal","punctuation","high",False
19660,"So that clients e.g. Shell or custom user program are insulated from REST details ala Cloud Foundry . May go even further if we want a Java DSL for stream definitions that may reuse Batch command POJOs btw Difference between xdClient.createStream mystream , http port 9000 file and import static stuff. ; StreamDef stream http .port 9000 .pipe file ; xdClient.createStream mystream , stream ;",NULL,"So that clients e.g. Shell or custom user program are insulated from REST details ala Cloud Foundry . May go even further if we want a Java DSL for stream definitions that may reuse Batch command POJOs btw Difference between xdClient.createStream mystream , http port 9000 file and import static stuff. ; StreamDef stream http .port 9000 .pipe file ; xdClient.createStream mystream , stream ;",NULL,"Add for who this story is","well_formed","no_role","high",False
19660,"So that clients e.g. Shell or custom user program are insulated from REST details ala Cloud Foundry . May go even further if we want a Java DSL for stream definitions that may reuse Batch command POJOs btw Difference between xdClient.createStream mystream , http port 9000 file and import static stuff. ; StreamDef stream http .port 9000 .pipe file ; xdClient.createStream mystream , stream ;",NULL,"So that clients e.g. Shell or custom user program are insulated from REST details ala Cloud Foundry . May go even further if we want a Java DSL for stream definitions that may reuse Batch command POJOs btw Difference between xdClient.createStream mystream , http port 9000 file and import static stuff. ; StreamDef stream http .port 9000 .pipe file ; xdClient.createStream mystream , stream ;",NULL,"So that clients e.g. Shell<span class='highlight-text severity-high'> or </span>custom user program are insulated from REST details ala Cloud Foundry . May go even further if we want a Java DSL for stream definitions that may reuse Batch command POJOs btw Difference between xdClient.createStream mystream , http port 9000 file<span class='highlight-text severity-high'> and </span>import static stuff. ; StreamDef stream http .port 9000 .pipe file ; xdClient.createStream mystream , stream ;","atomic","conjunctions","high",False
19660,"So that clients e.g. Shell or custom user program are insulated from REST details ala Cloud Foundry . May go even further if we want a Java DSL for stream definitions that may reuse Batch command POJOs btw Difference between xdClient.createStream mystream , http port 9000 file and import static stuff. ; StreamDef stream http .port 9000 .pipe file ; xdClient.createStream mystream , stream ;",NULL,"So that clients e.g. Shell or custom user program are insulated from REST details ala Cloud Foundry . May go even further if we want a Java DSL for stream definitions that may reuse Batch command POJOs btw Difference between xdClient.createStream mystream , http port 9000 file and import static stuff. ; StreamDef stream http .port 9000 .pipe file ; xdClient.createStream mystream , stream ;",NULL,"So that clients e<span class='highlight-text severity-high'>.g. Shell or custom user program are insulated from REST details ala Cloud Foundry . May go even further if we want a Java DSL for stream definitions that may reuse Batch command POJOs btw Difference between xdClient.createStream mystream , http port 9000 file and import static stuff. ; StreamDef stream http .port 9000 .pipe file ; xdClient.createStream mystream , stream ;</span>","minimal","punctuation","high",False
19637,"Examples 1. Be able to start the rabbitmq source just by pointing to modules source rabbit.xml, pass in some property file for parameters to be replaced, and outgoing message is placed in a in memory queue backed channel for use with assertions to verify functionality. 2. Test for as many source types as is reasonable , e.g. MQTT TCP testing might be harder than say rabbitmq. 3. Test that sending json, results in media type header is set to json 4. Test that sending POJO, POJO 5. Test that sending Tuple, Tuple 6. Test that sending raw bytes, raw bytes ",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19637,"Examples 1. Be able to start the rabbitmq source just by pointing to modules source rabbit.xml, pass in some property file for parameters to be replaced, and outgoing message is placed in a in memory queue backed channel for use with assertions to verify functionality. 2. Test for as many source types as is reasonable , e.g. MQTT TCP testing might be harder than say rabbitmq. 3. Test that sending json, results in media type header is set to json 4. Test that sending POJO, POJO 5. Test that sending Tuple, Tuple 6. Test that sending raw bytes, raw bytes ",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19637,"Examples 1. Be able to start the rabbitmq source just by pointing to modules source rabbit.xml, pass in some property file for parameters to be replaced, and outgoing message is placed in a in memory queue backed channel for use with assertions to verify functionality. 2. Test for as many source types as is reasonable , e.g. MQTT TCP testing might be harder than say rabbitmq. 3. Test that sending json, results in media type header is set to json 4. Test that sending POJO, POJO 5. Test that sending Tuple, Tuple 6. Test that sending raw bytes, raw bytes ",NULL,NULL,NULL,"Examples 1<span class='highlight-text severity-high'>. Be able to start the rabbitmq source just by pointing to modules source rabbit.xml, pass in some property file for parameters to be replaced, and outgoing message is placed in a in memory queue backed channel for use with assertions to verify functionality. 2. Test for as many source types as is reasonable , e.g. MQTT TCP testing might be harder than say rabbitmq. 3. Test that sending json, results in media type header is set to json 4. Test that sending POJO, POJO 5. Test that sending Tuple, Tuple 6. Test that sending raw bytes, raw bytes </span>","minimal","punctuation","high",False
19630,"Need to capture exceptions from the various projects that make up XD and wrap them in XD Specific exceptions. An example of this is when leaving out the channels in the module definitions, we see NoSuchBeanExceptions and IllegalArgumentExceptions thrown based on which module and what channel is missing. ",NULL,"Need to capture exceptions from the various projects that make up XD and wrap them in XD Specific exceptions. An example of this is when leaving out the channels in the module definitions, we see NoSuchBeanExceptions and IllegalArgumentExceptions thrown based on which module and what channel is missing. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19630,"Need to capture exceptions from the various projects that make up XD and wrap them in XD Specific exceptions. An example of this is when leaving out the channels in the module definitions, we see NoSuchBeanExceptions and IllegalArgumentExceptions thrown based on which module and what channel is missing. ",NULL,"Need to capture exceptions from the various projects that make up XD and wrap them in XD Specific exceptions. An example of this is when leaving out the channels in the module definitions, we see NoSuchBeanExceptions and IllegalArgumentExceptions thrown based on which module and what channel is missing. ",NULL,"Need to capture exceptions from the various projects that make up XD<span class='highlight-text severity-high'> and </span>wrap them in XD Specific exceptions. An example of this is when leaving out the channels in the module definitions, we see NoSuchBeanExceptions and IllegalArgumentExceptions thrown based on which module and what channel is missing. ","atomic","conjunctions","high",False
19630,"Need to capture exceptions from the various projects that make up XD and wrap them in XD Specific exceptions. An example of this is when leaving out the channels in the module definitions, we see NoSuchBeanExceptions and IllegalArgumentExceptions thrown based on which module and what channel is missing. ",NULL,"Need to capture exceptions from the various projects that make up XD and wrap them in XD Specific exceptions. An example of this is when leaving out the channels in the module definitions, we see NoSuchBeanExceptions and IllegalArgumentExceptions thrown based on which module and what channel is missing. ",NULL,"Need to capture exceptions from the various projects that make up XD and wrap them in XD Specific exceptions<span class='highlight-text severity-high'>. An example of this is when leaving out the channels in the module definitions, we see NoSuchBeanExceptions and IllegalArgumentExceptions thrown based on which module and what channel is missing. </span>","minimal","punctuation","high",False
19633,"Global option? Override for individual modules? module types?",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19633,"Global option? Override for individual modules? module types?",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19633,"Global option? Override for individual modules? module types?",NULL,NULL,NULL,"Global option<span class='highlight-text severity-high'>? Override for individual modules? module types?</span>","minimal","punctuation","high",False
19638,"Handled by 1245",NULL,"Handled by 1245",NULL,"Add for who this story is","well_formed","no_role","high",False
19635,"Probably needs support for Spring Profiles.",NULL,"Probably needs support for Spring Profiles.",NULL,"Add for who this story is","well_formed","no_role","high",False
19641,"To send a pre set message to process es ",NULL,"To send a pre set message to process es ",NULL,"Add for who this story is","well_formed","no_role","high",False
19650,"This could be an optimization, to be verified, that delegating the writing operations to Reactor e.g. with a backing ringbuffer implementation will increase the throughput performance. Other strategies, such as threads to handle writes to individual files concurrently, should be investigated.",NULL,"This could be an optimization, to be verified, that delegating the writing operations to Reactor e.g. with a backing ringbuffer implementation will increase the throughput performance. Other strategies, such as threads to handle writes to individual files concurrently, should be investigated.",NULL,"Add for who this story is","well_formed","no_role","high",False
19750,"It should provide an expression param for SpEL and have a default value of true accept everything .",NULL,"It should provide an expression param for SpEL and have a default value of true accept everything .",NULL,"Add for who this story is","well_formed","no_role","high",False
19650,"This could be an optimization, to be verified, that delegating the writing operations to Reactor e.g. with a backing ringbuffer implementation will increase the throughput performance. Other strategies, such as threads to handle writes to individual files concurrently, should be investigated.",NULL,"This could be an optimization, to be verified, that delegating the writing operations to Reactor e.g. with a backing ringbuffer implementation will increase the throughput performance. Other strategies, such as threads to handle writes to individual files concurrently, should be investigated.",NULL,"This could be an optimization, to be verified, that delegating the writing operations to Reactor e<span class='highlight-text severity-high'>.g. with a backing ringbuffer implementation will increase the throughput performance. Other strategies, such as threads to handle writes to individual files concurrently, should be investigated.</span>","minimal","punctuation","high",False
19649,"Writing POJOs using CDK Data Avro We should support both partitioned and un partitioned. This story addresses only un partitioned. Document limitations in terms of which Java types are supported and not supported by the Avro serialization ",NULL,"Writing POJOs using CDK Data Avro We should support both partitioned and un partitioned. This story addresses only un partitioned. Document limitations in terms of which Java types are supported and not supported by the Avro serialization ",NULL,"Add for who this story is","well_formed","no_role","high",False
19649,"Writing POJOs using CDK Data Avro We should support both partitioned and un partitioned. This story addresses only un partitioned. Document limitations in terms of which Java types are supported and not supported by the Avro serialization ",NULL,"Writing POJOs using CDK Data Avro We should support both partitioned and un partitioned. This story addresses only un partitioned. Document limitations in terms of which Java types are supported and not supported by the Avro serialization ",NULL,"Writing POJOs using CDK Data Avro We should support both partitioned<span class='highlight-text severity-high'> and </span>un partitioned. This story addresses only un partitioned. Document limitations in terms of which Java types are supported and not supported by the Avro serialization ","atomic","conjunctions","high",False
19649,"Writing POJOs using CDK Data Avro We should support both partitioned and un partitioned. This story addresses only un partitioned. Document limitations in terms of which Java types are supported and not supported by the Avro serialization ",NULL,"Writing POJOs using CDK Data Avro We should support both partitioned and un partitioned. This story addresses only un partitioned. Document limitations in terms of which Java types are supported and not supported by the Avro serialization ",NULL,"Writing POJOs using CDK Data Avro We should support both partitioned and un partitioned<span class='highlight-text severity-high'>. This story addresses only un partitioned. Document limitations in terms of which Java types are supported and not supported by the Avro serialization </span>","minimal","punctuation","high",False
19651,"This should be an optimization, to be verified, that aggregating data in memory, for example at the size of a HDFS block 64Mb often will result in increased performance vs. not aggregating data for writes.",NULL,"This should be an optimization, to be verified, that aggregating data in memory, for example at the size of a HDFS block 64Mb often will result in increased performance vs. not aggregating data for writes.",NULL,"Add for who this story is","well_formed","no_role","high",False
19651,"This should be an optimization, to be verified, that aggregating data in memory, for example at the size of a HDFS block 64Mb often will result in increased performance vs. not aggregating data for writes.",NULL,"This should be an optimization, to be verified, that aggregating data in memory, for example at the size of a HDFS block 64Mb often will result in increased performance vs. not aggregating data for writes.",NULL,"This should be an optimization, to be verified, that aggregating data in memory, for example at the size of a HDFS block 64Mb often will result in increased performance vs<span class='highlight-text severity-high'>. not aggregating data for writes.</span>","minimal","punctuation","high",False
19655,"A strategy to roll over files that allows the user to choose between 1 the size of the file 2 the number of events items in the file 3 an idle timeout value that if exceeded, will close the file",NULL,"A strategy to roll over files that allows the user to choose between 1 the size of the file 2 the number of events items in the file 3 an idle timeout value that if exceeded, will close the file",NULL,"Add for who this story is","well_formed","no_role","high",False
19652,"Based on message processing, a header in a Message can be added that contains the output file name. This will work together with the hdfs writer module so it can read the header and write the contents of the message to the specified file. ",NULL,"Based on message processing, a header in a Message can be added that contains the output file name. This will work together with the hdfs writer module","so it can read the header and write the contents of the message to the specified file.","Add for who this story is","well_formed","no_role","high",False
19652,"Based on message processing, a header in a Message can be added that contains the output file name. This will work together with the hdfs writer module so it can read the header and write the contents of the message to the specified file. ",NULL,"Based on message processing, a header in a Message can be added that contains the output file name. This will work together with the hdfs writer module","so it can read the header and write the contents of the message to the specified file.","Based on message processing, a header in a Message can be added that contains the output file name<span class='highlight-text severity-high'>. This will work together with the hdfs writer module so it can read the header and write the contents of the message to the specified file. </span>","minimal","punctuation","high",False
19652,"Based on message processing, a header in a Message can be added that contains the output file name. This will work together with the hdfs writer module so it can read the header and write the contents of the message to the specified file. ",NULL,"Based on message processing, a header in a Message can be added that contains the output file name. This will work together with the hdfs writer module","so it can read the header and write the contents of the message to the specified file.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19654,"A strategy that will automaticaly roll over files based time of day. For example New files will be created every hour, or every 6 hours etc. The directory for files can also be rotated so that directory structures such as data year month day can easily be supported with a minimum of configuration. ",NULL,"A strategy that will automaticaly roll over files based time of day. For example New files will be created every hour, or every 6 hours etc. The directory for files can also be rotated","so that directory structures such as data year month day can easily be supported with a minimum of configuration.","Add for who this story is","well_formed","no_role","high",False
19654,"A strategy that will automaticaly roll over files based time of day. For example New files will be created every hour, or every 6 hours etc. The directory for files can also be rotated so that directory structures such as data year month day can easily be supported with a minimum of configuration. ",NULL,"A strategy that will automaticaly roll over files based time of day. For example New files will be created every hour, or every 6 hours etc. The directory for files can also be rotated","so that directory structures such as data year month day can easily be supported with a minimum of configuration.","A strategy that will automaticaly roll over files based time of day<span class='highlight-text severity-high'>. For example New files will be created every hour, or every 6 hours etc. The directory for files can also be rotated so that directory structures such as data year month day can easily be supported with a minimum of configuration. </span>","minimal","punctuation","high",False
19654,"A strategy that will automaticaly roll over files based time of day. For example New files will be created every hour, or every 6 hours etc. The directory for files can also be rotated so that directory structures such as data year month day can easily be supported with a minimum of configuration. ",NULL,"A strategy that will automaticaly roll over files based time of day. For example New files will be created every hour, or every 6 hours etc. The directory for files can also be rotated","so that directory structures such as data year month day can easily be supported with a minimum of configuration.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19653,"The file name should allow the use of date and time patterns, either JDK or Joda TBD .",NULL,"The file name should allow the use of date and time patterns, either JDK or Joda TBD .",NULL,"Add for who this story is","well_formed","no_role","high",False
19653,"The file name should allow the use of date and time patterns, either JDK or Joda TBD .",NULL,"The file name should allow the use of date and time patterns, either JDK or Joda TBD .",NULL,"The file name should allow the use of date<span class='highlight-text severity-high'> and </span>time patterns, either JDK<span class='highlight-text severity-high'> or </span>Joda TBD .","atomic","conjunctions","high",False
19656,"A file that is in the process of being written to should have a customized suffix added to the name, e.g. temp . Once the file is closed, the suffix is removed and replaced with another value default value can be dependent on the serialization format used, but can be customized",NULL,"A file that is in the process of being written to should have a customized suffix added to the name, e.g. temp . Once the file is closed, the suffix is removed and replaced with another value default value can be dependent on the serialization format used, but can be customized",NULL,"Add for who this story is","well_formed","no_role","high",False
19656,"A file that is in the process of being written to should have a customized suffix added to the name, e.g. temp . Once the file is closed, the suffix is removed and replaced with another value default value can be dependent on the serialization format used, but can be customized",NULL,"A file that is in the process of being written to should have a customized suffix added to the name, e.g. temp . Once the file is closed, the suffix is removed and replaced with another value default value can be dependent on the serialization format used, but can be customized",NULL,"A file that is in the process of being written to should have a customized suffix added to the name, e.g. temp . Once the file is closed, the suffix is removed<span class='highlight-text severity-high'> and </span>replaced with another value default value can be dependent on the serialization format used, but can be customized","atomic","conjunctions","high",False
19656,"A file that is in the process of being written to should have a customized suffix added to the name, e.g. temp . Once the file is closed, the suffix is removed and replaced with another value default value can be dependent on the serialization format used, but can be customized",NULL,"A file that is in the process of being written to should have a customized suffix added to the name, e.g. temp . Once the file is closed, the suffix is removed and replaced with another value default value can be dependent on the serialization format used, but can be customized",NULL,"A file that is in the process of being written to should have a customized suffix added to the name, e<span class='highlight-text severity-high'>.g. temp . Once the file is closed, the suffix is removed and replaced with another value default value can be dependent on the serialization format used, but can be customized</span>","minimal","punctuation","high",False
19663,"This is the basic setup of the commands file no specific command implementations",NULL,"This is the basic setup of the commands file no specific command implementations",NULL,"Add for who this story is","well_formed","no_role","high",False
19658,"Command line arguments and especially their default values are currently scattered around different places. The aim is to regroup those in a common place Options classes make sense . Also, not very happy with how System properties are used as a vehicle for options.transport options.home",NULL,"Command line arguments and especially their default values are currently scattered around different places. The aim is to regroup those in a common place Options classes make sense . Also, not very happy with how System properties are used as a vehicle for options.transport options.home",NULL,"Add for who this story is","well_formed","no_role","high",False
19658,"Command line arguments and especially their default values are currently scattered around different places. The aim is to regroup those in a common place Options classes make sense . Also, not very happy with how System properties are used as a vehicle for options.transport options.home",NULL,"Command line arguments and especially their default values are currently scattered around different places. The aim is to regroup those in a common place Options classes make sense . Also, not very happy with how System properties are used as a vehicle for options.transport options.home",NULL,"Command line arguments and especially their default values are currently scattered around different places<span class='highlight-text severity-high'>. The aim is to regroup those in a common place Options classes make sense . Also, not very happy with how System properties are used as a vehicle for options.transport options.home</span>","minimal","punctuation","high",False
19659,"As an XD developer, I need to be able to use a batch job to stream data as a source. ","As an XD developer",", I need to be able to use a batch job to stream data as a source.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19661,"GET streams streamname modules and GET streams streamname modules modulename The former returning links to the latter ",NULL,"GET streams streamname modules and GET streams streamname modules modulename The former returning links to the latter ",NULL,"Add for who this story is","well_formed","no_role","high",False
19661,"GET streams streamname modules and GET streams streamname modules modulename The former returning links to the latter ",NULL,"GET streams streamname modules and GET streams streamname modules modulename The former returning links to the latter ",NULL,"GET streams streamname modules<span class='highlight-text severity-high'> and </span>GET streams streamname modules modulename The former returning links to the latter ","atomic","conjunctions","high",False
19662,"Pagination support, maybe querying by name as well",NULL,"Pagination support, maybe querying by name as well",NULL,"Add for who this story is","well_formed","no_role","high",False
19664,"Set up a basic Spring Shell project for XD Shell",NULL,"Set up a basic Spring Shell project for XD Shell",NULL,"Add for who this story is","well_formed","no_role","high",False
19657,"As XD, I need a persistent way to register job definitions beyond the map registry implementation provided by Spring Batch . ","As XD",", I need a persistent way to register job definitions beyond the map registry implementation provided by Spring Batch .",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19647,"See https github.com kevinweil elephant bird",NULL,"See https github.com kevinweil elephant bird",NULL,"Add for who this story is","well_formed","no_role","high",False
19648,"the key used in writing key value pairs should be able to be specified declaratively.",NULL,"the key used in writing key value pairs should be able to be specified declaratively.",NULL,"Add for who this story is","well_formed","no_role","high",False
19675,"This is needed for the use of the webhdfs scheme to talk to HDFS over http.",NULL,"This is needed for the use of the webhdfs scheme to talk to HDFS over http.",NULL,"Add for who this story is","well_formed","no_role","high",False
19676,"We need to cleanup some of the duplicate gradle tasks that bundle spring xd distributions. Currently, distXD does the copy of distributions from spring xd dirt , redis and spring xd gemfire server projects into rootDir dist spring xd . And, the task zipXD makes the zip archive. These tasks should be combined with the distZip docZip tasks. We also need to remove the duplicate artifacts configuration from these tasks.",NULL,"We need to cleanup some of the duplicate gradle tasks that bundle spring xd distributions. Currently, distXD does the copy of distributions from spring xd dirt , redis and spring xd gemfire server projects into rootDir dist spring xd . And, the task zipXD makes the zip archive. These tasks should be combined with the distZip docZip tasks. We also need to remove the duplicate artifacts configuration from these tasks.",NULL,"Add for who this story is","well_formed","no_role","high",False
19676,"We need to cleanup some of the duplicate gradle tasks that bundle spring xd distributions. Currently, distXD does the copy of distributions from spring xd dirt , redis and spring xd gemfire server projects into rootDir dist spring xd . And, the task zipXD makes the zip archive. These tasks should be combined with the distZip docZip tasks. We also need to remove the duplicate artifacts configuration from these tasks.",NULL,"We need to cleanup some of the duplicate gradle tasks that bundle spring xd distributions. Currently, distXD does the copy of distributions from spring xd dirt , redis and spring xd gemfire server projects into rootDir dist spring xd . And, the task zipXD makes the zip archive. These tasks should be combined with the distZip docZip tasks. We also need to remove the duplicate artifacts configuration from these tasks.",NULL,"We need to cleanup some of the duplicate gradle tasks that bundle spring xd distributions<span class='highlight-text severity-high'>. Currently, distXD does the copy of distributions from spring xd dirt , redis and spring xd gemfire server projects into rootDir dist spring xd . And, the task zipXD makes the zip archive. These tasks should be combined with the distZip docZip tasks. We also need to remove the duplicate artifacts configuration from these tasks.</span>","minimal","punctuation","high",False
19678,"https github.com SpringSource spring xd wiki Creating a Source Module uses the SI twittersearch inbound channel adapter, which is no longer going to work once Twitter disallows anonymous searches. Ideally we update the example to use a new version of SI twitter that adds support for this as opposed to the XD workaround. ",NULL,"https github.com SpringSource spring xd wiki Creating a Source Module uses the SI twittersearch inbound channel adapter, which is no longer going to work once Twitter disallows anonymous searches. Ideally we update the example to use a new version of SI twitter that adds support for this as opposed to the XD workaround. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19678,"https github.com SpringSource spring xd wiki Creating a Source Module uses the SI twittersearch inbound channel adapter, which is no longer going to work once Twitter disallows anonymous searches. Ideally we update the example to use a new version of SI twitter that adds support for this as opposed to the XD workaround. ",NULL,"https github.com SpringSource spring xd wiki Creating a Source Module uses the SI twittersearch inbound channel adapter, which is no longer going to work once Twitter disallows anonymous searches. Ideally we update the example to use a new version of SI twitter that adds support for this as opposed to the XD workaround. ",NULL,"https github<span class='highlight-text severity-high'>.com SpringSource spring xd wiki Creating a Source Module uses the SI twittersearch inbound channel adapter, which is no longer going to work once Twitter disallows anonymous searches. Ideally we update the example to use a new version of SI twitter that adds support for this as opposed to the XD workaround. </span>","minimal","punctuation","high",False
19677,"for example, when using the twittersearch source module, the hashTags are nested within entities , and the value for hashTags is itself an object with a text field, so the following would be needed to count the actual value of interest code tap tweets field value counter fieldName entities.hashTags.text code ",NULL,"for example, when using the twittersearch","source module, the hashTags are nested within entities , and the value for hashTags is itself an object with a text field, so the following would be needed to count the actual value of interest code tap tweets field value counter fieldName entities.hashTags.text code","Add for who this story is","well_formed","no_role","high",False
19677,"for example, when using the twittersearch source module, the hashTags are nested within entities , and the value for hashTags is itself an object with a text field, so the following would be needed to count the actual value of interest code tap tweets field value counter fieldName entities.hashTags.text code ",NULL,"for example, when using the twittersearch","source module, the hashTags are nested within entities , and the value for hashTags is itself an object with a text field, so the following would be needed to count the actual value of interest code tap tweets field value counter fieldName entities.hashTags.text code","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19682,"Since the changes for XD 202, twittersearch requires authentication. Need to update the docs to reflect this.",NULL,"Since the changes for XD 202, twittersearch requires authentication. Need to update the docs to reflect this.",NULL,"Add for who this story is","well_formed","no_role","high",False
19682,"Since the changes for XD 202, twittersearch requires authentication. Need to update the docs to reflect this.",NULL,"Since the changes for XD 202, twittersearch requires authentication. Need to update the docs to reflect this.",NULL,"Since the changes for XD 202, twittersearch requires authentication<span class='highlight-text severity-high'>. Need to update the docs to reflect this.</span>","minimal","punctuation","high",False
19681,"The transform, filter, and script processor modules support passing in a properties location for script variables. We need a default location on the classpath for users to provide custom properties files.",NULL,"The transform, filter, and script processor modules support passing in a properties location for script variables. We need a default location on the classpath for users to provide custom properties files.",NULL,"The transform, filter,<span class='highlight-text severity-high'> and </span>script processor modules support passing in a properties location for script variables. We need a default location on the classpath for users to provide custom properties files.","atomic","conjunctions","high",False
19681,"The transform, filter, and script processor modules support passing in a properties location for script variables. We need a default location on the classpath for users to provide custom properties files.",NULL,"The transform, filter, and script processor modules support passing in a properties location for script variables. We need a default location on the classpath for users to provide custom properties files.",NULL,"The transform, filter, and script processor modules support passing in a properties location for script variables<span class='highlight-text severity-high'>. We need a default location on the classpath for users to provide custom properties files.</span>","minimal","punctuation","high",False
19686,"The config file modules sink hdfs.xml has a hardcoded value to locate the namenode. hdp configuration register url handler false fs.default.name hdfs localhost 9000 hdp configuration the fs.default.name proprety should be configurable and we should also support loading an external configuration file using hdp configuration properties location xd.home config hadoop.properties hdp configuration ",NULL,"The config file modules sink hdfs.xml has a hardcoded value to locate the namenode. hdp configuration register url handler false fs.default.name hdfs localhost 9000 hdp configuration the fs.default.name proprety should be configurable and we should also support loading an external configuration file using hdp configuration properties location xd.home config hadoop.properties hdp configuration ",NULL,"Add for who this story is","well_formed","no_role","high",False
19686,"The config file modules sink hdfs.xml has a hardcoded value to locate the namenode. hdp configuration register url handler false fs.default.name hdfs localhost 9000 hdp configuration the fs.default.name proprety should be configurable and we should also support loading an external configuration file using hdp configuration properties location xd.home config hadoop.properties hdp configuration ",NULL,"The config file modules sink hdfs.xml has a hardcoded value to locate the namenode. hdp configuration register url handler false fs.default.name hdfs localhost 9000 hdp configuration the fs.default.name proprety should be configurable and we should also support loading an external configuration file using hdp configuration properties location xd.home config hadoop.properties hdp configuration ",NULL,"The config file modules sink hdfs.xml has a hardcoded value to locate the namenode. hdp configuration register url handler false fs.default.name hdfs localhost 9000 hdp configuration the fs.default.name proprety should be configurable<span class='highlight-text severity-high'> and </span>we should also support loading an external configuration file using hdp configuration properties location xd.home config hadoop.properties hdp configuration ","atomic","conjunctions","high",False
19686,"The config file modules sink hdfs.xml has a hardcoded value to locate the namenode. hdp configuration register url handler false fs.default.name hdfs localhost 9000 hdp configuration the fs.default.name proprety should be configurable and we should also support loading an external configuration file using hdp configuration properties location xd.home config hadoop.properties hdp configuration ",NULL,"The config file modules sink hdfs.xml has a hardcoded value to locate the namenode. hdp configuration register url handler false fs.default.name hdfs localhost 9000 hdp configuration the fs.default.name proprety should be configurable and we should also support loading an external configuration file using hdp configuration properties location xd.home config hadoop.properties hdp configuration ",NULL,"The config file modules sink hdfs<span class='highlight-text severity-high'>.xml has a hardcoded value to locate the namenode. hdp configuration register url handler false fs.default.name hdfs localhost 9000 hdp configuration the fs.default.name proprety should be configurable and we should also support loading an external configuration file using hdp configuration properties location xd.home config hadoop.properties hdp configuration </span>","minimal","punctuation","high",False
19685,"The wiki docs on creating custom modules have entire build.gradle files in them. Would be good to explore existing STS templates, maven archetypes, etc or create new ones for authoring custom modules.",NULL,"The wiki docs on creating custom modules have entire build.gradle files in them. Would be good to explore existing STS templates, maven archetypes, etc or create new ones for authoring custom modules.",NULL,"Add for who this story is","well_formed","no_role","high",False
19685,"The wiki docs on creating custom modules have entire build.gradle files in them. Would be good to explore existing STS templates, maven archetypes, etc or create new ones for authoring custom modules.",NULL,"The wiki docs on creating custom modules have entire build.gradle files in them. Would be good to explore existing STS templates, maven archetypes, etc or create new ones for authoring custom modules.",NULL,"The wiki docs on creating custom modules have entire build.gradle files in them. Would be good to explore existing STS templates, maven archetypes, etc<span class='highlight-text severity-high'> or </span>create new ones for authoring custom modules.","atomic","conjunctions","high",False
19685,"The wiki docs on creating custom modules have entire build.gradle files in them. Would be good to explore existing STS templates, maven archetypes, etc or create new ones for authoring custom modules.",NULL,"The wiki docs on creating custom modules have entire build.gradle files in them. Would be good to explore existing STS templates, maven archetypes, etc or create new ones for authoring custom modules.",NULL,"The wiki docs on creating custom modules have entire build<span class='highlight-text severity-high'>.gradle files in them. Would be good to explore existing STS templates, maven archetypes, etc or create new ones for authoring custom modules.</span>","minimal","punctuation","high",False
19687,"Filter, Transform, and Script modules all assume the provided script is written in Groovy. This is partly due to the fact that the lang attribute of int script script can t be set to a property value i.e. lang lang groovy , which would allow users to pass in the expected language. Or perhaps we could use a SPEL expression or script to pick the language based on the file extension?",NULL,"Filter, Transform, and Script modules all assume the provided script is written in Groovy. This is partly due to the fact that the lang attribute of int script script can t be set to a property value i.e. lang lang groovy , which would allow users to pass in the expected language. Or perhaps we could use a SPEL expression or script to pick the language based on the file extension?",NULL,"Add for who this story is","well_formed","no_role","high",False
19687,"Filter, Transform, and Script modules all assume the provided script is written in Groovy. This is partly due to the fact that the lang attribute of int script script can t be set to a property value i.e. lang lang groovy , which would allow users to pass in the expected language. Or perhaps we could use a SPEL expression or script to pick the language based on the file extension?",NULL,"Filter, Transform, and Script modules all assume the provided script is written in Groovy. This is partly due to the fact that the lang attribute of int script script can t be set to a property value i.e. lang lang groovy , which would allow users to pass in the expected language. Or perhaps we could use a SPEL expression or script to pick the language based on the file extension?",NULL,"Filter, Transform,<span class='highlight-text severity-high'> and </span>Script modules all assume the provided script is written in Groovy. This is partly due to the fact that the lang attribute of int script script can t be set to a property value i.e. lang lang groovy , which would allow users to pass in the expected language.<span class='highlight-text severity-high'> or </span>perhaps we could use a SPEL expression or script to pick the language based on the file extension?","atomic","conjunctions","high",False
19687,"Filter, Transform, and Script modules all assume the provided script is written in Groovy. This is partly due to the fact that the lang attribute of int script script can t be set to a property value i.e. lang lang groovy , which would allow users to pass in the expected language. Or perhaps we could use a SPEL expression or script to pick the language based on the file extension?",NULL,"Filter, Transform, and Script modules all assume the provided script is written in Groovy. This is partly due to the fact that the lang attribute of int script script can t be set to a property value i.e. lang lang groovy , which would allow users to pass in the expected language. Or perhaps we could use a SPEL expression or script to pick the language based on the file extension?",NULL,"Filter, Transform, and Script modules all assume the provided script is written in Groovy<span class='highlight-text severity-high'>. This is partly due to the fact that the lang attribute of int script script can t be set to a property value i.e. lang lang groovy , which would allow users to pass in the expected language. Or perhaps we could use a SPEL expression or script to pick the language based on the file extension?</span>","minimal","punctuation","high",False
19688,"Fill in https github.com SpringSource spring xd wiki Processors",NULL,"Fill in https github.com SpringSource spring xd wiki Processors",NULL,"Add for who this story is","well_formed","no_role","high",False
19684,"There shouldn t be a need to do a mkdir p before sending data to a file sink.",NULL,"There shouldn t be a need to do a mkdir p before sending data to a file sink.",NULL,"Add for who this story is","well_formed","no_role","high",False
19674,"configurable parameters should include the queue name s and optional binding key pattern connection info, such as host and port, should also be configurable but with defaults localhost and default port , and that should likely fallback to a rabbit.properties file in the XD HOME config directory",NULL,"configurable parameters should include the queue name s and optional binding key pattern connection info, such as host and port, should also be configurable but with defaults localhost and default port , and that should likely fallback to a rabbit.properties file in the XD HOME config directory",NULL,"Add for who this story is","well_formed","no_role","high",False
19674,"configurable parameters should include the queue name s and optional binding key pattern connection info, such as host and port, should also be configurable but with defaults localhost and default port , and that should likely fallback to a rabbit.properties file in the XD HOME config directory",NULL,"configurable parameters should include the queue name s and optional binding key pattern connection info, such as host and port, should also be configurable but with defaults localhost and default port , and that should likely fallback to a rabbit.properties file in the XD HOME config directory",NULL,"configurable parameters should include the queue name s<span class='highlight-text severity-high'> and </span>optional binding key pattern connection info, such as host and port, should also be configurable but with defaults localhost and default port , and that should likely fallback to a rabbit.properties file in the XD HOME config directory","atomic","conjunctions","high",False
19680,"Those property keys should then be provided as defaults for the placeholders in source twittersearch.xml",NULL,"Those property keys should then be provided as defaults for the placeholders in source twittersearch.xml",NULL,"Add for who this story is","well_formed","no_role","high",False
19690,"This will enable arbitrary processing logic to be used in a processing step. See http blog.springsource.org 2011 12 08 spring integration scripting support part 1 int service activator ... script script lang groovy location file scripts groovy myscript.groovy int service activator would be the essence of the module. Probably lang gets detected from the file extension.",NULL,"This will enable arbitrary processing logic to be used in a processing step. See http blog.springsource.org 2011 12 08 spring integration scripting support part 1 int service activator ... script script lang groovy location file scripts groovy myscript.groovy int service activator would be the essence of the module. Probably lang gets detected from the file extension.",NULL,"Add for who this story is","well_formed","no_role","high",False
19690,"This will enable arbitrary processing logic to be used in a processing step. See http blog.springsource.org 2011 12 08 spring integration scripting support part 1 int service activator ... script script lang groovy location file scripts groovy myscript.groovy int service activator would be the essence of the module. Probably lang gets detected from the file extension.",NULL,"This will enable arbitrary processing logic to be used in a processing step. See http blog.springsource.org 2011 12 08 spring integration scripting support part 1 int service activator ... script script lang groovy location file scripts groovy myscript.groovy int service activator would be the essence of the module. Probably lang gets detected from the file extension.",NULL,"This will enable arbitrary processing logic to be used in a processing step<span class='highlight-text severity-high'>. See http blog.springsource.org 2011 12 08 spring integration scripting support part 1 int service activator ... script script lang groovy location file scripts groovy myscript.groovy int service activator would be the essence of the module. Probably lang gets detected from the file extension.</span>","minimal","punctuation","high",False
19691,"Replace the existing DSL parser that uses string indexing with a more robust one based on a derivative of SpEL. This will provide a stable base on which to quickly iterate on syntax. ",NULL,"Replace the existing DSL parser that uses string indexing with a more robust one based on a derivative of SpEL. This will provide a stable base on which to quickly iterate on syntax. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19691,"Replace the existing DSL parser that uses string indexing with a more robust one based on a derivative of SpEL. This will provide a stable base on which to quickly iterate on syntax. ",NULL,"Replace the existing DSL parser that uses string indexing with a more robust one based on a derivative of SpEL. This will provide a stable base on which to quickly iterate on syntax. ",NULL,"Replace the existing DSL parser that uses string indexing with a more robust one based on a derivative of SpEL<span class='highlight-text severity-high'>. This will provide a stable base on which to quickly iterate on syntax. </span>","minimal","punctuation","high",False
19694,"Currently internal config files are in META INF spring with fairly generic names. To avoid potential collisions if users add their own configuration in the classpath, we should have a more unique location, e.g. META INF spring xd",NULL,"Currently internal config files are in META INF spring with fairly generic names. To avoid potential collisions if users add their own configuration in the classpath, we should have a more unique location, e.g. META INF spring xd",NULL,"Add for who this story is","well_formed","no_role","high",False
19694,"Currently internal config files are in META INF spring with fairly generic names. To avoid potential collisions if users add their own configuration in the classpath, we should have a more unique location, e.g. META INF spring xd",NULL,"Currently internal config files are in META INF spring with fairly generic names. To avoid potential collisions if users add their own configuration in the classpath, we should have a more unique location, e.g. META INF spring xd",NULL,"Currently internal config files are in META INF spring with fairly generic names<span class='highlight-text severity-high'>. To avoid potential collisions if users add their own configuration in the classpath, we should have a more unique location, e.g. META INF spring xd</span>","minimal","punctuation","high",False
19693,"If a user needs to deploy a module containing custom code, they have to build a jar for the lib dir and somehow deploy the app context file separately to a modules dir. This is a bit inconvenient to build from a project, since context files in src main resources typically get built into a jar. Might be better to accept both jar and xml files in the modules dir, though that brings up the issue of classpath isolation.",NULL,"If a user needs to deploy a module containing custom code, they have to build a jar for the lib dir and somehow deploy the app context file separately to a modules dir. This is a bit inconvenient to build from a project, since context files in src main resources typically get built into a jar. Might be better to accept both jar and xml files in the modules dir, though that brings up the issue of classpath isolation.",NULL,"Add for who this story is","well_formed","no_role","high",False
19693,"If a user needs to deploy a module containing custom code, they have to build a jar for the lib dir and somehow deploy the app context file separately to a modules dir. This is a bit inconvenient to build from a project, since context files in src main resources typically get built into a jar. Might be better to accept both jar and xml files in the modules dir, though that brings up the issue of classpath isolation.",NULL,"If a user needs to deploy a module containing custom code, they have to build a jar for the lib dir and somehow deploy the app context file separately to a modules dir. This is a bit inconvenient to build from a project, since context files in src main resources typically get built into a jar. Might be better to accept both jar and xml files in the modules dir, though that brings up the issue of classpath isolation.",NULL,"If a user needs to deploy a module containing custom code, they have to build a jar for the lib dir<span class='highlight-text severity-high'> and </span>somehow deploy the app context file separately to a modules dir. This is a bit inconvenient to build from a project, since context files in src main resources typically get built into a jar. Might be better to accept both jar and xml files in the modules dir, though that brings up the issue of classpath isolation.","atomic","conjunctions","high",False
19693,"If a user needs to deploy a module containing custom code, they have to build a jar for the lib dir and somehow deploy the app context file separately to a modules dir. This is a bit inconvenient to build from a project, since context files in src main resources typically get built into a jar. Might be better to accept both jar and xml files in the modules dir, though that brings up the issue of classpath isolation.",NULL,"If a user needs to deploy a module containing custom code, they have to build a jar for the lib dir and somehow deploy the app context file separately to a modules dir. This is a bit inconvenient to build from a project, since context files in src main resources typically get built into a jar. Might be better to accept both jar and xml files in the modules dir, though that brings up the issue of classpath isolation.",NULL,"If a user needs to deploy a module containing custom code, they have to build a jar for the lib dir and somehow deploy the app context file separately to a modules dir<span class='highlight-text severity-high'>. This is a bit inconvenient to build from a project, since context files in src main resources typically get built into a jar. Might be better to accept both jar and xml files in the modules dir, though that brings up the issue of classpath isolation.</span>","minimal","punctuation","high",False
19696,"XD 106 included detailed logging about the Redis metadata within the RedisContainerListener, but it seems as though that info could be logged somewhere closer to the establishment of a Redis connection for the XD runtime and could be logged even if this listener, whose main role is to capture Container related events, is not enabled .",NULL,"XD 106 included detailed logging about the Redis metadata within the RedisContainerListener, but it seems as though that info could be logged somewhere closer to the establishment of a Redis connection for the XD runtime and could be logged even if this listener, whose main role is to capture Container related events, is not enabled .",NULL,"Add for who this story is","well_formed","no_role","high",False
19696,"XD 106 included detailed logging about the Redis metadata within the RedisContainerListener, but it seems as though that info could be logged somewhere closer to the establishment of a Redis connection for the XD runtime and could be logged even if this listener, whose main role is to capture Container related events, is not enabled .",NULL,"XD 106 included detailed logging about the Redis metadata within the RedisContainerListener, but it seems as though that info could be logged somewhere closer to the establishment of a Redis connection for the XD runtime and could be logged even if this listener, whose main role is to capture Container related events, is not enabled .",NULL,"XD 106 included detailed logging about the Redis metadata within the RedisContainerListener, but it seems as though that info could be logged somewhere closer to the establishment of a Redis connection for the XD runtime<span class='highlight-text severity-high'> and </span>could be logged even if this listener, whose main role is to capture Container related events, is not enabled .","atomic","conjunctions","high",False
19697,"a stream such as time filter script oddMinuteFilter.groovy file would load the groovy script oddMinuteFilter.groovy that is located in the directory modules processor or perhaps in modules processor scripts. Not sure the benefit of having a subdirectory below processor just for scripts.",NULL,"a stream such as time filter script oddMinuteFilter.groovy file would load the groovy script oddMinuteFilter.groovy that is located in the directory modules processor or perhaps in modules processor scripts. Not sure the benefit of having a subdirectory below processor just for scripts.",NULL,"Add for who this story is","well_formed","no_role","high",False
19697,"a stream such as time filter script oddMinuteFilter.groovy file would load the groovy script oddMinuteFilter.groovy that is located in the directory modules processor or perhaps in modules processor scripts. Not sure the benefit of having a subdirectory below processor just for scripts.",NULL,"a stream such as time filter script oddMinuteFilter.groovy file would load the groovy script oddMinuteFilter.groovy that is located in the directory modules processor or perhaps in modules processor scripts. Not sure the benefit of having a subdirectory below processor just for scripts.",NULL,"a stream such as time filter script oddMinuteFilter<span class='highlight-text severity-high'>.groovy file would load the groovy script oddMinuteFilter.groovy that is located in the directory modules processor or perhaps in modules processor scripts. Not sure the benefit of having a subdirectory below processor just for scripts.</span>","minimal","punctuation","high",False
19698,"This script will launch XD admin along with the module container. As part of this implementation, we will also remove the embedded options for XD admin container scripts.",NULL,"This script will launch XD admin along with the module container. As part of this implementation, we will also remove the embedded options for XD admin container scripts.",NULL,"Add for who this story is","well_formed","no_role","high",False
19698,"This script will launch XD admin along with the module container. As part of this implementation, we will also remove the embedded options for XD admin container scripts.",NULL,"This script will launch XD admin along with the module container. As part of this implementation, we will also remove the embedded options for XD admin container scripts.",NULL,"This script will launch XD admin along with the module container<span class='highlight-text severity-high'>. As part of this implementation, we will also remove the embedded options for XD admin container scripts.</span>","minimal","punctuation","high",False
19700,"The current StreamServer depends on RedisStreamDeployer. Call this RedisStreamServer and extract interface to allow alternate implementations",NULL,"The current StreamServer depends on RedisStreamDeployer. Call this RedisStreamServer and extract interface to allow alternate implementations",NULL,"Add for who this story is","well_formed","no_role","high",False
19700,"The current StreamServer depends on RedisStreamDeployer. Call this RedisStreamServer and extract interface to allow alternate implementations",NULL,"The current StreamServer depends on RedisStreamDeployer. Call this RedisStreamServer and extract interface to allow alternate implementations",NULL,"The current StreamServer depends on RedisStreamDeployer. Call this RedisStreamServer<span class='highlight-text severity-high'> and </span>extract interface to allow alternate implementations","atomic","conjunctions","high",False
19700,"The current StreamServer depends on RedisStreamDeployer. Call this RedisStreamServer and extract interface to allow alternate implementations",NULL,"The current StreamServer depends on RedisStreamDeployer. Call this RedisStreamServer and extract interface to allow alternate implementations",NULL,"The current StreamServer depends on RedisStreamDeployer<span class='highlight-text severity-high'>. Call this RedisStreamServer and extract interface to allow alternate implementations</span>","minimal","punctuation","high",False
19701,"The redis specific beans that are defined in the current launcher.xml should move into this configuration file. ",NULL,"The redis specific beans that are defined in the current launcher.xml should move into this configuration file. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19699,"Create StreamDeployer that does not depend on an adapter implementation",NULL,"Create StreamDeployer that does not depend on an adapter implementation",NULL,"Add for who this story is","well_formed","no_role","high",False
19689,"I'don t see that we have automated tests for the modules we provide out of the box. We could make the modules folder an Eclipse project which would also help solve XD 198 and add some integration tests similar to those documented here https github.com SpringSource spring xd wiki Creating Custom Modules",NULL,"I'don t see that we have automated tests for the modules we provide out of the box. We could make the modules folder an Eclipse project which would also help solve XD 198 and add some integration tests similar to those documented here https github.com SpringSource spring xd wiki Creating Custom Modules",NULL,"Add for who this story is","well_formed","no_role","high",False
19689,"I'don t see that we have automated tests for the modules we provide out of the box. We could make the modules folder an Eclipse project which would also help solve XD 198 and add some integration tests similar to those documented here https github.com SpringSource spring xd wiki Creating Custom Modules",NULL,"I'don t see that we have automated tests for the modules we provide out of the box. We could make the modules folder an Eclipse project which would also help solve XD 198 and add some integration tests similar to those documented here https github.com SpringSource spring xd wiki Creating Custom Modules",NULL,"I'don t see that we have automated tests for the modules we provide out of the box. We could make the modules folder an Eclipse project which would also help solve XD 198<span class='highlight-text severity-high'> and </span>add some integration tests similar to those documented here https github.com SpringSource spring xd wiki Creating Custom Modules","atomic","conjunctions","high",False
19689,"I'don t see that we have automated tests for the modules we provide out of the box. We could make the modules folder an Eclipse project which would also help solve XD 198 and add some integration tests similar to those documented here https github.com SpringSource spring xd wiki Creating Custom Modules",NULL,"I'don t see that we have automated tests for the modules we provide out of the box. We could make the modules folder an Eclipse project which would also help solve XD 198 and add some integration tests similar to those documented here https github.com SpringSource spring xd wiki Creating Custom Modules",NULL,"I'don t see that we have automated tests for the modules we provide out of the box<span class='highlight-text severity-high'>. We could make the modules folder an Eclipse project which would also help solve XD 198 and add some integration tests similar to those documented here https github.com SpringSource spring xd wiki Creating Custom Modules</span>","minimal","punctuation","high",False
19695,"With the new option of starting without requiring redis, the getting started documentation should reflect this easier way to start processing data.",NULL,"With the new option of starting without requiring redis, the getting started documentation should reflect this easier way to start processing data.",NULL,"Add for who this story is","well_formed","no_role","high",False
19692,"When running in local mode no Redis time tcp no longer works. Change the time source to emit the date as a String, while allowing an option to emit a Date object.",NULL,"When running in local mode no Redis time tcp no longer works. Change the time source to emit the date as a String, while allowing an option to emit a Date object.",NULL,"Add for who this story is","well_formed","no_role","high",False
19692,"When running in local mode no Redis time tcp no longer works. Change the time source to emit the date as a String, while allowing an option to emit a Date object.",NULL,"When running in local mode no Redis time tcp no longer works. Change the time source to emit the date as a String, while allowing an option to emit a Date object.",NULL,"When running in local mode no Redis time tcp no longer works<span class='highlight-text severity-high'>. Change the time source to emit the date as a String, while allowing an option to emit a Date object.</span>","minimal","punctuation","high",False
19704,"The xd singlenode script will launch a main application that creates both the admin node to process http admin requests and the container node to execute modules for data processing within in the same process the xd admin script will launch a main application that creates only the admin node remove current embeddedContainer options the xd container script will launch a main application that creates only the container node as it is now ",NULL,"The xd singlenode script will launch a main application that creates both the admin node to process http admin requests and the container node to execute modules for data processing within in the same process the xd admin script will launch a main application that creates only the admin node remove current embeddedContainer options the xd container script will launch a main application that creates only the container node as it is now ",NULL,"Add for who this story is","well_formed","no_role","high",False
19704,"The xd singlenode script will launch a main application that creates both the admin node to process http admin requests and the container node to execute modules for data processing within in the same process the xd admin script will launch a main application that creates only the admin node remove current embeddedContainer options the xd container script will launch a main application that creates only the container node as it is now ",NULL,"The xd singlenode script will launch a main application that creates both the admin node to process http admin requests and the container node to execute modules for data processing within in the same process the xd admin script will launch a main application that creates only the admin node remove current embeddedContainer options the xd container script will launch a main application that creates only the container node as it is now ",NULL,"The xd singlenode script will launch a main application that creates both the admin node to process http admin requests<span class='highlight-text severity-high'> and </span>the container node to execute modules for data processing within in the same process the xd admin script will launch a main application that creates only the admin node remove current embeddedContainer options the xd container script will launch a main application that creates only the container node as it is now ","atomic","conjunctions","high",False
19706,"This could easily be supported in the existing gauge by adding a setAlpha method to RichGaugeService and adding the extra parameter alpha to the gauge data https en.wikipedia.org wiki Exponential moving average . If not set it would default to the current behaviour simple mean , otherwise it would calculate the exponential moving average in place of the mean.",NULL,"This could easily be supported in the existing gauge by adding a setAlpha method to RichGaugeService and adding the extra parameter alpha to the gauge data https en.wikipedia.org wiki Exponential moving average . If not set it would default to the current behaviour simple mean , otherwise it would calculate the exponential moving average in place of the mean.",NULL,"Add for who this story is","well_formed","no_role","high",False
19706,"This could easily be supported in the existing gauge by adding a setAlpha method to RichGaugeService and adding the extra parameter alpha to the gauge data https en.wikipedia.org wiki Exponential moving average . If not set it would default to the current behaviour simple mean , otherwise it would calculate the exponential moving average in place of the mean.",NULL,"This could easily be supported in the existing gauge by adding a setAlpha method to RichGaugeService and adding the extra parameter alpha to the gauge data https en.wikipedia.org wiki Exponential moving average . If not set it would default to the current behaviour simple mean , otherwise it would calculate the exponential moving average in place of the mean.",NULL,"This could easily be supported in the existing gauge by adding a setAlpha method to RichGaugeService<span class='highlight-text severity-high'> and </span>adding the extra parameter alpha to the gauge data https en.wikipedia.org wiki Exponential moving average . If not set it would default to the current behaviour simple mean , otherwise it would calculate the exponential moving average in place of the mean.","atomic","conjunctions","high",False
19706,"This could easily be supported in the existing gauge by adding a setAlpha method to RichGaugeService and adding the extra parameter alpha to the gauge data https en.wikipedia.org wiki Exponential moving average . If not set it would default to the current behaviour simple mean , otherwise it would calculate the exponential moving average in place of the mean.",NULL,"This could easily be supported in the existing gauge by adding a setAlpha method to RichGaugeService and adding the extra parameter alpha to the gauge data https en.wikipedia.org wiki Exponential moving average . If not set it would default to the current behaviour simple mean , otherwise it would calculate the exponential moving average in place of the mean.",NULL,"This could easily be supported in the existing gauge by adding a setAlpha method to RichGaugeService and adding the extra parameter alpha to the gauge data https en<span class='highlight-text severity-high'>.wikipedia.org wiki Exponential moving average . If not set it would default to the current behaviour simple mean , otherwise it would calculate the exponential moving average in place of the mean.</span>","minimal","punctuation","high",False
19705,"The current incrementAndGet approach based off redis will not easily be applicable in local model deployment",NULL,"The current incrementAndGet approach based off redis will not easily be applicable in local model deployment",NULL,"Add for who this story is","well_formed","no_role","high",False
19712,"Add more structure, more easily find the reference guide. The style that is here https github.com snowplow snowplow wiki is nice. ",NULL,"Add more structure, more easily find the reference guide. The style that is here https github.com snowplow snowplow wiki is nice. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19712,"Add more structure, more easily find the reference guide. The style that is here https github.com snowplow snowplow wiki is nice. ",NULL,"Add more structure, more easily find the reference guide. The style that is here https github.com snowplow snowplow wiki is nice. ",NULL,"Add more structure, more easily find the reference guide<span class='highlight-text severity-high'>. The style that is here https github.com snowplow snowplow wiki is nice. </span>","minimal","punctuation","high",False
19707,"2 3 containers separate processes that the stream syslog tcp 1 container separate process that aggregates the data sent from those conainers, tcp severityFilter hdfs ",NULL,"2 3 containers separate processes that the stream syslog tcp 1 container separate process that aggregates the data sent from those conainers, tcp severityFilter hdfs ",NULL,"Add for who this story is","well_formed","no_role","high",False
19708,"The use case is to write custom code that does processing on a specific domain class perhaps from twitter adapter or a tuple. Need to package up this code so that it can be used inside XD. ",NULL,"The use case is to write custom code that does processing on a specific domain class perhaps from twitter adapter or a tuple. Need to package up this code","so that it can be used inside XD.","Add for who this story is","well_formed","no_role","high",False
19708,"The use case is to write custom code that does processing on a specific domain class perhaps from twitter adapter or a tuple. Need to package up this code so that it can be used inside XD. ",NULL,"The use case is to write custom code that does processing on a specific domain class perhaps from twitter adapter or a tuple. Need to package up this code","so that it can be used inside XD.","The use case is to write custom code that does processing on a specific domain class perhaps from twitter adapter or a tuple<span class='highlight-text severity-high'>. Need to package up this code so that it can be used inside XD. </span>","minimal","punctuation","high",False
19708,"The use case is to write custom code that does processing on a specific domain class perhaps from twitter adapter or a tuple. Need to package up this code so that it can be used inside XD. ",NULL,"The use case is to write custom code that does processing on a specific domain class perhaps from twitter adapter or a tuple. Need to package up this code","so that it can be used inside XD.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19713,"Trying to run XD offline results in an error in redis.xml because the cloudfoundry schema file is missing. We need to add the cf runtime jar to the classpath to resolve this.",NULL,"Trying to run XD offline results in an error in redis.xml because the cloudfoundry schema file is missing. We need to add the cf runtime jar to the classpath to resolve this.",NULL,"Add for who this story is","well_formed","no_role","high",False
19713,"Trying to run XD offline results in an error in redis.xml because the cloudfoundry schema file is missing. We need to add the cf runtime jar to the classpath to resolve this.",NULL,"Trying to run XD offline results in an error in redis.xml because the cloudfoundry schema file is missing. We need to add the cf runtime jar to the classpath to resolve this.",NULL,"Trying to run XD offline results in an error in redis<span class='highlight-text severity-high'>.xml because the cloudfoundry schema file is missing. We need to add the cf runtime jar to the classpath to resolve this.</span>","minimal","punctuation","high",False
19710,"bottom home page list of projects data integration category landing pages related projects. ",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19710,"bottom home page list of projects data integration category landing pages related projects. ",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19748,"This should facilitate testing while avoiding any class dependencies. Also, log is a generally useful sink by itself and time is a more interesting source for testing should accept interval for the seconds between time messages .",NULL,"This should facilitate testing while avoiding any class dependencies. Also, log is a generally useful sink by itself and time is a more interesting source for testing should accept interval for the seconds between time messages .",NULL,"Add for who this story is","well_formed","no_role","high",False
19748,"This should facilitate testing while avoiding any class dependencies. Also, log is a generally useful sink by itself and time is a more interesting source for testing should accept interval for the seconds between time messages .",NULL,"This should facilitate testing while avoiding any class dependencies. Also, log is a generally useful sink by itself and time is a more interesting source for testing should accept interval for the seconds between time messages .",NULL,"This should facilitate testing while avoiding any class dependencies. Also, log is a generally useful sink by itself<span class='highlight-text severity-high'> and </span>time is a more interesting source for testing should accept interval for the seconds between time messages .","atomic","conjunctions","high",False
19748,"This should facilitate testing while avoiding any class dependencies. Also, log is a generally useful sink by itself and time is a more interesting source for testing should accept interval for the seconds between time messages .",NULL,"This should facilitate testing while avoiding any class dependencies. Also, log is a generally useful sink by itself and time is a more interesting source for testing should accept interval for the seconds between time messages .",NULL,"This should facilitate testing while avoiding any class dependencies<span class='highlight-text severity-high'>. Also, log is a generally useful sink by itself and time is a more interesting source for testing should accept interval for the seconds between time messages .</span>","minimal","punctuation","high",False
19703,"The name pipeProtocol is tentative. 1. The command line scripts for xd admin and xd container would support a pipeProtocol option, with the default being to use Redis. Otherwise use xd singlenode . 2. The xd admin and xd container scripts will use the value of pipeProtocol to set the java system property xd.pipeProtocol when launching the app. ",NULL,"The name pipeProtocol is tentative. 1. The command line scripts for xd admin and xd container would support a pipeProtocol option, with the default being to use Redis. Otherwise use xd singlenode . 2. The xd admin and xd container scripts will use the value of pipeProtocol to set the java system property xd.pipeProtocol when launching the app. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19750,"It should provide an expression param for SpEL and have a default value of true accept everything .",NULL,"It should provide an expression param for SpEL and have a default value of true accept everything .",NULL,"It should provide an expression param for SpEL<span class='highlight-text severity-high'> and </span>have a default value of true accept everything .","atomic","conjunctions","high",False
18221,"Currently we deploy a single instance, and ignore the ModuleDeploymentRequest instances setting. It is easy to change this in the ModuleDeployer, but there is no guarantee this will work in the ModuleLauncher, so we hold off until that can be verified.",NULL,"Currently we deploy a single instance, and ignore the ModuleDeploymentRequest instances setting. It is easy to change this in the ModuleDeployer, but there is no guarantee this will work in the ModuleLauncher,","so we hold off until that can be verified.","Add for who this story is","well_formed","no_role","high",False
19703,"The name pipeProtocol is tentative. 1. The command line scripts for xd admin and xd container would support a pipeProtocol option, with the default being to use Redis. Otherwise use xd singlenode . 2. The xd admin and xd container scripts will use the value of pipeProtocol to set the java system property xd.pipeProtocol when launching the app. ",NULL,"The name pipeProtocol is tentative. 1. The command line scripts for xd admin and xd container would support a pipeProtocol option, with the default being to use Redis. Otherwise use xd singlenode . 2. The xd admin and xd container scripts will use the value of pipeProtocol to set the java system property xd.pipeProtocol when launching the app. ",NULL,"The name pipeProtocol is tentative. 1. The command line scripts for xd admin<span class='highlight-text severity-high'> and </span>xd container would support a pipeProtocol option, with the default being to use Redis. Otherwise use xd singlenode . 2. The xd admin and xd container scripts will use the value of pipeProtocol to set the java system property xd.pipeProtocol when launching the app. ","atomic","conjunctions","high",False
19703,"The name pipeProtocol is tentative. 1. The command line scripts for xd admin and xd container would support a pipeProtocol option, with the default being to use Redis. Otherwise use xd singlenode . 2. The xd admin and xd container scripts will use the value of pipeProtocol to set the java system property xd.pipeProtocol when launching the app. ",NULL,"The name pipeProtocol is tentative. 1. The command line scripts for xd admin and xd container would support a pipeProtocol option, with the default being to use Redis. Otherwise use xd singlenode . 2. The xd admin and xd container scripts will use the value of pipeProtocol to set the java system property xd.pipeProtocol when launching the app. ",NULL,"The name pipeProtocol is tentative<span class='highlight-text severity-high'>. 1. The command line scripts for xd admin and xd container would support a pipeProtocol option, with the default being to use Redis. Otherwise use xd singlenode . 2. The xd admin and xd container scripts will use the value of pipeProtocol to set the java system property xd.pipeProtocol when launching the app. </span>","minimal","punctuation","high",False
19711,"A minimal project page of a top level project page that has basic information of docs and links to the github wiki page. No need to list maven coordinates.",NULL,"A minimal project page of a top level project page that has basic information of docs and links to the github wiki page. No need to list maven coordinates.",NULL,"Add for who this story is","well_formed","no_role","high",False
19711,"A minimal project page of a top level project page that has basic information of docs and links to the github wiki page. No need to list maven coordinates.",NULL,"A minimal project page of a top level project page that has basic information of docs and links to the github wiki page. No need to list maven coordinates.",NULL,"A minimal project page of a top level project page that has basic information of docs and links to the github wiki page<span class='highlight-text severity-high'>. No need to list maven coordinates.</span>","minimal","punctuation","high",False
19709,"Document how to take an existing input output channel adapters in spring integration and add them as a XD source sink module. Should be as end user focused, step by step guide as possible. Consider including a getting started gradle pom.xml ",NULL,"Document how to take an existing input output channel adapters in spring integration and add them as a XD source sink module. Should be as end user focused, step by step guide as possible. Consider including a getting started gradle pom.xml ",NULL,"Add for who this story is","well_formed","no_role","high",False
19709,"Document how to take an existing input output channel adapters in spring integration and add them as a XD source sink module. Should be as end user focused, step by step guide as possible. Consider including a getting started gradle pom.xml ",NULL,"Document how to take an existing input output channel adapters in spring integration and add them as a XD source sink module. Should be as end user focused, step by step guide as possible. Consider including a getting started gradle pom.xml ",NULL,"Document how to take an existing input output channel adapters in spring integration<span class='highlight-text severity-high'> and </span>add them as a XD source sink module. Should be as end user focused, step by step guide as possible. Consider including a getting started gradle pom.xml ","atomic","conjunctions","high",False
19709,"Document how to take an existing input output channel adapters in spring integration and add them as a XD source sink module. Should be as end user focused, step by step guide as possible. Consider including a getting started gradle pom.xml ",NULL,"Document how to take an existing input output channel adapters in spring integration and add them as a XD source sink module. Should be as end user focused, step by step guide as possible. Consider including a getting started gradle pom.xml ",NULL,"Document how to take an existing input output channel adapters in spring integration and add them as a XD source sink module<span class='highlight-text severity-high'>. Should be as end user focused, step by step guide as possible. Consider including a getting started gradle pom.xml </span>","minimal","punctuation","high",False
19716,"example code a b c d code ...where b and c modules are deployed together as a composite module. There are 2 options maybe more for how we could handle that. One would be defining a CompositeModule type that simply bridges the channels b s output to c s input in this example . The second option would be to deploy those together on the same node as modules but using the LocalChannelRegistry between them. ",NULL,"example code a b c d code ...where b and c modules are deployed together as a composite module. There are 2 options maybe more for how we could handle that. One would be defining a CompositeModule type that simply bridges the channels b s output to c s input in this example . The second option would be to deploy those together on the same node as modules but using the LocalChannelRegistry between them. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19716,"example code a b c d code ...where b and c modules are deployed together as a composite module. There are 2 options maybe more for how we could handle that. One would be defining a CompositeModule type that simply bridges the channels b s output to c s input in this example . The second option would be to deploy those together on the same node as modules but using the LocalChannelRegistry between them. ",NULL,"example code a b c d code ...where b and c modules are deployed together as a composite module. There are 2 options maybe more for how we could handle that. One would be defining a CompositeModule type that simply bridges the channels b s output to c s input in this example . The second option would be to deploy those together on the same node as modules but using the LocalChannelRegistry between them. ",NULL,"example code a b c d code <span class='highlight-text severity-high'>...where b and c modules are deployed together as a composite module. There are 2 options maybe more for how we could handle that. One would be defining a CompositeModule type that simply bridges the channels b s output to c s input in this example . The second option would be to deploy those together on the same node as modules but using the LocalChannelRegistry between them. </span>","minimal","punctuation","high",False
19717,"The conversion should be based on content type headers, similar to the way Spring s HttpMessageConverters work with mime types . Also, the map of available converters should be extensible while including the most common defaults for JSON, XML, etc . We most likely want to add a few of our own content types also e.g. for Tuples . Most likely, this logic and the configuration methods for extending the converter map, belong in AbstractChannelRegistry since it should be common across all implementations i.e. the logic should be the same regardless of the transport used after serialization before deserialization .",NULL,"The conversion should be based on content type headers, similar to the way Spring s HttpMessageConverters work with mime types . Also, the map of available converters should be extensible while including the most common defaults for JSON, XML, etc . We most likely want to add a few of our own content types also e.g. for Tuples . Most likely, this logic and the configuration methods for extending the converter map, belong in AbstractChannelRegistry since it should be common across all implementations i.e. the logic should be the same regardless of the transport used after serialization before deserialization .",NULL,"Add for who this story is","well_formed","no_role","high",False
19717,"The conversion should be based on content type headers, similar to the way Spring s HttpMessageConverters work with mime types . Also, the map of available converters should be extensible while including the most common defaults for JSON, XML, etc . We most likely want to add a few of our own content types also e.g. for Tuples . Most likely, this logic and the configuration methods for extending the converter map, belong in AbstractChannelRegistry since it should be common across all implementations i.e. the logic should be the same regardless of the transport used after serialization before deserialization .",NULL,"The conversion should be based on content type headers, similar to the way Spring s HttpMessageConverters work with mime types . Also, the map of available converters should be extensible while including the most common defaults for JSON, XML, etc . We most likely want to add a few of our own content types also e.g. for Tuples . Most likely, this logic and the configuration methods for extending the converter map, belong in AbstractChannelRegistry since it should be common across all implementations i.e. the logic should be the same regardless of the transport used after serialization before deserialization .",NULL,"The conversion should be based on content type headers, similar to the way Spring s HttpMessageConverters work with mime types . Also, the map of available converters should be extensible while including the most common defaults for JSON, XML, etc . We most likely want to add a few of our own content types also e.g. for Tuples . Most likely, this logic<span class='highlight-text severity-high'> and </span>the configuration methods for extending the converter map, belong in AbstractChannelRegistry since it should be common across all implementations i.e. the logic should be the same regardless of the transport used after serialization before deserialization .","atomic","conjunctions","high",False
19717,"The conversion should be based on content type headers, similar to the way Spring s HttpMessageConverters work with mime types . Also, the map of available converters should be extensible while including the most common defaults for JSON, XML, etc . We most likely want to add a few of our own content types also e.g. for Tuples . Most likely, this logic and the configuration methods for extending the converter map, belong in AbstractChannelRegistry since it should be common across all implementations i.e. the logic should be the same regardless of the transport used after serialization before deserialization .",NULL,"The conversion should be based on content type headers, similar to the way Spring s HttpMessageConverters work with mime types . Also, the map of available converters should be extensible while including the most common defaults for JSON, XML, etc . We most likely want to add a few of our own content types also e.g. for Tuples . Most likely, this logic and the configuration methods for extending the converter map, belong in AbstractChannelRegistry since it should be common across all implementations i.e. the logic should be the same regardless of the transport used after serialization before deserialization .",NULL,"The conversion should be based on content type headers, similar to the way Spring s HttpMessageConverters work with mime types <span class='highlight-text severity-high'>. Also, the map of available converters should be extensible while including the most common defaults for JSON, XML, etc . We most likely want to add a few of our own content types also e.g. for Tuples . Most likely, this logic and the configuration methods for extending the converter map, belong in AbstractChannelRegistry since it should be common across all implementations i.e. the logic should be the same regardless of the transport used after serialization before deserialization .</span>","minimal","punctuation","high",False
19718,"The wiki repo contains a script, gen docs.sh, that we are planning to use to generate a pretty HTML version of the Getting Started guide. We should consider using maven or gradle, but there is currently an issue documented in build.gradle to generate this and other reference docs and publish them automatically as part of a nightly build.",NULL,"The wiki repo contains a script, gen docs.sh, that we are planning to use to generate a pretty HTML version of the Getting Started guide. We should consider using maven or gradle, but there is currently an issue documented in build.gradle to generate this and other reference docs and publish them automatically as part of a nightly build.",NULL,"Add for who this story is","well_formed","no_role","high",False
19718,"The wiki repo contains a script, gen docs.sh, that we are planning to use to generate a pretty HTML version of the Getting Started guide. We should consider using maven or gradle, but there is currently an issue documented in build.gradle to generate this and other reference docs and publish them automatically as part of a nightly build.",NULL,"The wiki repo contains a script, gen docs.sh, that we are planning to use to generate a pretty HTML version of the Getting Started guide. We should consider using maven or gradle, but there is currently an issue documented in build.gradle to generate this and other reference docs and publish them automatically as part of a nightly build.",NULL,"The wiki repo contains a script, gen docs.sh, that we are planning to use to generate a pretty HTML version of the Getting Started guide. We should consider using maven<span class='highlight-text severity-high'> or </span>gradle, but there is currently an issue documented in build.gradle to generate this<span class='highlight-text severity-high'> and </span>other reference docs and publish them automatically as part of a nightly build.","atomic","conjunctions","high",False
19718,"The wiki repo contains a script, gen docs.sh, that we are planning to use to generate a pretty HTML version of the Getting Started guide. We should consider using maven or gradle, but there is currently an issue documented in build.gradle to generate this and other reference docs and publish them automatically as part of a nightly build.",NULL,"The wiki repo contains a script, gen docs.sh, that we are planning to use to generate a pretty HTML version of the Getting Started guide. We should consider using maven or gradle, but there is currently an issue documented in build.gradle to generate this and other reference docs and publish them automatically as part of a nightly build.",NULL,"The wiki repo contains a script, gen docs<span class='highlight-text severity-high'>.sh, that we are planning to use to generate a pretty HTML version of the Getting Started guide. We should consider using maven or gradle, but there is currently an issue documented in build.gradle to generate this and other reference docs and publish them automatically as part of a nightly build.</span>","minimal","punctuation","high",False
19719,"Parameter parsing does not work if an argument contains . For example code ... transform expression 42 transform expression payload ... code Also, I was surprised that this worked.. code transform expression new StringBuilder payload .reverse code ... but this didn t... code transform expression new StringBuilder payload .reverse code I think we need to tokenize the argument with if contains spaces and remove any surrounding ... from the result. This means if someone wants a SpEL literal they would have to use something like code expression Hello, world! code resulting in a SpEL literal Hello, world! ",NULL,"Parameter parsing does not work if an argument contains . For example code ... transform expression 42 transform expression payload ... code Also, I was surprised that this worked.. code transform expression new StringBuilder payload .reverse code ... but this didn t... code transform expression new StringBuilder payload .reverse code I think we need to tokenize the argument with if contains spaces and remove any surrounding ... from the result. This means if someone wants a SpEL literal they would have to use something like code expression Hello, world! code resulting in a SpEL literal Hello, world! ",NULL,"Add for who this story is","well_formed","no_role","high",False
19719,"Parameter parsing does not work if an argument contains . For example code ... transform expression 42 transform expression payload ... code Also, I was surprised that this worked.. code transform expression new StringBuilder payload .reverse code ... but this didn t... code transform expression new StringBuilder payload .reverse code I think we need to tokenize the argument with if contains spaces and remove any surrounding ... from the result. This means if someone wants a SpEL literal they would have to use something like code expression Hello, world! code resulting in a SpEL literal Hello, world! ",NULL,"Parameter parsing does not work if an argument contains . For example code ... transform expression 42 transform expression payload ... code Also, I was surprised that this worked.. code transform expression new StringBuilder payload .reverse code ... but this didn t... code transform expression new StringBuilder payload .reverse code I think we need to tokenize the argument with if contains spaces and remove any surrounding ... from the result. This means if someone wants a SpEL literal they would have to use something like code expression Hello, world! code resulting in a SpEL literal Hello, world! ",NULL,"Parameter parsing does not work if an argument contains . For example code ... transform expression 42 transform expression payload ... code Also, I was surprised that this worked.. code transform expression new StringBuilder payload .reverse code ... but this didn t... code transform expression new StringBuilder payload .reverse code I think we need to tokenize the argument with if contains spaces<span class='highlight-text severity-high'> and </span>remove any surrounding ... from the result. This means if someone wants a SpEL literal they would have to use something like code expression Hello, world! code resulting in a SpEL literal Hello, world! ","atomic","conjunctions","high",False
19719,"Parameter parsing does not work if an argument contains . For example code ... transform expression 42 transform expression payload ... code Also, I was surprised that this worked.. code transform expression new StringBuilder payload .reverse code ... but this didn t... code transform expression new StringBuilder payload .reverse code I think we need to tokenize the argument with if contains spaces and remove any surrounding ... from the result. This means if someone wants a SpEL literal they would have to use something like code expression Hello, world! code resulting in a SpEL literal Hello, world! ",NULL,"Parameter parsing does not work if an argument contains . For example code ... transform expression 42 transform expression payload ... code Also, I was surprised that this worked.. code transform expression new StringBuilder payload .reverse code ... but this didn t... code transform expression new StringBuilder payload .reverse code I think we need to tokenize the argument with if contains spaces and remove any surrounding ... from the result. This means if someone wants a SpEL literal they would have to use something like code expression Hello, world! code resulting in a SpEL literal Hello, world! ",NULL,"Parameter parsing does not work if an argument contains <span class='highlight-text severity-high'>. For example code ... transform expression 42 transform expression payload ... code Also, I was surprised that this worked.. code transform expression new StringBuilder payload .reverse code ... but this didn t... code transform expression new StringBuilder payload .reverse code I think we need to tokenize the argument with if contains spaces and remove any surrounding ... from the result. This means if someone wants a SpEL literal they would have to use something like code expression Hello, world! code resulting in a SpEL literal Hello, world! </span>","minimal","punctuation","high",False
19734,"The ModuleDeployer calls getBeansOfType before the context has had its PropertySourcesPlaceholderConfigurer attached. This can cause issues with FactoryBean s with placeholders in constructor args because the unresolved placeholder is used when the FactoryBean is pre instantiated to determine the type of object it will serve up.",NULL,"The ModuleDeployer calls getBeansOfType before the context has had its PropertySourcesPlaceholderConfigurer attached. This can cause issues with FactoryBean s with placeholders in constructor args because the unresolved placeholder is used when the FactoryBean is pre instantiated to determine the type of object it will serve up.",NULL,"Add for who this story is","well_formed","no_role","high",False
19734,"The ModuleDeployer calls getBeansOfType before the context has had its PropertySourcesPlaceholderConfigurer attached. This can cause issues with FactoryBean s with placeholders in constructor args because the unresolved placeholder is used when the FactoryBean is pre instantiated to determine the type of object it will serve up.",NULL,"The ModuleDeployer calls getBeansOfType before the context has had its PropertySourcesPlaceholderConfigurer attached. This can cause issues with FactoryBean s with placeholders in constructor args because the unresolved placeholder is used when the FactoryBean is pre instantiated to determine the type of object it will serve up.",NULL,"The ModuleDeployer calls getBeansOfType before the context has had its PropertySourcesPlaceholderConfigurer attached<span class='highlight-text severity-high'>. This can cause issues with FactoryBean s with placeholders in constructor args because the unresolved placeholder is used when the FactoryBean is pre instantiated to determine the type of object it will serve up.</span>","minimal","punctuation","high",False
19751,"It should provide an expression param for SpEL and have a default pass thru of the payload.",NULL,"It should provide an expression param for SpEL and have a default pass thru of the payload.",NULL,"Add for who this story is","well_formed","no_role","high",False
19751,"It should provide an expression param for SpEL and have a default pass thru of the payload.",NULL,"It should provide an expression param for SpEL and have a default pass thru of the payload.",NULL,"It should provide an expression param for SpEL<span class='highlight-text severity-high'> and </span>have a default pass thru of the payload.","atomic","conjunctions","high",False
18221,"Currently we deploy a single instance, and ignore the ModuleDeploymentRequest instances setting. It is easy to change this in the ModuleDeployer, but there is no guarantee this will work in the ModuleLauncher, so we hold off until that can be verified.",NULL,"Currently we deploy a single instance, and ignore the ModuleDeploymentRequest instances setting. It is easy to change this in the ModuleDeployer, but there is no guarantee this will work in the ModuleLauncher,","so we hold off until that can be verified.","Currently we deploy a single instance,<span class='highlight-text severity-high'> and </span>ignore the ModuleDeploymentRequest instances setting. It is easy to change this in the ModuleDeployer, but there is no guarantee this will work in the ModuleLauncher, so we hold off until that can be verified.","atomic","conjunctions","high",False
18221,"Currently we deploy a single instance, and ignore the ModuleDeploymentRequest instances setting. It is easy to change this in the ModuleDeployer, but there is no guarantee this will work in the ModuleLauncher, so we hold off until that can be verified.",NULL,"Currently we deploy a single instance, and ignore the ModuleDeploymentRequest instances setting. It is easy to change this in the ModuleDeployer, but there is no guarantee this will work in the ModuleLauncher,","so we hold off until that can be verified.","Currently we deploy a single instance, and ignore the ModuleDeploymentRequest instances setting<span class='highlight-text severity-high'>. It is easy to change this in the ModuleDeployer, but there is no guarantee this will work in the ModuleLauncher, so we hold off until that can be verified.</span>","minimal","punctuation","high",False
18221,"Currently we deploy a single instance, and ignore the ModuleDeploymentRequest instances setting. It is easy to change this in the ModuleDeployer, but there is no guarantee this will work in the ModuleLauncher, so we hold off until that can be verified.",NULL,"Currently we deploy a single instance, and ignore the ModuleDeploymentRequest instances setting. It is easy to change this in the ModuleDeployer, but there is no guarantee this will work in the ModuleLauncher,","so we hold off until that can be verified.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19715,"Validate that modules have required channels declared according to their type. Currently the stream deployer accepts processors with no input, but the stream doesn t complete. We should fail earlier and more loudly.",NULL,"Validate that modules have required channels declared according to their type. Currently the stream deployer accepts processors with no input, but the stream doesn t complete. We should fail earlier and more loudly.",NULL,"Add for who this story is","well_formed","no_role","high",False
19715,"Validate that modules have required channels declared according to their type. Currently the stream deployer accepts processors with no input, but the stream doesn t complete. We should fail earlier and more loudly.",NULL,"Validate that modules have required channels declared according to their type. Currently the stream deployer accepts processors with no input, but the stream doesn t complete. We should fail earlier and more loudly.",NULL,"Validate that modules have required channels declared according to their type<span class='highlight-text severity-high'>. Currently the stream deployer accepts processors with no input, but the stream doesn t complete. We should fail earlier and more loudly.</span>","minimal","punctuation","high",False
19749,"This will enable the use of groovy scripts within modules.",NULL,"This will enable the use of groovy scripts within modules.",NULL,"Add for who this story is","well_formed","no_role","high",False
19723,"A processor module that accepts either the location of a groovy script resource or an inline script string . Also some discussion about a default classpath location for scripts. ",NULL,"A processor module that accepts either the location of a groovy script resource or an inline script string . Also some discussion about a default classpath location for scripts. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19723,"A processor module that accepts either the location of a groovy script resource or an inline script string . Also some discussion about a default classpath location for scripts. ",NULL,"A processor module that accepts either the location of a groovy script resource or an inline script string . Also some discussion about a default classpath location for scripts. ",NULL,"A processor module that accepts either the location of a groovy script resource or an inline script string <span class='highlight-text severity-high'>. Also some discussion about a default classpath location for scripts. </span>","minimal","punctuation","high",False
19726,"Spring config for rich gauge plus message handler to coerce a numeric or string payload to a double.",NULL,"Spring config for rich gauge plus message handler to coerce a numeric or string payload to a double.",NULL,"Add for who this story is","well_formed","no_role","high",False
19727,"Presently, Spring XD does not ship Windows binaries for Redis. However, Microsoft is actively working 1 on supporting Redis on Windows. You can download Windows Redis binaries from https github.com MSOpenTech redis tree 2.6 bin release 1 http blogs.msdn.com b interoperability archive 2013 04 22 redis on windows stable and reliable.aspx",NULL,"Presently, Spring XD does not ship Windows binaries for Redis. However, Microsoft is actively working 1 on supporting Redis on Windows. You can download Windows Redis binaries from https github.com MSOpenTech redis tree 2.6 bin release 1 http blogs.msdn.com b interoperability archive 2013 04 22 redis on windows stable and reliable.aspx",NULL,"Add for who this story is","well_formed","no_role","high",False
19727,"Presently, Spring XD does not ship Windows binaries for Redis. However, Microsoft is actively working 1 on supporting Redis on Windows. You can download Windows Redis binaries from https github.com MSOpenTech redis tree 2.6 bin release 1 http blogs.msdn.com b interoperability archive 2013 04 22 redis on windows stable and reliable.aspx",NULL,"Presently, Spring XD does not ship Windows binaries for Redis. However, Microsoft is actively working 1 on supporting Redis on Windows. You can download Windows Redis binaries from https github.com MSOpenTech redis tree 2.6 bin release 1 http blogs.msdn.com b interoperability archive 2013 04 22 redis on windows stable and reliable.aspx",NULL,"Presently, Spring XD does not ship Windows binaries for Redis<span class='highlight-text severity-high'>. However, Microsoft is actively working 1 on supporting Redis on Windows. You can download Windows Redis binaries from https github.com MSOpenTech redis tree 2.6 bin release 1 http blogs.msdn.com b interoperability archive 2013 04 22 redis on windows stable and reliable.aspx</span>","minimal","punctuation","high",False
19725,"Spring config for simple gauge plus message handler to process message. There is some code common to RichGaugeHandler to coerce the payload to a double that should be refactored for reuse.",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19725,"Spring config for simple gauge plus message handler to process message. There is some code common to RichGaugeHandler to coerce the payload to a double that should be refactored for reuse.",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19725,"Spring config for simple gauge plus message handler to process message. There is some code common to RichGaugeHandler to coerce the payload to a double that should be refactored for reuse.",NULL,NULL,NULL,"Spring config for simple gauge plus message handler to process message<span class='highlight-text severity-high'>. There is some code common to RichGaugeHandler to coerce the payload to a double that should be refactored for reuse.</span>","minimal","punctuation","high",False
19730,"Currently redis project uses application plugin to bundle distribution. This also includes java plugin which causes java specific build behavior on this project. We should try removing the use of application plugin and use something similar or custom tasks that does the bundling. ",NULL,"Currently redis project uses application plugin to bundle distribution. This also includes java plugin which causes java specific build behavior on this project. We should try removing the use of application plugin and use something similar or custom tasks that does the bundling. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19730,"Currently redis project uses application plugin to bundle distribution. This also includes java plugin which causes java specific build behavior on this project. We should try removing the use of application plugin and use something similar or custom tasks that does the bundling. ",NULL,"Currently redis project uses application plugin to bundle distribution. This also includes java plugin which causes java specific build behavior on this project. We should try removing the use of application plugin and use something similar or custom tasks that does the bundling. ",NULL,"Currently redis project uses application plugin to bundle distribution. This also includes java plugin which causes java specific build behavior on this project. We should try removing the use of application plugin<span class='highlight-text severity-high'> and </span>use something similar<span class='highlight-text severity-high'> or </span>custom tasks that does the bundling. ","atomic","conjunctions","high",False
19730,"Currently redis project uses application plugin to bundle distribution. This also includes java plugin which causes java specific build behavior on this project. We should try removing the use of application plugin and use something similar or custom tasks that does the bundling. ",NULL,"Currently redis project uses application plugin to bundle distribution. This also includes java plugin which causes java specific build behavior on this project. We should try removing the use of application plugin and use something similar or custom tasks that does the bundling. ",NULL,"Currently redis project uses application plugin to bundle distribution<span class='highlight-text severity-high'>. This also includes java plugin which causes java specific build behavior on this project. We should try removing the use of application plugin and use something similar or custom tasks that does the bundling. </span>","minimal","punctuation","high",False
19728,"Currently, Bamboo s gradle artifactory plugin has the artifacts configured to projects target build directory archives . We need to have a way to set the final distribution archive as one of the gradle configurations in our build.gradle and refer it inside bamboo artifacts.",NULL,"Currently, Bamboo s gradle artifactory plugin has the artifacts configured to projects target build directory archives . We need to have a way to set the final distribution archive as one of the gradle configurations in our build.gradle and refer it inside bamboo artifacts.",NULL,"Add for who this story is","well_formed","no_role","high",False
19728,"Currently, Bamboo s gradle artifactory plugin has the artifacts configured to projects target build directory archives . We need to have a way to set the final distribution archive as one of the gradle configurations in our build.gradle and refer it inside bamboo artifacts.",NULL,"Currently, Bamboo s gradle artifactory plugin has the artifacts configured to projects target build directory archives . We need to have a way to set the final distribution archive as one of the gradle configurations in our build.gradle and refer it inside bamboo artifacts.",NULL,"Currently, Bamboo s gradle artifactory plugin has the artifacts configured to projects target build directory archives . We need to have a way to set the final distribution archive as one of the gradle configurations in our build.gradle<span class='highlight-text severity-high'> and </span>refer it inside bamboo artifacts.","atomic","conjunctions","high",False
19728,"Currently, Bamboo s gradle artifactory plugin has the artifacts configured to projects target build directory archives . We need to have a way to set the final distribution archive as one of the gradle configurations in our build.gradle and refer it inside bamboo artifacts.",NULL,"Currently, Bamboo s gradle artifactory plugin has the artifacts configured to projects target build directory archives . We need to have a way to set the final distribution archive as one of the gradle configurations in our build.gradle and refer it inside bamboo artifacts.",NULL,"Currently, Bamboo s gradle artifactory plugin has the artifacts configured to projects target build directory archives <span class='highlight-text severity-high'>. We need to have a way to set the final distribution archive as one of the gradle configurations in our build.gradle and refer it inside bamboo artifacts.</span>","minimal","punctuation","high",False
19729,"Publishing an html version of the guide that uses the toc2 style format, table of contents on the left. Looks like a stylesheet factory http asciidoctor.org docs produce custom themes using asciidoctor stylesheet factory needs to be installed. From the theme showcase, http themes.asciidoctor.org preview , the golo theme has a toc2 style. In the root of the git repo for the wiki is a build.gradle file that uses the asciidoctor gradle plugin, but it doesn t support using a single file with import as input. See See https github.com asciidoctor asciidoctor gradle plugin issues 15 The mvn plugin does support this, so maybe using mvn is an option or just a bash script.",NULL,"Publishing an html version of the guide that uses the toc2 style format, table of contents on the left. Looks like a stylesheet factory http asciidoctor.org docs produce custom themes using asciidoctor stylesheet factory needs to be installed. From the theme showcase, http themes.asciidoctor.org preview , the golo theme has a toc2 style. In the root of the git repo for the wiki is a build.gradle file that uses the asciidoctor gradle plugin, but it doesn t support using a single file with import as input. See See https github.com asciidoctor asciidoctor gradle plugin issues 15 The mvn plugin does support this,","so maybe using mvn is an option or just a bash script.","Add for who this story is","well_formed","no_role","high",False
19729,"Publishing an html version of the guide that uses the toc2 style format, table of contents on the left. Looks like a stylesheet factory http asciidoctor.org docs produce custom themes using asciidoctor stylesheet factory needs to be installed. From the theme showcase, http themes.asciidoctor.org preview , the golo theme has a toc2 style. In the root of the git repo for the wiki is a build.gradle file that uses the asciidoctor gradle plugin, but it doesn t support using a single file with import as input. See See https github.com asciidoctor asciidoctor gradle plugin issues 15 The mvn plugin does support this, so maybe using mvn is an option or just a bash script.",NULL,"Publishing an html version of the guide that uses the toc2 style format, table of contents on the left. Looks like a stylesheet factory http asciidoctor.org docs produce custom themes using asciidoctor stylesheet factory needs to be installed. From the theme showcase, http themes.asciidoctor.org preview , the golo theme has a toc2 style. In the root of the git repo for the wiki is a build.gradle file that uses the asciidoctor gradle plugin, but it doesn t support using a single file with import as input. See See https github.com asciidoctor asciidoctor gradle plugin issues 15 The mvn plugin does support this,","so maybe using mvn is an option or just a bash script.","Publishing an html version of the guide that uses the toc2 style format, table of contents on the left<span class='highlight-text severity-high'>. Looks like a stylesheet factory http asciidoctor.org docs produce custom themes using asciidoctor stylesheet factory needs to be installed. From the theme showcase, http themes.asciidoctor.org preview , the golo theme has a toc2 style. In the root of the git repo for the wiki is a build.gradle file that uses the asciidoctor gradle plugin, but it doesn t support using a single file with import as input. See See https github.com asciidoctor asciidoctor gradle plugin issues 15 The mvn plugin does support this, so maybe using mvn is an option or just a bash script.</span>","minimal","punctuation","high",False
19729,"Publishing an html version of the guide that uses the toc2 style format, table of contents on the left. Looks like a stylesheet factory http asciidoctor.org docs produce custom themes using asciidoctor stylesheet factory needs to be installed. From the theme showcase, http themes.asciidoctor.org preview , the golo theme has a toc2 style. In the root of the git repo for the wiki is a build.gradle file that uses the asciidoctor gradle plugin, but it doesn t support using a single file with import as input. See See https github.com asciidoctor asciidoctor gradle plugin issues 15 The mvn plugin does support this, so maybe using mvn is an option or just a bash script.",NULL,"Publishing an html version of the guide that uses the toc2 style format, table of contents on the left. Looks like a stylesheet factory http asciidoctor.org docs produce custom themes using asciidoctor stylesheet factory needs to be installed. From the theme showcase, http themes.asciidoctor.org preview , the golo theme has a toc2 style. In the root of the git repo for the wiki is a build.gradle file that uses the asciidoctor gradle plugin, but it doesn t support using a single file with import as input. See See https github.com asciidoctor asciidoctor gradle plugin issues 15 The mvn plugin does support this,","so maybe using mvn is an option or just a bash script.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19721,"In both cases there are multiple application contexts that can be running in the process. The JMX Managed bean should call close on those application contexts. The more detailed lifecycle of cleanly shutting down components within those application contexts is another story.",NULL,"In both cases there are multiple application contexts that can be running in the process. The JMX Managed bean should call close on those application contexts. The more detailed lifecycle of cleanly shutting down components within those application contexts is another story.",NULL,"Add for who this story is","well_formed","no_role","high",False
19721,"In both cases there are multiple application contexts that can be running in the process. The JMX Managed bean should call close on those application contexts. The more detailed lifecycle of cleanly shutting down components within those application contexts is another story.",NULL,"In both cases there are multiple application contexts that can be running in the process. The JMX Managed bean should call close on those application contexts. The more detailed lifecycle of cleanly shutting down components within those application contexts is another story.",NULL,"In both cases there are multiple application contexts that can be running in the process<span class='highlight-text severity-high'>. The JMX Managed bean should call close on those application contexts. The more detailed lifecycle of cleanly shutting down components within those application contexts is another story.</span>","minimal","punctuation","high",False
19722,"We would like to have Redis driven from a config property file under XD HOME.",NULL,"We would like to have Redis driven from a config property file under XD HOME.",NULL,"Add for who this story is","well_formed","no_role","high",False
19759,"This will launch the RedisContainerLauncher, in future will be able to select from a variety of middleware options.",NULL,"This will launch the RedisContainerLauncher, in future will be able to select from a variety of middleware options.",NULL,"Add for who this story is","well_formed","no_role","high",False
19755,"A ctrl c of xd admin results in exception messages about disconnecting from redis. 14 16 07,327 ERROR task scheduler 1 handler.LoggingHandler 136 org.springframework.data.redis.RedisSystemException Redis command interrupted; nested exception is com.lambdaworks.redis.RedisCommandInterruptedException Command interrupted ",NULL,"A ctrl c of xd admin results in exception messages about disconnecting from redis. 14 16 07,327 ERROR task scheduler 1 handler.LoggingHandler 136 org.springframework.data.redis.RedisSystemException Redis command interrupted; nested exception is com.lambdaworks.redis.RedisCommandInterruptedException Command interrupted ",NULL,"Add for who this story is","well_formed","no_role","high",False
19755,"A ctrl c of xd admin results in exception messages about disconnecting from redis. 14 16 07,327 ERROR task scheduler 1 handler.LoggingHandler 136 org.springframework.data.redis.RedisSystemException Redis command interrupted; nested exception is com.lambdaworks.redis.RedisCommandInterruptedException Command interrupted ",NULL,"A ctrl c of xd admin results in exception messages about disconnecting from redis. 14 16 07,327 ERROR task scheduler 1 handler.LoggingHandler 136 org.springframework.data.redis.RedisSystemException Redis command interrupted; nested exception is com.lambdaworks.redis.RedisCommandInterruptedException Command interrupted ",NULL,"A ctrl c of xd admin results in exception messages about disconnecting from redis<span class='highlight-text severity-high'>. 14 16 07,327 ERROR task scheduler 1 handler.LoggingHandler 136 org.springframework.data.redis.RedisSystemException Redis command interrupted; nested exception is com.lambdaworks.redis.RedisCommandInterruptedException Command interrupted </span>","minimal","punctuation","high",False
19756,". xd container processing module Module name file, type sink from group tailtest with index 1 processing module Module name tail, type source from group tailtest with index 0 Logging of processing module should have log level, time..",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19756,". xd container processing module Module name file, type sink from group tailtest with index 1 processing module Module name tail, type source from group tailtest with index 0 Logging of processing module should have log level, time..",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19756,". xd container processing module Module name file, type sink from group tailtest with index 1 processing module Module name tail, type source from group tailtest with index 0 Logging of processing module should have log level, time..",NULL,NULL,NULL,"<span class='highlight-text severity-high'>. xd container processing module Module name file, type sink from group tailtest with index 1 processing module Module name tail, type source from group tailtest with index 0 Logging of processing module should have log level, time..</span>","minimal","punctuation","high",False
19766,"in additional to existing tests that check for redis connection, we need to add tests that start stop stream server. ",NULL,"in additional to existing tests that check for redis connection, we need to add tests that start stop stream server. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19760,"Provide optional command line arg to embed the container launcher, aka xd admin server. XDContainer.sh embeddAdmin",NULL,"Provide optional command line arg to embed the container launcher, aka xd admin server. XDContainer.sh embeddAdmin",NULL,"Provide optional command line arg to embed the container launcher, aka xd admin server<span class='highlight-text severity-high'>. XDContainer.sh embeddAdmin</span>","minimal","punctuation","high",False
19764,"Put on the guide as a section in an streams wiki page. End user focused, no need to mention spring underpinning, impl details. ",NULL,"Put on the guide as a section in an streams wiki page. End user focused, no need to mention spring underpinning, impl details. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19764,"Put on the guide as a section in an streams wiki page. End user focused, no need to mention spring underpinning, impl details. ",NULL,"Put on the guide as a section in an streams wiki page. End user focused, no need to mention spring underpinning, impl details. ",NULL,"Put on the guide as a section in an streams wiki page<span class='highlight-text severity-high'>. End user focused, no need to mention spring underpinning, impl details. </span>","minimal","punctuation","high",False
19761,"https build.springsource.org browse XD SONAR 34 Caused by java.lang.ClassNotFoundException org.sonar.api.Plugin at org.codehaus.plexus.classworlds.strategy.SelfFirstStrategy.loadClass SelfFirstStrategy.java 50 at org.codehaus.plexus.classworlds.realm.ClassRealm.loadClass ClassRealm.java 244 at org.codehaus.plexus.classworlds.realm.ClassRealm.loadClass ClassRealm.java 230 ... 94 more",NULL,"https build.springsource.org browse XD SONAR 34 Caused by java.lang.ClassNotFoundException org.sonar.api.Plugin at org.codehaus.plexus.classworlds.strategy.SelfFirstStrategy.loadClass SelfFirstStrategy.java 50 at org.codehaus.plexus.classworlds.realm.ClassRealm.loadClass ClassRealm.java 244 at org.codehaus.plexus.classworlds.realm.ClassRealm.loadClass ClassRealm.java 230 ... 94 more",NULL,"Add for who this story is","well_formed","no_role","high",False
19761,"https build.springsource.org browse XD SONAR 34 Caused by java.lang.ClassNotFoundException org.sonar.api.Plugin at org.codehaus.plexus.classworlds.strategy.SelfFirstStrategy.loadClass SelfFirstStrategy.java 50 at org.codehaus.plexus.classworlds.realm.ClassRealm.loadClass ClassRealm.java 244 at org.codehaus.plexus.classworlds.realm.ClassRealm.loadClass ClassRealm.java 230 ... 94 more",NULL,"https build.springsource.org browse XD SONAR 34 Caused by java.lang.ClassNotFoundException org.sonar.api.Plugin at org.codehaus.plexus.classworlds.strategy.SelfFirstStrategy.loadClass SelfFirstStrategy.java 50 at org.codehaus.plexus.classworlds.realm.ClassRealm.loadClass ClassRealm.java 244 at org.codehaus.plexus.classworlds.realm.ClassRealm.loadClass ClassRealm.java 230 ... 94 more",NULL,"https build<span class='highlight-text severity-high'>.springsource.org browse XD SONAR 34 Caused by java.lang.ClassNotFoundException org.sonar.api.Plugin at org.codehaus.plexus.classworlds.strategy.SelfFirstStrategy.loadClass SelfFirstStrategy.java 50 at org.codehaus.plexus.classworlds.realm.ClassRealm.loadClass ClassRealm.java 244 at org.codehaus.plexus.classworlds.realm.ClassRealm.loadClass ClassRealm.java 230 ... 94 more</span>","minimal","punctuation","high",False
19762,"For people who are familiar with Spring Spring Integration provide documents that show how to add additional input sources sinks.",NULL,"For people who are familiar with Spring Spring Integration provide documents that show how to add additional input sources sinks.",NULL,"Add for who this story is","well_formed","no_role","high",False
19784,"There is duplicated code in Redis based repositories that related to expiry behavior, move into a common shared helper class and or base class.",NULL,"There is duplicated code in Redis based repositories that related to expiry behavior, move into a common shared helper class and or base class.",NULL,"Add for who this story is","well_formed","no_role","high",False
19784,"There is duplicated code in Redis based repositories that related to expiry behavior, move into a common shared helper class and or base class.",NULL,"There is duplicated code in Redis based repositories that related to expiry behavior, move into a common shared helper class and or base class.",NULL,"There is duplicated code in Redis based repositories that related to expiry behavior, move into a common shared helper class<span class='highlight-text severity-high'> and </span>span class='highlight-text severity-high'> or </span>base class.","atomic","conjunctions","high",False
19767,"stream should be able to ingest data from http ",NULL,"stream should be able to ingest data from http ",NULL,"Add for who this story is","well_formed","no_role","high",False
19765,"Put on the guide as a section in an input sources wiki page. https github.com springsource spring xd wiki GuideGettingStarted ",NULL,"Put on the guide as a section in an input sources wiki page. https github.com springsource spring xd wiki GuideGettingStarted ",NULL,"Add for who this story is","well_formed","no_role","high",False
19765,"Put on the guide as a section in an input sources wiki page. https github.com springsource spring xd wiki GuideGettingStarted ",NULL,"Put on the guide as a section in an input sources wiki page. https github.com springsource spring xd wiki GuideGettingStarted ",NULL,"Put on the guide as a section in an input sources wiki page<span class='highlight-text severity-high'>. https github.com springsource spring xd wiki GuideGettingStarted </span>","minimal","punctuation","high",False
19768,"The Java UUID class is known not to be the fasted implementation available. See https github.com stephenc eaio uuid and http mvnrepository.com artifact com.eaio.uuid uuid for high perf impls. ",NULL,"The Java UUID class is known not to be the fasted implementation available. See https github.com stephenc eaio uuid and http mvnrepository.com artifact com.eaio.uuid uuid for high perf impls. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19768,"The Java UUID class is known not to be the fasted implementation available. See https github.com stephenc eaio uuid and http mvnrepository.com artifact com.eaio.uuid uuid for high perf impls. ",NULL,"The Java UUID class is known not to be the fasted implementation available. See https github.com stephenc eaio uuid and http mvnrepository.com artifact com.eaio.uuid uuid for high perf impls. ",NULL,"The Java UUID class is known not to be the fasted implementation available. See https github.com stephenc eaio uuid<span class='highlight-text severity-high'> and </span>http mvnrepository.com artifact com.eaio.uuid uuid for high perf impls. ","atomic","conjunctions","high",False
19768,"The Java UUID class is known not to be the fasted implementation available. See https github.com stephenc eaio uuid and http mvnrepository.com artifact com.eaio.uuid uuid for high perf impls. ",NULL,"The Java UUID class is known not to be the fasted implementation available. See https github.com stephenc eaio uuid and http mvnrepository.com artifact com.eaio.uuid uuid for high perf impls. ",NULL,"The Java UUID class is known not to be the fasted implementation available<span class='highlight-text severity-high'>. See https github.com stephenc eaio uuid and http mvnrepository.com artifact com.eaio.uuid uuid for high perf impls. </span>","minimal","punctuation","high",False
19753,"This is in the AdminMain and ContainerMain. Can get the environment property directly in java code unless provided explicitly on the command line using xdHomeDir.",NULL,"This is in the AdminMain and ContainerMain. Can get the environment property directly in java code unless provided explicitly on the command line using xdHomeDir.",NULL,"Add for who this story is","well_formed","no_role","high",False
19753,"This is in the AdminMain and ContainerMain. Can get the environment property directly in java code unless provided explicitly on the command line using xdHomeDir.",NULL,"This is in the AdminMain and ContainerMain. Can get the environment property directly in java code unless provided explicitly on the command line using xdHomeDir.",NULL,"This is in the AdminMain and ContainerMain<span class='highlight-text severity-high'>. Can get the environment property directly in java code unless provided explicitly on the command line using xdHomeDir.</span>","minimal","punctuation","high",False
19757,"should contain apache licence",NULL,"should contain apache licence",NULL,"Add for who this story is","well_formed","no_role","high",False
19754,"We are packaging separate scripts to start XDAdmin and XDContainer. The Gradle application plugin will generate an unwanted spring xd dirt scripts, this should be removed from the bin directory when creating a distribution zip.",NULL,"We are packaging separate scripts to start XDAdmin and XDContainer. The Gradle application plugin will generate an unwanted spring xd dirt scripts, this should be removed from the bin directory when creating a distribution zip.",NULL,"Add for who this story is","well_formed","no_role","high",False
19754,"We are packaging separate scripts to start XDAdmin and XDContainer. The Gradle application plugin will generate an unwanted spring xd dirt scripts, this should be removed from the bin directory when creating a distribution zip.",NULL,"We are packaging separate scripts to start XDAdmin and XDContainer. The Gradle application plugin will generate an unwanted spring xd dirt scripts, this should be removed from the bin directory when creating a distribution zip.",NULL,"We are packaging separate scripts to start XDAdmin and XDContainer<span class='highlight-text severity-high'>. The Gradle application plugin will generate an unwanted spring xd dirt scripts, this should be removed from the bin directory when creating a distribution zip.</span>","minimal","punctuation","high",False
19763,"Put on the guide as a section in an input stream wiki page. ",NULL,"Put on the guide as a section in an input stream wiki page. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19758,"should explain basic layout of the distribution",NULL,"should explain basic layout of the distribution",NULL,"Add for who this story is","well_formed","no_role","high",False
19769,"Adopt Asciidoc as the markdown syntax, useful for generating pdf and more feature rich than standard github flavored markdown. Loosely following the conventions of https community.jboss.org wiki TheHolyGrailAsciiDocOnGitHubToDocBookTrain that have generate docbook pdf docs from the Asciidoc wiki. The asciidoctor project is a key element in the adoption of AsciiDoc for use as the format in github, it is the rendering engine used by github for AsciiDoc. See http asciidoctor.org docs asciidoc writers guide for guidance. ",NULL,"Adopt Asciidoc as the markdown syntax, useful for generating pdf and more feature rich than standard github flavored markdown. Loosely following the conventions of https community.jboss.org wiki TheHolyGrailAsciiDocOnGitHubToDocBookTrain that have generate docbook pdf docs from the Asciidoc wiki. The asciidoctor project is a key element in the adoption of AsciiDoc for use as the format in github, it is the rendering engine used by github for AsciiDoc. See http asciidoctor.org docs asciidoc writers guide for guidance. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19769,"Adopt Asciidoc as the markdown syntax, useful for generating pdf and more feature rich than standard github flavored markdown. Loosely following the conventions of https community.jboss.org wiki TheHolyGrailAsciiDocOnGitHubToDocBookTrain that have generate docbook pdf docs from the Asciidoc wiki. The asciidoctor project is a key element in the adoption of AsciiDoc for use as the format in github, it is the rendering engine used by github for AsciiDoc. See http asciidoctor.org docs asciidoc writers guide for guidance. ",NULL,"Adopt Asciidoc as the markdown syntax, useful for generating pdf and more feature rich than standard github flavored markdown. Loosely following the conventions of https community.jboss.org wiki TheHolyGrailAsciiDocOnGitHubToDocBookTrain that have generate docbook pdf docs from the Asciidoc wiki. The asciidoctor project is a key element in the adoption of AsciiDoc for use as the format in github, it is the rendering engine used by github for AsciiDoc. See http asciidoctor.org docs asciidoc writers guide for guidance. ",NULL,"Adopt Asciidoc as the markdown syntax, useful for generating pdf and more feature rich than standard github flavored markdown<span class='highlight-text severity-high'>. Loosely following the conventions of https community.jboss.org wiki TheHolyGrailAsciiDocOnGitHubToDocBookTrain that have generate docbook pdf docs from the Asciidoc wiki. The asciidoctor project is a key element in the adoption of AsciiDoc for use as the format in github, it is the rendering engine used by github for AsciiDoc. See http asciidoctor.org docs asciidoc writers guide for guidance. </span>","minimal","punctuation","high",False
19772,"Host the Spring XD distributable zip somewhere that is accessible by external http request. Create brew formula for Spring XD install while specifying redis as dependency. starting up stream server upon successful brew install couple of questions should we name the brew task springxd? name not taken yet should we start the stream server as part of the brew install process? should we specify redis as a recommended dependency? user can pass in brew install springxd without redis to skip redis installation. by default, brew install springxd will install redis as well.",NULL,"Host the Spring XD distributable zip somewhere that is accessible by external http request. Create brew formula for Spring XD install while specifying redis as dependency. starting up stream server upon successful brew install couple of questions should we name the brew task springxd? name not taken yet should we start the stream server as part of the brew install process? should we specify redis as a recommended dependency? user can pass in brew install springxd without redis to skip redis installation. by default, brew install springxd will install redis as well.",NULL,"Add for who this story is","well_formed","no_role","high",False
19772,"Host the Spring XD distributable zip somewhere that is accessible by external http request. Create brew formula for Spring XD install while specifying redis as dependency. starting up stream server upon successful brew install couple of questions should we name the brew task springxd? name not taken yet should we start the stream server as part of the brew install process? should we specify redis as a recommended dependency? user can pass in brew install springxd without redis to skip redis installation. by default, brew install springxd will install redis as well.",NULL,"Host the Spring XD distributable zip somewhere that is accessible by external http request. Create brew formula for Spring XD install while specifying redis as dependency. starting up stream server upon successful brew install couple of questions should we name the brew task springxd? name not taken yet should we start the stream server as part of the brew install process? should we specify redis as a recommended dependency? user can pass in brew install springxd without redis to skip redis installation. by default, brew install springxd will install redis as well.",NULL,"Host the Spring XD distributable zip somewhere that is accessible by external http request<span class='highlight-text severity-high'>. Create brew formula for Spring XD install while specifying redis as dependency. starting up stream server upon successful brew install couple of questions should we name the brew task springxd? name not taken yet should we start the stream server as part of the brew install process? should we specify redis as a recommended dependency? user can pass in brew install springxd without redis to skip redis installation. by default, brew install springxd will install redis as well.</span>","minimal","punctuation","high",False
19771,"Based on a single process running a Spring Batch job, support the ETL of data from HDFS to a RDBMS",NULL,"Based on a single process running a Spring Batch job, support the ETL of data from HDFS to a RDBMS",NULL,"Add for who this story is","well_formed","no_role","high",False
19773,"Update a gemfire region.",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19773,"Update a gemfire region.",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19780,"A Spring Integration based ServiceActivator that counts the number of messages using the Spring XD metrics support",NULL,"A Spring Integration based ServiceActivator that counts the number of messages using the Spring XD metrics support",NULL,"Add for who this story is","well_formed","no_role","high",False
19774,"Do not require a POJO in order to do end to end processing in a batch step.",NULL,"Do not require a POJO","in order to do end to end processing in a batch step.","Add for who this story is","well_formed","no_role","high",False
19774,"Do not require a POJO in order to do end to end processing in a batch step.",NULL,"Do not require a POJO","in order to do end to end processing in a batch step.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19777,"Nested tuple structures shoudl be supported, getTuple int index , getTuple String name ",NULL,"Nested tuple structures shoudl be supported, getTuple int index , getTuple String name ",NULL,"Add for who this story is","well_formed","no_role","high",False
19781,"Start to explore how the DSL can cover both advanced non linear spring integration flows as well as spring batch jobs.",NULL,"Start to explore how the DSL can cover both advanced non linear spring integration flows as well as spring batch jobs.",NULL,"Add for who this story is","well_formed","no_role","high",False
19782,"syntax code tap somechannel key value somecounter code ",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19782,"syntax code tap somechannel key value somecounter code ",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19783,"Currently these implementations are in the spring xd dirt module, but they should be moved into spring integration redis. We are already depending upon Spring Integration 3.0 snapshots since the ChannelRegistry implementation is not yet at a milestone, so this should be okay for the Redis Channel Adapters also until Spring Integration M2 is released.",NULL,"Currently these implementations are in the spring xd dirt module, but they should be moved into spring integration redis. We are already depending upon Spring Integration 3.0 snapshots since the ChannelRegistry implementation is not yet at a milestone,","so this should be okay for the Redis Channel Adapters also until Spring Integration M2 is released.","Add for who this story is","well_formed","no_role","high",False
19783,"Currently these implementations are in the spring xd dirt module, but they should be moved into spring integration redis. We are already depending upon Spring Integration 3.0 snapshots since the ChannelRegistry implementation is not yet at a milestone, so this should be okay for the Redis Channel Adapters also until Spring Integration M2 is released.",NULL,"Currently these implementations are in the spring xd dirt module, but they should be moved into spring integration redis. We are already depending upon Spring Integration 3.0 snapshots since the ChannelRegistry implementation is not yet at a milestone,","so this should be okay for the Redis Channel Adapters also until Spring Integration M2 is released.","Currently these implementations are in the spring xd dirt module, but they should be moved into spring integration redis<span class='highlight-text severity-high'>. We are already depending upon Spring Integration 3.0 snapshots since the ChannelRegistry implementation is not yet at a milestone, so this should be okay for the Redis Channel Adapters also until Spring Integration M2 is released.</span>","minimal","punctuation","high",False
19783,"Currently these implementations are in the spring xd dirt module, but they should be moved into spring integration redis. We are already depending upon Spring Integration 3.0 snapshots since the ChannelRegistry implementation is not yet at a milestone, so this should be okay for the Redis Channel Adapters also until Spring Integration M2 is released.",NULL,"Currently these implementations are in the spring xd dirt module, but they should be moved into spring integration redis. We are already depending upon Spring Integration 3.0 snapshots since the ChannelRegistry implementation is not yet at a milestone,","so this should be okay for the Redis Channel Adapters also until Spring Integration M2 is released.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19776,"The difference between saving a new metric and updating an existing one needs to be defined. Suggest that if we try to save when an existing counter is already in the database to throw exception, such as DataIntegrityViolationException.",NULL,"The difference between saving a new metric and updating an existing one needs to be defined. Suggest that if we try to save when an existing counter is already in the database to throw exception, such as DataIntegrityViolationException.",NULL,"Add for who this story is","well_formed","no_role","high",False
19795,"Initial simple handcoded implementation for straight through pipe and filter model, e.g. a b c",NULL,NULL,NULL,"Initial simple handcoded implementation for straight through pipe and filter model, e<span class='highlight-text severity-high'>.g. a b c</span>","minimal","punctuation","high",False
19801,"Base integration of core HDFS writer functionality with Spring Batch.",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19776,"The difference between saving a new metric and updating an existing one needs to be defined. Suggest that if we try to save when an existing counter is already in the database to throw exception, such as DataIntegrityViolationException.",NULL,"The difference between saving a new metric and updating an existing one needs to be defined. Suggest that if we try to save when an existing counter is already in the database to throw exception, such as DataIntegrityViolationException.",NULL,"The difference between saving a new metric<span class='highlight-text severity-high'> and </span>updating an existing one needs to be defined. Suggest that if we try to save when an existing counter is already in the database to throw exception, such as DataIntegrityViolationException.","atomic","conjunctions","high",False
19776,"The difference between saving a new metric and updating an existing one needs to be defined. Suggest that if we try to save when an existing counter is already in the database to throw exception, such as DataIntegrityViolationException.",NULL,"The difference between saving a new metric and updating an existing one needs to be defined. Suggest that if we try to save when an existing counter is already in the database to throw exception, such as DataIntegrityViolationException.",NULL,"The difference between saving a new metric and updating an existing one needs to be defined<span class='highlight-text severity-high'>. Suggest that if we try to save when an existing counter is already in the database to throw exception, such as DataIntegrityViolationException.</span>","minimal","punctuation","high",False
19778,"Replace the use of Jedis with Lettuce as it has higher performance",NULL,"Replace the use of Jedis with Lettuce as it has higher performance",NULL,"Add for who this story is","well_formed","no_role","high",False
19779,"A Spring Integration based ServiceActivator that counts the occurrence of field names, from either a tuple data structure or a POJO, using the Spring XD metrics support.",NULL,"A Spring Integration based ServiceActivator that counts the occurrence of field names, from either a tuple data structure or a POJO, using the Spring XD metrics support.",NULL,"Add for who this story is","well_formed","no_role","high",False
19779,"A Spring Integration based ServiceActivator that counts the occurrence of field names, from either a tuple data structure or a POJO, using the Spring XD metrics support.",NULL,"A Spring Integration based ServiceActivator that counts the occurrence of field names, from either a tuple data structure or a POJO, using the Spring XD metrics support.",NULL,"A Spring Integration based ServiceActivator that counts the occurrence of field names, from either a tuple data structure<span class='highlight-text severity-high'> or </span>a POJO, using the Spring XD metrics support.","atomic","conjunctions","high",False
19788,"TODO break out into sub task or other stories.",NULL,"TODO break out into sub task or other stories.",NULL,"Add for who this story is","well_formed","no_role","high",False
19787,"Gradle application plugin is a good starting point. this should be the main server that would host SI based modules to do syslog file ingestion as an example ",NULL,"Gradle application plugin is a good starting point. this should be the main server that would host SI based modules to do syslog file ingestion as an example ",NULL,"Add for who this story is","well_formed","no_role","high",False
19787,"Gradle application plugin is a good starting point. this should be the main server that would host SI based modules to do syslog file ingestion as an example ",NULL,"Gradle application plugin is a good starting point. this should be the main server that would host SI based modules to do syslog file ingestion as an example ",NULL,"Gradle application plugin is a good starting point<span class='highlight-text severity-high'>. this should be the main server that would host SI based modules to do syslog file ingestion as an example </span>","minimal","punctuation","high",False
19790,"bamboo based",NULL,"bamboo based",NULL,"Add for who this story is","well_formed","no_role","high",False
19789,"multi project build. look to Spring Framework for source of starting point.",NULL,"multi project build. look to Spring Framework for source of starting point.",NULL,"Add for who this story is","well_formed","no_role","high",False
19789,"multi project build. look to Spring Framework for source of starting point.",NULL,"multi project build. look to Spring Framework for source of starting point.",NULL,"multi project build<span class='highlight-text severity-high'>. look to Spring Framework for source of starting point.</span>","minimal","punctuation","high",False
19793,"A rich gauge stores a number and also rmd, min, max. Implementations for in memory and redis.",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19793,"A rich gauge stores a number and also rmd, min, max. Implementations for in memory and redis.",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19793,"A rich gauge stores a number and also rmd, min, max. Implementations for in memory and redis.",NULL,NULL,NULL,"A rich gauge stores a number and also rmd, min, max<span class='highlight-text severity-high'>. Implementations for in memory and redis.</span>","minimal","punctuation","high",False
19791,"A field value counter is useful for bar chart graphs, Strings on x axis and count on y axis. Maps well to zset in redis. Implementations for in memory and redis.",NULL,"A field value counter is useful for bar chart graphs, Strings on x axis and count on y axis. Maps well to zset in redis. Implementations for in memory and redis.",NULL,"Add for who this story is","well_formed","no_role","high",False
19791,"A field value counter is useful for bar chart graphs, Strings on x axis and count on y axis. Maps well to zset in redis. Implementations for in memory and redis.",NULL,"A field value counter is useful for bar chart graphs, Strings on x axis and count on y axis. Maps well to zset in redis. Implementations for in memory and redis.",NULL,"A field value counter is useful for bar chart graphs, Strings on x axis and count on y axis<span class='highlight-text severity-high'>. Maps well to zset in redis. Implementations for in memory and redis.</span>","minimal","punctuation","high",False
19796,"create enough of a design to develop additional stories.",NULL,"create enough of a design to develop additional stories.",NULL,"Add for who this story is","well_formed","no_role","high",False
19797,"When there is support for boostrapping a http server in the reactor project, and inbound SI adapter and associated XD source module should be created.",NULL,"When there is support for boostrapping a http server in the reactor project, and inbound SI adapter and associated XD source module should be created.",NULL,"Add for who this story is","well_formed","no_role","high",False
19797,"When there is support for boostrapping a http server in the reactor project, and inbound SI adapter and associated XD source module should be created.",NULL,"When there is support for boostrapping a http server in the reactor project, and inbound SI adapter and associated XD source module should be created.",NULL,"When there is support for boostrapping a http server in the reactor project,<span class='highlight-text severity-high'> and </span>inbound SI adapter and associated XD source module should be created.","atomic","conjunctions","high",False
19798,"Have a syslog.xml config file that can be added to a module and registered with a module registry.",NULL,"Have a syslog.xml config file that can be added to a module and registered with a module registry.",NULL,"Add for who this story is","well_formed","no_role","high",False
19798,"Have a syslog.xml config file that can be added to a module and registered with a module registry.",NULL,"Have a syslog.xml config file that can be added to a module and registered with a module registry.",NULL,"Have a syslog.xml config file that can be added to a module<span class='highlight-text severity-high'> and </span>registered with a module registry.","atomic","conjunctions","high",False
19799,"The tuple data structure should be backward compatible in functionality for use in spring batch. Porting over FieldSet tests in spring batch to use the tuple data structure is one way help ensure that compatibility.",NULL,"The tuple data structure should be backward compatible in functionality for use in spring batch. Porting over FieldSet tests in spring batch to use the tuple data structure is one way help ensure that compatibility.",NULL,"Add for who this story is","well_formed","no_role","high",False
19799,"The tuple data structure should be backward compatible in functionality for use in spring batch. Porting over FieldSet tests in spring batch to use the tuple data structure is one way help ensure that compatibility.",NULL,"The tuple data structure should be backward compatible in functionality for use in spring batch. Porting over FieldSet tests in spring batch to use the tuple data structure is one way help ensure that compatibility.",NULL,"The tuple data structure should be backward compatible in functionality for use in spring batch<span class='highlight-text severity-high'>. Porting over FieldSet tests in spring batch to use the tuple data structure is one way help ensure that compatibility.</span>","minimal","punctuation","high",False
19800,"Simple file writer that has existed in the spring hadoop samples.",NULL,"Simple file writer that has existed in the spring hadoop samples.",NULL,"Add for who this story is","well_formed","no_role","high",False
19665,"For stream creation we need to be able to specify source sink processor filter transformer script etc. ",NULL,"For stream creation we need to be able to specify source sink processor filter transformer script etc. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18103,"As a developer, I'd like to split admin artifact packaged with hadoop distro specific libraries, so I could avoid adding all variations of hadoop libraries under one project. ","As a developer",", I'd like to split admin artifact packaged with hadoop distro specific libraries,","so I could avoid adding all variations of hadoop libraries under one project.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18104,"As a developer, I'd like to add undeployed status for YARN SPI, so I can represent the correct status instead of the current unknown state.","As a developer",", I'd like to add undeployed status for YARN SPI,","so I can represent the correct status instead of the current unknown state.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18623,"As a user, I would like to be able disable snappy compression when using hdfs dataset sink with Avro files. I'd also like to be able to provide a different codec.","As a user",", I would like to be able disable snappy compression when using hdfs dataset sink with Avro files. I'd also like to be able to provide a different codec.",NULL,"As a user, I would like to be able disable snappy compression when using hdfs dataset sink with Avro files<span class='highlight-text severity-high'>. I'd also like to be able to provide a different codec.</span>","minimal","punctuation","high",False
18623,"As a user, I would like to be able disable snappy compression when using hdfs dataset sink with Avro files. I'd also like to be able to provide a different codec.","As a user",", I would like to be able disable snappy compression when using hdfs dataset sink with Avro files. I'd also like to be able to provide a different codec.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19667,"Need to create a basic Spring Shell implementation to provide easier access to the XD REST API via an XD REST API Client library. ",NULL,"Need to create a basic Spring Shell implementation to provide easier access to the XD REST API via an XD REST API Client library. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19669,"As a user of XD, I want to be able to use a job as a source. To do so, we need the output of a job to be written to a message channel ","As a user of XD,","I want to be able to use a job as a","source. To do so, we need the output of a job to be written to a message channel","As a user of XD, I want to be able to use a job as a source<span class='highlight-text severity-high'>. To do so, we need the output of a job to be written to a message channel </span>","minimal","punctuation","high",False
19735,"Currently, the install redis script uses relative path to determine redis source dist file. Since this is error prone, we need to fix it.",NULL,"Currently, the install redis script uses relative path to determine redis source dist file. Since this is error prone, we need to fix it.",NULL,"Add for who this story is","well_formed","no_role","high",False
19735,"Currently, the install redis script uses relative path to determine redis source dist file. Since this is error prone, we need to fix it.",NULL,"Currently, the install redis script uses relative path to determine redis source dist file. Since this is error prone, we need to fix it.",NULL,"Currently, the install redis script uses relative path to determine redis source dist file<span class='highlight-text severity-high'>. Since this is error prone, we need to fix it.</span>","minimal","punctuation","high",False
18107,"As a developer, I'd like to enhance test coverage to capture DSL and XML generation variants. ","As a developer",", I'd like to enhance test coverage to capture DSL and XML generation variants.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18108,"As a developer, I'd like to review and refactor JobLaunchingTasklet , so I can improve performance characteristics. ","As a developer",", I'd like to review and refactor JobLaunchingTasklet ,","so I can improve performance characteristics.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18110,"As a developer, I'd like to upgrade to SI 4.2.1 release, so I can take advantage of the latest improvements.","As a developer",", I'd like to upgrade to SI 4.2.1 release,","so I can take advantage of the latest improvements.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19792,"A simple counters can increment decrement a number. Implementations for in memory and redis.",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19792,"A simple counters can increment decrement a number. Implementations for in memory and redis.",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19792,"A simple counters can increment decrement a number. Implementations for in memory and redis.",NULL,NULL,NULL,"A simple counters can increment decrement a number<span class='highlight-text severity-high'>. Implementations for in memory and redis.</span>","minimal","punctuation","high",False
19785,"RedisCounterRepository and RedisGaugeRepository have duplicated code that needs to be factored out into a one place. One such duplication is the determination of the key name to use for persistence. This should be abstracted out into a strategy helper class.",NULL,"RedisCounterRepository and RedisGaugeRepository have duplicated code that needs to be factored out into a one place. One such duplication is the determination of the key name to use for persistence. This should be abstracted out into a strategy helper class.",NULL,"Add for who this story is","well_formed","no_role","high",False
19785,"RedisCounterRepository and RedisGaugeRepository have duplicated code that needs to be factored out into a one place. One such duplication is the determination of the key name to use for persistence. This should be abstracted out into a strategy helper class.",NULL,"RedisCounterRepository and RedisGaugeRepository have duplicated code that needs to be factored out into a one place. One such duplication is the determination of the key name to use for persistence. This should be abstracted out into a strategy helper class.",NULL,"RedisCounterRepository and RedisGaugeRepository have duplicated code that needs to be factored out into a one place<span class='highlight-text severity-high'>. One such duplication is the determination of the key name to use for persistence. This should be abstracted out into a strategy helper class.</span>","minimal","punctuation","high",False
19794,"A gauge just stores a number. Implementations for in memory and redis.",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19794,"A gauge just stores a number. Implementations for in memory and redis.",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19794,"A gauge just stores a number. Implementations for in memory and redis.",NULL,NULL,NULL,"A gauge just stores a number<span class='highlight-text severity-high'>. Implementations for in memory and redis.</span>","minimal","punctuation","high",False
19795,"Initial simple handcoded implementation for straight through pipe and filter model, e.g. a b c",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19795,"Initial simple handcoded implementation for straight through pipe and filter model, e.g. a b c",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
18248,"Spring cloud data admin requires lattice connector and spring cloud spring service connector dependencies so that the admin controllers get access to any services while running on lattice. One example is, CounterContoller using redis service for MetricRepository.",NULL,"Spring cloud data admin requires lattice connector and spring cloud spring service connector dependencies","so that the admin controllers get access to any services while running on lattice. One example is, CounterContoller using redis service for MetricRepository.","Add for who this story is","well_formed","no_role","high",False
18248,"Spring cloud data admin requires lattice connector and spring cloud spring service connector dependencies so that the admin controllers get access to any services while running on lattice. One example is, CounterContoller using redis service for MetricRepository.",NULL,"Spring cloud data admin requires lattice connector and spring cloud spring service connector dependencies","so that the admin controllers get access to any services while running on lattice. One example is, CounterContoller using redis service for MetricRepository.","Spring cloud data admin requires lattice connector and spring cloud spring service connector dependencies so that the admin controllers get access to any services while running on lattice<span class='highlight-text severity-high'>. One example is, CounterContoller using redis service for MetricRepository.</span>","minimal","punctuation","high",False
18248,"Spring cloud data admin requires lattice connector and spring cloud spring service connector dependencies so that the admin controllers get access to any services while running on lattice. One example is, CounterContoller using redis service for MetricRepository.",NULL,"Spring cloud data admin requires lattice connector and spring cloud spring service connector dependencies","so that the admin controllers get access to any services while running on lattice. One example is, CounterContoller using redis service for MetricRepository.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18249,"Improve Spring Cloud Stream module launcher resolver properties 1 Support comma separated remoteRepositories 2 Classify group the properties",NULL,"Improve Spring Cloud Stream module launcher resolver properties 1 Support comma separated remoteRepositories 2 Classify group the properties",NULL,"Add for who this story is","well_formed","no_role","high",False
18278,"As an s c s developer, I'd like to brainstorm and design the foundation to port XD modules as s c s modules, so I can use it as the base and start migrating the modules.","As an s c","s developer, I'd like to brainstorm and design the foundation to port XD modules as s c s modules,","so I can use it as the base and start migrating the modules.","As an s c s developer, I'd like to brainstorm<span class='highlight-text severity-high'> and </span>design the foundation to port XD modules as s c s modules, so I can use it as the base and start migrating the modules.","atomic","conjunctions","high",False
18278,"As an s c s developer, I'd like to brainstorm and design the foundation to port XD modules as s c s modules, so I can use it as the base and start migrating the modules.","As an s c","s developer, I'd like to brainstorm and design the foundation to port XD modules as s c s modules,","so I can use it as the base and start migrating the modules.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18279,"As a s c d developer, I'd like to add support for profiles to the core Admin application, so I can back the stream repository with respective backend strategy. For example local profile would use in memory strategy to store the metadata.","As a s c d developer",", I'd like to add support for profiles to the core Admin application,","so I can back the stream repository with respective backend strategy. For example local profile would use in memory strategy to store the metadata.","As a s c d developer, I'd like to add support for profiles to the core Admin application, so I can back the stream repository with respective backend strategy<span class='highlight-text severity-high'>. For example local profile would use in memory strategy to store the metadata.</span>","minimal","punctuation","high",False
18279,"As a s c d developer, I'd like to add support for profiles to the core Admin application, so I can back the stream repository with respective backend strategy. For example local profile would use in memory strategy to store the metadata.","As a s c d developer",", I'd like to add support for profiles to the core Admin application,","so I can back the stream repository with respective backend strategy. For example local profile would use in memory strategy to store the metadata.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18300,"As a s c s developer, I'd like to adapt redis counter from XD to s c s, so I can build streaming pipes using s c s modules with simple counters to feed dashboards. ","As a s c","s developer, I'd like to adapt redis counter from XD to s c s,","so I can build streaming pipes using s c s modules with simple counters to feed dashboards.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18705,"Pre requisite for Rabbit MQ Benchmarks Infrastructure setup Configuration changes Tool chain setup IPerf",NULL,"Pre requisite for Rabbit MQ Benchmarks Infrastructure setup Configuration changes Tool chain setup IPerf",NULL,"Add for who this story is","well_formed","no_role","high",False
18708,"Using the iperf tool https iperf.fr , find out the transfer rate in MB sec between three machines in a four machine configuration.",NULL,"Using the iperf tool https iperf.fr , find out the transfer rate in MB sec between three machines in a four machine configuration.",NULL,"Add for who this story is","well_formed","no_role","high",False
18808,"We need to make sure there is no conflicting missing dependency with build.gradle using spring IO platform dependencies. https jira.spring.io browse XD 1929 is one such scenario where jolokia dependency went missing.",NULL,"We need to make sure there is no conflicting missing dependency with build.gradle using spring IO platform dependencies. https jira.spring.io browse XD 1929 is one such scenario where jolokia dependency went missing.",NULL,"Add for who this story is","well_formed","no_role","high",False
18808,"We need to make sure there is no conflicting missing dependency with build.gradle using spring IO platform dependencies. https jira.spring.io browse XD 1929 is one such scenario where jolokia dependency went missing.",NULL,"We need to make sure there is no conflicting missing dependency with build.gradle using spring IO platform dependencies. https jira.spring.io browse XD 1929 is one such scenario where jolokia dependency went missing.",NULL,"We need to make sure there is no conflicting missing dependency with build<span class='highlight-text severity-high'>.gradle using spring IO platform dependencies. https jira.spring.io browse XD 1929 is one such scenario where jolokia dependency went missing.</span>","minimal","punctuation","high",False
19018,"When sending data to the TCP Source if the data is not terminated with a r n when the socket is closed by the client, XD throws an exception. ",NULL,"When sending data to the TCP Source if the data is not terminated with a r n when the socket is closed by the client, XD throws an exception. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18991,"By default, XD admin server uses hsqldb as data source for batch job repository and uses the default port 9101, specific database location and dbname. This makes the existence of multiple admin server on a same host not possible. ",NULL,"By default, XD admin server uses hsqldb as data source for batch job repository and uses the default port 9101, specific database location and dbname. This makes the existence of multiple admin server on a same host not possible. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18991,"By default, XD admin server uses hsqldb as data source for batch job repository and uses the default port 9101, specific database location and dbname. This makes the existence of multiple admin server on a same host not possible. ",NULL,"By default, XD admin server uses hsqldb as data source for batch job repository and uses the default port 9101, specific database location and dbname. This makes the existence of multiple admin server on a same host not possible. ",NULL,"By default, XD admin server uses hsqldb as data source for batch job repository<span class='highlight-text severity-high'> and </span>uses the default port 9101, specific database location and dbname. This makes the existence of multiple admin server on a same host not possible. ","atomic","conjunctions","high",False
18991,"By default, XD admin server uses hsqldb as data source for batch job repository and uses the default port 9101, specific database location and dbname. This makes the existence of multiple admin server on a same host not possible. ",NULL,"By default, XD admin server uses hsqldb as data source for batch job repository and uses the default port 9101, specific database location and dbname. This makes the existence of multiple admin server on a same host not possible. ",NULL,"By default, XD admin server uses hsqldb as data source for batch job repository and uses the default port 9101, specific database location and dbname<span class='highlight-text severity-high'>. This makes the existence of multiple admin server on a same host not possible. </span>","minimal","punctuation","high",False
18992,"3.4.6 was released on 2014.03.10 http zookeeper.apache.org doc r3.4.6 releasenotes.html Especially relevant for us, they updated Netty from 3.2.2 to 3.6.6 https issues.apache.org jira browse ZOOKEEPER 1715 ",NULL,"3.4.6 was released on 2014.03.10 http zookeeper.apache.org doc r3.4.6 releasenotes.html Especially relevant for us, they updated Netty from 3.2.2 to 3.6.6 https issues.apache.org jira browse ZOOKEEPER 1715 ",NULL,"Add for who this story is","well_formed","no_role","high",False
18303,"As a s c s developer, I'd like to move spring cloud stream modules from s c s to s c repo, so I can cleanup s c s project and at the same time make these modules visible outside of s c s.","As a s c","s developer, I'd like to move spring cloud stream modules from s c s to s c repo,","so I can cleanup s c s project and at the same time make these modules visible outside of s c s.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18306,"As a s c s user, I'd like to have the option to direct bind modules , so I'don t have to use messaging middleware and I can eliminate latency between them. This is important for high throughput and low latency use cases. ","As a s c","s user, I'd like to have the option to direct bind modules ,","so I'don t have to use messaging middleware and I can eliminate latency between them. This is important for high throughput and low latency use cases.","As a s c s user, I'd like to have the option to direct bind modules , so I'don t have to use messaging middleware and I can eliminate latency between them<span class='highlight-text severity-high'>. This is important for high throughput and low latency use cases. </span>","minimal","punctuation","high",False
18306,"As a s c s user, I'd like to have the option to direct bind modules , so I'don t have to use messaging middleware and I can eliminate latency between them. This is important for high throughput and low latency use cases. ","As a s c","s user, I'd like to have the option to direct bind modules ,","so I'don t have to use messaging middleware and I can eliminate latency between them. This is important for high throughput and low latency use cases.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18310,"As a developer, I'd like to be able to configure common dependencies for the entire environment. An example could be that I use MySql for my databases. I want to be able to configure the MySql driver once and have all modules use it. ","As a developer, I'd like to be able to configure common dependencies for the entire environment. An example could be that I use MySql for my databases.","I want to be able to configure the MySql driver once and have all modules use it.",NULL,"As a developer, I'd like to be able to configure common dependencies for the entire environment. An example could be that I use MySql for my databases. I want to be able to configure the MySql driver once<span class='highlight-text severity-high'> and </span>have all modules use it. ","atomic","conjunctions","high",False
18310,"As a developer, I'd like to be able to configure common dependencies for the entire environment. An example could be that I use MySql for my databases. I want to be able to configure the MySql driver once and have all modules use it. ","As a developer, I'd like to be able to configure common dependencies for the entire environment. An example could be that I use MySql for my databases.","I want to be able to configure the MySql driver once and have all modules use it.",NULL,"As a developer, I'd like to be able to configure common dependencies for the entire environment<span class='highlight-text severity-high'>. An example could be that I use MySql for my databases. I want to be able to configure the MySql driver once and have all modules use it. </span>","minimal","punctuation","high",False
18375,"The stream definition example uses old style syntax, should be mode ref instead of ref true ",NULL,"The stream definition example uses old style syntax, should be mode ref instead of ref true ",NULL,"Add for who this story is","well_formed","no_role","high",False
18378,"As a user, I'd like to have a landing page with higher order links for sources, processors, sinks and jobs, so I can jump to right section from one place. ","As a user",", I'd like to have a landing page with higher order links for","sources, processors, sinks and jobs, so I can jump to right section from one place.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18473,"When uploading a new version of a module the admin container if there is already an existing module, the behavior should be to delete the existing contents of the module directory and replace it with that of the new upload jar. This would be an optional parameter.",NULL,"When uploading a new version of a module the admin container if there is already an existing module, the behavior should be to delete the existing contents of the module directory and replace it with that of the new upload jar. This would be an optional parameter.",NULL,"Add for who this story is","well_formed","no_role","high",False
18473,"When uploading a new version of a module the admin container if there is already an existing module, the behavior should be to delete the existing contents of the module directory and replace it with that of the new upload jar. This would be an optional parameter.",NULL,"When uploading a new version of a module the admin container if there is already an existing module, the behavior should be to delete the existing contents of the module directory and replace it with that of the new upload jar. This would be an optional parameter.",NULL,"When uploading a new version of a module the admin container if there is already an existing module, the behavior should be to delete the existing contents of the module directory<span class='highlight-text severity-high'> and </span>replace it with that of the new upload jar. This would be an optional parameter.","atomic","conjunctions","high",False
18473,"When uploading a new version of a module the admin container if there is already an existing module, the behavior should be to delete the existing contents of the module directory and replace it with that of the new upload jar. This would be an optional parameter.",NULL,"When uploading a new version of a module the admin container if there is already an existing module, the behavior should be to delete the existing contents of the module directory and replace it with that of the new upload jar. This would be an optional parameter.",NULL,"When uploading a new version of a module the admin container if there is already an existing module, the behavior should be to delete the existing contents of the module directory and replace it with that of the new upload jar<span class='highlight-text severity-high'>. This would be an optional parameter.</span>","minimal","punctuation","high",False
18408,"As a developer, I'd like to move the project reactor based gpfdist https github.com spring projects spring xd modules tree master gpfdist from spring xd module repo to the core, so I can natively use this sink to write to GPDB HAWQ. ","As a developer",", I'd like to move the project reactor based gpfdist https github.com spring projects spring xd modules tree master gpfdist from spring xd module repo to the core,","so I can natively use this sink to write to GPDB HAWQ.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18409,"As a developer, I'd like to move the project reactor based data processor module https github.com spring projects spring xd modules tree master spring xd reactor from spring xd module repo to the core, so I can natively use Reactor s Stream API to build processor modules. ","As a developer",", I'd like to move the project reactor based data proces","sor module https github.com spring projects spring xd modules tree master spring xd reactor from spring xd module repo to the core, so I can natively use Reactor s Stream API to build processor modules.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18744,"As a user, I'd like to have the option to provide LDAP based security configurations so that I can access the endpoints in a secured manner. Ideally, all the listed endpoints http localhost 9393 needs to be encapsulated within LDAP based security layer. Reference Authentication using LDAP https spring.io guides gs authenticating ldap ","As a user",", I'd like to have the option to provide LDAP based security configurations","so that I can access the endpoints in a secured manner. Ideally, all the listed endpoints http localhost 9393 needs to be encapsulated within LDAP based security layer. Reference Authentication using LDAP https spring.io guides gs authenticating ldap","As a user, I'd like to have the option to provide LDAP based security configurations so that I can access the endpoints in a secured manner<span class='highlight-text severity-high'>. Ideally, all the listed endpoints http localhost 9393 needs to be encapsulated within LDAP based security layer. Reference Authentication using LDAP https spring.io guides gs authenticating ldap </span>","minimal","punctuation","high",False
18744,"As a user, I'd like to have the option to provide LDAP based security configurations so that I can access the endpoints in a secured manner. Ideally, all the listed endpoints http localhost 9393 needs to be encapsulated within LDAP based security layer. Reference Authentication using LDAP https spring.io guides gs authenticating ldap ","As a user",", I'd like to have the option to provide LDAP based security configurations","so that I can access the endpoints in a secured manner. Ideally, all the listed endpoints http localhost 9393 needs to be encapsulated within LDAP based security layer. Reference Authentication using LDAP https spring.io guides gs authenticating ldap","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18447,"As a user, I'd like to have the ability to use expressions, so I can dynamically name directories files based on the timestamp or other intermediate data point.","As a user",", I'd like to have the ability to use expressions,","so I can dynamically name directories files based on the timestamp or other intermediate data point.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18450,"As a user, I'd like to parameterize all Import Options, so I can eliminate the need for args option since it gets confusing.","As a user",", I'd like to parameterize all Import Options,","so I can eliminate the need for args option since it gets confusing.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18468,"As a developer, I would like to connect to the broker that hosts the Rabbit queue, so I can connect to a Rabbit cluster that s setup for HA FT. Perhaps consider having this feature natively supported in spring amqp itself.","As a developer",", I would like to connect to the broker that hosts the Rabbit queue,","so I can connect to a Rabbit cluster that s setup for HA FT. Perhaps consider having this feature natively supported in spring amqp itself.","As a developer, I would like to connect to the broker that hosts the Rabbit queue, so I can connect to a Rabbit cluster that s setup for HA FT<span class='highlight-text severity-high'>. Perhaps consider having this feature natively supported in spring amqp itself.</span>","minimal","punctuation","high",False
18468,"As a developer, I would like to connect to the broker that hosts the Rabbit queue, so I can connect to a Rabbit cluster that s setup for HA FT. Perhaps consider having this feature natively supported in spring amqp itself.","As a developer",", I would like to connect to the broker that hosts the Rabbit queue,","so I can connect to a Rabbit cluster that s setup for HA FT. Perhaps consider having this feature natively supported in spring amqp itself.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18116,"As a s c d developer, I'd like to break the build lifecycle to bundle SPI'deployers individually, so I'don t have to build admin with all the deployer variations as one whole thing.","As a s c d developer",", I'd like to break the build lifecycle to bundle SPI'deployers individually,","so I'don t have to build admin with all the deployer variations as one whole thing.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18120,"There is a need to customize the ModuleLauncher behavior itself, NOT pass options to modules that are launched, which is already supported for example to set the location of the maven repository. ",NULL,"There is a need to customize the ModuleLauncher behavior itself, NOT pass options to modules that are launched, which is already supported for example to set the location of the maven repository. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18772,"Update XD EC2 configs to Pull from 1.0.1 Repo Update XD EC2 Configs to use spring xd 1.0.1.BUILD SNAPSHOT dir Update test configs XD HOME to spring xd 1.0.1.BUILD SNAPSHOT instead of spring xd 1.0.0.BUILD SNAPSHOT",NULL,"Update XD EC2 configs to Pull from 1.0.1 Repo Update XD EC2 Configs to use spring xd 1.0.1.BUILD SNAPSHOT dir Update test configs XD HOME to spring xd 1.0.1.BUILD SNAPSHOT instead of spring xd 1.0.0.BUILD SNAPSHOT",NULL,"Add for who this story is","well_formed","no_role","high",False
18129,"As a s c d user, I should be prevented from creating streams with duplicate name. I'd expect streams to have unique names all the time. ","As a s c d user",", I should be prevented from creating streams with duplicate name. I'd expect streams to have unique names all the time.",NULL,"As a s c d user, I should be prevented from creating streams with duplicate name<span class='highlight-text severity-high'>. I'd expect streams to have unique names all the time. </span>","minimal","punctuation","high",False
18129,"As a s c d user, I should be prevented from creating streams with duplicate name. I'd expect streams to have unique names all the time. ","As a s c d user",", I should be prevented from creating streams with duplicate name. I'd expect streams to have unique names all the time.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18132,"As a SCDF user, I want to be able to register artifacts as libraries, so that I can reference them in include and exclude statements.","As a SCDF user,","I want to be able to register artifacts as libraries,","so that I can reference them in include and exclude statements.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18134,"As an XD developer, I need to be able to create a composed job module as XML from the DSL an store it in the Module File repository. While the user uses the composed job as if it is a normal job including seeing only the DSL. In the background the JobFactory will deploy the composed job module. When the user destroys the job the module will be deleted from the file module repository. When the user creates the job a module will be created in the file Module repository. ","As an XD developer",", I need to be able to create a composed job module as XML from the DSL an store it in the Module File repository. While the user uses the composed job as if it is a normal job including seeing only the DSL. In the background the JobFactory will deploy the composed job module. When the user destroys the job the module will be deleted from the file module repository. When the user creates the job a module will be created in the file Module repository.",NULL,"As an XD developer, I need to be able to create a composed job module as XML from the DSL an store it in the Module File repository<span class='highlight-text severity-high'>. While the user uses the composed job as if it is a normal job including seeing only the DSL. In the background the JobFactory will deploy the composed job module. When the user destroys the job the module will be deleted from the file module repository. When the user creates the job a module will be created in the file Module repository. </span>","minimal","punctuation","high",False
18134,"As an XD developer, I need to be able to create a composed job module as XML from the DSL an store it in the Module File repository. While the user uses the composed job as if it is a normal job including seeing only the DSL. In the background the JobFactory will deploy the composed job module. When the user destroys the job the module will be deleted from the file module repository. When the user creates the job a module will be created in the file Module repository. ","As an XD developer",", I need to be able to create a composed job module as XML from the DSL an store it in the Module File repository. While the user uses the composed job as if it is a normal job including seeing only the DSL. In the background the JobFactory will deploy the composed job module. When the user destroys the job the module will be deleted from the file module repository. When the user creates the job a module will be created in the file Module repository.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18174,"As a Spring XD developer, I'd like to move mongo module from XD to s c s repo, so I can use it as sink to build streaming pipeline. ","As a Spring XD developer",", I'd like to move mongo module from XD to s c s repo,","so I can use it as sink to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18179,"As a Spring XD developer, I'd like to move tcp module from XD to s c s repo, so I can use it as source to build streaming pipeline.","As a Spring XD developer",", I'd like to move tcp module from XD to s c s repo,","so I can use it as source to build streaming pipeline.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18478,"As a user, I logged in with ROLE CREATE and I get an error while trying job creation from admin ui. I can create job from the shell successfully. Trying the same workflow with ROLE ADMIN results with the same error as well. I'don t see anything in the admin container logs about the error itself. ","As a user, I logged in with ROLE CREATE and I get an error while trying job creation from admin ui.","I can create job from the shell successfully. Trying the same workflow with ROLE ADMIN results with the same error as well. I'don t see anything in the admin container logs about the error itself.",NULL,"As a user, I logged in with ROLE CREATE<span class='highlight-text severity-high'> and </span>I get an error while trying job creation from admin ui. I can create job from the shell successfully. Trying the same workflow with ROLE ADMIN results with the same error as well. I'don t see anything in the admin container logs about the error itself. ","atomic","conjunctions","high",False
18478,"As a user, I logged in with ROLE CREATE and I get an error while trying job creation from admin ui. I can create job from the shell successfully. Trying the same workflow with ROLE ADMIN results with the same error as well. I'don t see anything in the admin container logs about the error itself. ","As a user, I logged in with ROLE CREATE and I get an error while trying job creation from admin ui.","I can create job from the shell successfully. Trying the same workflow with ROLE ADMIN results with the same error as well. I'don t see anything in the admin container logs about the error itself.",NULL,"As a user, I logged in with ROLE CREATE and I get an error while trying job creation from admin ui<span class='highlight-text severity-high'>. I can create job from the shell successfully. Trying the same workflow with ROLE ADMIN results with the same error as well. I'don t see anything in the admin container logs about the error itself. </span>","minimal","punctuation","high",False
18478,"As a user, I logged in with ROLE CREATE and I get an error while trying job creation from admin ui. I can create job from the shell successfully. Trying the same workflow with ROLE ADMIN results with the same error as well. I'don t see anything in the admin container logs about the error itself. ","As a user, I logged in with ROLE CREATE and I get an error while trying job creation from admin ui.","I can create job from the shell successfully. Trying the same workflow with ROLE ADMIN results with the same error as well. I'don t see anything in the admin container logs about the error itself.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18755,"When the container starts up, it has a random http port for management configurations. If the container has management port enabled, then we can store it as container attribute. If the admin can reach out to the container on that port, then we can provide an option to shutdown container from the admin server.",NULL,"When the container starts up, it has a random http port for management configurations. If the container has management port enabled, then we can store it as container attribute. If the admin can reach out to the container on that port, then we can provide an option to shutdown container from the admin server.",NULL,"Add for who this story is","well_formed","no_role","high",False
18755,"When the container starts up, it has a random http port for management configurations. If the container has management port enabled, then we can store it as container attribute. If the admin can reach out to the container on that port, then we can provide an option to shutdown container from the admin server.",NULL,"When the container starts up, it has a random http port for management configurations. If the container has management port enabled, then we can store it as container attribute. If the admin can reach out to the container on that port, then we can provide an option to shutdown container from the admin server.",NULL,"When the container starts up, it has a random http port for management configurations<span class='highlight-text severity-high'>. If the container has management port enabled, then we can store it as container attribute. If the admin can reach out to the container on that port, then we can provide an option to shutdown container from the admin server.</span>","minimal","punctuation","high",False
18495,"This apparently is not tested or used internally, but I expect it to fail having tried a similar approach to derive the class of a generic type in a different situation. This method does not always work due to type erasure http stackoverflow.com questions 3403909 get generic type of class at runtime. We need to verify if this is working, if not fix it. The API may require it, so possibly UnsupportedOperationException... code Infers the type from this class s generic type argument param kryo param input return protected T doDeserialize Kryo kryo, Input input Class T type Class T ParameterizedType this.getClass .getGenericSuperclass .getActualTypeArguments 0 ; return doDeserialize kryo, input, type ; code ",NULL,"This apparently is not tested or used internally, but I expect it to fail having tried a similar approach to derive the class of a generic type in a different situation. This method does not always work due to type erasure http stackoverflow.com questions 3403909 get generic type of class at runtime. We need to verify if this is working, if not fix it. The API may require it,","so possibly UnsupportedOperationException... code Infers the type from this class s generic type argument param kryo param input return protected T doDeserialize Kryo kryo, Input input Class T type Class T ParameterizedType this.getClass .getGenericSuperclass .getActualTypeArguments 0 ; return doDeserialize kryo, input, type ; code","Add for who this story is","well_formed","no_role","high",False
18495,"This apparently is not tested or used internally, but I expect it to fail having tried a similar approach to derive the class of a generic type in a different situation. This method does not always work due to type erasure http stackoverflow.com questions 3403909 get generic type of class at runtime. We need to verify if this is working, if not fix it. The API may require it, so possibly UnsupportedOperationException... code Infers the type from this class s generic type argument param kryo param input return protected T doDeserialize Kryo kryo, Input input Class T type Class T ParameterizedType this.getClass .getGenericSuperclass .getActualTypeArguments 0 ; return doDeserialize kryo, input, type ; code ",NULL,"This apparently is not tested or used internally, but I expect it to fail having tried a similar approach to derive the class of a generic type in a different situation. This method does not always work due to type erasure http stackoverflow.com questions 3403909 get generic type of class at runtime. We need to verify if this is working, if not fix it. The API may require it,","so possibly UnsupportedOperationException... code Infers the type from this class s generic type argument param kryo param input return protected T doDeserialize Kryo kryo, Input input Class T type Class T ParameterizedType this.getClass .getGenericSuperclass .getActualTypeArguments 0 ; return doDeserialize kryo, input, type ; code","This apparently is not tested<span class='highlight-text severity-high'> or </span>used internally, but I expect it to fail having tried a similar approach to derive the class of a generic type in a different situation. This method does not always work due to type erasure http stackoverflow.com questions 3403909 get generic type of class at runtime. We need to verify if this is working, if not fix it. The API may require it, so possibly UnsupportedOperationException... code Infers the type from this class s generic type argument param kryo param input return protected T doDeserialize Kryo kryo, Input input Class T type Class T ParameterizedType this.getClass .getGenericSuperclass .getActualTypeArguments 0 ; return doDeserialize kryo, input, type ; code ","atomic","conjunctions","high",False
18495,"This apparently is not tested or used internally, but I expect it to fail having tried a similar approach to derive the class of a generic type in a different situation. This method does not always work due to type erasure http stackoverflow.com questions 3403909 get generic type of class at runtime. We need to verify if this is working, if not fix it. The API may require it, so possibly UnsupportedOperationException... code Infers the type from this class s generic type argument param kryo param input return protected T doDeserialize Kryo kryo, Input input Class T type Class T ParameterizedType this.getClass .getGenericSuperclass .getActualTypeArguments 0 ; return doDeserialize kryo, input, type ; code ",NULL,"This apparently is not tested or used internally, but I expect it to fail having tried a similar approach to derive the class of a generic type in a different situation. This method does not always work due to type erasure http stackoverflow.com questions 3403909 get generic type of class at runtime. We need to verify if this is working, if not fix it. The API may require it,","so possibly UnsupportedOperationException... code Infers the type from this class s generic type argument param kryo param input return protected T doDeserialize Kryo kryo, Input input Class T type Class T ParameterizedType this.getClass .getGenericSuperclass .getActualTypeArguments 0 ; return doDeserialize kryo, input, type ; code","This apparently is not tested or used internally, but I expect it to fail having tried a similar approach to derive the class of a generic type in a different situation<span class='highlight-text severity-high'>. This method does not always work due to type erasure http stackoverflow.com questions 3403909 get generic type of class at runtime. We need to verify if this is working, if not fix it. The API may require it, so possibly UnsupportedOperationException... code Infers the type from this class s generic type argument param kryo param input return protected T doDeserialize Kryo kryo, Input input Class T type Class T ParameterizedType this.getClass .getGenericSuperclass .getActualTypeArguments 0 ; return doDeserialize kryo, input, type ; code </span>","minimal","punctuation","high",False
18495,"This apparently is not tested or used internally, but I expect it to fail having tried a similar approach to derive the class of a generic type in a different situation. This method does not always work due to type erasure http stackoverflow.com questions 3403909 get generic type of class at runtime. We need to verify if this is working, if not fix it. The API may require it, so possibly UnsupportedOperationException... code Infers the type from this class s generic type argument param kryo param input return protected T doDeserialize Kryo kryo, Input input Class T type Class T ParameterizedType this.getClass .getGenericSuperclass .getActualTypeArguments 0 ; return doDeserialize kryo, input, type ; code ",NULL,"This apparently is not tested or used internally, but I expect it to fail having tried a similar approach to derive the class of a generic type in a different situation. This method does not always work due to type erasure http stackoverflow.com questions 3403909 get generic type of class at runtime. We need to verify if this is working, if not fix it. The API may require it,","so possibly UnsupportedOperationException... code Infers the type from this class s generic type argument param kryo param input return protected T doDeserialize Kryo kryo, Input input Class T type Class T ParameterizedType this.getClass .getGenericSuperclass .getActualTypeArguments 0 ; return doDeserialize kryo, input, type ; code","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19491,"e.g. http localhost 8080 post httpsource target http localhost 9090 data 10 I believe this will also help to avoid ugly syntax to escaping quotes for json as in the gemfire example.",NULL,"e.g. http localhost 8080 post httpsource target http localhost 9090 data 10 I believe this will also help to avoid ugly syntax to escaping quotes for json as in the gemfire example.",NULL,"Add for who this story is","well_formed","no_role","high",False
19491,"e.g. http localhost 8080 post httpsource target http localhost 9090 data 10 I believe this will also help to avoid ugly syntax to escaping quotes for json as in the gemfire example.",NULL,"e.g. http localhost 8080 post httpsource target http localhost 9090 data 10 I believe this will also help to avoid ugly syntax to escaping quotes for json as in the gemfire example.",NULL,"e<span class='highlight-text severity-high'>.g. http localhost 8080 post httpsource target http localhost 9090 data 10 I believe this will also help to avoid ugly syntax to escaping quotes for json as in the gemfire example.</span>","minimal","punctuation","high",False
18203,"As a s c d developer, I'd like to add support for having different binder types for module s channels, so I can plug rabbit , redis , or kafka as the source or sink to read and write respectively.","As a s c d developer",", I'd like to add support for having different binder types for module s channels,","so I can plug rabbit , redis , or kafka as the source or sink to read and write respectively.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18209,"As a XD user, I'd like to orchestrate composed jobs, so I can bring multiple jobs into single workflow and operationalize.","As a XD user",", I'd like to orchestrate composed jobs,","so I can bring multiple jobs into single workflow and operationalize.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18211,"The CF implementation requires that a route be created for each new app. This works fine on the happy path, but is brittle. For example, it will fail if the route required already exists.",NULL,"The CF implementation requires that a route be created for each new app. This works fine on the happy path, but is brittle. For example, it will fail if the route required already exists.",NULL,"Add for who this story is","well_formed","no_role","high",False
18211,"The CF implementation requires that a route be created for each new app. This works fine on the happy path, but is brittle. For example, it will fail if the route required already exists.",NULL,"The CF implementation requires that a route be created for each new app. This works fine on the happy path, but is brittle. For example, it will fail if the route required already exists.",NULL,"The CF implementation requires that a route be created for each new app<span class='highlight-text severity-high'>. This works fine on the happy path, but is brittle. For example, it will fail if the route required already exists.</span>","minimal","punctuation","high",False
19164,"Wherever they come from cmd line args or ENV VARS , options such as transport, analytics, etc should be validated and issues should be reported to users",NULL,"Wherever they come from cmd line args or ENV VARS , options such as transport, analytics, etc should be validated and issues should be reported to users",NULL,"Add for who this story is","well_formed","no_role","high",False
19164,"Wherever they come from cmd line args or ENV VARS , options such as transport, analytics, etc should be validated and issues should be reported to users",NULL,"Wherever they come from cmd line args or ENV VARS , options such as transport, analytics, etc should be validated and issues should be reported to users",NULL,"Wherever they come from cmd line args<span class='highlight-text severity-high'> or </span>ENV VARS , options such as transport, analytics, etc should be validated<span class='highlight-text severity-high'> and </span>issues should be reported to users","atomic","conjunctions","high",False
18214,"As a s c d user, I'd like to create a new banner, so I can embed and display the banner when the shell server boots up. Perhaps use this banner generator http patorjk.com software taag p display f Standard t Spring 20Cloud 0AData 20Flow 20 20 3E 3E 3E 3E 3E 20 ?","As a s c d user",", I'd like to create a new banner,","so I can embed and display the banner when the shell server boots up. Perhaps use this banner generator http patorjk.com software taag p display f Standard t Spring 20Cloud 0AData 20Flow 20 20 3E 3E 3E 3E 3E 20 ?","As a s c d user, I'd like to create a new banner, so I can embed and display the banner when the shell server boots up<span class='highlight-text severity-high'>. Perhaps use this banner generator http patorjk.com software taag p display f Standard t Spring 20Cloud 0AData 20Flow 20 20 3E 3E 3E 3E 3E 20 ?</span>","minimal","punctuation","high",False
18214,"As a s c d user, I'd like to create a new banner, so I can embed and display the banner when the shell server boots up. Perhaps use this banner generator http patorjk.com software taag p display f Standard t Spring 20Cloud 0AData 20Flow 20 20 3E 3E 3E 3E 3E 20 ?","As a s c d user",", I'd like to create a new banner,","so I can embed and display the banner when the shell server boots up. Perhaps use this banner generator http patorjk.com software taag p display f Standard t Spring 20Cloud 0AData 20Flow 20 20 3E 3E 3E 3E 3E 20 ?","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18927,"Currently the reactor syslog source module only supports TCP. Once we add UDP support, we can probably remove the existing syslog tcp and syslog udp modules.",NULL,"Currently the reactor syslog source module only supports TCP. Once we add UDP support, we can probably remove the existing syslog tcp and syslog udp modules.",NULL,"Add for who this story is","well_formed","no_role","high",False
18927,"Currently the reactor syslog source module only supports TCP. Once we add UDP support, we can probably remove the existing syslog tcp and syslog udp modules.",NULL,"Currently the reactor syslog source module only supports TCP. Once we add UDP support, we can probably remove the existing syslog tcp and syslog udp modules.",NULL,"Currently the reactor syslog source module only supports TCP<span class='highlight-text severity-high'>. Once we add UDP support, we can probably remove the existing syslog tcp and syslog udp modules.</span>","minimal","punctuation","high",False
18242,"As an s c d developer, I'd like to add support to negotiate with the ResourceManager REST APIs to deploy modules by groups. so I can build instrumentation to start the App instances automatically. Perhaps also take into account of the App specifics such as appType CLOUDDATA and appName spring cloud data yarn app . ","As an s c d developer",", I'd like to add support to negotiate with the Re","sourceManager REST APIs to deploy modules by groups. so I can build instrumentation to start the App instances automatically. Perhaps also take into account of the App specifics such as appType CLOUDDATA and appName spring cloud data yarn app .","As an s c d developer, I'd like to add support to negotiate with the ResourceManager REST APIs to deploy modules by groups<span class='highlight-text severity-high'>. so I can build instrumentation to start the App instances automatically. Perhaps also take into account of the App specifics such as appType CLOUDDATA and appName spring cloud data yarn app . </span>","minimal","punctuation","high",False
18242,"As an s c d developer, I'd like to add support to negotiate with the ResourceManager REST APIs to deploy modules by groups. so I can build instrumentation to start the App instances automatically. Perhaps also take into account of the App specifics such as appType CLOUDDATA and appName spring cloud data yarn app . ","As an s c d developer",", I'd like to add support to negotiate with the Re","sourceManager REST APIs to deploy modules by groups. so I can build instrumentation to start the App instances automatically. Perhaps also take into account of the App specifics such as appType CLOUDDATA and appName spring cloud data yarn app .","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18975,"Typical case is with a module that contains noformat ... user username ... noformat say, as part of a jdbc connection configuration and no value has been given at deployment time. The module may pickup a value from the environment by mistake typically from an environment variable of the same name . This was even more problematic when ordering of property sources was unclear, but should be prevented entirely anyway.",NULL,"Typical case is with a module that contains noformat ... user username ... noformat say, as part of a jdbc connection configuration and no value has been given at deployment time. The module may pickup a value from the environment by mistake typically from an environment variable of the same name . This was even more problematic when ordering of property sources was unclear, but should be prevented entirely anyway.",NULL,"Add for who this story is","well_formed","no_role","high",False
18975,"Typical case is with a module that contains noformat ... user username ... noformat say, as part of a jdbc connection configuration and no value has been given at deployment time. The module may pickup a value from the environment by mistake typically from an environment variable of the same name . This was even more problematic when ordering of property sources was unclear, but should be prevented entirely anyway.",NULL,"Typical case is with a module that contains noformat ... user username ... noformat say, as part of a jdbc connection configuration and no value has been given at deployment time. The module may pickup a value from the environment by mistake typically from an environment variable of the same name . This was even more problematic when ordering of property sources was unclear, but should be prevented entirely anyway.",NULL,"Typical case is with a module that contains noformat ... user username ... noformat say, as part of a jdbc connection configuration<span class='highlight-text severity-high'> and </span>no value has been given at deployment time. The module may pickup a value from the environment by mistake typically from an environment variable of the same name . This was even more problematic when ordering of property sources was unclear, but should be prevented entirely anyway.","atomic","conjunctions","high",False
18975,"Typical case is with a module that contains noformat ... user username ... noformat say, as part of a jdbc connection configuration and no value has been given at deployment time. The module may pickup a value from the environment by mistake typically from an environment variable of the same name . This was even more problematic when ordering of property sources was unclear, but should be prevented entirely anyway.",NULL,"Typical case is with a module that contains noformat ... user username ... noformat say, as part of a jdbc connection configuration and no value has been given at deployment time. The module may pickup a value from the environment by mistake typically from an environment variable of the same name . This was even more problematic when ordering of property sources was unclear, but should be prevented entirely anyway.",NULL,"Typical case is with a module that contains noformat <span class='highlight-text severity-high'>... user username ... noformat say, as part of a jdbc connection configuration and no value has been given at deployment time. The module may pickup a value from the environment by mistake typically from an environment variable of the same name . This was even more problematic when ordering of property sources was unclear, but should be prevented entirely anyway.</span>","minimal","punctuation","high",False
18088,"Need to add the following instructions to setup the configurations for the Batch Repo to Composed Job Docs to support parallel jobs 1 uncomment and change the following from spring batch Configure other Spring Batch repository values. Most are typically not needed isolationLevel ISOLATION SERIALIZATION to spring batch Configure other Spring Batch repository values. Most are typically not needed isolationLevel ISOLATION READ COMMITTED And update the hsqldb datasource to spring datasource url jdbc hsqldb hsql hsql.server.host localhost hsql.server.port 9101 hsql.server.dbname xdjob ;sql.enforce strict size true;hsqldb.tx mvcc",NULL,"Need to add the following instructions to setup the configurations for the Batch Repo to Composed Job Docs to support parallel jobs 1 uncomment and change the following from spring batch Configure other Spring Batch repository values. Most are typically not needed isolationLevel ISOLATION SERIALIZATION to spring batch Configure other Spring Batch repository values. Most are typically not needed isolationLevel ISOLATION READ COMMITTED And update the hsqldb datasource to spring datasource url jdbc hsqldb hsql hsql.server.host localhost hsql.server.port 9101 hsql.server.dbname xdjob ;sql.enforce strict size true;hsqldb.tx mvcc",NULL,"Add for who this story is","well_formed","no_role","high",False
18088,"Need to add the following instructions to setup the configurations for the Batch Repo to Composed Job Docs to support parallel jobs 1 uncomment and change the following from spring batch Configure other Spring Batch repository values. Most are typically not needed isolationLevel ISOLATION SERIALIZATION to spring batch Configure other Spring Batch repository values. Most are typically not needed isolationLevel ISOLATION READ COMMITTED And update the hsqldb datasource to spring datasource url jdbc hsqldb hsql hsql.server.host localhost hsql.server.port 9101 hsql.server.dbname xdjob ;sql.enforce strict size true;hsqldb.tx mvcc",NULL,"Need to add the following instructions to setup the configurations for the Batch Repo to Composed Job Docs to support parallel jobs 1 uncomment and change the following from spring batch Configure other Spring Batch repository values. Most are typically not needed isolationLevel ISOLATION SERIALIZATION to spring batch Configure other Spring Batch repository values. Most are typically not needed isolationLevel ISOLATION READ COMMITTED And update the hsqldb datasource to spring datasource url jdbc hsqldb hsql hsql.server.host localhost hsql.server.port 9101 hsql.server.dbname xdjob ;sql.enforce strict size true;hsqldb.tx mvcc",NULL,"Need to add the following instructions to setup the configurations for the Batch Repo to Composed Job Docs to support parallel jobs 1 uncomment<span class='highlight-text severity-high'> and </span>change the following from spring batch Configure other Spring Batch repository values. Most are typically not needed isolationLevel ISOLATION SERIALIZATION to spring batch Configure other Spring Batch repository values. Most are typically not needed isolationLevel ISOLATION READ COMMITTED And update the hsqldb datasource to spring datasource url jdbc hsqldb hsql hsql.server.host localhost hsql.server.port 9101 hsql.server.dbname xdjob ;sql.enforce strict size true;hsqldb.tx mvcc","atomic","conjunctions","high",False
18088,"Need to add the following instructions to setup the configurations for the Batch Repo to Composed Job Docs to support parallel jobs 1 uncomment and change the following from spring batch Configure other Spring Batch repository values. Most are typically not needed isolationLevel ISOLATION SERIALIZATION to spring batch Configure other Spring Batch repository values. Most are typically not needed isolationLevel ISOLATION READ COMMITTED And update the hsqldb datasource to spring datasource url jdbc hsqldb hsql hsql.server.host localhost hsql.server.port 9101 hsql.server.dbname xdjob ;sql.enforce strict size true;hsqldb.tx mvcc",NULL,"Need to add the following instructions to setup the configurations for the Batch Repo to Composed Job Docs to support parallel jobs 1 uncomment and change the following from spring batch Configure other Spring Batch repository values. Most are typically not needed isolationLevel ISOLATION SERIALIZATION to spring batch Configure other Spring Batch repository values. Most are typically not needed isolationLevel ISOLATION READ COMMITTED And update the hsqldb datasource to spring datasource url jdbc hsqldb hsql hsql.server.host localhost hsql.server.port 9101 hsql.server.dbname xdjob ;sql.enforce strict size true;hsqldb.tx mvcc",NULL,"Need to add the following instructions to setup the configurations for the Batch Repo to Composed Job Docs to support parallel jobs 1 uncomment and change the following from spring batch Configure other Spring Batch repository values<span class='highlight-text severity-high'>. Most are typically not needed isolationLevel ISOLATION SERIALIZATION to spring batch Configure other Spring Batch repository values. Most are typically not needed isolationLevel ISOLATION READ COMMITTED And update the hsqldb datasource to spring datasource url jdbc hsqldb hsql hsql.server.host localhost hsql.server.port 9101 hsql.server.dbname xdjob ;sql.enforce strict size true;hsqldb.tx mvcc</span>","minimal","punctuation","high",False
18149,"In https github.com spring cloud spring cloud dataflow blob master README.md running on cloud foundry the section starting Now we can configure the app needs to be revised the information is both out of date and, even if up to date, misleading it includes some values as if they are universal, when they are really just examples .",NULL,"In https github.com spring cloud spring cloud dataflow blob master README.md running on cloud foundry the section starting Now we can configure the app needs to be revised the information is both out of date and, even if up to date, misleading it includes some values as if they are universal, when they are really just examples .",NULL,"Add for who this story is","well_formed","no_role","high",False
18188,"As an s c d user, I'd like to contribute modules that immediately reflects in module registry, so I can create stream or task definitions using the shell rest api s. Currently the registry isn t flexible, as it is pretty much hard coded at registry bootstrap level https github.com spring cloud spring cloud dataflow blob master spring cloud dataflow admin src main java org springframework cloud dataflow admin config ModuleRegistryPopulator.java L75 . ","As an s c d user",", I'd like to contribute modules that immediately reflects in module registry,","so I can create stream or task definitions using the shell rest api s. Currently the registry isn t flexible, as it is pretty much hard coded at registry bootstrap level https github.com spring cloud spring cloud dataflow blob master spring cloud dataflow admin src main java org springframework cloud dataflow admin config ModuleRegistryPopulator.java L75 .","As an s c d user, I'd like to contribute modules that immediately reflects in module registry, so I can create stream or task definitions using the shell rest api s<span class='highlight-text severity-high'>. Currently the registry isn t flexible, as it is pretty much hard coded at registry bootstrap level https github.com spring cloud spring cloud dataflow blob master spring cloud dataflow admin src main java org springframework cloud dataflow admin config ModuleRegistryPopulator.java L75 . </span>","minimal","punctuation","high",False
18188,"As an s c d user, I'd like to contribute modules that immediately reflects in module registry, so I can create stream or task definitions using the shell rest api s. Currently the registry isn t flexible, as it is pretty much hard coded at registry bootstrap level https github.com spring cloud spring cloud dataflow blob master spring cloud dataflow admin src main java org springframework cloud dataflow admin config ModuleRegistryPopulator.java L75 . ","As an s c d user",", I'd like to contribute modules that immediately reflects in module registry,","so I can create stream or task definitions using the shell rest api s. Currently the registry isn t flexible, as it is pretty much hard coded at registry bootstrap level https github.com spring cloud spring cloud dataflow blob master spring cloud dataflow admin src main java org springframework cloud dataflow admin config ModuleRegistryPopulator.java L75 .","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18191,"As an s c d user, I'd like to refer to documentation on direct binding , so I can use it as a reference to deploy a stream that includes directly bound modules. Example code java jar spring cloud stream module launcher target spring cloud stream module launcher 1.0.0.BUILD SNAPSHOT.jar modules org.springframework.cloud.stream.module time source 1.0.0.BUILD SNAPSHOT,org.springframework.cloud.stream.module filter processor 1.0.0.BUILD SNAPSHOT,org.springframework.cloud.stream.module filter processor 1.0.0.BUILD SNAPSHOT args.0.fixedDelay 7 args.1.expression payload.contains 6 aggregate true spring.cloud.stream.bindings.output filtered code ","As an s c d user",", I'd like to refer to documentation on direct binding ,","so I can use it as a reference to deploy a stream that includes directly bound modules. Example code java jar spring cloud stream module launcher target spring cloud stream module launcher 1.0.0.BUILD SNAPSHOT.jar modules org.springframework.cloud.stream.module time source 1.0.0.BUILD SNAPSHOT,org.springframework.cloud.stream.module filter processor 1.0.0.BUILD SNAPSHOT,org.springframework.cloud.stream.module filter processor 1.0.0.BUILD SNAPSHOT args.0.fixedDelay 7 args.1.expression payload.contains 6 aggregate true spring.cloud.stream.bindings.output filtered code","As an s c d user, I'd like to refer to documentation on direct binding , so I can use it as a reference to deploy a stream that includes directly bound modules<span class='highlight-text severity-high'>. Example code java jar spring cloud stream module launcher target spring cloud stream module launcher 1.0.0.BUILD SNAPSHOT.jar modules org.springframework.cloud.stream.module time source 1.0.0.BUILD SNAPSHOT,org.springframework.cloud.stream.module filter processor 1.0.0.BUILD SNAPSHOT,org.springframework.cloud.stream.module filter processor 1.0.0.BUILD SNAPSHOT args.0.fixedDelay 7 args.1.expression payload.contains 6 aggregate true spring.cloud.stream.bindings.output filtered code </span>","minimal","punctuation","high",False
18191,"As an s c d user, I'd like to refer to documentation on direct binding , so I can use it as a reference to deploy a stream that includes directly bound modules. Example code java jar spring cloud stream module launcher target spring cloud stream module launcher 1.0.0.BUILD SNAPSHOT.jar modules org.springframework.cloud.stream.module time source 1.0.0.BUILD SNAPSHOT,org.springframework.cloud.stream.module filter processor 1.0.0.BUILD SNAPSHOT,org.springframework.cloud.stream.module filter processor 1.0.0.BUILD SNAPSHOT args.0.fixedDelay 7 args.1.expression payload.contains 6 aggregate true spring.cloud.stream.bindings.output filtered code ","As an s c d user",", I'd like to refer to documentation on direct binding ,","so I can use it as a reference to deploy a stream that includes directly bound modules. Example code java jar spring cloud stream module launcher target spring cloud stream module launcher 1.0.0.BUILD SNAPSHOT.jar modules org.springframework.cloud.stream.module time source 1.0.0.BUILD SNAPSHOT,org.springframework.cloud.stream.module filter processor 1.0.0.BUILD SNAPSHOT,org.springframework.cloud.stream.module filter processor 1.0.0.BUILD SNAPSHOT args.0.fixedDelay 7 args.1.expression payload.contains 6 aggregate true spring.cloud.stream.bindings.output filtered code","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18195,"We do set a default value in xd lib spring xd dirt 1.2.1.RELEASE.jar application.yml code ... xd data home file XD HOME data config home file XD HOME config module home file XD HOME modules customModule home file XD HOME custom modules ui home file XD HOME spring xd ui dist allow origin http localhost 9889 ... code We need to document how this property can be used to allow for external services to use the XD Rest API and how you can customize it using servers.yml ",NULL,"We do set a default value in xd lib spring xd dirt 1.2.1.RELEASE.jar application.yml code ... xd data home file XD HOME data config home file XD HOME config module home file XD HOME modules customModule home file XD HOME custom modules ui home file XD HOME spring xd ui dist allow origin http localhost 9889 ... code We need to document how this property can be used to allow for external services to use the XD Rest API and how you can customize it using servers.yml ",NULL,"Add for who this story is","well_formed","no_role","high",False
18195,"We do set a default value in xd lib spring xd dirt 1.2.1.RELEASE.jar application.yml code ... xd data home file XD HOME data config home file XD HOME config module home file XD HOME modules customModule home file XD HOME custom modules ui home file XD HOME spring xd ui dist allow origin http localhost 9889 ... code We need to document how this property can be used to allow for external services to use the XD Rest API and how you can customize it using servers.yml ",NULL,"We do set a default value in xd lib spring xd dirt 1.2.1.RELEASE.jar application.yml code ... xd data home file XD HOME data config home file XD HOME config module home file XD HOME modules customModule home file XD HOME custom modules ui home file XD HOME spring xd ui dist allow origin http localhost 9889 ... code We need to document how this property can be used to allow for external services to use the XD Rest API and how you can customize it using servers.yml ",NULL,"We do set a default value in xd lib spring xd dirt 1.2.1.RELEASE.jar application.yml code ... xd data home file XD HOME data config home file XD HOME config module home file XD HOME modules customModule home file XD HOME custom modules ui home file XD HOME spring xd ui dist allow origin http localhost 9889 ... code We need to document how this property can be used to allow for external services to use the XD Rest API<span class='highlight-text severity-high'> and </span>how you can customize it using servers.yml ","atomic","conjunctions","high",False
18195,"We do set a default value in xd lib spring xd dirt 1.2.1.RELEASE.jar application.yml code ... xd data home file XD HOME data config home file XD HOME config module home file XD HOME modules customModule home file XD HOME custom modules ui home file XD HOME spring xd ui dist allow origin http localhost 9889 ... code We need to document how this property can be used to allow for external services to use the XD Rest API and how you can customize it using servers.yml ",NULL,"We do set a default value in xd lib spring xd dirt 1.2.1.RELEASE.jar application.yml code ... xd data home file XD HOME data config home file XD HOME config module home file XD HOME modules customModule home file XD HOME custom modules ui home file XD HOME spring xd ui dist allow origin http localhost 9889 ... code We need to document how this property can be used to allow for external services to use the XD Rest API and how you can customize it using servers.yml ",NULL,"We do set a default value in xd lib spring xd dirt 1<span class='highlight-text severity-high'>.2.1.RELEASE.jar application.yml code ... xd data home file XD HOME data config home file XD HOME config module home file XD HOME modules customModule home file XD HOME custom modules ui home file XD HOME spring xd ui dist allow origin http localhost 9889 ... code We need to document how this property can be used to allow for external services to use the XD Rest API and how you can customize it using servers.yml </span>","minimal","punctuation","high",False
18255,"As a module author, I want to be able to test my code in next to real world conditions ie Integration Testing, but not really I want all my module wiring to be testable I want all my module configuration ConfigurationProperties to be in effect, and I want to be able to test various combination of props I want to be able to send data to my module and assert what is coming at the other end I want an idiomatic way of asserting the above eg integration with Hamcrest, etc I DONT want to have to send data to an actual bus redis, rabbit, etc ","As a module author,","I want to be able to test my code in next to real world conditions ie Integration Testing, but not really I want all my module wiring to be testable I want all my module configuration ConfigurationProperties to be in effect, and I want to be able to test various combination of props I want to be able to send data to my module and assert what is coming at the other end I want an idiomatic way of asserting the above eg integration with Hamcrest, etc I DONT want to have to send data to an actual bus redis, rabbit, etc",NULL,"As a module author, I want to be able to test my code in next to real world conditions ie Integration Testing, but not really I want all my module wiring to be testable I want all my module configuration ConfigurationProperties to be in effect,<span class='highlight-text severity-high'> and </span>I want to be able to test various combination of props I want to be able to send data to my module and assert what is coming at the other end I want an idiomatic way of asserting the above eg integration with Hamcrest, etc I DONT want to have to send data to an actual bus redis, rabbit, etc ","atomic","conjunctions","high",False
18255,"As a module author, I want to be able to test my code in next to real world conditions ie Integration Testing, but not really I want all my module wiring to be testable I want all my module configuration ConfigurationProperties to be in effect, and I want to be able to test various combination of props I want to be able to send data to my module and assert what is coming at the other end I want an idiomatic way of asserting the above eg integration with Hamcrest, etc I DONT want to have to send data to an actual bus redis, rabbit, etc ","As a module author,","I want to be able to test my code in next to real world conditions ie Integration Testing, but not really I want all my module wiring to be testable I want all my module configuration ConfigurationProperties to be in effect, and I want to be able to test various combination of props I want to be able to send data to my module and assert what is coming at the other end I want an idiomatic way of asserting the above eg integration with Hamcrest, etc I DONT want to have to send data to an actual bus redis, rabbit, etc",NULL,"As a module author,<span class='highlight-text severity-high'> I want to </span>be able to test my code in next to real world conditions ie Integration Testing, but not really<span class='highlight-text severity-high'> I want </span>all my module wiring to be testable<span class='highlight-text severity-high'> I want </span>all my module configuration ConfigurationProperties to be in effect, and<span class='highlight-text severity-high'> I want to </span>be able to test various combination of props<span class='highlight-text severity-high'> I want to </span>be able to send data to my module and assert what is coming at the other end<span class='highlight-text severity-high'> I want </span>an idiomatic way of asserting the above eg integration with Hamcrest, etc I DONT want to have to send data to an actual bus redis, rabbit, etc ","minimal","indicator_repetition","high",False
18272,"User can configure spring cloud data via via Spring Cloud Config, data admin.yml or Spring Cloud Connector Add bootstrap.yml to spring cloud data create a default data admin.yml and configure spring data to look for this vs application.yml. Spring Cloud Data will have Spring Cloud Config enabled by default User has the ability to disable it via the bootstrap.yml",NULL,"User can configure spring cloud data via via Spring Cloud Config, data admin.yml or Spring Cloud Connector Add bootstrap.yml to spring cloud data create a default data admin.yml and configure spring data to look for this vs application.yml. Spring Cloud Data will have Spring Cloud Config enabled by default User has the ability to disable it via the bootstrap.yml",NULL,"Add for who this story is","well_formed","no_role","high",False
18910,"Add configuration for the partition strategy to HDFS sink to support writing files into subdirectories based on a partition key provided in the header or field in the message of the stream data. The writing using HDFS Store DataWriter should pass in the partition key value to be used for the write operation. Partition configuration could be made available to the sink using a format parameter that could then be used in XML config like code expression new java.text.SimpleDateFormat format .format timestamp code Similar to the time source.",NULL,"Add configuration for the partition strategy to HDFS sink to support writing files into subdirectories based on a partition key provided in the header or field in the message of the stream data. The writing using HDFS Store DataWriter should pass in the partition key value to be used for the write operation. Partition configuration could be made available to the sink using a format parameter that could then be used in XML config like code expression new java.text.SimpleDateFormat format .format timestamp code Similar to the time source.",NULL,"Add for who this story is","well_formed","no_role","high",False
18272,"User can configure spring cloud data via via Spring Cloud Config, data admin.yml or Spring Cloud Connector Add bootstrap.yml to spring cloud data create a default data admin.yml and configure spring data to look for this vs application.yml. Spring Cloud Data will have Spring Cloud Config enabled by default User has the ability to disable it via the bootstrap.yml",NULL,"User can configure spring cloud data via via Spring Cloud Config, data admin.yml or Spring Cloud Connector Add bootstrap.yml to spring cloud data create a default data admin.yml and configure spring data to look for this vs application.yml. Spring Cloud Data will have Spring Cloud Config enabled by default User has the ability to disable it via the bootstrap.yml",NULL,"User can configure spring cloud data via via Spring Cloud Config, data admin.yml<span class='highlight-text severity-high'> or </span>Spring Cloud Connector Add bootstrap.yml to spring cloud data create a default data admin.yml<span class='highlight-text severity-high'> and </span>configure spring data to look for this vs application.yml. Spring Cloud Data will have Spring Cloud Config enabled by default User has the ability to disable it via the bootstrap.yml","atomic","conjunctions","high",False
18272,"User can configure spring cloud data via via Spring Cloud Config, data admin.yml or Spring Cloud Connector Add bootstrap.yml to spring cloud data create a default data admin.yml and configure spring data to look for this vs application.yml. Spring Cloud Data will have Spring Cloud Config enabled by default User has the ability to disable it via the bootstrap.yml",NULL,"User can configure spring cloud data via via Spring Cloud Config, data admin.yml or Spring Cloud Connector Add bootstrap.yml to spring cloud data create a default data admin.yml and configure spring data to look for this vs application.yml. Spring Cloud Data will have Spring Cloud Config enabled by default User has the ability to disable it via the bootstrap.yml",NULL,"User can configure spring cloud data via via Spring Cloud Config, data admin<span class='highlight-text severity-high'>.yml or Spring Cloud Connector Add bootstrap.yml to spring cloud data create a default data admin.yml and configure spring data to look for this vs application.yml. Spring Cloud Data will have Spring Cloud Config enabled by default User has the ability to disable it via the bootstrap.yml</span>","minimal","punctuation","high",False
18281,"As a s c d developer, I'd like to implement undeploy operation for singlenode single JVM , so I can use this target to undeploy a running stream. More details in this PR https github.com spring cloud spring cloud data pull 19 . Note Its a prerequisite to determine consistent undeploy strategy for both jobs and streams . ","As a s c d developer",", I'd like to implement undeploy operation for singlenode single JVM ,","so I can use this target to undeploy a running stream. More details in this PR https github.com spring cloud spring cloud data pull 19 . Note Its a prerequisite to determine consistent undeploy strategy for both jobs and streams .","As a s c d developer, I'd like to implement undeploy operation for singlenode single JVM , so I can use this target to undeploy a running stream<span class='highlight-text severity-high'>. More details in this PR https github.com spring cloud spring cloud data pull 19 . Note Its a prerequisite to determine consistent undeploy strategy for both jobs and streams . </span>","minimal","punctuation","high",False
18281,"As a s c d developer, I'd like to implement undeploy operation for singlenode single JVM , so I can use this target to undeploy a running stream. More details in this PR https github.com spring cloud spring cloud data pull 19 . Note Its a prerequisite to determine consistent undeploy strategy for both jobs and streams . ","As a s c d developer",", I'd like to implement undeploy operation for singlenode single JVM ,","so I can use this target to undeploy a running stream. More details in this PR https github.com spring cloud spring cloud data pull 19 . Note Its a prerequisite to determine consistent undeploy strategy for both jobs and streams .","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18325,"Add Pagination to Containers Page",NULL,"Add Pagination to Containers Page",NULL,"Add for who this story is","well_formed","no_role","high",False
18338,"As a developer, I'd like to create an annotation EnableModule driven programming model for modules, so instead of explicitly defining I O channels as beans on the module, for classes annotated with EnableModule , the application would be responsible for creating the actual channel beans and channel adapters vs. the developer creating concrete channel instance types. The Input and Output annotations will be used to indicate the input and output channels of the module. ","As a developer",", I'd like to create an annotation EnableModule driven programming model for modules,","so instead of explicitly defining I O channels as beans on the module, for classes annotated with EnableModule , the application would be responsible for creating the actual channel beans and channel adapters vs. the developer creating concrete channel instance types. The Input and Output annotations will be used to indicate the input and output channels of the module.","As a developer, I'd like to create an annotation EnableModule driven programming model for modules, so instead of explicitly defining I O channels as beans on the module, for classes annotated with EnableModule , the application would be responsible for creating the actual channel beans and channel adapters vs<span class='highlight-text severity-high'>. the developer creating concrete channel instance types. The Input and Output annotations will be used to indicate the input and output channels of the module. </span>","minimal","punctuation","high",False
18338,"As a developer, I'd like to create an annotation EnableModule driven programming model for modules, so instead of explicitly defining I O channels as beans on the module, for classes annotated with EnableModule , the application would be responsible for creating the actual channel beans and channel adapters vs. the developer creating concrete channel instance types. The Input and Output annotations will be used to indicate the input and output channels of the module. ","As a developer",", I'd like to create an annotation EnableModule driven programming model for modules,","so instead of explicitly defining I O channels as beans on the module, for classes annotated with EnableModule , the application would be responsible for creating the actual channel beans and channel adapters vs. the developer creating concrete channel instance types. The Input and Output annotations will be used to indicate the input and output channels of the module.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18348,"As a user I would like to connect the Sqoop batch job to Teradata for import jobs. I have tried the Teradata JDBC driver directly using code job create tdTest definition sqoop command import args table Frequent Flyers connect jdbc teradata tdexpress DATABASE transportation driver com.teradata.jdbc.TeraDriver username dbc password dbc target dir xd teradata num mappers 1 code but that results in an NPE. The only way so far is to use the Hortonworks Connector for Teradata http public repo 1.hortonworks.com HDP tools 2.2.4.2 hdp connector for teradata 1.3.4.2.2.4.2 2 distro.tar.gz That one allows me to use the following code job create tdTest definition sqoop command import args table Frequent Flyers connect jdbc teradata tdexpress DATABASE transportation connection manager org.apache.sqoop.teradata.TeradataConnManager username dbc password dbc target dir xd teradata num mappers 1 code ","As a user","I would like to connect the Sqoop batch job to Teradata for import jobs. I have tried the Teradata JDBC driver directly using code job create tdTest definition sqoop command import args table Frequent Flyers connect jdbc teradata tdexpress DATABASE transportation driver com.teradata.jdbc.TeraDriver username dbc password dbc target dir xd teradata num mappers 1 code but that results in an NPE. The only way","so far is to use the Hortonworks Connector for Teradata http public repo 1.hortonworks.com HDP tools 2.2.4.2 hdp connector for teradata 1.3.4.2.2.4.2 2 distro.tar.gz That one allows me to use the following code job create tdTest definition sqoop command import args table Frequent Flyers connect jdbc teradata tdexpress DATABASE transportation connection manager org.apache.sqoop.teradata.TeradataConnManager username dbc password dbc target dir xd teradata num mappers 1 code","As a user I would like to connect the Sqoop batch job to Teradata for import jobs<span class='highlight-text severity-high'>. I have tried the Teradata JDBC driver directly using code job create tdTest definition sqoop command import args table Frequent Flyers connect jdbc teradata tdexpress DATABASE transportation driver com.teradata.jdbc.TeraDriver username dbc password dbc target dir xd teradata num mappers 1 code but that results in an NPE. The only way so far is to use the Hortonworks Connector for Teradata http public repo 1.hortonworks.com HDP tools 2.2.4.2 hdp connector for teradata 1.3.4.2.2.4.2 2 distro.tar.gz That one allows me to use the following code job create tdTest definition sqoop command import args table Frequent Flyers connect jdbc teradata tdexpress DATABASE transportation connection manager org.apache.sqoop.teradata.TeradataConnManager username dbc password dbc target dir xd teradata num mappers 1 code </span>","minimal","punctuation","high",False
18348,"As a user I would like to connect the Sqoop batch job to Teradata for import jobs. I have tried the Teradata JDBC driver directly using code job create tdTest definition sqoop command import args table Frequent Flyers connect jdbc teradata tdexpress DATABASE transportation driver com.teradata.jdbc.TeraDriver username dbc password dbc target dir xd teradata num mappers 1 code but that results in an NPE. The only way so far is to use the Hortonworks Connector for Teradata http public repo 1.hortonworks.com HDP tools 2.2.4.2 hdp connector for teradata 1.3.4.2.2.4.2 2 distro.tar.gz That one allows me to use the following code job create tdTest definition sqoop command import args table Frequent Flyers connect jdbc teradata tdexpress DATABASE transportation connection manager org.apache.sqoop.teradata.TeradataConnManager username dbc password dbc target dir xd teradata num mappers 1 code ","As a user","I would like to connect the Sqoop batch job to Teradata for import jobs. I have tried the Teradata JDBC driver directly using code job create tdTest definition sqoop command import args table Frequent Flyers connect jdbc teradata tdexpress DATABASE transportation driver com.teradata.jdbc.TeraDriver username dbc password dbc target dir xd teradata num mappers 1 code but that results in an NPE. The only way","so far is to use the Hortonworks Connector for Teradata http public repo 1.hortonworks.com HDP tools 2.2.4.2 hdp connector for teradata 1.3.4.2.2.4.2 2 distro.tar.gz That one allows me to use the following code job create tdTest definition sqoop command import args table Frequent Flyers connect jdbc teradata tdexpress DATABASE transportation connection manager org.apache.sqoop.teradata.TeradataConnManager username dbc password dbc target dir xd teradata num mappers 1 code","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18356,"As a user, I'd like to have the module app specific metrics consumed directly from Boot actuator export https github.com spring projects spring boot blob master spring boot actuator src main java org springframework boot actuate autoconfigure MetricRepositoryAutoConfiguration.java API, so I can have insight on how it is performing, being used and that it works etc. ","As a user",", I'd like to have the module app specific metrics consumed directly from Boot actuator export https github.com spring projects spring boot blob master spring boot actuator src main java org springframework boot actuate autoconfigure MetricRepositoryAutoConfiguration.java API,","so I can have insight on how it is performing, being used and that it works etc.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18827,"Port https github.com spring projects spring xd blob master src test scripts gemfire stream tests Need to consider how to start the server, maybe use the jvm fork utilities? Look into spring data gemfire as well.",NULL,"Port https github.com spring projects spring xd blob master src test scripts gemfire stream tests Need to consider how to start the server, maybe use the jvm fork utilities? Look into spring data gemfire as well.",NULL,"Add for who this story is","well_formed","no_role","high",False
18827,"Port https github.com spring projects spring xd blob master src test scripts gemfire stream tests Need to consider how to start the server, maybe use the jvm fork utilities? Look into spring data gemfire as well.",NULL,"Port https github.com spring projects spring xd blob master src test scripts gemfire stream tests Need to consider how to start the server, maybe use the jvm fork utilities? Look into spring data gemfire as well.",NULL,"Port https github.com spring projects spring xd blob master src test scripts gemfire stream tests Need to consider how to start the server, maybe use the jvm fork utilities<span class='highlight-text severity-high'>? Look into spring data gemfire as well.</span>","minimal","punctuation","high",False
18844,"This will eliminate any race conditions between deployments and containers joining leaving the cluster.",NULL,"This will eliminate any race conditions between deployments and containers joining leaving the cluster.",NULL,"Add for who this story is","well_formed","no_role","high",False
18844,"This will eliminate any race conditions between deployments and containers joining leaving the cluster.",NULL,"This will eliminate any race conditions between deployments and containers joining leaving the cluster.",NULL,"This will eliminate any race conditions between deployments<span class='highlight-text severity-high'> and </span>containers joining leaving the cluster.","atomic","conjunctions","high",False
18386,"Occasional CI test build failures quote Caused by java.lang.IllegalStateException Container cache not initialized likely as a result of a ZooKeeper connection error at org.springframework.util.Assert.state Assert.java 385 at org.springframework.xd.dirt.container.store.ZooKeeperContainerRepository.ensureCache ZooKeeperContainerRepository.java 184 at org.springframework.xd.dirt.container.store.ZooKeeperContainerRepository.findOne ZooKeeperContainerRepository.java 263 quote e.g. https build.spring.io browse XD JDK8 JOB1 1514 Add logging to ensureCache e.g. in childEvent and closeCache to log that the cache was closed; it appears that s the only way the cache not initialized message can be emitted.",NULL,"Occasional CI test build failures quote Caused by java.lang.IllegalStateException Container cache not initialized likely as a result of a ZooKeeper connection error at org.springframework.util.Assert.state Assert.java 385 at org.springframework.xd.dirt.container.store.ZooKeeperContainerRepository.ensureCache ZooKeeperContainerRepository.java 184 at org.springframework.xd.dirt.container.store.ZooKeeperContainerRepository.findOne ZooKeeperContainerRepository.java 263 quote e.g. https build.spring.io browse XD JDK8 JOB1 1514 Add logging to ensureCache e.g. in childEvent and closeCache to log that the cache was closed; it appears that s the only way the cache not initialized message can be emitted.",NULL,"Add for who this story is","well_formed","no_role","high",False
18386,"Occasional CI test build failures quote Caused by java.lang.IllegalStateException Container cache not initialized likely as a result of a ZooKeeper connection error at org.springframework.util.Assert.state Assert.java 385 at org.springframework.xd.dirt.container.store.ZooKeeperContainerRepository.ensureCache ZooKeeperContainerRepository.java 184 at org.springframework.xd.dirt.container.store.ZooKeeperContainerRepository.findOne ZooKeeperContainerRepository.java 263 quote e.g. https build.spring.io browse XD JDK8 JOB1 1514 Add logging to ensureCache e.g. in childEvent and closeCache to log that the cache was closed; it appears that s the only way the cache not initialized message can be emitted.",NULL,"Occasional CI test build failures quote Caused by java.lang.IllegalStateException Container cache not initialized likely as a result of a ZooKeeper connection error at org.springframework.util.Assert.state Assert.java 385 at org.springframework.xd.dirt.container.store.ZooKeeperContainerRepository.ensureCache ZooKeeperContainerRepository.java 184 at org.springframework.xd.dirt.container.store.ZooKeeperContainerRepository.findOne ZooKeeperContainerRepository.java 263 quote e.g. https build.spring.io browse XD JDK8 JOB1 1514 Add logging to ensureCache e.g. in childEvent and closeCache to log that the cache was closed; it appears that s the only way the cache not initialized message can be emitted.",NULL,"Occasional CI test build failures quote Caused by java.lang.IllegalStateException Container cache not initialized likely as a result of a ZooKeeper connection error at org.springframework.util.Assert.state Assert.java 385 at org.springframework.xd.dirt.container.store.ZooKeeperContainerRepository.ensureCache ZooKeeperContainerRepository.java 184 at org.springframework.xd.dirt.container.store.ZooKeeperContainerRepository.findOne ZooKeeperContainerRepository.java 263 quote e.g. https build.spring.io browse XD JDK8 JOB1 1514 Add logging to ensureCache e.g. in childEvent<span class='highlight-text severity-high'> and </span>closeCache to log that the cache was closed; it appears that s the only way the cache not initialized message can be emitted.","atomic","conjunctions","high",False
18386,"Occasional CI test build failures quote Caused by java.lang.IllegalStateException Container cache not initialized likely as a result of a ZooKeeper connection error at org.springframework.util.Assert.state Assert.java 385 at org.springframework.xd.dirt.container.store.ZooKeeperContainerRepository.ensureCache ZooKeeperContainerRepository.java 184 at org.springframework.xd.dirt.container.store.ZooKeeperContainerRepository.findOne ZooKeeperContainerRepository.java 263 quote e.g. https build.spring.io browse XD JDK8 JOB1 1514 Add logging to ensureCache e.g. in childEvent and closeCache to log that the cache was closed; it appears that s the only way the cache not initialized message can be emitted.",NULL,"Occasional CI test build failures quote Caused by java.lang.IllegalStateException Container cache not initialized likely as a result of a ZooKeeper connection error at org.springframework.util.Assert.state Assert.java 385 at org.springframework.xd.dirt.container.store.ZooKeeperContainerRepository.ensureCache ZooKeeperContainerRepository.java 184 at org.springframework.xd.dirt.container.store.ZooKeeperContainerRepository.findOne ZooKeeperContainerRepository.java 263 quote e.g. https build.spring.io browse XD JDK8 JOB1 1514 Add logging to ensureCache e.g. in childEvent and closeCache to log that the cache was closed; it appears that s the only way the cache not initialized message can be emitted.",NULL,"Occasional CI test build failures quote Caused by java<span class='highlight-text severity-high'>.lang.IllegalStateException Container cache not initialized likely as a result of a ZooKeeper connection error at org.springframework.util.Assert.state Assert.java 385 at org.springframework.xd.dirt.container.store.ZooKeeperContainerRepository.ensureCache ZooKeeperContainerRepository.java 184 at org.springframework.xd.dirt.container.store.ZooKeeperContainerRepository.findOne ZooKeeperContainerRepository.java 263 quote e.g. https build.spring.io browse XD JDK8 JOB1 1514 Add logging to ensureCache e.g. in childEvent and closeCache to log that the cache was closed; it appears that s the only way the cache not initialized message can be emitted.</span>","minimal","punctuation","high",False
18403,"As a developer, I'd like to bench Rabbit on rackspace infrastructure, so I can have a sense on how it scales as we add more xd container nodes.","As a developer",", I'd like to bench Rabbit on rackspace infrastructure,","so I can have a sense on how it scales as we add more xd container nodes.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18422,"The file and ftp sources allow working with either the java.io.File or its contents. For consistency, the sftp source should support the same mechanism. ",NULL,"The file and ftp sources allow working with either the java.io.File or its contents. For consistency, the sftp source should support the same mechanism. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18422,"The file and ftp sources allow working with either the java.io.File or its contents. For consistency, the sftp source should support the same mechanism. ",NULL,"The file and ftp sources allow working with either the java.io.File or its contents. For consistency, the sftp source should support the same mechanism. ",NULL,"The file<span class='highlight-text severity-high'> and </span>ftp sources allow working with either the java.io.File<span class='highlight-text severity-high'> or </span>its contents. For consistency, the sftp source should support the same mechanism. ","atomic","conjunctions","high",False
18422,"The file and ftp sources allow working with either the java.io.File or its contents. For consistency, the sftp source should support the same mechanism. ",NULL,"The file and ftp sources allow working with either the java.io.File or its contents. For consistency, the sftp source should support the same mechanism. ",NULL,"The file and ftp sources allow working with either the java<span class='highlight-text severity-high'>.io.File or its contents. For consistency, the sftp source should support the same mechanism. </span>","minimal","punctuation","high",False
18429,"As a Flo developer, I'd like to have a new DSL parser, so I can easily detect incorrect module option values when supplied from the Flo UI. Example MyStream mail log tap stream MyStream.bar log If parsed separately which Flo UI'does , the current parser endpoint will barf on the second stream because it doesn t know about the first stream MyStream . ","As a Flo developer",", I'd like to have a new DSL parser,","so I can easily detect incorrect module option values when supplied from the Flo UI. Example MyStream mail log tap stream MyStream.bar log If parsed separately which Flo UI'does , the current parser endpoint will barf on the second stream because it doesn t know about the first stream MyStream .","As a Flo developer, I'd like to have a new DSL parser, so I can easily detect incorrect module option values when supplied from the Flo UI<span class='highlight-text severity-high'>. Example MyStream mail log tap stream MyStream.bar log If parsed separately which Flo UI'does , the current parser endpoint will barf on the second stream because it doesn t know about the first stream MyStream . </span>","minimal","punctuation","high",False
18429,"As a Flo developer, I'd like to have a new DSL parser, so I can easily detect incorrect module option values when supplied from the Flo UI. Example MyStream mail log tap stream MyStream.bar log If parsed separately which Flo UI'does , the current parser endpoint will barf on the second stream because it doesn t know about the first stream MyStream . ","As a Flo developer",", I'd like to have a new DSL parser,","so I can easily detect incorrect module option values when supplied from the Flo UI. Example MyStream mail log tap stream MyStream.bar log If parsed separately which Flo UI'does , the current parser endpoint will barf on the second stream because it doesn t know about the first stream MyStream .","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18437,"In XD today we use commons logging or slf4j APIs bound to log4j at runtime configured with log4j.properties . Boot uses slf4j APIs backed by logback. This causes some build incompatibilities building a component that depends on spring xd dirt and spring boot, requiring specific dependency exclusions. In order to simplify building and troubleshooting log dependencies, XD should standardize on slf4j APIs replace any commons logging Loggers with Slf4j . This is internal only, and would not impact users who are used to seeing log4j.properties. An additional step is to replace log4j with logback. This change would be visible to end users but will provide us greater affinity with boot and improve the developer experience. If we make this change it should go into 1.2 GA. ",NULL,"In XD today we use commons logging or slf4j APIs bound to log4j at runtime configured with log4j.properties . Boot uses slf4j APIs backed by logback. This causes some build incompatibilities building a component that depends on spring xd dirt and spring boot, requiring specific dependency exclusions.","In order to simplify building and troubleshooting log dependencies, XD should standardize on slf4j APIs replace any commons logging Loggers with Slf4j . This is internal only, and would not impact users who are used to seeing log4j.properties. An additional step is to replace log4j with logback. This change would be visible to end users but will provide us greater affinity with boot and improve the developer experience. If we make this change it should go into 1.2 GA.","Add for who this story is","well_formed","no_role","high",False
19246,"Here is an example the following request for streams http ec2 23 20 25 30.compute 1.amazonaws.com 9393 streams Returns This XML file does not appear to have any style information associated with it. The document tree is shown below. errors xmlns atom http www.w3.org 2005 Atom error logref HttpMessageNotWritableException message Could not marshal PagedResource content links http ec2 23 20 25 30.compute 1.amazonaws.com 9393 streams ticktock ;rel self , metadata Metadata number 0, total pages 1, total elements 1, size 20 , links null; nested exception is javax.xml.bind.MarshalException with linked exception com.sun.istack.SAXException2 unable to marshal type org.springframework.xd.rest.client.domain.StreamDefinitionResource as an element because it is not known to this context. message error errors ",NULL,"Here is an example the following request for streams http ec2 23 20 25 30.compute 1.amazonaws.com 9393 streams Returns This XML file does not appear to have any style information associated with it. The document tree is shown below. errors xmlns atom http www.w3.org 2005 Atom error logref HttpMessageNotWritableException message Could not marshal PagedResource content links http ec2 23 20 25 30.compute 1.amazonaws.com 9393 streams ticktock ;rel self , metadata Metadata number 0, total pages 1, total elements 1, size 20 , links null; nested exception is javax.xml.bind.MarshalException with linked exception com.sun.istack.SAXException2 unable to marshal type org.springframework.xd.rest.client.domain.StreamDefinitionResource as an element because it is not known to this context. message error errors ",NULL,"Add for who this story is","well_formed","no_role","high",False
18437,"In XD today we use commons logging or slf4j APIs bound to log4j at runtime configured with log4j.properties . Boot uses slf4j APIs backed by logback. This causes some build incompatibilities building a component that depends on spring xd dirt and spring boot, requiring specific dependency exclusions. In order to simplify building and troubleshooting log dependencies, XD should standardize on slf4j APIs replace any commons logging Loggers with Slf4j . This is internal only, and would not impact users who are used to seeing log4j.properties. An additional step is to replace log4j with logback. This change would be visible to end users but will provide us greater affinity with boot and improve the developer experience. If we make this change it should go into 1.2 GA. ",NULL,"In XD today we use commons logging or slf4j APIs bound to log4j at runtime configured with log4j.properties . Boot uses slf4j APIs backed by logback. This causes some build incompatibilities building a component that depends on spring xd dirt and spring boot, requiring specific dependency exclusions.","In order to simplify building and troubleshooting log dependencies, XD should standardize on slf4j APIs replace any commons logging Loggers with Slf4j . This is internal only, and would not impact users who are used to seeing log4j.properties. An additional step is to replace log4j with logback. This change would be visible to end users but will provide us greater affinity with boot and improve the developer experience. If we make this change it should go into 1.2 GA.","In XD today we use commons logging<span class='highlight-text severity-high'> or </span>slf4j APIs bound to log4j at runtime configured with log4j.properties . Boot uses slf4j APIs backed by logback. This causes some build incompatibilities building a component that depends on spring xd dirt<span class='highlight-text severity-high'> and </span>spring boot, requiring specific dependency exclusions. In order to simplify building and troubleshooting log dependencies, XD should standardize on slf4j APIs replace any commons logging Loggers with Slf4j . This is internal only, and would not impact users who are used to seeing log4j.properties. An additional step is to replace log4j with logback. This change would be visible to end users but will provide us greater affinity with boot and improve the developer experience. If we make this change it should go into 1.2 GA. ","atomic","conjunctions","high",False
18437,"In XD today we use commons logging or slf4j APIs bound to log4j at runtime configured with log4j.properties . Boot uses slf4j APIs backed by logback. This causes some build incompatibilities building a component that depends on spring xd dirt and spring boot, requiring specific dependency exclusions. In order to simplify building and troubleshooting log dependencies, XD should standardize on slf4j APIs replace any commons logging Loggers with Slf4j . This is internal only, and would not impact users who are used to seeing log4j.properties. An additional step is to replace log4j with logback. This change would be visible to end users but will provide us greater affinity with boot and improve the developer experience. If we make this change it should go into 1.2 GA. ",NULL,"In XD today we use commons logging or slf4j APIs bound to log4j at runtime configured with log4j.properties . Boot uses slf4j APIs backed by logback. This causes some build incompatibilities building a component that depends on spring xd dirt and spring boot, requiring specific dependency exclusions.","In order to simplify building and troubleshooting log dependencies, XD should standardize on slf4j APIs replace any commons logging Loggers with Slf4j . This is internal only, and would not impact users who are used to seeing log4j.properties. An additional step is to replace log4j with logback. This change would be visible to end users but will provide us greater affinity with boot and improve the developer experience. If we make this change it should go into 1.2 GA.","In XD today we use commons logging or slf4j APIs bound to log4j at runtime configured with log4j<span class='highlight-text severity-high'>.properties . Boot uses slf4j APIs backed by logback. This causes some build incompatibilities building a component that depends on spring xd dirt and spring boot, requiring specific dependency exclusions. In order to simplify building and troubleshooting log dependencies, XD should standardize on slf4j APIs replace any commons logging Loggers with Slf4j . This is internal only, and would not impact users who are used to seeing log4j.properties. An additional step is to replace log4j with logback. This change would be visible to end users but will provide us greater affinity with boot and improve the developer experience. If we make this change it should go into 1.2 GA. </span>","minimal","punctuation","high",False
18437,"In XD today we use commons logging or slf4j APIs bound to log4j at runtime configured with log4j.properties . Boot uses slf4j APIs backed by logback. This causes some build incompatibilities building a component that depends on spring xd dirt and spring boot, requiring specific dependency exclusions. In order to simplify building and troubleshooting log dependencies, XD should standardize on slf4j APIs replace any commons logging Loggers with Slf4j . This is internal only, and would not impact users who are used to seeing log4j.properties. An additional step is to replace log4j with logback. This change would be visible to end users but will provide us greater affinity with boot and improve the developer experience. If we make this change it should go into 1.2 GA. ",NULL,"In XD today we use commons logging or slf4j APIs bound to log4j at runtime configured with log4j.properties . Boot uses slf4j APIs backed by logback. This causes some build incompatibilities building a component that depends on spring xd dirt and spring boot, requiring specific dependency exclusions.","In order to simplify building and troubleshooting log dependencies, XD should standardize on slf4j APIs replace any commons logging Loggers with Slf4j . This is internal only, and would not impact users who are used to seeing log4j.properties. An additional step is to replace log4j with logback. This change would be visible to end users but will provide us greater affinity with boot and improve the developer experience. If we make this change it should go into 1.2 GA.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18452,"The Sqoop module is generating a SQL statement for table argument that is not correct for Oracle source. The Job definition is job create sqoop lookup definition sqoop command import args connect jdbc oracle thin XXXXXXXXX driver oracle.jdbc.OracleDriver direct username password table W LOOKUP D target dir user zeybeb ingest gdw masterdata lookup d num mappers 1 deploy the table W LOOKUP D results in Sqoop Object generation 13 22 59,798 INFO main manager.SqlManager Executing SQL statement SELECT t. FROM W LOOKUP D AS t WHERE 1 0 13 22 59,861 ERROR main manager.SqlManager Error executing statement java.sql.SQLSyntaxErrorException ORA 00933 SQL command not properly ended the SQL shoudl be generate with table name t instead of table name AS t The table argument does not except a schema name. User should be able to provide schema.table name syntax. ",NULL,"The Sqoop module is generating a SQL statement for table argument that is not correct for Oracle source. The Job definition is job create sqoop lookup definition sqoop command import args connect jdbc oracle thin XXXXXXXXX driver oracle.jdbc.OracleDriver direct username password table W LOOKUP D target dir user zeybeb ingest gdw masterdata lookup d num mappers 1 deploy the table W LOOKUP D results in Sqoop Object generation 13 22 59,798 INFO main manager.SqlManager Executing SQL statement SELECT t. FROM W LOOKUP D AS t WHERE 1 0 13 22 59,861 ERROR main manager.SqlManager Error executing statement java.sql.SQLSyntaxErrorException ORA 00933 SQL command not properly ended the SQL shoudl be generate with table name t instead of table name AS t The table argument does not except a schema name. User should be able to provide schema.table name syntax. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18452,"The Sqoop module is generating a SQL statement for table argument that is not correct for Oracle source. The Job definition is job create sqoop lookup definition sqoop command import args connect jdbc oracle thin XXXXXXXXX driver oracle.jdbc.OracleDriver direct username password table W LOOKUP D target dir user zeybeb ingest gdw masterdata lookup d num mappers 1 deploy the table W LOOKUP D results in Sqoop Object generation 13 22 59,798 INFO main manager.SqlManager Executing SQL statement SELECT t. FROM W LOOKUP D AS t WHERE 1 0 13 22 59,861 ERROR main manager.SqlManager Error executing statement java.sql.SQLSyntaxErrorException ORA 00933 SQL command not properly ended the SQL shoudl be generate with table name t instead of table name AS t The table argument does not except a schema name. User should be able to provide schema.table name syntax. ",NULL,"The Sqoop module is generating a SQL statement for table argument that is not correct for Oracle source. The Job definition is job create sqoop lookup definition sqoop command import args connect jdbc oracle thin XXXXXXXXX driver oracle.jdbc.OracleDriver direct username password table W LOOKUP D target dir user zeybeb ingest gdw masterdata lookup d num mappers 1 deploy the table W LOOKUP D results in Sqoop Object generation 13 22 59,798 INFO main manager.SqlManager Executing SQL statement SELECT t. FROM W LOOKUP D AS t WHERE 1 0 13 22 59,861 ERROR main manager.SqlManager Error executing statement java.sql.SQLSyntaxErrorException ORA 00933 SQL command not properly ended the SQL shoudl be generate with table name t instead of table name AS t The table argument does not except a schema name. User should be able to provide schema.table name syntax. ",NULL,"The Sqoop module is generating a SQL statement for table argument that is not correct for Oracle source<span class='highlight-text severity-high'>. The Job definition is job create sqoop lookup definition sqoop command import args connect jdbc oracle thin XXXXXXXXX driver oracle.jdbc.OracleDriver direct username password table W LOOKUP D target dir user zeybeb ingest gdw masterdata lookup d num mappers 1 deploy the table W LOOKUP D results in Sqoop Object generation 13 22 59,798 INFO main manager.SqlManager Executing SQL statement SELECT t. FROM W LOOKUP D AS t WHERE 1 0 13 22 59,861 ERROR main manager.SqlManager Error executing statement java.sql.SQLSyntaxErrorException ORA 00933 SQL command not properly ended the SQL shoudl be generate with table name t instead of table name AS t The table argument does not except a schema name. User should be able to provide schema.table name syntax. </span>","minimal","punctuation","high",False
18481,"As a developer, I'd like to add support for dynamic classpath for modules, so we can have the flexibility to load the right dependencies either based on module options 0 or via other properties such as including the dependencies from a specific location 1 . 0 code lib .jar lib distro .jar code 1 code xd.home lib hadoop distro .jar code Example code http hdfs distro PHD22 http myCustomModule classpath my funky dir http jpa provider eclipse jpa config lib something that is common.jar eclipse eclipse link 3.2.jar hibernate hibernate core 5.0.jar module.classpath lib .jar lib provider .jar code ","As a developer",", I'd like to add support for dynamic classpath for modules,","so we can have the flexibility to load the right dependencies either based on module options 0 or via other properties such as including the dependencies from a specific location 1 . 0 code lib .jar lib distro .jar code 1 code xd.home lib hadoop distro .jar code Example code http hdfs distro PHD22 http myCustomModule classpath my funky dir http jpa provider eclipse jpa config lib something that is common.jar eclipse eclipse link 3.2.jar hibernate hibernate core 5.0.jar module.classpath lib .jar lib provider .jar code","As a developer, I'd like to add support for dynamic classpath for modules, so we can have the flexibility to load the right dependencies either based on module options 0 or via other properties such as including the dependencies from a specific location 1 <span class='highlight-text severity-high'>. 0 code lib .jar lib distro .jar code 1 code xd.home lib hadoop distro .jar code Example code http hdfs distro PHD22 http myCustomModule classpath my funky dir http jpa provider eclipse jpa config lib something that is common.jar eclipse eclipse link 3.2.jar hibernate hibernate core 5.0.jar module.classpath lib .jar lib provider .jar code </span>","minimal","punctuation","high",False
18481,"As a developer, I'd like to add support for dynamic classpath for modules, so we can have the flexibility to load the right dependencies either based on module options 0 or via other properties such as including the dependencies from a specific location 1 . 0 code lib .jar lib distro .jar code 1 code xd.home lib hadoop distro .jar code Example code http hdfs distro PHD22 http myCustomModule classpath my funky dir http jpa provider eclipse jpa config lib something that is common.jar eclipse eclipse link 3.2.jar hibernate hibernate core 5.0.jar module.classpath lib .jar lib provider .jar code ","As a developer",", I'd like to add support for dynamic classpath for modules,","so we can have the flexibility to load the right dependencies either based on module options 0 or via other properties such as including the dependencies from a specific location 1 . 0 code lib .jar lib distro .jar code 1 code xd.home lib hadoop distro .jar code Example code http hdfs distro PHD22 http myCustomModule classpath my funky dir http jpa provider eclipse jpa config lib something that is common.jar eclipse eclipse link 3.2.jar hibernate hibernate core 5.0.jar module.classpath lib .jar lib provider .jar code","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18500,"It should be possible to configure a short description for a module that is display above the module options via module info name .... . The description could contain a few lines describing the core functionality and potentially hyperlinks to additional information for a module. This information should be exposed via the REST interface as well. Currently only the module options are printed.",NULL,"It should be possible to configure a short description for a module that is display above the module options via module info name .... . The description could contain a few lines describing the core functionality and potentially hyperlinks to additional information for a module. This information should be exposed via the REST interface as well. Currently only the module options are printed.",NULL,"Add for who this story is","well_formed","no_role","high",False
18500,"It should be possible to configure a short description for a module that is display above the module options via module info name .... . The description could contain a few lines describing the core functionality and potentially hyperlinks to additional information for a module. This information should be exposed via the REST interface as well. Currently only the module options are printed.",NULL,"It should be possible to configure a short description for a module that is display above the module options via module info name .... . The description could contain a few lines describing the core functionality and potentially hyperlinks to additional information for a module. This information should be exposed via the REST interface as well. Currently only the module options are printed.",NULL,"It should be possible to configure a short description for a module that is display above the module options via module info name <span class='highlight-text severity-high'>.... . The description could contain a few lines describing the core functionality and potentially hyperlinks to additional information for a module. This information should be exposed via the REST interface as well. Currently only the module options are printed.</span>","minimal","punctuation","high",False
18518,"As a developer, I'd like to document the changes to message headers so that users can refer to the troubleshooting section if there are any serialization errors when reusing the 1.0 batch jobs in 1.1 release. Perhaps this could be part of troubleshooting https github.com spring projects spring xd wiki Deployment troubleshooting section in our wiki.","As a developer",", I'd like to document the changes to message headers","so that users can refer to the troubleshooting section if there are any serialization errors when reusing the 1.0 batch jobs in 1.1 release. Perhaps this could be part of troubleshooting https github.com spring projects spring xd wiki Deployment troubleshooting section in our wiki.","As a developer, I'd like to document the changes to message headers so that users can refer to the troubleshooting section if there are any serialization errors when reusing the 1<span class='highlight-text severity-high'>.0 batch jobs in 1.1 release. Perhaps this could be part of troubleshooting https github.com spring projects spring xd wiki Deployment troubleshooting section in our wiki.</span>","minimal","punctuation","high",False
18518,"As a developer, I'd like to document the changes to message headers so that users can refer to the troubleshooting section if there are any serialization errors when reusing the 1.0 batch jobs in 1.1 release. Perhaps this could be part of troubleshooting https github.com spring projects spring xd wiki Deployment troubleshooting section in our wiki.","As a developer",", I'd like to document the changes to message headers","so that users can refer to the troubleshooting section if there are any serialization errors when reusing the 1.0 batch jobs in 1.1 release. Perhaps this could be part of troubleshooting https github.com spring projects spring xd wiki Deployment troubleshooting section in our wiki.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18523,"As a build manager, I'd like to have Spring XD RPMs published in spring.io repository so that users can directly download the bits without having to go through appsuite repo or the EULA. Location for 1.1.0 RELEASE http repo.spring.io libs release local org springframework xd spring xd 1.1.0.RELEASE ",NULL,"As a build manager, I'd like to have Spring XD RPMs published in spring.io repository","so that users can directly download the bits without having to go through appsuite repo or the EULA. Location for 1.1.0 RELEASE http repo.spring.io libs release local org springframework xd spring xd 1.1.0.RELEASE","Add for who this story is","well_formed","no_role","high",False
18523,"As a build manager, I'd like to have Spring XD RPMs published in spring.io repository so that users can directly download the bits without having to go through appsuite repo or the EULA. Location for 1.1.0 RELEASE http repo.spring.io libs release local org springframework xd spring xd 1.1.0.RELEASE ",NULL,"As a build manager, I'd like to have Spring XD RPMs published in spring.io repository","so that users can directly download the bits without having to go through appsuite repo or the EULA. Location for 1.1.0 RELEASE http repo.spring.io libs release local org springframework xd spring xd 1.1.0.RELEASE","As a build manager, I'd like to have Spring XD RPMs published in spring<span class='highlight-text severity-high'>.io repository so that users can directly download the bits without having to go through appsuite repo or the EULA. Location for 1.1.0 RELEASE http repo.spring.io libs release local org springframework xd spring xd 1.1.0.RELEASE </span>","minimal","punctuation","high",False
18523,"As a build manager, I'd like to have Spring XD RPMs published in spring.io repository so that users can directly download the bits without having to go through appsuite repo or the EULA. Location for 1.1.0 RELEASE http repo.spring.io libs release local org springframework xd spring xd 1.1.0.RELEASE ",NULL,"As a build manager, I'd like to have Spring XD RPMs published in spring.io repository","so that users can directly download the bits without having to go through appsuite repo or the EULA. Location for 1.1.0 RELEASE http repo.spring.io libs release local org springframework xd spring xd 1.1.0.RELEASE","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18531,"As a user, I'd like to have an optional arbitrary side channels created so that when creating a module channels other than the primary stream channels input, output could be added to the bus i.e. creating a tap channel within a flow . The optional side channels can be used to trace track module progress.","As a user",", I'd like to have an optional arbitrary side channels created","so that when creating a module channels other than the primary stream channels input, output could be added to the bus i.e. creating a tap channel within a flow . The optional side channels can be used to trace track module progress.","As a user, I'd like to have an optional arbitrary side channels created so that when creating a module channels other than the primary stream channels input, output could be added to the bus i<span class='highlight-text severity-high'>.e. creating a tap channel within a flow . The optional side channels can be used to trace track module progress.</span>","minimal","punctuation","high",False
18531,"As a user, I'd like to have an optional arbitrary side channels created so that when creating a module channels other than the primary stream channels input, output could be added to the bus i.e. creating a tap channel within a flow . The optional side channels can be used to trace track module progress.","As a user",", I'd like to have an optional arbitrary side channels created","so that when creating a module channels other than the primary stream channels input, output could be added to the bus i.e. creating a tap channel within a flow . The optional side channels can be used to trace track module progress.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18546,"Currently all message bus implementations are removed from the runtime classpath and loaded on demand from the file system according to the transport setting. Custom module projects that include in container testing must install messagebus local on the local file system. This is currently configured as a task for module build scripts. This is also a dependency for testing in the IDE and developers need to execute the build task or configure the messagebus manually. Embedding the local MB for the singlenode application local is not a valid transport for distributed eliminates this step.",NULL,"Currently all message bus implementations are removed from the runtime classpath and loaded on demand from the file system according to the transport setting. Custom module projects that include in container testing must install messagebus local on the local file system. This is currently configured as a task for module build scripts. This is also a dependency for testing in the IDE and developers need to execute the build task or configure the messagebus manually. Embedding the local MB for the singlenode application local is not a valid transport for distributed eliminates this step.",NULL,"Add for who this story is","well_formed","no_role","high",False
18546,"Currently all message bus implementations are removed from the runtime classpath and loaded on demand from the file system according to the transport setting. Custom module projects that include in container testing must install messagebus local on the local file system. This is currently configured as a task for module build scripts. This is also a dependency for testing in the IDE and developers need to execute the build task or configure the messagebus manually. Embedding the local MB for the singlenode application local is not a valid transport for distributed eliminates this step.",NULL,"Currently all message bus implementations are removed from the runtime classpath and loaded on demand from the file system according to the transport setting. Custom module projects that include in container testing must install messagebus local on the local file system. This is currently configured as a task for module build scripts. This is also a dependency for testing in the IDE and developers need to execute the build task or configure the messagebus manually. Embedding the local MB for the singlenode application local is not a valid transport for distributed eliminates this step.",NULL,"Currently all message bus implementations are removed from the runtime classpath<span class='highlight-text severity-high'> and </span>loaded on demand from the file system according to the transport setting. Custom module projects that include in container testing must install messagebus local on the local file system. This is currently configured as a task for module build scripts. This is also a dependency for testing in the IDE and developers need to execute the build task<span class='highlight-text severity-high'> or </span>configure the messagebus manually. Embedding the local MB for the singlenode application local is not a valid transport for distributed eliminates this step.","atomic","conjunctions","high",False
18546,"Currently all message bus implementations are removed from the runtime classpath and loaded on demand from the file system according to the transport setting. Custom module projects that include in container testing must install messagebus local on the local file system. This is currently configured as a task for module build scripts. This is also a dependency for testing in the IDE and developers need to execute the build task or configure the messagebus manually. Embedding the local MB for the singlenode application local is not a valid transport for distributed eliminates this step.",NULL,"Currently all message bus implementations are removed from the runtime classpath and loaded on demand from the file system according to the transport setting. Custom module projects that include in container testing must install messagebus local on the local file system. This is currently configured as a task for module build scripts. This is also a dependency for testing in the IDE and developers need to execute the build task or configure the messagebus manually. Embedding the local MB for the singlenode application local is not a valid transport for distributed eliminates this step.",NULL,"Currently all message bus implementations are removed from the runtime classpath and loaded on demand from the file system according to the transport setting<span class='highlight-text severity-high'>. Custom module projects that include in container testing must install messagebus local on the local file system. This is currently configured as a task for module build scripts. This is also a dependency for testing in the IDE and developers need to execute the build task or configure the messagebus manually. Embedding the local MB for the singlenode application local is not a valid transport for distributed eliminates this step.</span>","minimal","punctuation","high",False
18532,"As a user, I'd like to have an optional trace as inline deployment properties for stream so that I can declare which module in the stream needs to be traced for logging or notifications. This gives the flexibility to track the stage progress between individual modules. Example code xml xd stream create foo http log xd stream deploy foo properties module.http.trace,module.log.trace or xd stream deploy foo properties module. .trace code Wildcard wiretap config http docs.spring.io spring integration reference html messaging channels section.html channel global wiretap","As a user",", I'd like to have an optional trace as inline deployment properties for stream","so that I can declare which module in the stream needs to be traced for logging or notifications. This gives the flexibility to track the stage progress between individual modules. Example code xml xd stream create foo http log xd stream deploy foo properties module.http.trace,module.log.trace or xd stream deploy foo properties module. .trace code Wildcard wiretap config http docs.spring.io spring integration reference html messaging channels section.html channel global wiretap","As a user, I'd like to have an optional trace as inline deployment properties for stream so that I can declare which module in the stream needs to be traced for logging or notifications<span class='highlight-text severity-high'>. This gives the flexibility to track the stage progress between individual modules. Example code xml xd stream create foo http log xd stream deploy foo properties module.http.trace,module.log.trace or xd stream deploy foo properties module. .trace code Wildcard wiretap config http docs.spring.io spring integration reference html messaging channels section.html channel global wiretap</span>","minimal","punctuation","high",False
18532,"As a user, I'd like to have an optional trace as inline deployment properties for stream so that I can declare which module in the stream needs to be traced for logging or notifications. This gives the flexibility to track the stage progress between individual modules. Example code xml xd stream create foo http log xd stream deploy foo properties module.http.trace,module.log.trace or xd stream deploy foo properties module. .trace code Wildcard wiretap config http docs.spring.io spring integration reference html messaging channels section.html channel global wiretap","As a user",", I'd like to have an optional trace as inline deployment properties for stream","so that I can declare which module in the stream needs to be traced for logging or notifications. This gives the flexibility to track the stage progress between individual modules. Example code xml xd stream create foo http log xd stream deploy foo properties module.http.trace,module.log.trace or xd stream deploy foo properties module. .trace code Wildcard wiretap config http docs.spring.io spring integration reference html messaging channels section.html channel global wiretap","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18548,"See original report here https github.com spring projects spring xd issues 1300 I ve created a module with a custom Metric, Handler and Repository patterned after the AggregateCounter but then it appears that there doesn t seem to be a way to add anything to the Admin Context. The diagram at https github.com spring projects spring xd wiki Extending XD shows the Plugin Context as a sibling to the Admin Context which seems to verify my fears. It would be convenient to be able to add custom Metrics Controllers Repositories into the Admin Context so that they can be accessed through the same REST api as the other Metrics. ",NULL,"See original report here https github.com spring projects spring xd issues 1300 I ve created a module with a custom Metric, Handler and Repository patterned after the AggregateCounter but then it appears that there doesn t seem to be a way to add anything to the Admin Context. The diagram at https github.com spring projects spring xd wiki Extending XD shows the Plugin Context as a sibling to the Admin Context which seems to verify my fears. It would be convenient to be able to add custom Metrics Controllers Repositories into the Admin Context","so that they can be accessed through the same REST api as the other Metrics.","Add for who this story is","well_formed","no_role","high",False
18548,"See original report here https github.com spring projects spring xd issues 1300 I ve created a module with a custom Metric, Handler and Repository patterned after the AggregateCounter but then it appears that there doesn t seem to be a way to add anything to the Admin Context. The diagram at https github.com spring projects spring xd wiki Extending XD shows the Plugin Context as a sibling to the Admin Context which seems to verify my fears. It would be convenient to be able to add custom Metrics Controllers Repositories into the Admin Context so that they can be accessed through the same REST api as the other Metrics. ",NULL,"See original report here https github.com spring projects spring xd issues 1300 I ve created a module with a custom Metric, Handler and Repository patterned after the AggregateCounter but then it appears that there doesn t seem to be a way to add anything to the Admin Context. The diagram at https github.com spring projects spring xd wiki Extending XD shows the Plugin Context as a sibling to the Admin Context which seems to verify my fears. It would be convenient to be able to add custom Metrics Controllers Repositories into the Admin Context","so that they can be accessed through the same REST api as the other Metrics.","See original report here https github.com spring projects spring xd issues 1300 I ve created a module with a custom Metric, Handler<span class='highlight-text severity-high'> and </span>Repository patterned after the AggregateCounter but then it appears that there doesn t seem to be a way to add anything to the Admin Context. The diagram at https github.com spring projects spring xd wiki Extending XD shows the Plugin Context as a sibling to the Admin Context which seems to verify my fears. It would be convenient to be able to add custom Metrics Controllers Repositories into the Admin Context so that they can be accessed through the same REST api as the other Metrics. ","atomic","conjunctions","high",False
18548,"See original report here https github.com spring projects spring xd issues 1300 I ve created a module with a custom Metric, Handler and Repository patterned after the AggregateCounter but then it appears that there doesn t seem to be a way to add anything to the Admin Context. The diagram at https github.com spring projects spring xd wiki Extending XD shows the Plugin Context as a sibling to the Admin Context which seems to verify my fears. It would be convenient to be able to add custom Metrics Controllers Repositories into the Admin Context so that they can be accessed through the same REST api as the other Metrics. ",NULL,"See original report here https github.com spring projects spring xd issues 1300 I ve created a module with a custom Metric, Handler and Repository patterned after the AggregateCounter but then it appears that there doesn t seem to be a way to add anything to the Admin Context. The diagram at https github.com spring projects spring xd wiki Extending XD shows the Plugin Context as a sibling to the Admin Context which seems to verify my fears. It would be convenient to be able to add custom Metrics Controllers Repositories into the Admin Context","so that they can be accessed through the same REST api as the other Metrics.","See original report here https github<span class='highlight-text severity-high'>.com spring projects spring xd issues 1300 I ve created a module with a custom Metric, Handler and Repository patterned after the AggregateCounter but then it appears that there doesn t seem to be a way to add anything to the Admin Context. The diagram at https github.com spring projects spring xd wiki Extending XD shows the Plugin Context as a sibling to the Admin Context which seems to verify my fears. It would be convenient to be able to add custom Metrics Controllers Repositories into the Admin Context so that they can be accessed through the same REST api as the other Metrics. </span>","minimal","punctuation","high",False
18548,"See original report here https github.com spring projects spring xd issues 1300 I ve created a module with a custom Metric, Handler and Repository patterned after the AggregateCounter but then it appears that there doesn t seem to be a way to add anything to the Admin Context. The diagram at https github.com spring projects spring xd wiki Extending XD shows the Plugin Context as a sibling to the Admin Context which seems to verify my fears. It would be convenient to be able to add custom Metrics Controllers Repositories into the Admin Context so that they can be accessed through the same REST api as the other Metrics. ",NULL,"See original report here https github.com spring projects spring xd issues 1300 I ve created a module with a custom Metric, Handler and Repository patterned after the AggregateCounter but then it appears that there doesn t seem to be a way to add anything to the Admin Context. The diagram at https github.com spring projects spring xd wiki Extending XD shows the Plugin Context as a sibling to the Admin Context which seems to verify my fears. It would be convenient to be able to add custom Metrics Controllers Repositories into the Admin Context","so that they can be accessed through the same REST api as the other Metrics.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18657,"As a user, I'd like to have a reactor stream processor module so that I can ingest data using XD source modules and process them as time window operations. Example 1 http reactor stream timeWindow 10s field payload.sensorData expressions min,avg This would give you 10 second time window of the min and avg values. Example 2 Reactor as a module Example 3 Integration with Spark streaming and reactor","As a user",", I'd like to have a reactor stream processor module","so that I can ingest data using XD source modules and process them as time window operations. Example 1 http reactor stream timeWindow 10s field payload.sensorData expressions min,avg This would give you 10 second time window of the min and avg values. Example 2 Reactor as a module Example 3 Integration with Spark streaming and reactor","As a user, I'd like to have a reactor stream processor module so that I can ingest data using XD source modules and process them as time window operations<span class='highlight-text severity-high'>. Example 1 http reactor stream timeWindow 10s field payload.sensorData expressions min,avg This would give you 10 second time window of the min and avg values. Example 2 Reactor as a module Example 3 Integration with Spark streaming and reactor</span>","minimal","punctuation","high",False
18657,"As a user, I'd like to have a reactor stream processor module so that I can ingest data using XD source modules and process them as time window operations. Example 1 http reactor stream timeWindow 10s field payload.sensorData expressions min,avg This would give you 10 second time window of the min and avg values. Example 2 Reactor as a module Example 3 Integration with Spark streaming and reactor","As a user",", I'd like to have a reactor stream processor module","so that I can ingest data using XD source modules and process them as time window operations. Example 1 http reactor stream timeWindow 10s field payload.sensorData expressions min,avg This would give you 10 second time window of the min and avg values. Example 2 Reactor as a module Example 3 Integration with Spark streaming and reactor","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18562,"As a developer, I'd like to refer to wiki so that I can configure machines with recommended ulimit setting for XD s distributed setup. Note Recommended ulimit setting is 10K under Troubleshooting new section Exception reason to increase ulimit 8 25 52,266 1.1.0.SNAP ERROR DeploymentsPathChildrenCache 0 server.DeploymentListener Exception deploying module java.lang.IllegalStateException java.io.FileNotFoundException var vcap data packages springxd ee02bd3482eeb620a65862fb54e1f23fcece8022.1 bd a341640a5de2f922fd3db906ce504b85819c1e spring xd 1.1.0.BUILD SNAPSHOT xd config modules modules.yml Too many open files ","As a developer",", I'd like to refer to wiki","so that I can configure machines with recommended ulimit setting for XD s distributed setup. Note Recommended ulimit setting is 10K under Troubleshooting new section Exception reason to increase ulimit 8 25 52,266 1.1.0.SNAP ERROR DeploymentsPathChildrenCache 0 server.DeploymentListener Exception deploying module java.lang.IllegalStateException java.io.FileNotFoundException var vcap data packages springxd ee02bd3482eeb620a65862fb54e1f23fcece8022.1 bd a341640a5de2f922fd3db906ce504b85819c1e spring xd 1.1.0.BUILD SNAPSHOT xd config modules modules.yml Too many open files","As a developer, I'd like to refer to wiki so that I can configure machines with recommended ulimit setting for XD s distributed setup<span class='highlight-text severity-high'>. Note Recommended ulimit setting is 10K under Troubleshooting new section Exception reason to increase ulimit 8 25 52,266 1.1.0.SNAP ERROR DeploymentsPathChildrenCache 0 server.DeploymentListener Exception deploying module java.lang.IllegalStateException java.io.FileNotFoundException var vcap data packages springxd ee02bd3482eeb620a65862fb54e1f23fcece8022.1 bd a341640a5de2f922fd3db906ce504b85819c1e spring xd 1.1.0.BUILD SNAPSHOT xd config modules modules.yml Too many open files </span>","minimal","punctuation","high",False
18562,"As a developer, I'd like to refer to wiki so that I can configure machines with recommended ulimit setting for XD s distributed setup. Note Recommended ulimit setting is 10K under Troubleshooting new section Exception reason to increase ulimit 8 25 52,266 1.1.0.SNAP ERROR DeploymentsPathChildrenCache 0 server.DeploymentListener Exception deploying module java.lang.IllegalStateException java.io.FileNotFoundException var vcap data packages springxd ee02bd3482eeb620a65862fb54e1f23fcece8022.1 bd a341640a5de2f922fd3db906ce504b85819c1e spring xd 1.1.0.BUILD SNAPSHOT xd config modules modules.yml Too many open files ","As a developer",", I'd like to refer to wiki","so that I can configure machines with recommended ulimit setting for XD s distributed setup. Note Recommended ulimit setting is 10K under Troubleshooting new section Exception reason to increase ulimit 8 25 52,266 1.1.0.SNAP ERROR DeploymentsPathChildrenCache 0 server.DeploymentListener Exception deploying module java.lang.IllegalStateException java.io.FileNotFoundException var vcap data packages springxd ee02bd3482eeb620a65862fb54e1f23fcece8022.1 bd a341640a5de2f922fd3db906ce504b85819c1e spring xd 1.1.0.BUILD SNAPSHOT xd config modules modules.yml Too many open files","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18578,"As a user, I'd like to refer to the wiki so that I can create a job with partitions that in turn expects tableName and columns be explicitly included in the job definition. It is also beneficial to call out sql and tableName metadata options are mutually exclusive. Following logic in JdbcHdfsOptionsMetadata needs documented. code java AssertTrue message Use tableName AND columns when using partition column boolean isPartitionedWithTableName if StringUtils.hasText partitionColumn return StringUtils.hasText tableName !StringUtils.hasText sql ; else return true; code ","As a user",", I'd like to refer to the wiki","so that I can create a job with partitions that in turn expects tableName and columns be explicitly included in the job definition. It is also beneficial to call out sql and tableName metadata options are mutually exclusive. Following logic in JdbcHdfsOptionsMetadata needs documented. code java AssertTrue message Use tableName AND columns when using partition column boolean isPartitionedWithTableName if StringUtils.hasText partitionColumn return StringUtils.hasText tableName !StringUtils.hasText sql ; else return true; code","As a user, I'd like to refer to the wiki so that I can create a job with partitions that in turn expects tableName and columns be explicitly included in the job definition<span class='highlight-text severity-high'>. It is also beneficial to call out sql and tableName metadata options are mutually exclusive. Following logic in JdbcHdfsOptionsMetadata needs documented. code java AssertTrue message Use tableName AND columns when using partition column boolean isPartitionedWithTableName if StringUtils.hasText partitionColumn return StringUtils.hasText tableName !StringUtils.hasText sql ; else return true; code </span>","minimal","punctuation","high",False
18578,"As a user, I'd like to refer to the wiki so that I can create a job with partitions that in turn expects tableName and columns be explicitly included in the job definition. It is also beneficial to call out sql and tableName metadata options are mutually exclusive. Following logic in JdbcHdfsOptionsMetadata needs documented. code java AssertTrue message Use tableName AND columns when using partition column boolean isPartitionedWithTableName if StringUtils.hasText partitionColumn return StringUtils.hasText tableName !StringUtils.hasText sql ; else return true; code ","As a user",", I'd like to refer to the wiki","so that I can create a job with partitions that in turn expects tableName and columns be explicitly included in the job definition. It is also beneficial to call out sql and tableName metadata options are mutually exclusive. Following logic in JdbcHdfsOptionsMetadata needs documented. code java AssertTrue message Use tableName AND columns when using partition column boolean isPartitionedWithTableName if StringUtils.hasText partitionColumn return StringUtils.hasText tableName !StringUtils.hasText sql ; else return true; code","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18590,"Create port of https github.com spring projects spring xd samples tree master reactor moving average based on RxJava",NULL,"Create port of https github.com spring projects spring xd samples tree master reactor moving average based on RxJava",NULL,"Add for who this story is","well_formed","no_role","high",False
18608,"As a user, I'd like to have a gradle build option so that I can support module projects that will declare the Spring XD dependencies as provided, configure the boot plugin for MODULE layout and other boilerplate build configuration. This is dependent on Boot s module layout scoping issue https github.com spring projects spring boot issues 2187","As a user",", I'd like to have a gradle build option","so that I can support module projects that will declare the Spring XD dependencies as provided, configure the boot plugin for MODULE layout and other boilerplate build configuration. This is dependent on Boot s module layout scoping issue https github.com spring projects spring boot issues 2187","As a user, I'd like to have a gradle build option so that I can support module projects that will declare the Spring XD dependencies as provided, configure the boot plugin for MODULE layout and other boilerplate build configuration<span class='highlight-text severity-high'>. This is dependent on Boot s module layout scoping issue https github.com spring projects spring boot issues 2187</span>","minimal","punctuation","high",False
18608,"As a user, I'd like to have a gradle build option so that I can support module projects that will declare the Spring XD dependencies as provided, configure the boot plugin for MODULE layout and other boilerplate build configuration. This is dependent on Boot s module layout scoping issue https github.com spring projects spring boot issues 2187","As a user",", I'd like to have a gradle build option","so that I can support module projects that will declare the Spring XD dependencies as provided, configure the boot plugin for MODULE layout and other boilerplate build configuration. This is dependent on Boot s module layout scoping issue https github.com spring projects spring boot issues 2187","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18616,"As a XD Admin, I'd like to upgrade to Spring Boot 1.2.0 RELEASE and the associated dependencies so that we can catch up with the latest features, bug fixes and enhancements. Following XD dependencies needs upgraded to sync up with Boot 1.2.0 RELEASE activemq.version 5.10.0 activemq.version aspectj.version 1.8.4 aspectj.version commons dbcp2.version 2.0.1 commons dbcp2.version ","As a XD Admin",", I'd like to upgrade to Spring Boot 1.2.0 RELEASE and the associated dependencies","so that we can catch up with the latest features, bug fixes and enhancements. Following XD dependencies needs upgraded to sync up with Boot 1.2.0 RELEASE activemq.version 5.10.0 activemq.version aspectj.version 1.8.4 aspectj.version commons dbcp2.version 2.0.1 commons dbcp2.version","As a XD Admin, I'd like to upgrade to Spring Boot 1.2.0 RELEASE<span class='highlight-text severity-high'> and </span>the associated dependencies so that we can catch up with the latest features, bug fixes and enhancements. Following XD dependencies needs upgraded to sync up with Boot 1.2.0 RELEASE activemq.version 5.10.0 activemq.version aspectj.version 1.8.4 aspectj.version commons dbcp2.version 2.0.1 commons dbcp2.version ","atomic","conjunctions","high",False
18616,"As a XD Admin, I'd like to upgrade to Spring Boot 1.2.0 RELEASE and the associated dependencies so that we can catch up with the latest features, bug fixes and enhancements. Following XD dependencies needs upgraded to sync up with Boot 1.2.0 RELEASE activemq.version 5.10.0 activemq.version aspectj.version 1.8.4 aspectj.version commons dbcp2.version 2.0.1 commons dbcp2.version ","As a XD Admin",", I'd like to upgrade to Spring Boot 1.2.0 RELEASE and the associated dependencies","so that we can catch up with the latest features, bug fixes and enhancements. Following XD dependencies needs upgraded to sync up with Boot 1.2.0 RELEASE activemq.version 5.10.0 activemq.version aspectj.version 1.8.4 aspectj.version commons dbcp2.version 2.0.1 commons dbcp2.version","As a XD Admin, I'd like to upgrade to Spring Boot 1<span class='highlight-text severity-high'>.2.0 RELEASE and the associated dependencies so that we can catch up with the latest features, bug fixes and enhancements. Following XD dependencies needs upgraded to sync up with Boot 1.2.0 RELEASE activemq.version 5.10.0 activemq.version aspectj.version 1.8.4 aspectj.version commons dbcp2.version 2.0.1 commons dbcp2.version </span>","minimal","punctuation","high",False
18616,"As a XD Admin, I'd like to upgrade to Spring Boot 1.2.0 RELEASE and the associated dependencies so that we can catch up with the latest features, bug fixes and enhancements. Following XD dependencies needs upgraded to sync up with Boot 1.2.0 RELEASE activemq.version 5.10.0 activemq.version aspectj.version 1.8.4 aspectj.version commons dbcp2.version 2.0.1 commons dbcp2.version ","As a XD Admin",", I'd like to upgrade to Spring Boot 1.2.0 RELEASE and the associated dependencies","so that we can catch up with the latest features, bug fixes and enhancements. Following XD dependencies needs upgraded to sync up with Boot 1.2.0 RELEASE activemq.version 5.10.0 activemq.version aspectj.version 1.8.4 aspectj.version commons dbcp2.version 2.0.1 commons dbcp2.version","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18650,"Version XD 1.1 M1 Problem Trying to use tcp client source module and observing an exception while deploying the stream. Stream Definition code xml curl data name dummy firehose data definition tcp client decoder LF port 8080 log data deploy true http localhost 9393 streams definitions name dummy firehose , status null, definition tcp client decoder LF port 8080 log , links self href http localhost 9393 streams dummy firehose code The same curl command works fine against XD 1.0.1 release. ",NULL,"Version XD 1.1 M1 Problem Trying to use tcp client source module and observing an exception while deploying the stream. Stream Definition code xml curl data name dummy firehose data definition tcp client decoder LF port 8080 log data deploy true http localhost 9393 streams definitions name dummy firehose , status null, definition tcp client decoder LF port 8080 log , links self href http localhost 9393 streams dummy firehose code The same curl command works fine against XD 1.0.1 release. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18650,"Version XD 1.1 M1 Problem Trying to use tcp client source module and observing an exception while deploying the stream. Stream Definition code xml curl data name dummy firehose data definition tcp client decoder LF port 8080 log data deploy true http localhost 9393 streams definitions name dummy firehose , status null, definition tcp client decoder LF port 8080 log , links self href http localhost 9393 streams dummy firehose code The same curl command works fine against XD 1.0.1 release. ",NULL,"Version XD 1.1 M1 Problem Trying to use tcp client source module and observing an exception while deploying the stream. Stream Definition code xml curl data name dummy firehose data definition tcp client decoder LF port 8080 log data deploy true http localhost 9393 streams definitions name dummy firehose , status null, definition tcp client decoder LF port 8080 log , links self href http localhost 9393 streams dummy firehose code The same curl command works fine against XD 1.0.1 release. ",NULL,"Version XD 1.1 M1 Problem Trying to use tcp client source module<span class='highlight-text severity-high'> and </span>observing an exception while deploying the stream. Stream Definition code xml curl data name dummy firehose data definition tcp client decoder LF port 8080 log data deploy true http localhost 9393 streams definitions name dummy firehose , status null, definition tcp client decoder LF port 8080 log , links self href http localhost 9393 streams dummy firehose code The same curl command works fine against XD 1.0.1 release. ","atomic","conjunctions","high",False
19552,"If we want spring xd to support interactions through a UI, then it would make sense that we should support the UI coming from a separate origin. This way, the UI could remain a separate project and be served from wherever we want. So, we would need to add the header to all outgoing REST responses. We may also need to add a Access Control Allow Methods header as well. In the short term, the Access Control Allow Origin header could be hard coded to a specific url I m using http localhost 9889 for now , but in the long term we would need this configurable.",NULL,"If we want spring xd to support interactions through a UI, then it would make sense that we should support the UI coming from a separate origin. This way, the UI could remain a separate project and be served from wherever we want.","So, we would need to add the header to all outgoing REST responses. We may also need to add a Access Control Allow Methods header as well. In the short term, the Access Control Allow Origin header could be hard coded to a specific url I m using http localhost 9889 for now , but in the long term we would need this configurable.","Add for who this story is","well_formed","no_role","high",False
18650,"Version XD 1.1 M1 Problem Trying to use tcp client source module and observing an exception while deploying the stream. Stream Definition code xml curl data name dummy firehose data definition tcp client decoder LF port 8080 log data deploy true http localhost 9393 streams definitions name dummy firehose , status null, definition tcp client decoder LF port 8080 log , links self href http localhost 9393 streams dummy firehose code The same curl command works fine against XD 1.0.1 release. ",NULL,"Version XD 1.1 M1 Problem Trying to use tcp client source module and observing an exception while deploying the stream. Stream Definition code xml curl data name dummy firehose data definition tcp client decoder LF port 8080 log data deploy true http localhost 9393 streams definitions name dummy firehose , status null, definition tcp client decoder LF port 8080 log , links self href http localhost 9393 streams dummy firehose code The same curl command works fine against XD 1.0.1 release. ",NULL,"Version XD 1<span class='highlight-text severity-high'>.1 M1 Problem Trying to use tcp client source module and observing an exception while deploying the stream. Stream Definition code xml curl data name dummy firehose data definition tcp client decoder LF port 8080 log data deploy true http localhost 9393 streams definitions name dummy firehose , status null, definition tcp client decoder LF port 8080 log , links self href http localhost 9393 streams dummy firehose code The same curl command works fine against XD 1.0.1 release. </span>","minimal","punctuation","high",False
18667,"After updating to Boot 1.2 RC1 the following replacement doesn t work. code xml bean id messageConverter class converterClass code which appears in the rabbit source and sink. Feels like a core spring thing, but that was updated earlier. Current workaround that was commited already so the build can pass is code xml bean id clazz class java.lang.Class factory method forName constructor arg value converterClass bean bean id messageConverter factory bean clazz factory method newInstance code ",NULL,"After updating to Boot 1.2 RC1 the following replacement doesn t work. code xml bean id messageConverter class converterClass code which appears in the rabbit","source and sink. Feels like a core spring thing, but that was updated earlier. Current workaround that was commited already so the build can pass is code xml bean id clazz class java.lang.Class factory method forName constructor arg value converterClass bean bean id messageConverter factory bean clazz factory method newInstance code","Add for who this story is","well_formed","no_role","high",False
18667,"After updating to Boot 1.2 RC1 the following replacement doesn t work. code xml bean id messageConverter class converterClass code which appears in the rabbit source and sink. Feels like a core spring thing, but that was updated earlier. Current workaround that was commited already so the build can pass is code xml bean id clazz class java.lang.Class factory method forName constructor arg value converterClass bean bean id messageConverter factory bean clazz factory method newInstance code ",NULL,"After updating to Boot 1.2 RC1 the following replacement doesn t work. code xml bean id messageConverter class converterClass code which appears in the rabbit","source and sink. Feels like a core spring thing, but that was updated earlier. Current workaround that was commited already so the build can pass is code xml bean id clazz class java.lang.Class factory method forName constructor arg value converterClass bean bean id messageConverter factory bean clazz factory method newInstance code","After updating to Boot 1<span class='highlight-text severity-high'>.2 RC1 the following replacement doesn t work. code xml bean id messageConverter class converterClass code which appears in the rabbit source and sink. Feels like a core spring thing, but that was updated earlier. Current workaround that was commited already so the build can pass is code xml bean id clazz class java.lang.Class factory method forName constructor arg value converterClass bean bean id messageConverter factory bean clazz factory method newInstance code </span>","minimal","punctuation","high",False
18667,"After updating to Boot 1.2 RC1 the following replacement doesn t work. code xml bean id messageConverter class converterClass code which appears in the rabbit source and sink. Feels like a core spring thing, but that was updated earlier. Current workaround that was commited already so the build can pass is code xml bean id clazz class java.lang.Class factory method forName constructor arg value converterClass bean bean id messageConverter factory bean clazz factory method newInstance code ",NULL,"After updating to Boot 1.2 RC1 the following replacement doesn t work. code xml bean id messageConverter class converterClass code which appears in the rabbit","source and sink. Feels like a core spring thing, but that was updated earlier. Current workaround that was commited already so the build can pass is code xml bean id clazz class java.lang.Class factory method forName constructor arg value converterClass bean bean id messageConverter factory bean clazz factory method newInstance code","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18671,"Using single node deployment of Spring XD 1.0 GA, we needed to redefine several batch jobs. We deleted the jobs job destroy all . When attempting to re add, we received an error that a job with the name already exists. Performing job list confirms the jobs were gone. To workaround, I needed to terminate the instance server of Spring XD and restart it. Since this was the single node deployment without a live stream of data coming in this was okay, but would have been a major problem if bouncing the Spring XD server was not acceptable i.e., live data being actively received .",NULL,"Using single node deployment of Spring XD 1.0 GA, we needed to redefine several batch jobs. We deleted the jobs job destroy all . When attempting to re add, we received an error that a job with the name already exists. Performing job list confirms the jobs were gone. To workaround, I needed to terminate the instance server of Spring XD and restart it. Since this was the single node deployment without a live stream of data coming in this was okay, but would have been a major problem if bouncing the Spring XD server was not acceptable i.e., live data being actively received .",NULL,"Add for who this story is","well_formed","no_role","high",False
18671,"Using single node deployment of Spring XD 1.0 GA, we needed to redefine several batch jobs. We deleted the jobs job destroy all . When attempting to re add, we received an error that a job with the name already exists. Performing job list confirms the jobs were gone. To workaround, I needed to terminate the instance server of Spring XD and restart it. Since this was the single node deployment without a live stream of data coming in this was okay, but would have been a major problem if bouncing the Spring XD server was not acceptable i.e., live data being actively received .",NULL,"Using single node deployment of Spring XD 1.0 GA, we needed to redefine several batch jobs. We deleted the jobs job destroy all . When attempting to re add, we received an error that a job with the name already exists. Performing job list confirms the jobs were gone. To workaround, I needed to terminate the instance server of Spring XD and restart it. Since this was the single node deployment without a live stream of data coming in this was okay, but would have been a major problem if bouncing the Spring XD server was not acceptable i.e., live data being actively received .",NULL,"Using single node deployment of Spring XD 1.0 GA, we needed to redefine several batch jobs. We deleted the jobs job destroy all . When attempting to re add, we received an error that a job with the name already exists. Performing job list confirms the jobs were gone. To workaround, I needed to terminate the instance server of Spring XD<span class='highlight-text severity-high'> and </span>restart it. Since this was the single node deployment without a live stream of data coming in this was okay, but would have been a major problem if bouncing the Spring XD server was not acceptable i.e., live data being actively received .","atomic","conjunctions","high",False
18671,"Using single node deployment of Spring XD 1.0 GA, we needed to redefine several batch jobs. We deleted the jobs job destroy all . When attempting to re add, we received an error that a job with the name already exists. Performing job list confirms the jobs were gone. To workaround, I needed to terminate the instance server of Spring XD and restart it. Since this was the single node deployment without a live stream of data coming in this was okay, but would have been a major problem if bouncing the Spring XD server was not acceptable i.e., live data being actively received .",NULL,"Using single node deployment of Spring XD 1.0 GA, we needed to redefine several batch jobs. We deleted the jobs job destroy all . When attempting to re add, we received an error that a job with the name already exists. Performing job list confirms the jobs were gone. To workaround, I needed to terminate the instance server of Spring XD and restart it. Since this was the single node deployment without a live stream of data coming in this was okay, but would have been a major problem if bouncing the Spring XD server was not acceptable i.e., live data being actively received .",NULL,"Using single node deployment of Spring XD 1<span class='highlight-text severity-high'>.0 GA, we needed to redefine several batch jobs. We deleted the jobs job destroy all . When attempting to re add, we received an error that a job with the name already exists. Performing job list confirms the jobs were gone. To workaround, I needed to terminate the instance server of Spring XD and restart it. Since this was the single node deployment without a live stream of data coming in this was okay, but would have been a major problem if bouncing the Spring XD server was not acceptable i.e., live data being actively received .</span>","minimal","punctuation","high",False
18685,"Update Angular Growl to v2 Allowing for stoppable notifications in case you want to see it for longer than 5 secs ",NULL,"Update Angular Growl to v2 Allowing for stoppable notifications in case you want to see it for longer than 5 secs ",NULL,"Add for who this story is","well_formed","no_role","high",False
18709,"When answering support questions, the first step is to determine what version of the software the customer is using. This question can be easily answered if we log the version as one of the fields in the log file. For example noformat 10 44 21,212 1.0.2.BUILD SNAPSHOT INFO DeploymentSupervisorCacheListener 0 server.ContainerListener Container arrived Container name 431baa56 b23b 48fc b37d 18b52231e799 , attributes ip 192.168.25.177, host Patrick Peralta MacBook Pro.local, groups , pid 38004, id 431baa56 b23b 48fc b37d 18b52231e799 noformat This way when we receive log snippets initial support inquires rarely include the entire log file we can immediately determine if the issue has already been fixed in a later release. ",NULL,"When answering support questions, the first step is to determine what version of the software the customer is using. This question can be easily answered if we log the version as one of the fields in the log file. For example noformat 10 44 21,212 1.0.2.BUILD SNAPSHOT INFO DeploymentSupervisorCacheListener 0 server.ContainerListener Container arrived Container name 431baa56 b23b 48fc b37d 18b52231e799 , attributes ip 192.168.25.177, host Patrick Peralta MacBook Pro.local, groups , pid 38004, id 431baa56 b23b 48fc b37d 18b52231e799 noformat This way when we receive log snippets initial support inquires rarely include the entire log file we can immediately determine if the issue has already been fixed in a later release. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18709,"When answering support questions, the first step is to determine what version of the software the customer is using. This question can be easily answered if we log the version as one of the fields in the log file. For example noformat 10 44 21,212 1.0.2.BUILD SNAPSHOT INFO DeploymentSupervisorCacheListener 0 server.ContainerListener Container arrived Container name 431baa56 b23b 48fc b37d 18b52231e799 , attributes ip 192.168.25.177, host Patrick Peralta MacBook Pro.local, groups , pid 38004, id 431baa56 b23b 48fc b37d 18b52231e799 noformat This way when we receive log snippets initial support inquires rarely include the entire log file we can immediately determine if the issue has already been fixed in a later release. ",NULL,"When answering support questions, the first step is to determine what version of the software the customer is using. This question can be easily answered if we log the version as one of the fields in the log file. For example noformat 10 44 21,212 1.0.2.BUILD SNAPSHOT INFO DeploymentSupervisorCacheListener 0 server.ContainerListener Container arrived Container name 431baa56 b23b 48fc b37d 18b52231e799 , attributes ip 192.168.25.177, host Patrick Peralta MacBook Pro.local, groups , pid 38004, id 431baa56 b23b 48fc b37d 18b52231e799 noformat This way when we receive log snippets initial support inquires rarely include the entire log file we can immediately determine if the issue has already been fixed in a later release. ",NULL,"When answering support questions, the first step is to determine what version of the software the customer is using<span class='highlight-text severity-high'>. This question can be easily answered if we log the version as one of the fields in the log file. For example noformat 10 44 21,212 1.0.2.BUILD SNAPSHOT INFO DeploymentSupervisorCacheListener 0 server.ContainerListener Container arrived Container name 431baa56 b23b 48fc b37d 18b52231e799 , attributes ip 192.168.25.177, host Patrick Peralta MacBook Pro.local, groups , pid 38004, id 431baa56 b23b 48fc b37d 18b52231e799 noformat This way when we receive log snippets initial support inquires rarely include the entire log file we can immediately determine if the issue has already been fixed in a later release. </span>","minimal","punctuation","high",False
18698,"Using a single producer, message size of 1000 bytes, Pretch of 100. Send 1M messages and increase or decrease so that a given test iteration takes about 2 minutes. Vary the number of consumers. Measure the msg sec rate and calculate the data transfer rate in MB sec. Number of consumers 1 2 4 6 10 50 During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",NULL,"Using a single producer, message size of 1000 bytes, Pretch of 100. Send 1M messages and increase or decrease","so that a given test iteration takes about 2 minutes. Vary the number of consumers. Measure the msg sec rate and calculate the data transfer rate in MB sec. Number of consumers 1 2 4 6 10 50 During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.","Add for who this story is","well_formed","no_role","high",False
18698,"Using a single producer, message size of 1000 bytes, Pretch of 100. Send 1M messages and increase or decrease so that a given test iteration takes about 2 minutes. Vary the number of consumers. Measure the msg sec rate and calculate the data transfer rate in MB sec. Number of consumers 1 2 4 6 10 50 During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",NULL,"Using a single producer, message size of 1000 bytes, Pretch of 100. Send 1M messages and increase or decrease","so that a given test iteration takes about 2 minutes. Vary the number of consumers. Measure the msg sec rate and calculate the data transfer rate in MB sec. Number of consumers 1 2 4 6 10 50 During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.","Using a single producer, message size of 1000 bytes, Pretch of 100. Send 1M messages<span class='highlight-text severity-high'> and </span>increase<span class='highlight-text severity-high'> or </span>decrease so that a given test iteration takes about 2 minutes. Vary the number of consumers. Measure the msg sec rate and calculate the data transfer rate in MB sec. Number of consumers 1 2 4 6 10 50 During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.","atomic","conjunctions","high",False
18698,"Using a single producer, message size of 1000 bytes, Pretch of 100. Send 1M messages and increase or decrease so that a given test iteration takes about 2 minutes. Vary the number of consumers. Measure the msg sec rate and calculate the data transfer rate in MB sec. Number of consumers 1 2 4 6 10 50 During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",NULL,"Using a single producer, message size of 1000 bytes, Pretch of 100. Send 1M messages and increase or decrease","so that a given test iteration takes about 2 minutes. Vary the number of consumers. Measure the msg sec rate and calculate the data transfer rate in MB sec. Number of consumers 1 2 4 6 10 50 During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.","Using a single producer, message size of 1000 bytes, Pretch of 100<span class='highlight-text severity-high'>. Send 1M messages and increase or decrease so that a given test iteration takes about 2 minutes. Vary the number of consumers. Measure the msg sec rate and calculate the data transfer rate in MB sec. Number of consumers 1 2 4 6 10 50 During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.</span>","minimal","punctuation","high",False
18698,"Using a single producer, message size of 1000 bytes, Pretch of 100. Send 1M messages and increase or decrease so that a given test iteration takes about 2 minutes. Vary the number of consumers. Measure the msg sec rate and calculate the data transfer rate in MB sec. Number of consumers 1 2 4 6 10 50 During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",NULL,"Using a single producer, message size of 1000 bytes, Pretch of 100. Send 1M messages and increase or decrease","so that a given test iteration takes about 2 minutes. Vary the number of consumers. Measure the msg sec rate and calculate the data transfer rate in MB sec. Number of consumers 1 2 4 6 10 50 During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18714,"With the implementation of XD 1864, we need to make sure that the paginated data returned from the REST endpoints has proper default ordering. Up to now we have done client side ordering in the Admin UI, but with server side pagination, the server side should support proper pagination as well. Eventually, we may even decide to provide more flexible ordering options ASC vs DESC, sort on different properties etc. , which may be a separate Jira.",NULL,"With the implementation of XD 1864, we need to make sure that the paginated data returned from the REST endpoints has proper default ordering. Up to now we have done client side ordering in the Admin UI, but with server side pagination, the server side should support proper pagination as well. Eventually, we may even decide to provide more flexible ordering options ASC vs DESC, sort on different properties etc. , which may be a separate Jira.",NULL,"Add for who this story is","well_formed","no_role","high",False
18714,"With the implementation of XD 1864, we need to make sure that the paginated data returned from the REST endpoints has proper default ordering. Up to now we have done client side ordering in the Admin UI, but with server side pagination, the server side should support proper pagination as well. Eventually, we may even decide to provide more flexible ordering options ASC vs DESC, sort on different properties etc. , which may be a separate Jira.",NULL,"With the implementation of XD 1864, we need to make sure that the paginated data returned from the REST endpoints has proper default ordering. Up to now we have done client side ordering in the Admin UI, but with server side pagination, the server side should support proper pagination as well. Eventually, we may even decide to provide more flexible ordering options ASC vs DESC, sort on different properties etc. , which may be a separate Jira.",NULL,"With the implementation of XD 1864, we need to make sure that the paginated data returned from the REST endpoints has proper default ordering<span class='highlight-text severity-high'>. Up to now we have done client side ordering in the Admin UI, but with server side pagination, the server side should support proper pagination as well. Eventually, we may even decide to provide more flexible ordering options ASC vs DESC, sort on different properties etc. , which may be a separate Jira.</span>","minimal","punctuation","high",False
18745,"As a user, I'd like to have the option to provide file based security configurations so that I can access the endpoints in a secured manner. Ideally, all the listed endpoints http localhost 9393 needs to be encapsulated within file based security layer. Reference Securing Web App https spring.io guides gs securing web ","As a user",", I'd like to have the option to provide file based security configurations","so that I can access the endpoints in a secured manner. Ideally, all the listed endpoints http localhost 9393 needs to be encapsulated within file based security layer. Reference Securing Web App https spring.io guides gs securing web","As a user, I'd like to have the option to provide file based security configurations so that I can access the endpoints in a secured manner<span class='highlight-text severity-high'>. Ideally, all the listed endpoints http localhost 9393 needs to be encapsulated within file based security layer. Reference Securing Web App https spring.io guides gs securing web </span>","minimal","punctuation","high",False
18745,"As a user, I'd like to have the option to provide file based security configurations so that I can access the endpoints in a secured manner. Ideally, all the listed endpoints http localhost 9393 needs to be encapsulated within file based security layer. Reference Securing Web App https spring.io guides gs securing web ","As a user",", I'd like to have the option to provide file based security configurations","so that I can access the endpoints in a secured manner. Ideally, all the listed endpoints http localhost 9393 needs to be encapsulated within file based security layer. Reference Securing Web App https spring.io guides gs securing web","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18752,"As a user, I'd like to have the option to write into Kafka sink so that I can publish mass data into Kafka broker.","As a user",", I'd like to have the option to write into Kafka sink","so that I can publish mass data into Kafka broker.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18793,"Some hadoop commands generate warnings deprecation messages. We should try to get rid of most of them. code xd hadoop fs ls xd recursive Hadoop configuration changed, re initializing shell... lsr DEPRECATED Please use ls R instead. 13 01 07,120 WARN Spring Shell util.NativeCodeLoader 62 Unable to load native hadoop library for your platform... using builtin java classes where applicable drwxr xr x trisberg supergroup 0 2014 07 17 11 19 xd hashtagcount drwxr xr x trisberg supergroup 0 2014 07 17 11 19 xd hashtagcount output rw r r 3 trisberg supergroup 0 2014 07 17 11 19 xd hashtagcount output SUCCESS rw r r 3 trisberg supergroup 833 2014 07 17 11 19 xd hashtagcount output part r 00000 drwxr xr x trisberg supergroup 0 2014 07 16 18 28 xd tweets rw r r 3 trisberg supergroup 982993 2014 07 16 18 28 xd tweets tweets 0.txt code ",NULL,"Some hadoop commands generate warnings deprecation messages. We should try to get rid of most of them. code xd hadoop fs ls xd recursive Hadoop configuration changed, re initializing shell... lsr DEPRECATED Please use ls R instead. 13 01 07,120 WARN Spring Shell util.NativeCodeLoader 62 Unable to load native hadoop library for your platform... using builtin java classes where applicable drwxr xr x trisberg supergroup 0 2014 07 17 11 19 xd hashtagcount drwxr xr x trisberg supergroup 0 2014 07 17 11 19 xd hashtagcount output rw r r 3 trisberg supergroup 0 2014 07 17 11 19 xd hashtagcount output SUCCESS rw r r 3 trisberg supergroup 833 2014 07 17 11 19 xd hashtagcount output part r 00000 drwxr xr x trisberg supergroup 0 2014 07 16 18 28 xd tweets rw r r 3 trisberg supergroup 982993 2014 07 16 18 28 xd tweets tweets 0.txt code ",NULL,"Add for who this story is","well_formed","no_role","high",False
18793,"Some hadoop commands generate warnings deprecation messages. We should try to get rid of most of them. code xd hadoop fs ls xd recursive Hadoop configuration changed, re initializing shell... lsr DEPRECATED Please use ls R instead. 13 01 07,120 WARN Spring Shell util.NativeCodeLoader 62 Unable to load native hadoop library for your platform... using builtin java classes where applicable drwxr xr x trisberg supergroup 0 2014 07 17 11 19 xd hashtagcount drwxr xr x trisberg supergroup 0 2014 07 17 11 19 xd hashtagcount output rw r r 3 trisberg supergroup 0 2014 07 17 11 19 xd hashtagcount output SUCCESS rw r r 3 trisberg supergroup 833 2014 07 17 11 19 xd hashtagcount output part r 00000 drwxr xr x trisberg supergroup 0 2014 07 16 18 28 xd tweets rw r r 3 trisberg supergroup 982993 2014 07 16 18 28 xd tweets tweets 0.txt code ",NULL,"Some hadoop commands generate warnings deprecation messages. We should try to get rid of most of them. code xd hadoop fs ls xd recursive Hadoop configuration changed, re initializing shell... lsr DEPRECATED Please use ls R instead. 13 01 07,120 WARN Spring Shell util.NativeCodeLoader 62 Unable to load native hadoop library for your platform... using builtin java classes where applicable drwxr xr x trisberg supergroup 0 2014 07 17 11 19 xd hashtagcount drwxr xr x trisberg supergroup 0 2014 07 17 11 19 xd hashtagcount output rw r r 3 trisberg supergroup 0 2014 07 17 11 19 xd hashtagcount output SUCCESS rw r r 3 trisberg supergroup 833 2014 07 17 11 19 xd hashtagcount output part r 00000 drwxr xr x trisberg supergroup 0 2014 07 16 18 28 xd tweets rw r r 3 trisberg supergroup 982993 2014 07 16 18 28 xd tweets tweets 0.txt code ",NULL,"Some hadoop commands generate warnings deprecation messages<span class='highlight-text severity-high'>. We should try to get rid of most of them. code xd hadoop fs ls xd recursive Hadoop configuration changed, re initializing shell... lsr DEPRECATED Please use ls R instead. 13 01 07,120 WARN Spring Shell util.NativeCodeLoader 62 Unable to load native hadoop library for your platform... using builtin java classes where applicable drwxr xr x trisberg supergroup 0 2014 07 17 11 19 xd hashtagcount drwxr xr x trisberg supergroup 0 2014 07 17 11 19 xd hashtagcount output rw r r 3 trisberg supergroup 0 2014 07 17 11 19 xd hashtagcount output SUCCESS rw r r 3 trisberg supergroup 833 2014 07 17 11 19 xd hashtagcount output part r 00000 drwxr xr x trisberg supergroup 0 2014 07 16 18 28 xd tweets rw r r 3 trisberg supergroup 982993 2014 07 16 18 28 xd tweets tweets 0.txt code </span>","minimal","punctuation","high",False
18841,"As part of XD 1591, DeploymentVerifier was modified to take the node structure into account. As indicated in the review below, the implementation does not take module properties such as count into account https github.com spring projects spring xd pull 939 files r13730134 This means the implementation is incorrect. For now this won t affect us since all tests at the moment are single node. However this can be drastically improved and simplified once XD 1270 is completed. At that point we ll be able to simply read a single ZK node to determine if when a deployment succeeded. ","As part of XD 1591, Deploymen","tVerifier was modified to take the node structure into account. As indicated in the review below, the implementation does not take module properties such as count into account https github.com spring projects spring xd pull 939 files r13730134 This means the implementation is incorrect. For now this won t affect us since all tests at the moment are single node. However this can be drastically improved and simplified once XD 1270 is completed. At that point we ll be able to simply read a single ZK node to determine if when a deployment succeeded.",NULL,"As part of XD 1591, DeploymentVerifier was modified to take the node structure into account. As indicated in the review below, the implementation does not take module properties such as count into account https github.com spring projects spring xd pull 939 files r13730134 This means the implementation is incorrect. For now this won t affect us since all tests at the moment are single node. However this can be drastically improved<span class='highlight-text severity-high'> and </span>simplified once XD 1270 is completed. At that point we ll be able to simply read a single ZK node to determine if when a deployment succeeded. ","atomic","conjunctions","high",False
18841,"As part of XD 1591, DeploymentVerifier was modified to take the node structure into account. As indicated in the review below, the implementation does not take module properties such as count into account https github.com spring projects spring xd pull 939 files r13730134 This means the implementation is incorrect. For now this won t affect us since all tests at the moment are single node. However this can be drastically improved and simplified once XD 1270 is completed. At that point we ll be able to simply read a single ZK node to determine if when a deployment succeeded. ","As part of XD 1591, Deploymen","tVerifier was modified to take the node structure into account. As indicated in the review below, the implementation does not take module properties such as count into account https github.com spring projects spring xd pull 939 files r13730134 This means the implementation is incorrect. For now this won t affect us since all tests at the moment are single node. However this can be drastically improved and simplified once XD 1270 is completed. At that point we ll be able to simply read a single ZK node to determine if when a deployment succeeded.",NULL,"As part of XD 1591, DeploymentVerifier was modified to take the node structure into account<span class='highlight-text severity-high'>. As indicated in the review below, the implementation does not take module properties such as count into account https github.com spring projects spring xd pull 939 files r13730134 This means the implementation is incorrect. For now this won t affect us since all tests at the moment are single node. However this can be drastically improved and simplified once XD 1270 is completed. At that point we ll be able to simply read a single ZK node to determine if when a deployment succeeded. </span>","minimal","punctuation","high",False
18841,"As part of XD 1591, DeploymentVerifier was modified to take the node structure into account. As indicated in the review below, the implementation does not take module properties such as count into account https github.com spring projects spring xd pull 939 files r13730134 This means the implementation is incorrect. For now this won t affect us since all tests at the moment are single node. However this can be drastically improved and simplified once XD 1270 is completed. At that point we ll be able to simply read a single ZK node to determine if when a deployment succeeded. ","As part of XD 1591, Deploymen","tVerifier was modified to take the node structure into account. As indicated in the review below, the implementation does not take module properties such as count into account https github.com spring projects spring xd pull 939 files r13730134 This means the implementation is incorrect. For now this won t affect us since all tests at the moment are single node. However this can be drastically improved and simplified once XD 1270 is completed. At that point we ll be able to simply read a single ZK node to determine if when a deployment succeeded.",NULL,"Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18852,"Theoretically I would have liked to centralize logging of Http Resource calls global more substantially but see this limitation https github.com angular angular.js issues 4013",NULL,"Theoretically I would have liked to centralize logging of Http Resource calls global more substantially but see this limitation https github.com angular angular.js issues 4013",NULL,"Add for who this story is","well_formed","no_role","high",False
18858,"XD EC2 applies the environment variables to all container instances that are created. This behavior has to be altered such that a environment variable can be applied to to a specific container instance. For example if we create a 3 node cluster Admin, Container1, Container2 Container3. For Example XD1 XD CONTAINER GROUPS GROUP1 XD2 XD CONTAINER GROUPS GROUP2 In this example XD1 XD CONTAINER GROUPS GROUP1 would apply XD CONTAINER GROUPS GROUP1 to container1 s environment. XD2 XD CONTAINER GROUPS GROUP2 would apply XD CONTAINER GROUPS GROUP2 to container2 s environment. While container3 would not receive a specific environment setting for XD CONTAINER GROUPS. ",NULL,"XD EC2 applies the environment variables to all container instances that are created. This behavior has to be altered such that a environment variable can be applied to to a specific container instance. For example if we create a 3 node cluster Admin, Container1, Container2 Container3. For Example XD1 XD CONTAINER GROUPS GROUP1 XD2 XD CONTAINER GROUPS GROUP2 In this example XD1 XD CONTAINER GROUPS GROUP1 would apply XD CONTAINER GROUPS GROUP1 to container1 s environment. XD2 XD CONTAINER GROUPS GROUP2 would apply XD CONTAINER GROUPS GROUP2 to container2 s environment. While container3 would not receive a specific environment setting for XD CONTAINER GROUPS. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18858,"XD EC2 applies the environment variables to all container instances that are created. This behavior has to be altered such that a environment variable can be applied to to a specific container instance. For example if we create a 3 node cluster Admin, Container1, Container2 Container3. For Example XD1 XD CONTAINER GROUPS GROUP1 XD2 XD CONTAINER GROUPS GROUP2 In this example XD1 XD CONTAINER GROUPS GROUP1 would apply XD CONTAINER GROUPS GROUP1 to container1 s environment. XD2 XD CONTAINER GROUPS GROUP2 would apply XD CONTAINER GROUPS GROUP2 to container2 s environment. While container3 would not receive a specific environment setting for XD CONTAINER GROUPS. ",NULL,"XD EC2 applies the environment variables to all container instances that are created. This behavior has to be altered such that a environment variable can be applied to to a specific container instance. For example if we create a 3 node cluster Admin, Container1, Container2 Container3. For Example XD1 XD CONTAINER GROUPS GROUP1 XD2 XD CONTAINER GROUPS GROUP2 In this example XD1 XD CONTAINER GROUPS GROUP1 would apply XD CONTAINER GROUPS GROUP1 to container1 s environment. XD2 XD CONTAINER GROUPS GROUP2 would apply XD CONTAINER GROUPS GROUP2 to container2 s environment. While container3 would not receive a specific environment setting for XD CONTAINER GROUPS. ",NULL,"XD EC2 applies the environment variables to all container instances that are created<span class='highlight-text severity-high'>. This behavior has to be altered such that a environment variable can be applied to to a specific container instance. For example if we create a 3 node cluster Admin, Container1, Container2 Container3. For Example XD1 XD CONTAINER GROUPS GROUP1 XD2 XD CONTAINER GROUPS GROUP2 In this example XD1 XD CONTAINER GROUPS GROUP1 would apply XD CONTAINER GROUPS GROUP1 to container1 s environment. XD2 XD CONTAINER GROUPS GROUP2 would apply XD CONTAINER GROUPS GROUP2 to container2 s environment. While container3 would not receive a specific environment setting for XD CONTAINER GROUPS. </span>","minimal","punctuation","high",False
18880,"We are getting test failures such as https build.spring.io browse XD MASTER 1381 quite often in the CI environment recently. I suspect the ordering of checks in AbstractSingleNodeStreamDeploymentIntegrationTests Deploys in reverse order assertModuleRequest streamName, log , false ; assertModuleRequest streamName, filter , false ; assertModuleRequest streamName, transform , false ; assertModuleRequest streamName, http , false ; Undeploys in stream order assertModuleRequest streamName, http , true ; assertModuleRequest streamName, transform , true ; assertModuleRequest streamName, filter , true ; assertModuleRequest streamName, log , true ; isn t happening perhaps changing nextUndeployEvent poll time from 5 seconds to higher is appropriate no idea why CI environment seems so slow ",NULL,"We are getting test failures such as https build.spring.io browse XD MASTER 1381 quite often in the CI environment recently. I suspect the ordering of checks in AbstractSingleNodeStreamDeploymentIntegrationTests Deploys in reverse order assertModuleRequest streamName, log , false ; assertModuleRequest streamName, filter , false ; assertModuleRequest streamName, transform , false ; assertModuleRequest streamName, http , false ; Undeploys in stream order assertModuleRequest streamName, http , true ; assertModuleRequest streamName, transform , true ; assertModuleRequest streamName, filter , true ; assertModuleRequest streamName, log , true ; isn t happening perhaps changing nextUndeployEvent poll time from 5 seconds to higher is appropriate no idea why CI environment seems","so slow","Add for who this story is","well_formed","no_role","high",False
18880,"We are getting test failures such as https build.spring.io browse XD MASTER 1381 quite often in the CI environment recently. I suspect the ordering of checks in AbstractSingleNodeStreamDeploymentIntegrationTests Deploys in reverse order assertModuleRequest streamName, log , false ; assertModuleRequest streamName, filter , false ; assertModuleRequest streamName, transform , false ; assertModuleRequest streamName, http , false ; Undeploys in stream order assertModuleRequest streamName, http , true ; assertModuleRequest streamName, transform , true ; assertModuleRequest streamName, filter , true ; assertModuleRequest streamName, log , true ; isn t happening perhaps changing nextUndeployEvent poll time from 5 seconds to higher is appropriate no idea why CI environment seems so slow ",NULL,"We are getting test failures such as https build.spring.io browse XD MASTER 1381 quite often in the CI environment recently. I suspect the ordering of checks in AbstractSingleNodeStreamDeploymentIntegrationTests Deploys in reverse order assertModuleRequest streamName, log , false ; assertModuleRequest streamName, filter , false ; assertModuleRequest streamName, transform , false ; assertModuleRequest streamName, http , false ; Undeploys in stream order assertModuleRequest streamName, http , true ; assertModuleRequest streamName, transform , true ; assertModuleRequest streamName, filter , true ; assertModuleRequest streamName, log , true ; isn t happening perhaps changing nextUndeployEvent poll time from 5 seconds to higher is appropriate no idea why CI environment seems","so slow","We are getting test failures such as https build<span class='highlight-text severity-high'>.spring.io browse XD MASTER 1381 quite often in the CI environment recently. I suspect the ordering of checks in AbstractSingleNodeStreamDeploymentIntegrationTests Deploys in reverse order assertModuleRequest streamName, log , false ; assertModuleRequest streamName, filter , false ; assertModuleRequest streamName, transform , false ; assertModuleRequest streamName, http , false ; Undeploys in stream order assertModuleRequest streamName, http , true ; assertModuleRequest streamName, transform , true ; assertModuleRequest streamName, filter , true ; assertModuleRequest streamName, log , true ; isn t happening perhaps changing nextUndeployEvent poll time from 5 seconds to higher is appropriate no idea why CI environment seems so slow </span>","minimal","punctuation","high",False
18880,"We are getting test failures such as https build.spring.io browse XD MASTER 1381 quite often in the CI environment recently. I suspect the ordering of checks in AbstractSingleNodeStreamDeploymentIntegrationTests Deploys in reverse order assertModuleRequest streamName, log , false ; assertModuleRequest streamName, filter , false ; assertModuleRequest streamName, transform , false ; assertModuleRequest streamName, http , false ; Undeploys in stream order assertModuleRequest streamName, http , true ; assertModuleRequest streamName, transform , true ; assertModuleRequest streamName, filter , true ; assertModuleRequest streamName, log , true ; isn t happening perhaps changing nextUndeployEvent poll time from 5 seconds to higher is appropriate no idea why CI environment seems so slow ",NULL,"We are getting test failures such as https build.spring.io browse XD MASTER 1381 quite often in the CI environment recently. I suspect the ordering of checks in AbstractSingleNodeStreamDeploymentIntegrationTests Deploys in reverse order assertModuleRequest streamName, log , false ; assertModuleRequest streamName, filter , false ; assertModuleRequest streamName, transform , false ; assertModuleRequest streamName, http , false ; Undeploys in stream order assertModuleRequest streamName, http , true ; assertModuleRequest streamName, transform , true ; assertModuleRequest streamName, filter , true ; assertModuleRequest streamName, log , true ; isn t happening perhaps changing nextUndeployEvent poll time from 5 seconds to higher is appropriate no idea why CI environment seems","so slow","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18867,"As part of XD 1338 we modified how module deployment works. Now module deployment requests include deployment properties as the data for the ZooKeeper node. This allows us to reuse those properties when a container exit the cluster and the module is redeployed to another container. However if there are no other containers to handle the deployment, the module deployment node is erased, along with the properties. This mean no module will ever handle the partition that module was responsible for. This condition needs to be handled so that partitioned streams continue to function in cases where the cluster temporarily doesn t have enough containers to support the stream.","As part of","XD 1338 we modified how module deployment works. Now module deployment requests include deployment properties as the data for the ZooKeeper node. This allows us to reuse those properties when a container exit the cluster and the module is redeployed to another container. However if there are no other containers to handle the deployment, the module deployment node is erased, along with the properties. This mean no module will ever handle the partition that module was responsible for. This condition needs to be handled","so that partitioned streams continue to function in cases where the cluster temporarily doesn t have enough containers to support the stream.","As part of XD 1338 we modified how module deployment works. Now module deployment requests include deployment properties as the data for the ZooKeeper node. This allows us to reuse those properties when a container exit the cluster<span class='highlight-text severity-high'> and </span>the module is redeployed to another container. However if there are no other containers to handle the deployment, the module deployment node is erased, along with the properties. This mean no module will ever handle the partition that module was responsible for. This condition needs to be handled so that partitioned streams continue to function in cases where the cluster temporarily doesn t have enough containers to support the stream.","atomic","conjunctions","high",False
18867,"As part of XD 1338 we modified how module deployment works. Now module deployment requests include deployment properties as the data for the ZooKeeper node. This allows us to reuse those properties when a container exit the cluster and the module is redeployed to another container. However if there are no other containers to handle the deployment, the module deployment node is erased, along with the properties. This mean no module will ever handle the partition that module was responsible for. This condition needs to be handled so that partitioned streams continue to function in cases where the cluster temporarily doesn t have enough containers to support the stream.","As part of","XD 1338 we modified how module deployment works. Now module deployment requests include deployment properties as the data for the ZooKeeper node. This allows us to reuse those properties when a container exit the cluster and the module is redeployed to another container. However if there are no other containers to handle the deployment, the module deployment node is erased, along with the properties. This mean no module will ever handle the partition that module was responsible for. This condition needs to be handled","so that partitioned streams continue to function in cases where the cluster temporarily doesn t have enough containers to support the stream.","As part of XD 1338 we modified how module deployment works<span class='highlight-text severity-high'>. Now module deployment requests include deployment properties as the data for the ZooKeeper node. This allows us to reuse those properties when a container exit the cluster and the module is redeployed to another container. However if there are no other containers to handle the deployment, the module deployment node is erased, along with the properties. This mean no module will ever handle the partition that module was responsible for. This condition needs to be handled so that partitioned streams continue to function in cases where the cluster temporarily doesn t have enough containers to support the stream.</span>","minimal","punctuation","high",False
18867,"As part of XD 1338 we modified how module deployment works. Now module deployment requests include deployment properties as the data for the ZooKeeper node. This allows us to reuse those properties when a container exit the cluster and the module is redeployed to another container. However if there are no other containers to handle the deployment, the module deployment node is erased, along with the properties. This mean no module will ever handle the partition that module was responsible for. This condition needs to be handled so that partitioned streams continue to function in cases where the cluster temporarily doesn t have enough containers to support the stream.","As part of","XD 1338 we modified how module deployment works. Now module deployment requests include deployment properties as the data for the ZooKeeper node. This allows us to reuse those properties when a container exit the cluster and the module is redeployed to another container. However if there are no other containers to handle the deployment, the module deployment node is erased, along with the properties. This mean no module will ever handle the partition that module was responsible for. This condition needs to be handled","so that partitioned streams continue to function in cases where the cluster temporarily doesn t have enough containers to support the stream.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18873,"build 22 May 2014 08 45 04 Creating stream tcptofile with definition tcp port 3D21234 socketTimeout 3D2000 7C file dir 3D 2Ftmp 2Fxdtest 2Fbasic ... build 22 May 2014 08 45 04 name tcptofile , deployed null, definition tcp port 21234 socketTimeout 2000 file dir tmp xdtest basic , links rel self , href http 127.0.0.1 9393 streams tcptofile build 22 May 2014 08 45 04 build 22 May 2014 08 45 11 Destroying stream tcptofile ... build 22 May 2014 08 45 11 build 22 May 2014 08 45 11 build 22 May 2014 08 45 11 Expected blahblah does not match actual value 98,108,97,104,98,108,97,104 simple 22 May 2014 08 45 11 Failing task since return code of bin sh tmp XD SCRIPTS RS 513 ScriptBuildTask 7280766559152712153.sh was 1 while expected 0 simple 22 May 2014 08 45 11 Finished task Run basic stream tests See https build.spring.io download XD SCRIPTS RS build logs XD SCRIPTS RS 513.log",NULL,"build 22 May 2014 08 45 04 Creating stream tcptofile with definition tcp port 3D21234 socketTimeout 3D2000 7C file dir 3D 2Ftmp 2Fxdtest 2Fbasic ... build 22 May 2014 08 45 04 name tcptofile , deployed null, definition tcp port 21234 socketTimeout 2000 file dir tmp xdtest basic , links rel self , href http 127.0.0.1 9393 streams tcptofile build 22 May 2014 08 45 04 build 22 May 2014 08 45 11 Destroying stream tcptofile ... build 22 May 2014 08 45 11 build 22 May 2014 08 45 11 build 22 May 2014 08 45 11 Expected blahblah does not match actual value 98,108,97,104,98,108,97,104 simple 22 May 2014 08 45 11 Failing task since return code of bin sh tmp XD SCRIPTS RS 513 ScriptBuildTask 7280766559152712153.sh was 1 while expected 0 simple 22 May 2014 08 45 11 Finished task Run basic stream tests See https build.spring.io download XD SCRIPTS RS build logs XD SCRIPTS RS 513.log",NULL,"Add for who this story is","well_formed","no_role","high",False
18873,"build 22 May 2014 08 45 04 Creating stream tcptofile with definition tcp port 3D21234 socketTimeout 3D2000 7C file dir 3D 2Ftmp 2Fxdtest 2Fbasic ... build 22 May 2014 08 45 04 name tcptofile , deployed null, definition tcp port 21234 socketTimeout 2000 file dir tmp xdtest basic , links rel self , href http 127.0.0.1 9393 streams tcptofile build 22 May 2014 08 45 04 build 22 May 2014 08 45 11 Destroying stream tcptofile ... build 22 May 2014 08 45 11 build 22 May 2014 08 45 11 build 22 May 2014 08 45 11 Expected blahblah does not match actual value 98,108,97,104,98,108,97,104 simple 22 May 2014 08 45 11 Failing task since return code of bin sh tmp XD SCRIPTS RS 513 ScriptBuildTask 7280766559152712153.sh was 1 while expected 0 simple 22 May 2014 08 45 11 Finished task Run basic stream tests See https build.spring.io download XD SCRIPTS RS build logs XD SCRIPTS RS 513.log",NULL,"build 22 May 2014 08 45 04 Creating stream tcptofile with definition tcp port 3D21234 socketTimeout 3D2000 7C file dir 3D 2Ftmp 2Fxdtest 2Fbasic ... build 22 May 2014 08 45 04 name tcptofile , deployed null, definition tcp port 21234 socketTimeout 2000 file dir tmp xdtest basic , links rel self , href http 127.0.0.1 9393 streams tcptofile build 22 May 2014 08 45 04 build 22 May 2014 08 45 11 Destroying stream tcptofile ... build 22 May 2014 08 45 11 build 22 May 2014 08 45 11 build 22 May 2014 08 45 11 Expected blahblah does not match actual value 98,108,97,104,98,108,97,104 simple 22 May 2014 08 45 11 Failing task since return code of bin sh tmp XD SCRIPTS RS 513 ScriptBuildTask 7280766559152712153.sh was 1 while expected 0 simple 22 May 2014 08 45 11 Finished task Run basic stream tests See https build.spring.io download XD SCRIPTS RS build logs XD SCRIPTS RS 513.log",NULL,"build 22 May 2014 08 45 04 Creating stream tcptofile with definition tcp port 3D21234 socketTimeout 3D2000 7C file dir 3D 2Ftmp 2Fxdtest 2Fbasic <span class='highlight-text severity-high'>... build 22 May 2014 08 45 04 name tcptofile , deployed null, definition tcp port 21234 socketTimeout 2000 file dir tmp xdtest basic , links rel self , href http 127.0.0.1 9393 streams tcptofile build 22 May 2014 08 45 04 build 22 May 2014 08 45 11 Destroying stream tcptofile ... build 22 May 2014 08 45 11 build 22 May 2014 08 45 11 build 22 May 2014 08 45 11 Expected blahblah does not match actual value 98,108,97,104,98,108,97,104 simple 22 May 2014 08 45 11 Failing task since return code of bin sh tmp XD SCRIPTS RS 513 ScriptBuildTask 7280766559152712153.sh was 1 while expected 0 simple 22 May 2014 08 45 11 Finished task Run basic stream tests See https build.spring.io download XD SCRIPTS RS build logs XD SCRIPTS RS 513.log</span>","minimal","punctuation","high",False
18888,"Create a new section in the docs regaring shell usage, in particular how to represent single and double quotes. Include some discussion of basic commands to manipulate streams, jobs and list modules. How to pass in a file that can be executed when the shell starts up. Also point to spring shell ref docs for extensibility in terms of adding custom commands.",NULL,"Create a new section in the docs regaring shell usage, in particular how to represent single and double quotes. Include some discussion of basic commands to manipulate streams, jobs and list modules. How to pass in a file that can be executed when the shell starts up. Also point to spring shell ref docs for extensibility in terms of adding custom commands.",NULL,"Add for who this story is","well_formed","no_role","high",False
18888,"Create a new section in the docs regaring shell usage, in particular how to represent single and double quotes. Include some discussion of basic commands to manipulate streams, jobs and list modules. How to pass in a file that can be executed when the shell starts up. Also point to spring shell ref docs for extensibility in terms of adding custom commands.",NULL,"Create a new section in the docs regaring shell usage, in particular how to represent single and double quotes. Include some discussion of basic commands to manipulate streams, jobs and list modules. How to pass in a file that can be executed when the shell starts up. Also point to spring shell ref docs for extensibility in terms of adding custom commands.",NULL,"Create a new section in the docs regaring shell usage, in particular how to represent single and double quotes<span class='highlight-text severity-high'>. Include some discussion of basic commands to manipulate streams, jobs and list modules. How to pass in a file that can be executed when the shell starts up. Also point to spring shell ref docs for extensibility in terms of adding custom commands.</span>","minimal","punctuation","high",False
18896,"As a user, I'd like to have the option to provide security configurations so that I can access REST endpoints in a secured manner. Ideally, all the listed REST https github.com spring projects spring xd wiki REST API xd resources endpoints needs to be wrapped within a security layer. Scope of this spike Research Spring Security and Spring Boot and the OOTB features Design considerations and approach for XD Developer experience How users will be configuring security credentials? How DSL shell will be handled? How Admin UI will be handled?","As a user",", I'd like to have the option to provide security configurations","so that I can access REST endpoints in a secured manner. Ideally, all the listed REST https github.com spring projects spring xd wiki REST API xd resources endpoints needs to be wrapped within a security layer. Scope of this spike Research Spring Security and Spring Boot and the OOTB features Design considerations and approach for XD Developer experience How users will be configuring security credentials? How DSL shell will be handled? How Admin UI will be handled?","As a user, I'd like to have the option to provide security configurations so that I can access REST endpoints in a secured manner<span class='highlight-text severity-high'>. Ideally, all the listed REST https github.com spring projects spring xd wiki REST API xd resources endpoints needs to be wrapped within a security layer. Scope of this spike Research Spring Security and Spring Boot and the OOTB features Design considerations and approach for XD Developer experience How users will be configuring security credentials? How DSL shell will be handled? How Admin UI will be handled?</span>","minimal","punctuation","high",False
18896,"As a user, I'd like to have the option to provide security configurations so that I can access REST endpoints in a secured manner. Ideally, all the listed REST https github.com spring projects spring xd wiki REST API xd resources endpoints needs to be wrapped within a security layer. Scope of this spike Research Spring Security and Spring Boot and the OOTB features Design considerations and approach for XD Developer experience How users will be configuring security credentials? How DSL shell will be handled? How Admin UI will be handled?","As a user",", I'd like to have the option to provide security configurations","so that I can access REST endpoints in a secured manner. Ideally, all the listed REST https github.com spring projects spring xd wiki REST API xd resources endpoints needs to be wrapped within a security layer. Scope of this spike Research Spring Security and Spring Boot and the OOTB features Design considerations and approach for XD Developer experience How users will be configuring security credentials? How DSL shell will be handled? How Admin UI will be handled?","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
18910,"Add configuration for the partition strategy to HDFS sink to support writing files into subdirectories based on a partition key provided in the header or field in the message of the stream data. The writing using HDFS Store DataWriter should pass in the partition key value to be used for the write operation. Partition configuration could be made available to the sink using a format parameter that could then be used in XML config like code expression new java.text.SimpleDateFormat format .format timestamp code Similar to the time source.",NULL,"Add configuration for the partition strategy to HDFS sink to support writing files into subdirectories based on a partition key provided in the header or field in the message of the stream data. The writing using HDFS Store DataWriter should pass in the partition key value to be used for the write operation. Partition configuration could be made available to the sink using a format parameter that could then be used in XML config like code expression new java.text.SimpleDateFormat format .format timestamp code Similar to the time source.",NULL,"Add configuration for the partition strategy to HDFS sink to support writing files into subdirectories based on a partition key provided in the header or field in the message of the stream data<span class='highlight-text severity-high'>. The writing using HDFS Store DataWriter should pass in the partition key value to be used for the write operation. Partition configuration could be made available to the sink using a format parameter that could then be used in XML config like code expression new java.text.SimpleDateFormat format .format timestamp code Similar to the time source.</span>","minimal","punctuation","high",False
18936,"Currently uses m5 dependency. ",NULL,"Currently uses m5 dependency. ",NULL,"Add for who this story is","well_formed","no_role","high",False
18942,"When using hdp13, the XdConfigLoggingInitializer throws this info 12 02 06,064 INFO main util.XdConfigLoggingInitializer 77 Hadoop Distro hdp13 12 02 06,068 WARN main util.XdConfigLoggingInitializer 84 Hadoop version detected from classpath 1.2.0 but you provided hadoopDistro hdp13 . Did you mean hadoopDistro hdp20 ? 12 02 06,069 WARN main util.XdConfigLoggingInitializer 84 Hadoop version detected from classpath 1.2.0 but you provided hadoopDistro hdp13 . Did you mean hadoopDistro hadoop12 ? Since hdp13 uses hadoop 1.2.0, we need to fix that in the versions map ContainerOptions.getHadoopDistroVersions ",NULL,"When using hdp13, the XdConfigLoggingInitializer throws this info 12 02 06,064 INFO main util.XdConfigLoggingInitializer 77 Hadoop Distro hdp13 12 02 06,068 WARN main util.XdConfigLoggingInitializer 84 Hadoop version detected from classpath 1.2.0 but you provided hadoopDistro hdp13 . Did you mean hadoopDistro hdp20 ? 12 02 06,069 WARN main util.XdConfigLoggingInitializer 84 Hadoop version detected from classpath 1.2.0 but you provided hadoopDistro hdp13 . Did you mean hadoopDistro hadoop12 ? Since hdp13 uses hadoop 1.2.0, we need to fix that in the versions map ContainerOptions.getHadoopDistroVersions ",NULL,"Add for who this story is","well_formed","no_role","high",False
18942,"When using hdp13, the XdConfigLoggingInitializer throws this info 12 02 06,064 INFO main util.XdConfigLoggingInitializer 77 Hadoop Distro hdp13 12 02 06,068 WARN main util.XdConfigLoggingInitializer 84 Hadoop version detected from classpath 1.2.0 but you provided hadoopDistro hdp13 . Did you mean hadoopDistro hdp20 ? 12 02 06,069 WARN main util.XdConfigLoggingInitializer 84 Hadoop version detected from classpath 1.2.0 but you provided hadoopDistro hdp13 . Did you mean hadoopDistro hadoop12 ? Since hdp13 uses hadoop 1.2.0, we need to fix that in the versions map ContainerOptions.getHadoopDistroVersions ",NULL,"When using hdp13, the XdConfigLoggingInitializer throws this info 12 02 06,064 INFO main util.XdConfigLoggingInitializer 77 Hadoop Distro hdp13 12 02 06,068 WARN main util.XdConfigLoggingInitializer 84 Hadoop version detected from classpath 1.2.0 but you provided hadoopDistro hdp13 . Did you mean hadoopDistro hdp20 ? 12 02 06,069 WARN main util.XdConfigLoggingInitializer 84 Hadoop version detected from classpath 1.2.0 but you provided hadoopDistro hdp13 . Did you mean hadoopDistro hadoop12 ? Since hdp13 uses hadoop 1.2.0, we need to fix that in the versions map ContainerOptions.getHadoopDistroVersions ",NULL,"When using hdp13, the XdConfigLoggingInitializer throws this info 12 02 06,064 INFO main util<span class='highlight-text severity-high'>.XdConfigLoggingInitializer 77 Hadoop Distro hdp13 12 02 06,068 WARN main util.XdConfigLoggingInitializer 84 Hadoop version detected from classpath 1.2.0 but you provided hadoopDistro hdp13 . Did you mean hadoopDistro hdp20 ? 12 02 06,069 WARN main util.XdConfigLoggingInitializer 84 Hadoop version detected from classpath 1.2.0 but you provided hadoopDistro hdp13 . Did you mean hadoopDistro hadoop12 ? Since hdp13 uses hadoop 1.2.0, we need to fix that in the versions map ContainerOptions.getHadoopDistroVersions </span>","minimal","punctuation","high",False
18965,"Currently the index is used globally but applied to a range of candidates that can differ based on the match criteria per invocation.",NULL,"Currently the index is used globally but applied to a range of candidates that can differ based on the match criteria per invocation.",NULL,"Add for who this story is","well_formed","no_role","high",False
18968,"noformat correlation strategy expression correlation xd.stream.name noformat should be noformat correlation strategy expression correlation xd.stream.name noformat Add a test to make sure correlation expressions here work.",NULL,"noformat correlation strategy expression correlation xd.stream.name noformat should be noformat correlation strategy expression correlation xd.stream.name noformat Add a test to make sure correlation expressions here work.",NULL,"Add for who this story is","well_formed","no_role","high",False
18997,"This project contains core abstractions that will allow for multiple implementations of a machine learning algorithm to be implemented via integration with various existing libraries or custom code implementations. The initial code for this has been developed in a separate github repo and is located here https github.com thomasdarimont spring xd tree feature advanced analytics support spring xd analytics src main java org springframework xd analytics model The model can assume its use in evaluation of the model inside a stream where the data structure is a Tuple. Note, it maybe useful to consider Message Tuple in case any metadata outside the core input data is required to help guide the evaluation. The build.gradle file should be updated such that there is a new build artifact spring xd machine learning analytics.jar along the lines of our other build artifacts. Open to other naming suggestions.",NULL,"This project contains core abstractions that will allow for multiple implementations of a machine learning algorithm to be implemented via integration with various existing libraries or custom code implementations. The initial code for this has been developed in a separate github repo and is located here https github.com thomasdarimont spring xd tree feature advanced analytics support spring xd analytics src main java org springframework xd analytics model The model can assume its use in evaluation of the model inside a stream where the data structure is a Tuple. Note, it maybe useful to consider Message Tuple in case any metadata outside the core input data is required to help guide the evaluation. The build.gradle file should be updated such that there is a new build artifact spring xd machine learning analytics.jar along the lines of our other build artifacts. Open to other naming suggestions.",NULL,"Add for who this story is","well_formed","no_role","high",False
18997,"This project contains core abstractions that will allow for multiple implementations of a machine learning algorithm to be implemented via integration with various existing libraries or custom code implementations. The initial code for this has been developed in a separate github repo and is located here https github.com thomasdarimont spring xd tree feature advanced analytics support spring xd analytics src main java org springframework xd analytics model The model can assume its use in evaluation of the model inside a stream where the data structure is a Tuple. Note, it maybe useful to consider Message Tuple in case any metadata outside the core input data is required to help guide the evaluation. The build.gradle file should be updated such that there is a new build artifact spring xd machine learning analytics.jar along the lines of our other build artifacts. Open to other naming suggestions.",NULL,"This project contains core abstractions that will allow for multiple implementations of a machine learning algorithm to be implemented via integration with various existing libraries or custom code implementations. The initial code for this has been developed in a separate github repo and is located here https github.com thomasdarimont spring xd tree feature advanced analytics support spring xd analytics src main java org springframework xd analytics model The model can assume its use in evaluation of the model inside a stream where the data structure is a Tuple. Note, it maybe useful to consider Message Tuple in case any metadata outside the core input data is required to help guide the evaluation. The build.gradle file should be updated such that there is a new build artifact spring xd machine learning analytics.jar along the lines of our other build artifacts. Open to other naming suggestions.",NULL,"This project contains core abstractions that will allow for multiple implementations of a machine learning algorithm to be implemented via integration with various existing libraries<span class='highlight-text severity-high'> or </span>custom code implementations. The initial code for this has been developed in a separate github repo<span class='highlight-text severity-high'> and </span>is located here https github.com thomasdarimont spring xd tree feature advanced analytics support spring xd analytics src main java org springframework xd analytics model The model can assume its use in evaluation of the model inside a stream where the data structure is a Tuple. Note, it maybe useful to consider Message Tuple in case any metadata outside the core input data is required to help guide the evaluation. The build.gradle file should be updated such that there is a new build artifact spring xd machine learning analytics.jar along the lines of our other build artifacts. Open to other naming suggestions.","atomic","conjunctions","high",False
18997,"This project contains core abstractions that will allow for multiple implementations of a machine learning algorithm to be implemented via integration with various existing libraries or custom code implementations. The initial code for this has been developed in a separate github repo and is located here https github.com thomasdarimont spring xd tree feature advanced analytics support spring xd analytics src main java org springframework xd analytics model The model can assume its use in evaluation of the model inside a stream where the data structure is a Tuple. Note, it maybe useful to consider Message Tuple in case any metadata outside the core input data is required to help guide the evaluation. The build.gradle file should be updated such that there is a new build artifact spring xd machine learning analytics.jar along the lines of our other build artifacts. Open to other naming suggestions.",NULL,"This project contains core abstractions that will allow for multiple implementations of a machine learning algorithm to be implemented via integration with various existing libraries or custom code implementations. The initial code for this has been developed in a separate github repo and is located here https github.com thomasdarimont spring xd tree feature advanced analytics support spring xd analytics src main java org springframework xd analytics model The model can assume its use in evaluation of the model inside a stream where the data structure is a Tuple. Note, it maybe useful to consider Message Tuple in case any metadata outside the core input data is required to help guide the evaluation. The build.gradle file should be updated such that there is a new build artifact spring xd machine learning analytics.jar along the lines of our other build artifacts. Open to other naming suggestions.",NULL,"This project contains core abstractions that will allow for multiple implementations of a machine learning algorithm to be implemented via integration with various existing libraries or custom code implementations<span class='highlight-text severity-high'>. The initial code for this has been developed in a separate github repo and is located here https github.com thomasdarimont spring xd tree feature advanced analytics support spring xd analytics src main java org springframework xd analytics model The model can assume its use in evaluation of the model inside a stream where the data structure is a Tuple. Note, it maybe useful to consider Message Tuple in case any metadata outside the core input data is required to help guide the evaluation. The build.gradle file should be updated such that there is a new build artifact spring xd machine learning analytics.jar along the lines of our other build artifacts. Open to other naming suggestions.</span>","minimal","punctuation","high",False
19020,"Problem Can t use tcp source, sink and http together on Single Node. While creating tests for CI I tried to create the following Steps to Reproduce xd stream create fooOut definition tcp file Created new stream fooOut xd stream create fooIn definition http port 9002 tcp Command failed org.springframework.xd.rest.client.impl.SpringXDException Failed to bind to 0.0.0.0 0.0.0.0 9000. Possibly the port is already in use. Even if I use different ports for the tcp I still get failures pointing to 9000. Extra Notes The stream below is works. xd stream create fooOut definition tcp file Created new stream fooOut xd stream create fooIN definition time tcp Stack Trace Attached ",NULL,"Problem Can t use tcp source, sink and http together on Single Node. While creating tests for CI I tried to create the following Steps to Reproduce xd stream create fooOut definition tcp file Created new stream fooOut xd stream create fooIn definition http port 9002 tcp Command failed org.springframework.xd.rest.client.impl.SpringXDException Failed to bind to 0.0.0.0 0.0.0.0 9000. Possibly the port is already in use. Even if I use different ports for the tcp I still get failures pointing to 9000. Extra Notes The stream below is works. xd stream create fooOut definition tcp file Created new stream fooOut xd stream create fooIN definition time tcp Stack Trace Attached ",NULL,"Add for who this story is","well_formed","no_role","high",False
19020,"Problem Can t use tcp source, sink and http together on Single Node. While creating tests for CI I tried to create the following Steps to Reproduce xd stream create fooOut definition tcp file Created new stream fooOut xd stream create fooIn definition http port 9002 tcp Command failed org.springframework.xd.rest.client.impl.SpringXDException Failed to bind to 0.0.0.0 0.0.0.0 9000. Possibly the port is already in use. Even if I use different ports for the tcp I still get failures pointing to 9000. Extra Notes The stream below is works. xd stream create fooOut definition tcp file Created new stream fooOut xd stream create fooIN definition time tcp Stack Trace Attached ",NULL,"Problem Can t use tcp source, sink and http together on Single Node. While creating tests for CI I tried to create the following Steps to Reproduce xd stream create fooOut definition tcp file Created new stream fooOut xd stream create fooIn definition http port 9002 tcp Command failed org.springframework.xd.rest.client.impl.SpringXDException Failed to bind to 0.0.0.0 0.0.0.0 9000. Possibly the port is already in use. Even if I use different ports for the tcp I still get failures pointing to 9000. Extra Notes The stream below is works. xd stream create fooOut definition tcp file Created new stream fooOut xd stream create fooIN definition time tcp Stack Trace Attached ",NULL,"Problem Can t use tcp source, sink<span class='highlight-text severity-high'> and </span>http together on Single Node. While creating tests for CI I tried to create the following Steps to Reproduce xd stream create fooOut definition tcp file Created new stream fooOut xd stream create fooIn definition http port 9002 tcp Command failed org.springframework.xd.rest.client.impl.SpringXDException Failed to bind to 0.0.0.0 0.0.0.0 9000. Possibly the port is already in use. Even if I use different ports for the tcp I still get failures pointing to 9000. Extra Notes The stream below is works. xd stream create fooOut definition tcp file Created new stream fooOut xd stream create fooIN definition time tcp Stack Trace Attached ","atomic","conjunctions","high",False
19020,"Problem Can t use tcp source, sink and http together on Single Node. While creating tests for CI I tried to create the following Steps to Reproduce xd stream create fooOut definition tcp file Created new stream fooOut xd stream create fooIn definition http port 9002 tcp Command failed org.springframework.xd.rest.client.impl.SpringXDException Failed to bind to 0.0.0.0 0.0.0.0 9000. Possibly the port is already in use. Even if I use different ports for the tcp I still get failures pointing to 9000. Extra Notes The stream below is works. xd stream create fooOut definition tcp file Created new stream fooOut xd stream create fooIN definition time tcp Stack Trace Attached ",NULL,"Problem Can t use tcp source, sink and http together on Single Node. While creating tests for CI I tried to create the following Steps to Reproduce xd stream create fooOut definition tcp file Created new stream fooOut xd stream create fooIn definition http port 9002 tcp Command failed org.springframework.xd.rest.client.impl.SpringXDException Failed to bind to 0.0.0.0 0.0.0.0 9000. Possibly the port is already in use. Even if I use different ports for the tcp I still get failures pointing to 9000. Extra Notes The stream below is works. xd stream create fooOut definition tcp file Created new stream fooOut xd stream create fooIN definition time tcp Stack Trace Attached ",NULL,"Problem Can t use tcp source, sink and http together on Single Node<span class='highlight-text severity-high'>. While creating tests for CI I tried to create the following Steps to Reproduce xd stream create fooOut definition tcp file Created new stream fooOut xd stream create fooIn definition http port 9002 tcp Command failed org.springframework.xd.rest.client.impl.SpringXDException Failed to bind to 0.0.0.0 0.0.0.0 9000. Possibly the port is already in use. Even if I use different ports for the tcp I still get failures pointing to 9000. Extra Notes The stream below is works. xd stream create fooOut definition tcp file Created new stream fooOut xd stream create fooIN definition time tcp Stack Trace Attached </span>","minimal","punctuation","high",False
19029,"This will be used by SingleNodeApplication if no ZooKeeper process is available based on the provided client connect string. It will still be recommended that user s run ZooKeeper externally, at least in standalone mode, when running SingleNodeApplication. We should clarify that in the documentation and logs. It should implement SmartLifecycle so that it can be managed as a bean within an ApplicationContext.",NULL,"This will be used by SingleNodeApplication if no ZooKeeper process is available based on the provided client connect string. It will still be recommended that user s run ZooKeeper externally, at least in standalone mode, when running SingleNodeApplication. We should clarify that in the documentation and logs. It should implement SmartLifecycle","so that it can be managed as a bean within an ApplicationContext.","Add for who this story is","well_formed","no_role","high",False
19029,"This will be used by SingleNodeApplication if no ZooKeeper process is available based on the provided client connect string. It will still be recommended that user s run ZooKeeper externally, at least in standalone mode, when running SingleNodeApplication. We should clarify that in the documentation and logs. It should implement SmartLifecycle so that it can be managed as a bean within an ApplicationContext.",NULL,"This will be used by SingleNodeApplication if no ZooKeeper process is available based on the provided client connect string. It will still be recommended that user s run ZooKeeper externally, at least in standalone mode, when running SingleNodeApplication. We should clarify that in the documentation and logs. It should implement SmartLifecycle","so that it can be managed as a bean within an ApplicationContext.","This will be used by SingleNodeApplication if no ZooKeeper process is available based on the provided client connect string<span class='highlight-text severity-high'>. It will still be recommended that user s run ZooKeeper externally, at least in standalone mode, when running SingleNodeApplication. We should clarify that in the documentation and logs. It should implement SmartLifecycle so that it can be managed as a bean within an ApplicationContext.</span>","minimal","punctuation","high",False
19029,"This will be used by SingleNodeApplication if no ZooKeeper process is available based on the provided client connect string. It will still be recommended that user s run ZooKeeper externally, at least in standalone mode, when running SingleNodeApplication. We should clarify that in the documentation and logs. It should implement SmartLifecycle so that it can be managed as a bean within an ApplicationContext.",NULL,"This will be used by SingleNodeApplication if no ZooKeeper process is available based on the provided client connect string. It will still be recommended that user s run ZooKeeper externally, at least in standalone mode, when running SingleNodeApplication. We should clarify that in the documentation and logs. It should implement SmartLifecycle","so that it can be managed as a bean within an ApplicationContext.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19042,"Currently, hsqldb, postgres and mysql job repositories are supported. We need to add configurable oracle jdbc settings.",NULL,"Currently, hsqldb, postgres and mysql job repositories are supported. We need to add configurable oracle jdbc settings.",NULL,"Add for who this story is","well_formed","no_role","high",False
19042,"Currently, hsqldb, postgres and mysql job repositories are supported. We need to add configurable oracle jdbc settings.",NULL,"Currently, hsqldb, postgres and mysql job repositories are supported. We need to add configurable oracle jdbc settings.",NULL,"Currently, hsqldb, postgres and mysql job repositories are supported<span class='highlight-text severity-high'>. We need to add configurable oracle jdbc settings.</span>","minimal","punctuation","high",False
19049,"Command such as yarn app list yarn deploy xd zipFile tmp myapp.zip config tmp myconfig.yml ",NULL,"Command such as yarn app list yarn deploy xd zipFile tmp myapp.zip config tmp myconfig.yml ",NULL,"Add for who this story is","well_formed","no_role","high",False
19061,"Commit https github.com spring projects spring xd commit 761cd5e8250c055878caf3a789ab5b3254ba48e8 introduced support for FTP and added a bunch of .x classes. These should not belong to DIRT proper though, and should be added to an extension style project. The job s module would then depend on them",NULL,"Commit https github.com spring projects spring xd commit 761cd5e8250c055878caf3a789ab5b3254ba48e8 introduced support for FTP and added a bunch of .x classes. These should not belong to DIRT proper though, and should be added to an extension style project. The job s module would then depend on them",NULL,"Add for who this story is","well_formed","no_role","high",False
19061,"Commit https github.com spring projects spring xd commit 761cd5e8250c055878caf3a789ab5b3254ba48e8 introduced support for FTP and added a bunch of .x classes. These should not belong to DIRT proper though, and should be added to an extension style project. The job s module would then depend on them",NULL,"Commit https github.com spring projects spring xd commit 761cd5e8250c055878caf3a789ab5b3254ba48e8 introduced support for FTP and added a bunch of .x classes. These should not belong to DIRT proper though, and should be added to an extension style project. The job s module would then depend on them",NULL,"Commit https github.com spring projects spring xd commit 761cd5e8250c055878caf3a789ab5b3254ba48e8 introduced support for FTP<span class='highlight-text severity-high'> and </span>added a bunch of .x classes. These should not belong to DIRT proper though, and should be added to an extension style project. The job s module would then depend on them","atomic","conjunctions","high",False
19061,"Commit https github.com spring projects spring xd commit 761cd5e8250c055878caf3a789ab5b3254ba48e8 introduced support for FTP and added a bunch of .x classes. These should not belong to DIRT proper though, and should be added to an extension style project. The job s module would then depend on them",NULL,"Commit https github.com spring projects spring xd commit 761cd5e8250c055878caf3a789ab5b3254ba48e8 introduced support for FTP and added a bunch of .x classes. These should not belong to DIRT proper though, and should be added to an extension style project. The job s module would then depend on them",NULL,"Commit https github<span class='highlight-text severity-high'>.com spring projects spring xd commit 761cd5e8250c055878caf3a789ab5b3254ba48e8 introduced support for FTP and added a bunch of .x classes. These should not belong to DIRT proper though, and should be added to an extension style project. The job s module would then depend on them</span>","minimal","punctuation","high",False
19246,"Here is an example the following request for streams http ec2 23 20 25 30.compute 1.amazonaws.com 9393 streams Returns This XML file does not appear to have any style information associated with it. The document tree is shown below. errors xmlns atom http www.w3.org 2005 Atom error logref HttpMessageNotWritableException message Could not marshal PagedResource content links http ec2 23 20 25 30.compute 1.amazonaws.com 9393 streams ticktock ;rel self , metadata Metadata number 0, total pages 1, total elements 1, size 20 , links null; nested exception is javax.xml.bind.MarshalException with linked exception com.sun.istack.SAXException2 unable to marshal type org.springframework.xd.rest.client.domain.StreamDefinitionResource as an element because it is not known to this context. message error errors ",NULL,"Here is an example the following request for streams http ec2 23 20 25 30.compute 1.amazonaws.com 9393 streams Returns This XML file does not appear to have any style information associated with it. The document tree is shown below. errors xmlns atom http www.w3.org 2005 Atom error logref HttpMessageNotWritableException message Could not marshal PagedResource content links http ec2 23 20 25 30.compute 1.amazonaws.com 9393 streams ticktock ;rel self , metadata Metadata number 0, total pages 1, total elements 1, size 20 , links null; nested exception is javax.xml.bind.MarshalException with linked exception com.sun.istack.SAXException2 unable to marshal type org.springframework.xd.rest.client.domain.StreamDefinitionResource as an element because it is not known to this context. message error errors ",NULL,"Here is an example the following request for streams http ec2 23 20 25 30<span class='highlight-text severity-high'>.compute 1.amazonaws.com 9393 streams Returns This XML file does not appear to have any style information associated with it. The document tree is shown below. errors xmlns atom http www.w3.org 2005 Atom error logref HttpMessageNotWritableException message Could not marshal PagedResource content links http ec2 23 20 25 30.compute 1.amazonaws.com 9393 streams ticktock ;rel self , metadata Metadata number 0, total pages 1, total elements 1, size 20 , links null; nested exception is javax.xml.bind.MarshalException with linked exception com.sun.istack.SAXException2 unable to marshal type org.springframework.xd.rest.client.domain.StreamDefinitionResource as an element because it is not known to this context. message error errors </span>","minimal","punctuation","high",False
19064,"Currently, the fileDeletion listeners are added to filepollhdfs and filejdbc OOTB job modules so that the files are deleted after successful completion of jobs that write the file into hdfs jdbc. We have deleteFiles option in ResourcesIntoJdbcJobModuleOptionsMetadata from https github.com spring projects spring xd pull 562 which makes it available for hdfsjdbc job module as well. But it is not supported yet as it involves deletion of HDFS files. We need the file deletion listeners for the hdfsjdbc and hdfsmongodb job modules so that if opted to delete files, it can be supported. ",NULL,"Currently, the fileDeletion listeners are added to filepollhdfs and filejdbc OOTB job modules","so that the files are deleted after successful completion of jobs that write the file into hdfs jdbc. We have deleteFiles option in ResourcesIntoJdbcJobModuleOptionsMetadata from https github.com spring projects spring xd pull 562 which makes it available for hdfsjdbc job module as well. But it is not supported yet as it involves deletion of HDFS files. We need the file deletion listeners for the hdfsjdbc and hdfsmongodb job modules so that if opted to delete files, it can be supported.","Add for who this story is","well_formed","no_role","high",False
19064,"Currently, the fileDeletion listeners are added to filepollhdfs and filejdbc OOTB job modules so that the files are deleted after successful completion of jobs that write the file into hdfs jdbc. We have deleteFiles option in ResourcesIntoJdbcJobModuleOptionsMetadata from https github.com spring projects spring xd pull 562 which makes it available for hdfsjdbc job module as well. But it is not supported yet as it involves deletion of HDFS files. We need the file deletion listeners for the hdfsjdbc and hdfsmongodb job modules so that if opted to delete files, it can be supported. ",NULL,"Currently, the fileDeletion listeners are added to filepollhdfs and filejdbc OOTB job modules","so that the files are deleted after successful completion of jobs that write the file into hdfs jdbc. We have deleteFiles option in ResourcesIntoJdbcJobModuleOptionsMetadata from https github.com spring projects spring xd pull 562 which makes it available for hdfsjdbc job module as well. But it is not supported yet as it involves deletion of HDFS files. We need the file deletion listeners for the hdfsjdbc and hdfsmongodb job modules so that if opted to delete files, it can be supported.","Currently, the fileDeletion listeners are added to filepollhdfs and filejdbc OOTB job modules so that the files are deleted after successful completion of jobs that write the file into hdfs jdbc<span class='highlight-text severity-high'>. We have deleteFiles option in ResourcesIntoJdbcJobModuleOptionsMetadata from https github.com spring projects spring xd pull 562 which makes it available for hdfsjdbc job module as well. But it is not supported yet as it involves deletion of HDFS files. We need the file deletion listeners for the hdfsjdbc and hdfsmongodb job modules so that if opted to delete files, it can be supported. </span>","minimal","punctuation","high",False
19064,"Currently, the fileDeletion listeners are added to filepollhdfs and filejdbc OOTB job modules so that the files are deleted after successful completion of jobs that write the file into hdfs jdbc. We have deleteFiles option in ResourcesIntoJdbcJobModuleOptionsMetadata from https github.com spring projects spring xd pull 562 which makes it available for hdfsjdbc job module as well. But it is not supported yet as it involves deletion of HDFS files. We need the file deletion listeners for the hdfsjdbc and hdfsmongodb job modules so that if opted to delete files, it can be supported. ",NULL,"Currently, the fileDeletion listeners are added to filepollhdfs and filejdbc OOTB job modules","so that the files are deleted after successful completion of jobs that write the file into hdfs jdbc. We have deleteFiles option in ResourcesIntoJdbcJobModuleOptionsMetadata from https github.com spring projects spring xd pull 562 which makes it available for hdfsjdbc job module as well. But it is not supported yet as it involves deletion of HDFS files. We need the file deletion listeners for the hdfsjdbc and hdfsmongodb job modules so that if opted to delete files, it can be supported.","Currently, the fileDeletion listeners are added to filepollhdfs and filejdbc OOTB job modules<span class='highlight-text severity-high'> so that </span>the files are deleted after successful completion of jobs that write the file into hdfs jdbc. We have deleteFiles option in ResourcesIntoJdbcJobModuleOptionsMetadata from https github.com spring projects spring xd pull 562 which makes it available for hdfsjdbc job module as well. But it is not supported yet as it involves deletion of HDFS files. We need the file deletion listeners for the hdfsjdbc and hdfsmongodb job modules<span class='highlight-text severity-high'> so that </span>if opted to delete files, it can be supported. ","minimal","indicator_repetition","high",False
19064,"Currently, the fileDeletion listeners are added to filepollhdfs and filejdbc OOTB job modules so that the files are deleted after successful completion of jobs that write the file into hdfs jdbc. We have deleteFiles option in ResourcesIntoJdbcJobModuleOptionsMetadata from https github.com spring projects spring xd pull 562 which makes it available for hdfsjdbc job module as well. But it is not supported yet as it involves deletion of HDFS files. We need the file deletion listeners for the hdfsjdbc and hdfsmongodb job modules so that if opted to delete files, it can be supported. ",NULL,"Currently, the fileDeletion listeners are added to filepollhdfs and filejdbc OOTB job modules","so that the files are deleted after successful completion of jobs that write the file into hdfs jdbc. We have deleteFiles option in ResourcesIntoJdbcJobModuleOptionsMetadata from https github.com spring projects spring xd pull 562 which makes it available for hdfsjdbc job module as well. But it is not supported yet as it involves deletion of HDFS files. We need the file deletion listeners for the hdfsjdbc and hdfsmongodb job modules so that if opted to delete files, it can be supported.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19082,"Currently, few of the boot s actuator endpoints go missing in the EndpointHandler mapping. They are BeansEndpoint, dumpEndpoint, traceEndpoint, healthEndpoint, infoEndpoint. Also, the EndpointHandler mapping doesn t even happen in case of LauncherApplication. I think this is because the LauncherApplication context starts with port 0 and the TomcatEmbeddedServletContainer sets the local port for it later. With port 0 , the Endpointhandler mapping is disabled during the EndpointHandler mapping bean creation.",NULL,"Currently, few of the boot s actuator endpoints go missing in the EndpointHandler mapping. They are BeansEndpoint, dumpEndpoint, traceEndpoint, healthEndpoint, infoEndpoint. Also, the EndpointHandler mapping doesn t even happen in case of LauncherApplication. I think this is because the LauncherApplication context starts with port 0 and the TomcatEmbeddedServletContainer sets the local port for it later. With port 0 , the Endpointhandler mapping is disabled during the EndpointHandler mapping bean creation.",NULL,"Add for who this story is","well_formed","no_role","high",False
19082,"Currently, few of the boot s actuator endpoints go missing in the EndpointHandler mapping. They are BeansEndpoint, dumpEndpoint, traceEndpoint, healthEndpoint, infoEndpoint. Also, the EndpointHandler mapping doesn t even happen in case of LauncherApplication. I think this is because the LauncherApplication context starts with port 0 and the TomcatEmbeddedServletContainer sets the local port for it later. With port 0 , the Endpointhandler mapping is disabled during the EndpointHandler mapping bean creation.",NULL,"Currently, few of the boot s actuator endpoints go missing in the EndpointHandler mapping. They are BeansEndpoint, dumpEndpoint, traceEndpoint, healthEndpoint, infoEndpoint. Also, the EndpointHandler mapping doesn t even happen in case of LauncherApplication. I think this is because the LauncherApplication context starts with port 0 and the TomcatEmbeddedServletContainer sets the local port for it later. With port 0 , the Endpointhandler mapping is disabled during the EndpointHandler mapping bean creation.",NULL,"Currently, few of the boot s actuator endpoints go missing in the EndpointHandler mapping. They are BeansEndpoint, dumpEndpoint, traceEndpoint, healthEndpoint, infoEndpoint. Also, the EndpointHandler mapping doesn t even happen in case of LauncherApplication. I think this is because the LauncherApplication context starts with port 0<span class='highlight-text severity-high'> and </span>the TomcatEmbeddedServletContainer sets the local port for it later. With port 0 , the Endpointhandler mapping is disabled during the EndpointHandler mapping bean creation.","atomic","conjunctions","high",False
19082,"Currently, few of the boot s actuator endpoints go missing in the EndpointHandler mapping. They are BeansEndpoint, dumpEndpoint, traceEndpoint, healthEndpoint, infoEndpoint. Also, the EndpointHandler mapping doesn t even happen in case of LauncherApplication. I think this is because the LauncherApplication context starts with port 0 and the TomcatEmbeddedServletContainer sets the local port for it later. With port 0 , the Endpointhandler mapping is disabled during the EndpointHandler mapping bean creation.",NULL,"Currently, few of the boot s actuator endpoints go missing in the EndpointHandler mapping. They are BeansEndpoint, dumpEndpoint, traceEndpoint, healthEndpoint, infoEndpoint. Also, the EndpointHandler mapping doesn t even happen in case of LauncherApplication. I think this is because the LauncherApplication context starts with port 0 and the TomcatEmbeddedServletContainer sets the local port for it later. With port 0 , the Endpointhandler mapping is disabled during the EndpointHandler mapping bean creation.",NULL,"Currently, few of the boot s actuator endpoints go missing in the EndpointHandler mapping<span class='highlight-text severity-high'>. They are BeansEndpoint, dumpEndpoint, traceEndpoint, healthEndpoint, infoEndpoint. Also, the EndpointHandler mapping doesn t even happen in case of LauncherApplication. I think this is because the LauncherApplication context starts with port 0 and the TomcatEmbeddedServletContainer sets the local port for it later. With port 0 , the Endpointhandler mapping is disabled during the EndpointHandler mapping bean creation.</span>","minimal","punctuation","high",False
19095,"Add a stage to the plan that will stop any instances that the CI process may have started before and relaunch a 2 node install based on rabbit. https build.springsource.org browse XD ATEC2 was created as an empty shell. The running of across different transports will be handled in a separate story along with adding a stage to run a hello world acceptance test. ",NULL,"Add a stage to the plan that will stop any instances that the CI process may have started before and relaunch a 2 node install based on rabbit. https build.springsource.org browse XD ATEC2 was created as an empty shell. The running of across different transports will be handled in a separate story along with adding a stage to run a hello world acceptance test. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19095,"Add a stage to the plan that will stop any instances that the CI process may have started before and relaunch a 2 node install based on rabbit. https build.springsource.org browse XD ATEC2 was created as an empty shell. The running of across different transports will be handled in a separate story along with adding a stage to run a hello world acceptance test. ",NULL,"Add a stage to the plan that will stop any instances that the CI process may have started before and relaunch a 2 node install based on rabbit. https build.springsource.org browse XD ATEC2 was created as an empty shell. The running of across different transports will be handled in a separate story along with adding a stage to run a hello world acceptance test. ",NULL,"Add a stage to the plan that will stop any instances that the CI process may have started before<span class='highlight-text severity-high'> and </span>relaunch a 2 node install based on rabbit. https build.springsource.org browse XD ATEC2 was created as an empty shell. The running of across different transports will be handled in a separate story along with adding a stage to run a hello world acceptance test. ","atomic","conjunctions","high",False
19095,"Add a stage to the plan that will stop any instances that the CI process may have started before and relaunch a 2 node install based on rabbit. https build.springsource.org browse XD ATEC2 was created as an empty shell. The running of across different transports will be handled in a separate story along with adding a stage to run a hello world acceptance test. ",NULL,"Add a stage to the plan that will stop any instances that the CI process may have started before and relaunch a 2 node install based on rabbit. https build.springsource.org browse XD ATEC2 was created as an empty shell. The running of across different transports will be handled in a separate story along with adding a stage to run a hello world acceptance test. ",NULL,"Add a stage to the plan that will stop any instances that the CI process may have started before and relaunch a 2 node install based on rabbit<span class='highlight-text severity-high'>. https build.springsource.org browse XD ATEC2 was created as an empty shell. The running of across different transports will be handled in a separate story along with adding a stage to run a hello world acceptance test. </span>","minimal","punctuation","high",False
19099,"https sonar.springsource.org dashboard index 7173?did 3 Shows which of our current tests take the most time to execute. xd.dirt.stream and xd.shell.command are where the most time is spent. In xd.dirt.stream it seems likely that time can be reduced by not restarting a new single node server per test, but sharing it across tests, e.g. RabbitSingleNodeStreamDeploymentIntegrationTests LocalSingleNodeStreamDeploymentIntegrationTests RedisSingleNodeStreamDeploymentIntegrationTests As a first pass, the test that take longer than 15 seconds in that report should be investigated. ",NULL,NULL,NULL,"Add what you want to achieve","well_formed","no_means","high",False
19099,"https sonar.springsource.org dashboard index 7173?did 3 Shows which of our current tests take the most time to execute. xd.dirt.stream and xd.shell.command are where the most time is spent. In xd.dirt.stream it seems likely that time can be reduced by not restarting a new single node server per test, but sharing it across tests, e.g. RabbitSingleNodeStreamDeploymentIntegrationTests LocalSingleNodeStreamDeploymentIntegrationTests RedisSingleNodeStreamDeploymentIntegrationTests As a first pass, the test that take longer than 15 seconds in that report should be investigated. ",NULL,NULL,NULL,"Add for who this story is","well_formed","no_role","high",False
19099,"https sonar.springsource.org dashboard index 7173?did 3 Shows which of our current tests take the most time to execute. xd.dirt.stream and xd.shell.command are where the most time is spent. In xd.dirt.stream it seems likely that time can be reduced by not restarting a new single node server per test, but sharing it across tests, e.g. RabbitSingleNodeStreamDeploymentIntegrationTests LocalSingleNodeStreamDeploymentIntegrationTests RedisSingleNodeStreamDeploymentIntegrationTests As a first pass, the test that take longer than 15 seconds in that report should be investigated. ",NULL,NULL,NULL,"https sonar<span class='highlight-text severity-high'>.springsource.org dashboard index 7173?did 3 Shows which of our current tests take the most time to execute. xd.dirt.stream and xd.shell.command are where the most time is spent. In xd.dirt.stream it seems likely that time can be reduced by not restarting a new single node server per test, but sharing it across tests, e.g. RabbitSingleNodeStreamDeploymentIntegrationTests LocalSingleNodeStreamDeploymentIntegrationTests RedisSingleNodeStreamDeploymentIntegrationTests As a first pass, the test that take longer than 15 seconds in that report should be investigated. </span>","minimal","punctuation","high",False
19107,"In distributed batch processing in XD, the JobLocator implementation for getJob String jobName should return a valid Job FlowJob SimpleJob . Since we won t we relying on the MapJobRegistry s joblocator implementation which doesn t work in distributed use case, we need to have an appropriate way to return FlowJob SimpleJob using XD s BatchJobLocator.",NULL,"In distributed batch processing in XD, the JobLocator implementation for getJob String jobName should return a valid Job FlowJob SimpleJob . Since we won t we relying on the MapJobRegistry s joblocator implementation which doesn t work in distributed use case, we need to have an appropriate way to return FlowJob SimpleJob using XD s BatchJobLocator.",NULL,"Add for who this story is","well_formed","no_role","high",False
19107,"In distributed batch processing in XD, the JobLocator implementation for getJob String jobName should return a valid Job FlowJob SimpleJob . Since we won t we relying on the MapJobRegistry s joblocator implementation which doesn t work in distributed use case, we need to have an appropriate way to return FlowJob SimpleJob using XD s BatchJobLocator.",NULL,"In distributed batch processing in XD, the JobLocator implementation for getJob String jobName should return a valid Job FlowJob SimpleJob . Since we won t we relying on the MapJobRegistry s joblocator implementation which doesn t work in distributed use case, we need to have an appropriate way to return FlowJob SimpleJob using XD s BatchJobLocator.",NULL,"In distributed batch processing in XD, the JobLocator implementation for getJob String jobName should return a valid Job FlowJob SimpleJob <span class='highlight-text severity-high'>. Since we won t we relying on the MapJobRegistry s joblocator implementation which doesn t work in distributed use case, we need to have an appropriate way to return FlowJob SimpleJob using XD s BatchJobLocator.</span>","minimal","punctuation","high",False
19116,"hdfsjdbc throws an exception code org.springframework.beans.factory.BeanDefinitionStoreException Invalid bean definition with name itemReader defined in URL file Users trisberg Projects spring xd build dist spring xd xd modules job hdfsjdbc config hdfsjdbc.xml Could not resolve placeholder columns in string value columns code The hdfsjdbc job uses columns instead of names as the parameter for the column names. Should we make this usage consistent between jobs? There is a comment in the docs there is also a limitation in that the database table must be created manually. This is due to a bug in Spring Hadoop and will be fixed in the future. Think this is this solved in Spring Hadoop now? initializeDatabase should default to false now to be consistent with jdbc sink Rename batch jdbc mongo import.properties to batch jdbc mongo.properties since these aren t just for import",NULL,"hdfsjdbc throws an exception code org.springframework.beans.factory.BeanDefinitionStoreException Invalid bean definition with name itemReader defined in URL file Users trisberg Projects spring xd build dist spring xd xd modules job hdfsjdbc config hdfsjdbc.xml Could not resolve placeholder columns in string value columns code The hdfsjdbc job uses columns instead of names as the parameter for the column names. Should we make this usage consistent between jobs? There is a comment in the docs there is also a limitation in that the database table must be created manually. This is due to a bug in Spring Hadoop and will be fixed in the future. Think this is this solved in Spring Hadoop now? initializeDatabase should default to false now to be consistent with jdbc sink Rename batch jdbc mongo import.properties to batch jdbc mongo.properties since these aren t just for import",NULL,"Add for who this story is","well_formed","no_role","high",False
19116,"hdfsjdbc throws an exception code org.springframework.beans.factory.BeanDefinitionStoreException Invalid bean definition with name itemReader defined in URL file Users trisberg Projects spring xd build dist spring xd xd modules job hdfsjdbc config hdfsjdbc.xml Could not resolve placeholder columns in string value columns code The hdfsjdbc job uses columns instead of names as the parameter for the column names. Should we make this usage consistent between jobs? There is a comment in the docs there is also a limitation in that the database table must be created manually. This is due to a bug in Spring Hadoop and will be fixed in the future. Think this is this solved in Spring Hadoop now? initializeDatabase should default to false now to be consistent with jdbc sink Rename batch jdbc mongo import.properties to batch jdbc mongo.properties since these aren t just for import",NULL,"hdfsjdbc throws an exception code org.springframework.beans.factory.BeanDefinitionStoreException Invalid bean definition with name itemReader defined in URL file Users trisberg Projects spring xd build dist spring xd xd modules job hdfsjdbc config hdfsjdbc.xml Could not resolve placeholder columns in string value columns code The hdfsjdbc job uses columns instead of names as the parameter for the column names. Should we make this usage consistent between jobs? There is a comment in the docs there is also a limitation in that the database table must be created manually. This is due to a bug in Spring Hadoop and will be fixed in the future. Think this is this solved in Spring Hadoop now? initializeDatabase should default to false now to be consistent with jdbc sink Rename batch jdbc mongo import.properties to batch jdbc mongo.properties since these aren t just for import",NULL,"hdfsjdbc throws an exception code org.springframework.beans.factory.BeanDefinitionStoreException Invalid bean definition with name itemReader defined in URL file Users trisberg Projects spring xd build dist spring xd xd modules job hdfsjdbc config hdfsjdbc.xml Could not resolve placeholder columns in string value columns code The hdfsjdbc job uses columns instead of names as the parameter for the column names. Should we make this usage consistent between jobs? There is a comment in the docs there is also a limitation in that the database table must be created manually. This is due to a bug in Spring Hadoop<span class='highlight-text severity-high'> and </span>will be fixed in the future. Think this is this solved in Spring Hadoop now? initializeDatabase should default to false now to be consistent with jdbc sink Rename batch jdbc mongo import.properties to batch jdbc mongo.properties since these aren t just for import","atomic","conjunctions","high",False
19116,"hdfsjdbc throws an exception code org.springframework.beans.factory.BeanDefinitionStoreException Invalid bean definition with name itemReader defined in URL file Users trisberg Projects spring xd build dist spring xd xd modules job hdfsjdbc config hdfsjdbc.xml Could not resolve placeholder columns in string value columns code The hdfsjdbc job uses columns instead of names as the parameter for the column names. Should we make this usage consistent between jobs? There is a comment in the docs there is also a limitation in that the database table must be created manually. This is due to a bug in Spring Hadoop and will be fixed in the future. Think this is this solved in Spring Hadoop now? initializeDatabase should default to false now to be consistent with jdbc sink Rename batch jdbc mongo import.properties to batch jdbc mongo.properties since these aren t just for import",NULL,"hdfsjdbc throws an exception code org.springframework.beans.factory.BeanDefinitionStoreException Invalid bean definition with name itemReader defined in URL file Users trisberg Projects spring xd build dist spring xd xd modules job hdfsjdbc config hdfsjdbc.xml Could not resolve placeholder columns in string value columns code The hdfsjdbc job uses columns instead of names as the parameter for the column names. Should we make this usage consistent between jobs? There is a comment in the docs there is also a limitation in that the database table must be created manually. This is due to a bug in Spring Hadoop and will be fixed in the future. Think this is this solved in Spring Hadoop now? initializeDatabase should default to false now to be consistent with jdbc sink Rename batch jdbc mongo import.properties to batch jdbc mongo.properties since these aren t just for import",NULL,"hdfsjdbc throws an exception code org<span class='highlight-text severity-high'>.springframework.beans.factory.BeanDefinitionStoreException Invalid bean definition with name itemReader defined in URL file Users trisberg Projects spring xd build dist spring xd xd modules job hdfsjdbc config hdfsjdbc.xml Could not resolve placeholder columns in string value columns code The hdfsjdbc job uses columns instead of names as the parameter for the column names. Should we make this usage consistent between jobs? There is a comment in the docs there is also a limitation in that the database table must be created manually. This is due to a bug in Spring Hadoop and will be fixed in the future. Think this is this solved in Spring Hadoop now? initializeDatabase should default to false now to be consistent with jdbc sink Rename batch jdbc mongo import.properties to batch jdbc mongo.properties since these aren t just for import</span>","minimal","punctuation","high",False
19274,"Once XD 887 is merged, gradually convert more modules. Recipe 1 Move the module .xml file to module config module .xml 2 Declare a module. type . module gradle project 3 Move dependencies from dirt project to newly created module project 4 gradle build picks it up. gradle clean build manual test Also have a look at gradle cleanEclipse eclipse",NULL,"Once XD 887 is merged, gradually convert more modules. Recipe 1 Move the module .xml file to module config module .xml 2 Declare a module. type . module gradle project 3 Move dependencies from dirt project to newly created module project 4 gradle build picks it up. gradle clean build manual test Also have a look at gradle cleanEclipse eclipse",NULL,"Add for who this story is","well_formed","no_role","high",False
19274,"Once XD 887 is merged, gradually convert more modules. Recipe 1 Move the module .xml file to module config module .xml 2 Declare a module. type . module gradle project 3 Move dependencies from dirt project to newly created module project 4 gradle build picks it up. gradle clean build manual test Also have a look at gradle cleanEclipse eclipse",NULL,"Once XD 887 is merged, gradually convert more modules. Recipe 1 Move the module .xml file to module config module .xml 2 Declare a module. type . module gradle project 3 Move dependencies from dirt project to newly created module project 4 gradle build picks it up. gradle clean build manual test Also have a look at gradle cleanEclipse eclipse",NULL,"Once XD 887 is merged, gradually convert more modules<span class='highlight-text severity-high'>. Recipe 1 Move the module .xml file to module config module .xml 2 Declare a module. type . module gradle project 3 Move dependencies from dirt project to newly created module project 4 gradle build picks it up. gradle clean build manual test Also have a look at gradle cleanEclipse eclipse</span>","minimal","punctuation","high",False
19280,"It should be possible to supply a start or end date or none for the present , plus a count value for the number of points required i.e. after or prior to the given time .",NULL,"It should be possible to supply a start or end date or none for the present , plus a count value for the number of points required i.e. after or prior to the given time .",NULL,"Add for who this story is","well_formed","no_role","high",False
19280,"It should be possible to supply a start or end date or none for the present , plus a count value for the number of points required i.e. after or prior to the given time .",NULL,"It should be possible to supply a start or end date or none for the present , plus a count value for the number of points required i.e. after or prior to the given time .",NULL,"It should be possible to supply a start<span class='highlight-text severity-high'> or </span>end date or none for the present , plus a count value for the number of points required i.e. after or prior to the given time .","atomic","conjunctions","high",False
19280,"It should be possible to supply a start or end date or none for the present , plus a count value for the number of points required i.e. after or prior to the given time .",NULL,"It should be possible to supply a start or end date or none for the present , plus a count value for the number of points required i.e. after or prior to the given time .",NULL,"It should be possible to supply a start or end date or none for the present , plus a count value for the number of points required i<span class='highlight-text severity-high'>.e. after or prior to the given time .</span>","minimal","punctuation","high",False
19120,"If we have default values from Container Admin options, then they can not be overridden by the system properties or system environment. Currently, the only default we have for the Container Admin options is for jmxEnabled option and since it is a boolean it can never be overridden by sys env property XD JMX ENABLED. I think we need to make sure there are no default values assigned for the non boolean Container Admin options and handle the boolean type option separately.",NULL,"If we have default values from Container Admin options, then they can not be overridden by the system properties or system environment. Currently, the only default we have for the Container Admin options is for jmxEnabled option and since it is a boolean it can never be overridden by sys env property XD JMX ENABLED. I think we need to make sure there are no default values assigned for the non boolean Container Admin options and handle the boolean type option separately.",NULL,"Add for who this story is","well_formed","no_role","high",False
19120,"If we have default values from Container Admin options, then they can not be overridden by the system properties or system environment. Currently, the only default we have for the Container Admin options is for jmxEnabled option and since it is a boolean it can never be overridden by sys env property XD JMX ENABLED. I think we need to make sure there are no default values assigned for the non boolean Container Admin options and handle the boolean type option separately.",NULL,"If we have default values from Container Admin options, then they can not be overridden by the system properties or system environment. Currently, the only default we have for the Container Admin options is for jmxEnabled option and since it is a boolean it can never be overridden by sys env property XD JMX ENABLED. I think we need to make sure there are no default values assigned for the non boolean Container Admin options and handle the boolean type option separately.",NULL,"If we have default values from Container Admin options, then they can not be overridden by the system properties<span class='highlight-text severity-high'> or </span>system environment. Currently, the only default we have for the Container Admin options is for jmxEnabled option<span class='highlight-text severity-high'> and </span>since it is a boolean it can never be overridden by sys env property XD JMX ENABLED. I think we need to make sure there are no default values assigned for the non boolean Container Admin options and handle the boolean type option separately.","atomic","conjunctions","high",False
19120,"If we have default values from Container Admin options, then they can not be overridden by the system properties or system environment. Currently, the only default we have for the Container Admin options is for jmxEnabled option and since it is a boolean it can never be overridden by sys env property XD JMX ENABLED. I think we need to make sure there are no default values assigned for the non boolean Container Admin options and handle the boolean type option separately.",NULL,"If we have default values from Container Admin options, then they can not be overridden by the system properties or system environment. Currently, the only default we have for the Container Admin options is for jmxEnabled option and since it is a boolean it can never be overridden by sys env property XD JMX ENABLED. I think we need to make sure there are no default values assigned for the non boolean Container Admin options and handle the boolean type option separately.",NULL,"If we have default values from Container Admin options, then they can not be overridden by the system properties or system environment<span class='highlight-text severity-high'>. Currently, the only default we have for the Container Admin options is for jmxEnabled option and since it is a boolean it can never be overridden by sys env property XD JMX ENABLED. I think we need to make sure there are no default values assigned for the non boolean Container Admin options and handle the boolean type option separately.</span>","minimal","punctuation","high",False
19131,"XD container s id is set to use its application context id which is derived from vcap.application.name spring.application.name spring.config.name application vcap.application.instance index spring.application.index server.port PORT null With the default values the PORT is not set and embedded tomcat uses local port , the launcher id is set to application 0 When I have multiple launchers then all the launchers have the same id as application 0 which doesn t seem correct. Do we need to use the Id that is generated at the XDContainer s constructor here? public XDContainer this.id UUID.randomUUID .toString ; ",NULL,"XD container s id is set to use its application context id which is derived from vcap.application.name spring.application.name spring.config.name application vcap.application.instance index spring.application.index server.port PORT null With the default values the PORT is not set and embedded tomcat uses local port , the launcher id is set to application 0 When I have multiple launchers then all the launchers have the same id as application 0 which doesn t seem correct. Do we need to use the Id that is generated at the XDContainer s constructor here? public XDContainer this.id UUID.randomUUID .toString ; ",NULL,"Add for who this story is","well_formed","no_role","high",False
19131,"XD container s id is set to use its application context id which is derived from vcap.application.name spring.application.name spring.config.name application vcap.application.instance index spring.application.index server.port PORT null With the default values the PORT is not set and embedded tomcat uses local port , the launcher id is set to application 0 When I have multiple launchers then all the launchers have the same id as application 0 which doesn t seem correct. Do we need to use the Id that is generated at the XDContainer s constructor here? public XDContainer this.id UUID.randomUUID .toString ; ",NULL,"XD container s id is set to use its application context id which is derived from vcap.application.name spring.application.name spring.config.name application vcap.application.instance index spring.application.index server.port PORT null With the default values the PORT is not set and embedded tomcat uses local port , the launcher id is set to application 0 When I have multiple launchers then all the launchers have the same id as application 0 which doesn t seem correct. Do we need to use the Id that is generated at the XDContainer s constructor here? public XDContainer this.id UUID.randomUUID .toString ; ",NULL,"XD container s id is set to use its application context id which is derived from vcap.application.name spring.application.name spring.config.name application vcap.application.instance index spring.application.index server.port PORT null With the default values the PORT is not set<span class='highlight-text severity-high'> and </span>embedded tomcat uses local port , the launcher id is set to application 0 When I have multiple launchers then all the launchers have the same id as application 0 which doesn t seem correct. Do we need to use the Id that is generated at the XDContainer s constructor here? public XDContainer this.id UUID.randomUUID .toString ; ","atomic","conjunctions","high",False
19131,"XD container s id is set to use its application context id which is derived from vcap.application.name spring.application.name spring.config.name application vcap.application.instance index spring.application.index server.port PORT null With the default values the PORT is not set and embedded tomcat uses local port , the launcher id is set to application 0 When I have multiple launchers then all the launchers have the same id as application 0 which doesn t seem correct. Do we need to use the Id that is generated at the XDContainer s constructor here? public XDContainer this.id UUID.randomUUID .toString ; ",NULL,"XD container s id is set to use its application context id which is derived from vcap.application.name spring.application.name spring.config.name application vcap.application.instance index spring.application.index server.port PORT null With the default values the PORT is not set and embedded tomcat uses local port , the launcher id is set to application 0 When I have multiple launchers then all the launchers have the same id as application 0 which doesn t seem correct. Do we need to use the Id that is generated at the XDContainer s constructor here? public XDContainer this.id UUID.randomUUID .toString ; ",NULL,"XD container s id is set to use its application context id which is derived from vcap<span class='highlight-text severity-high'>.application.name spring.application.name spring.config.name application vcap.application.instance index spring.application.index server.port PORT null With the default values the PORT is not set and embedded tomcat uses local port , the launcher id is set to application 0 When I have multiple launchers then all the launchers have the same id as application 0 which doesn t seem correct. Do we need to use the Id that is generated at the XDContainer s constructor here? public XDContainer this.id UUID.randomUUID .toString ; </span>","minimal","punctuation","high",False
19150,"This could be inside gradlew or a .settings file.",NULL,"This could be inside gradlew or a .settings file.",NULL,"Add for who this story is","well_formed","no_role","high",False
19159,"We have observed in unit tests see AbstractSingleNodeStreamIntegrationTests that Redis SingleNode occasionally fail. The root cause must be investigated further but there is some evidence to suggest that the control messages ModuleDeploymentRequests are not always received and handled by the ModuleDeployer. This does not produce an error but results in runtime stream failures. This problem may be resolved as part of the planned Deployment SPI but is being tracked here until we are certain that it has been resolved.",NULL,"We have observed in unit tests see AbstractSingleNodeStreamIntegrationTests that Redis SingleNode occasionally fail. The root cause must be investigated further but there is some evidence to suggest that the control messages ModuleDeploymentRequests are not always received and handled by the ModuleDeployer. This does not produce an error but results in runtime stream failures. This problem may be resolved as part of the planned Deployment SPI but is being tracked here until we are certain that it has been resolved.",NULL,"Add for who this story is","well_formed","no_role","high",False
19159,"We have observed in unit tests see AbstractSingleNodeStreamIntegrationTests that Redis SingleNode occasionally fail. The root cause must be investigated further but there is some evidence to suggest that the control messages ModuleDeploymentRequests are not always received and handled by the ModuleDeployer. This does not produce an error but results in runtime stream failures. This problem may be resolved as part of the planned Deployment SPI but is being tracked here until we are certain that it has been resolved.",NULL,"We have observed in unit tests see AbstractSingleNodeStreamIntegrationTests that Redis SingleNode occasionally fail. The root cause must be investigated further but there is some evidence to suggest that the control messages ModuleDeploymentRequests are not always received and handled by the ModuleDeployer. This does not produce an error but results in runtime stream failures. This problem may be resolved as part of the planned Deployment SPI but is being tracked here until we are certain that it has been resolved.",NULL,"We have observed in unit tests see AbstractSingleNodeStreamIntegrationTests that Redis SingleNode occasionally fail. The root cause must be investigated further but there is some evidence to suggest that the control messages ModuleDeploymentRequests are not always received<span class='highlight-text severity-high'> and </span>handled by the ModuleDeployer. This does not produce an error but results in runtime stream failures. This problem may be resolved as part of the planned Deployment SPI but is being tracked here until we are certain that it has been resolved.","atomic","conjunctions","high",False
19159,"We have observed in unit tests see AbstractSingleNodeStreamIntegrationTests that Redis SingleNode occasionally fail. The root cause must be investigated further but there is some evidence to suggest that the control messages ModuleDeploymentRequests are not always received and handled by the ModuleDeployer. This does not produce an error but results in runtime stream failures. This problem may be resolved as part of the planned Deployment SPI but is being tracked here until we are certain that it has been resolved.",NULL,"We have observed in unit tests see AbstractSingleNodeStreamIntegrationTests that Redis SingleNode occasionally fail. The root cause must be investigated further but there is some evidence to suggest that the control messages ModuleDeploymentRequests are not always received and handled by the ModuleDeployer. This does not produce an error but results in runtime stream failures. This problem may be resolved as part of the planned Deployment SPI but is being tracked here until we are certain that it has been resolved.",NULL,"We have observed in unit tests see AbstractSingleNodeStreamIntegrationTests that Redis SingleNode occasionally fail<span class='highlight-text severity-high'>. The root cause must be investigated further but there is some evidence to suggest that the control messages ModuleDeploymentRequests are not always received and handled by the ModuleDeployer. This does not produce an error but results in runtime stream failures. This problem may be resolved as part of the planned Deployment SPI but is being tracked here until we are certain that it has been resolved.</span>","minimal","punctuation","high",False
19175,"This is along the lines of what is in this blog post http coreyreil.wordpress.com 2012 12 21 spring batch creating an ftp tasklet to get remote files Note that there have been some new developments in SI to get at the underlying stream for FTP. The way to test this is to create a new batch job in XD that has this as it s tasklet. Going forward the target file system will also be HDFS.",NULL,"This is along the lines of what is in this blog post http coreyreil.wordpress.com 2012 12 21 spring batch creating an ftp tasklet to get remote files Note that there have been some new developments in SI to get at the underlying stream for FTP. The way to test this is to create a new batch job in XD that has this as it s tasklet. Going forward the target file system will also be HDFS.",NULL,"Add for who this story is","well_formed","no_role","high",False
19175,"This is along the lines of what is in this blog post http coreyreil.wordpress.com 2012 12 21 spring batch creating an ftp tasklet to get remote files Note that there have been some new developments in SI to get at the underlying stream for FTP. The way to test this is to create a new batch job in XD that has this as it s tasklet. Going forward the target file system will also be HDFS.",NULL,"This is along the lines of what is in this blog post http coreyreil.wordpress.com 2012 12 21 spring batch creating an ftp tasklet to get remote files Note that there have been some new developments in SI to get at the underlying stream for FTP. The way to test this is to create a new batch job in XD that has this as it s tasklet. Going forward the target file system will also be HDFS.",NULL,"This is along the lines of what is in this blog post http coreyreil<span class='highlight-text severity-high'>.wordpress.com 2012 12 21 spring batch creating an ftp tasklet to get remote files Note that there have been some new developments in SI to get at the underlying stream for FTP. The way to test this is to create a new batch job in XD that has this as it s tasklet. Going forward the target file system will also be HDFS.</span>","minimal","punctuation","high",False
19179,"for example, the following should work code xd stream create a1 definition queue a transform expression payload a log Created new stream a1 xd stream create b1 definition queue b transform expression payload b log Created new stream b1 xd stream create s1 definition http router expression payload.contains a ? queue a queue b Created new stream s1 xd http post data ha POST text plain;Charset UTF 8 http localhost 9000 ha 200 OK log shows ha a xd http post data hi POST text plain;Charset UTF 8 http localhost 9000 hi 200 OK log shows ha b code This needs to be tested against all transports local, redis, and rabbit ",NULL,"for example, the following should work code xd stream create a1 definition queue a transform expression payload a log Created new stream a1 xd stream create b1 definition queue b transform expression payload b log Created new stream b1 xd stream create s1 definition http router expression payload.contains a ? queue a queue b Created new stream s1 xd http post data ha POST text plain;Charset UTF 8 http localhost 9000 ha 200 OK log shows ha a xd http post data hi POST text plain;Charset UTF 8 http localhost 9000 hi 200 OK log shows ha b code This needs to be tested against all transports local, redis, and rabbit ",NULL,"Add for who this story is","well_formed","no_role","high",False
19179,"for example, the following should work code xd stream create a1 definition queue a transform expression payload a log Created new stream a1 xd stream create b1 definition queue b transform expression payload b log Created new stream b1 xd stream create s1 definition http router expression payload.contains a ? queue a queue b Created new stream s1 xd http post data ha POST text plain;Charset UTF 8 http localhost 9000 ha 200 OK log shows ha a xd http post data hi POST text plain;Charset UTF 8 http localhost 9000 hi 200 OK log shows ha b code This needs to be tested against all transports local, redis, and rabbit ",NULL,"for example, the following should work code xd stream create a1 definition queue a transform expression payload a log Created new stream a1 xd stream create b1 definition queue b transform expression payload b log Created new stream b1 xd stream create s1 definition http router expression payload.contains a ? queue a queue b Created new stream s1 xd http post data ha POST text plain;Charset UTF 8 http localhost 9000 ha 200 OK log shows ha a xd http post data hi POST text plain;Charset UTF 8 http localhost 9000 hi 200 OK log shows ha b code This needs to be tested against all transports local, redis, and rabbit ",NULL,"for example, the following should work code xd stream create a1 definition queue a transform expression payload a log Created new stream a1 xd stream create b1 definition queue b transform expression payload b log Created new stream b1 xd stream create s1 definition http router expression payload.contains a <span class='highlight-text severity-high'>? queue a queue b Created new stream s1 xd http post data ha POST text plain;Charset UTF 8 http localhost 9000 ha 200 OK log shows ha a xd http post data hi POST text plain;Charset UTF 8 http localhost 9000 hi 200 OK log shows ha b code This needs to be tested against all transports local, redis, and rabbit </span>","minimal","punctuation","high",False
19182,"apologies if a ticket already exists for this, but I'didn t see one I spun up the Hortonworks Data Platform 2.0 sandbox, but see it isn t supported by Spring XD yet. How hard would it be to add these Distro s in? Is it just a matter of dropping in a lib folder for hadoop22 and or hdp20, and allowing those and options to be passed in via the hadoopDistro option? I m currently trying to work through the following tutorial, but using the HDP 2.0 sandbox instead of the 1.3 sandbox http hortonworks.com hadoop tutorial using spring xd to stream tweets to hadoop for sentiment analysis Thanks!",NULL,"apologies if a ticket already exists for this, but I'didn t see one I spun up the Hortonworks Data Platform 2.0 sandbox, but see it isn t supported by Spring XD yet. How hard would it be to add these Distro s in? Is it just a matter of dropping in a lib folder for hadoop22 and or hdp20, and allowing those and options to be passed in via the hadoopDistro option? I m currently trying to work through the following tutorial, but using the HDP 2.0 sandbox instead of the 1.3 sandbox http hortonworks.com hadoop tutorial using spring xd to stream tweets to hadoop for sentiment analysis Thanks!",NULL,"Add for who this story is","well_formed","no_role","high",False
19182,"apologies if a ticket already exists for this, but I'didn t see one I spun up the Hortonworks Data Platform 2.0 sandbox, but see it isn t supported by Spring XD yet. How hard would it be to add these Distro s in? Is it just a matter of dropping in a lib folder for hadoop22 and or hdp20, and allowing those and options to be passed in via the hadoopDistro option? I m currently trying to work through the following tutorial, but using the HDP 2.0 sandbox instead of the 1.3 sandbox http hortonworks.com hadoop tutorial using spring xd to stream tweets to hadoop for sentiment analysis Thanks!",NULL,"apologies if a ticket already exists for this, but I'didn t see one I spun up the Hortonworks Data Platform 2.0 sandbox, but see it isn t supported by Spring XD yet. How hard would it be to add these Distro s in? Is it just a matter of dropping in a lib folder for hadoop22 and or hdp20, and allowing those and options to be passed in via the hadoopDistro option? I m currently trying to work through the following tutorial, but using the HDP 2.0 sandbox instead of the 1.3 sandbox http hortonworks.com hadoop tutorial using spring xd to stream tweets to hadoop for sentiment analysis Thanks!",NULL,"apologies if a ticket already exists for this, but I'didn t see one I spun up the Hortonworks Data Platform 2.0 sandbox, but see it isn t supported by Spring XD yet. How hard would it be to add these Distro s in? Is it just a matter of dropping in a lib folder for hadoop22<span class='highlight-text severity-high'> and </span>span class='highlight-text severity-high'> or </span>hdp20, and allowing those and options to be passed in via the hadoopDistro option? I m currently trying to work through the following tutorial, but using the HDP 2.0 sandbox instead of the 1.3 sandbox http hortonworks.com hadoop tutorial using spring xd to stream tweets to hadoop for sentiment analysis Thanks!","atomic","conjunctions","high",False
19182,"apologies if a ticket already exists for this, but I'didn t see one I spun up the Hortonworks Data Platform 2.0 sandbox, but see it isn t supported by Spring XD yet. How hard would it be to add these Distro s in? Is it just a matter of dropping in a lib folder for hadoop22 and or hdp20, and allowing those and options to be passed in via the hadoopDistro option? I m currently trying to work through the following tutorial, but using the HDP 2.0 sandbox instead of the 1.3 sandbox http hortonworks.com hadoop tutorial using spring xd to stream tweets to hadoop for sentiment analysis Thanks!",NULL,"apologies if a ticket already exists for this, but I'didn t see one I spun up the Hortonworks Data Platform 2.0 sandbox, but see it isn t supported by Spring XD yet. How hard would it be to add these Distro s in? Is it just a matter of dropping in a lib folder for hadoop22 and or hdp20, and allowing those and options to be passed in via the hadoopDistro option? I m currently trying to work through the following tutorial, but using the HDP 2.0 sandbox instead of the 1.3 sandbox http hortonworks.com hadoop tutorial using spring xd to stream tweets to hadoop for sentiment analysis Thanks!",NULL,"apologies if a ticket already exists for this, but I'didn t see one I spun up the Hortonworks Data Platform 2<span class='highlight-text severity-high'>.0 sandbox, but see it isn t supported by Spring XD yet. How hard would it be to add these Distro s in? Is it just a matter of dropping in a lib folder for hadoop22 and or hdp20, and allowing those and options to be passed in via the hadoopDistro option? I m currently trying to work through the following tutorial, but using the HDP 2.0 sandbox instead of the 1.3 sandbox http hortonworks.com hadoop tutorial using spring xd to stream tweets to hadoop for sentiment analysis Thanks!</span>","minimal","punctuation","high",False
19189,"note from PR 365 quote We should probably create a new story to at least rename module display to module showconfig or something, but possibly even reconsider it altogether. In some sense, it s even violating the encapsulation of ModuleRegistry. It wont work for java config style modules or spring integration Groovy DSL modules. Personally, if anything, I'd rather see those config files themselves exposed as part of an admin UI. What you are doing with the options here fits better with the encapsulation principle and the fact that typical usage should not require detailed knowledge of the actual underlying configuration of a Module. quote ",NULL,"note from PR 365 quote We should probably create a new story to at least rename module display to module showconfig or something, but possibly even reconsider it altogether. In some sense, it s even violating the encapsulation of ModuleRegistry. It wont work for java config style modules or spring integration Groovy DSL modules. Personally, if anything, I'd rather see those config files themselves exposed as part of an admin UI. What you are doing with the options here fits better with the encapsulation principle and the fact that typical usage should not require detailed knowledge of the actual underlying configuration of a Module. quote ",NULL,"Add for who this story is","well_formed","no_role","high",False
19189,"note from PR 365 quote We should probably create a new story to at least rename module display to module showconfig or something, but possibly even reconsider it altogether. In some sense, it s even violating the encapsulation of ModuleRegistry. It wont work for java config style modules or spring integration Groovy DSL modules. Personally, if anything, I'd rather see those config files themselves exposed as part of an admin UI. What you are doing with the options here fits better with the encapsulation principle and the fact that typical usage should not require detailed knowledge of the actual underlying configuration of a Module. quote ",NULL,"note from PR 365 quote We should probably create a new story to at least rename module display to module showconfig or something, but possibly even reconsider it altogether. In some sense, it s even violating the encapsulation of ModuleRegistry. It wont work for java config style modules or spring integration Groovy DSL modules. Personally, if anything, I'd rather see those config files themselves exposed as part of an admin UI. What you are doing with the options here fits better with the encapsulation principle and the fact that typical usage should not require detailed knowledge of the actual underlying configuration of a Module. quote ",NULL,"note from PR 365 quote We should probably create a new story to at least rename module display to module showconfig<span class='highlight-text severity-high'> or </span>something, but possibly even reconsider it altogether. In some sense, it s even violating the encapsulation of ModuleRegistry. It wont work for java config style modules or spring integration Groovy DSL modules. Personally, if anything, I'd rather see those config files themselves exposed as part of an admin UI. What you are doing with the options here fits better with the encapsulation principle<span class='highlight-text severity-high'> and </span>the fact that typical usage should not require detailed knowledge of the actual underlying configuration of a Module. quote ","atomic","conjunctions","high",False
19189,"note from PR 365 quote We should probably create a new story to at least rename module display to module showconfig or something, but possibly even reconsider it altogether. In some sense, it s even violating the encapsulation of ModuleRegistry. It wont work for java config style modules or spring integration Groovy DSL modules. Personally, if anything, I'd rather see those config files themselves exposed as part of an admin UI. What you are doing with the options here fits better with the encapsulation principle and the fact that typical usage should not require detailed knowledge of the actual underlying configuration of a Module. quote ",NULL,"note from PR 365 quote We should probably create a new story to at least rename module display to module showconfig or something, but possibly even reconsider it altogether. In some sense, it s even violating the encapsulation of ModuleRegistry. It wont work for java config style modules or spring integration Groovy DSL modules. Personally, if anything, I'd rather see those config files themselves exposed as part of an admin UI. What you are doing with the options here fits better with the encapsulation principle and the fact that typical usage should not require detailed knowledge of the actual underlying configuration of a Module. quote ",NULL,"note from PR 365 quote We should probably create a new story to at least rename module display to module showconfig or something, but possibly even reconsider it altogether<span class='highlight-text severity-high'>. In some sense, it s even violating the encapsulation of ModuleRegistry. It wont work for java config style modules or spring integration Groovy DSL modules. Personally, if anything, I'd rather see those config files themselves exposed as part of an admin UI. What you are doing with the options here fits better with the encapsulation principle and the fact that typical usage should not require detailed knowledge of the actual underlying configuration of a Module. quote </span>","minimal","punctuation","high",False
19202,"Use a modal dialog to specify runtime parameters. There should be a little text are that gives hints as to the spring batch parameter key value conventions, e.g. for type. Might be a good idea to have a checkbox that lets you select to auto increment job instance number. 4 columns key, value, type, identifying and an add parameter button that adds a new row. This would appear as a modal dialog box, polling of the state of the deployments would be suspended while the job parameter modal dialog box is shown. ",NULL,"Use a modal dialog to specify runtime parameters. There should be a little text are that gives hints as to the spring batch parameter key value conventions, e.g. for type. Might be a good idea to have a checkbox that lets you select to auto increment job instance number. 4 columns key, value, type, identifying and an add parameter button that adds a new row. This would appear as a modal dialog box, polling of the state of the deployments would be suspended while the job parameter modal dialog box is shown. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19202,"Use a modal dialog to specify runtime parameters. There should be a little text are that gives hints as to the spring batch parameter key value conventions, e.g. for type. Might be a good idea to have a checkbox that lets you select to auto increment job instance number. 4 columns key, value, type, identifying and an add parameter button that adds a new row. This would appear as a modal dialog box, polling of the state of the deployments would be suspended while the job parameter modal dialog box is shown. ",NULL,"Use a modal dialog to specify runtime parameters. There should be a little text are that gives hints as to the spring batch parameter key value conventions, e.g. for type. Might be a good idea to have a checkbox that lets you select to auto increment job instance number. 4 columns key, value, type, identifying and an add parameter button that adds a new row. This would appear as a modal dialog box, polling of the state of the deployments would be suspended while the job parameter modal dialog box is shown. ",NULL,"Use a modal dialog to specify runtime parameters. There should be a little text are that gives hints as to the spring batch parameter key value conventions, e.g. for type. Might be a good idea to have a checkbox that lets you select to auto increment job instance number. 4 columns key, value, type, identifying<span class='highlight-text severity-high'> and </span>an add parameter button that adds a new row. This would appear as a modal dialog box, polling of the state of the deployments would be suspended while the job parameter modal dialog box is shown. ","atomic","conjunctions","high",False
19202,"Use a modal dialog to specify runtime parameters. There should be a little text are that gives hints as to the spring batch parameter key value conventions, e.g. for type. Might be a good idea to have a checkbox that lets you select to auto increment job instance number. 4 columns key, value, type, identifying and an add parameter button that adds a new row. This would appear as a modal dialog box, polling of the state of the deployments would be suspended while the job parameter modal dialog box is shown. ",NULL,"Use a modal dialog to specify runtime parameters. There should be a little text are that gives hints as to the spring batch parameter key value conventions, e.g. for type. Might be a good idea to have a checkbox that lets you select to auto increment job instance number. 4 columns key, value, type, identifying and an add parameter button that adds a new row. This would appear as a modal dialog box, polling of the state of the deployments would be suspended while the job parameter modal dialog box is shown. ",NULL,"Use a modal dialog to specify runtime parameters<span class='highlight-text severity-high'>. There should be a little text are that gives hints as to the spring batch parameter key value conventions, e.g. for type. Might be a good idea to have a checkbox that lets you select to auto increment job instance number. 4 columns key, value, type, identifying and an add parameter button that adds a new row. This would appear as a modal dialog box, polling of the state of the deployments would be suspended while the job parameter modal dialog box is shown. </span>","minimal","punctuation","high",False
19213,"Currently HSQLDB is the only option for batch jobs. This should be configurable so that a user may select another JDBC data store option. Steps hsqldb.properties needs to be changed to batch jdbc.properties hsql prefixes should be changed to batch jdbc JDBC Connection String needs to be configurable JDBC Driver needs to be configurable The Setup Scripts to be used for spring batch need to be configurable. HSQLServerBean should be renamed to something batch jdbc.properties Tests Should be able write tests that support HSQLDB",NULL,"Currently HSQLDB is the only option for batch jobs. This should be configurable","so that a user may select another JDBC data store option. Steps hsqldb.properties needs to be changed to batch jdbc.properties hsql prefixes should be changed to batch jdbc JDBC Connection String needs to be configurable JDBC Driver needs to be configurable The Setup Scripts to be used for spring batch need to be configurable. HSQLServerBean should be renamed to something batch jdbc.properties Tests Should be able write tests that support HSQLDB","Add for who this story is","well_formed","no_role","high",False
19213,"Currently HSQLDB is the only option for batch jobs. This should be configurable so that a user may select another JDBC data store option. Steps hsqldb.properties needs to be changed to batch jdbc.properties hsql prefixes should be changed to batch jdbc JDBC Connection String needs to be configurable JDBC Driver needs to be configurable The Setup Scripts to be used for spring batch need to be configurable. HSQLServerBean should be renamed to something batch jdbc.properties Tests Should be able write tests that support HSQLDB",NULL,"Currently HSQLDB is the only option for batch jobs. This should be configurable","so that a user may select another JDBC data store option. Steps hsqldb.properties needs to be changed to batch jdbc.properties hsql prefixes should be changed to batch jdbc JDBC Connection String needs to be configurable JDBC Driver needs to be configurable The Setup Scripts to be used for spring batch need to be configurable. HSQLServerBean should be renamed to something batch jdbc.properties Tests Should be able write tests that support HSQLDB","Currently HSQLDB is the only option for batch jobs<span class='highlight-text severity-high'>. This should be configurable so that a user may select another JDBC data store option. Steps hsqldb.properties needs to be changed to batch jdbc.properties hsql prefixes should be changed to batch jdbc JDBC Connection String needs to be configurable JDBC Driver needs to be configurable The Setup Scripts to be used for spring batch need to be configurable. HSQLServerBean should be renamed to something batch jdbc.properties Tests Should be able write tests that support HSQLDB</span>","minimal","punctuation","high",False
19213,"Currently HSQLDB is the only option for batch jobs. This should be configurable so that a user may select another JDBC data store option. Steps hsqldb.properties needs to be changed to batch jdbc.properties hsql prefixes should be changed to batch jdbc JDBC Connection String needs to be configurable JDBC Driver needs to be configurable The Setup Scripts to be used for spring batch need to be configurable. HSQLServerBean should be renamed to something batch jdbc.properties Tests Should be able write tests that support HSQLDB",NULL,"Currently HSQLDB is the only option for batch jobs. This should be configurable","so that a user may select another JDBC data store option. Steps hsqldb.properties needs to be changed to batch jdbc.properties hsql prefixes should be changed to batch jdbc JDBC Connection String needs to be configurable JDBC Driver needs to be configurable The Setup Scripts to be used for spring batch need to be configurable. HSQLServerBean should be renamed to something batch jdbc.properties Tests Should be able write tests that support HSQLDB","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19232,"Same processing as XD 984, but the job instacne is launched via an event from the input file source. Supporting a single file per job launch is OK. job create blah definition filehdfs stream create csvStream definition file ref true dir Users luke pattern .csv job blah the job should be documented ",NULL,"Same processing as XD 984, but the job instacne is launched via an event from the input file source. Supporting a single file per job launch is OK. job create blah definition filehdfs stream create csvStream definition file ref true dir Users luke pattern .csv job blah the job should be documented ",NULL,"Add for who this story is","well_formed","no_role","high",False
19232,"Same processing as XD 984, but the job instacne is launched via an event from the input file source. Supporting a single file per job launch is OK. job create blah definition filehdfs stream create csvStream definition file ref true dir Users luke pattern .csv job blah the job should be documented ",NULL,"Same processing as XD 984, but the job instacne is launched via an event from the input file source. Supporting a single file per job launch is OK. job create blah definition filehdfs stream create csvStream definition file ref true dir Users luke pattern .csv job blah the job should be documented ",NULL,"Same processing as XD 984, but the job instacne is launched via an event from the input file source<span class='highlight-text severity-high'>. Supporting a single file per job launch is OK. job create blah definition filehdfs stream create csvStream definition file ref true dir Users luke pattern .csv job blah the job should be documented </span>","minimal","punctuation","high",False
19320,"The syntax for both taps and job channels will be prefixed with the word tap. The parser will search both the all stream and job for now registries to find the name . If the name is present in only in one registry then that definition will be used. If the name is present both registries a syntax exception will be issued. The user then can optionally specify which registry to use by adding a second token to the definition that specifies the type of channel the user wants. i.e. tap stream streamName mySink If the user attempts to place a job on the greater left hand side without specifying a notification a syntax error will be thrown. Format tap type name tap type name Example tap streamName mySink tap stream streamName mySink mySource myProcessor tap stream mySink myEmailSource tap job jobName.step1 myEmailSource tap jobName.step1 tap job jobName notifications myEmailSink",NULL,"The syntax for both taps and job channels will be prefixed with the word tap. The parser will search both the all stream and job for now registries to find the name . If the name is present in only in one registry then that definition will be used. If the name is present both registries a syntax exception will be issued. The user then can optionally specify which registry to use by adding a second token to the definition that specifies the type of channel the user wants. i.e. tap stream streamName mySink If the user attempts to place a job on the greater left hand side without specifying a notification a syntax error will be thrown. Format tap type name tap type name Example tap streamName mySink tap stream streamName mySink mySource myProcessor tap stream mySink myEmailSource tap job jobName.step1 myEmailSource tap jobName.step1 tap job jobName notifications myEmailSink",NULL,"Add for who this story is","well_formed","no_role","high",False
19320,"The syntax for both taps and job channels will be prefixed with the word tap. The parser will search both the all stream and job for now registries to find the name . If the name is present in only in one registry then that definition will be used. If the name is present both registries a syntax exception will be issued. The user then can optionally specify which registry to use by adding a second token to the definition that specifies the type of channel the user wants. i.e. tap stream streamName mySink If the user attempts to place a job on the greater left hand side without specifying a notification a syntax error will be thrown. Format tap type name tap type name Example tap streamName mySink tap stream streamName mySink mySource myProcessor tap stream mySink myEmailSource tap job jobName.step1 myEmailSource tap jobName.step1 tap job jobName notifications myEmailSink",NULL,"The syntax for both taps and job channels will be prefixed with the word tap. The parser will search both the all stream and job for now registries to find the name . If the name is present in only in one registry then that definition will be used. If the name is present both registries a syntax exception will be issued. The user then can optionally specify which registry to use by adding a second token to the definition that specifies the type of channel the user wants. i.e. tap stream streamName mySink If the user attempts to place a job on the greater left hand side without specifying a notification a syntax error will be thrown. Format tap type name tap type name Example tap streamName mySink tap stream streamName mySink mySource myProcessor tap stream mySink myEmailSource tap job jobName.step1 myEmailSource tap jobName.step1 tap job jobName notifications myEmailSink",NULL,"The syntax for both taps<span class='highlight-text severity-high'> and </span>job channels will be prefixed with the word tap. The parser will search both the all stream and job for now registries to find the name . If the name is present in only in one registry then that definition will be used. If the name is present both registries a syntax exception will be issued. The user then can optionally specify which registry to use by adding a second token to the definition that specifies the type of channel the user wants. i.e. tap stream streamName mySink If the user attempts to place a job on the greater left hand side without specifying a notification a syntax error will be thrown. Format tap type name tap type name Example tap streamName mySink tap stream streamName mySink mySource myProcessor tap stream mySink myEmailSource tap job jobName.step1 myEmailSource tap jobName.step1 tap job jobName notifications myEmailSink","atomic","conjunctions","high",False
19320,"The syntax for both taps and job channels will be prefixed with the word tap. The parser will search both the all stream and job for now registries to find the name . If the name is present in only in one registry then that definition will be used. If the name is present both registries a syntax exception will be issued. The user then can optionally specify which registry to use by adding a second token to the definition that specifies the type of channel the user wants. i.e. tap stream streamName mySink If the user attempts to place a job on the greater left hand side without specifying a notification a syntax error will be thrown. Format tap type name tap type name Example tap streamName mySink tap stream streamName mySink mySource myProcessor tap stream mySink myEmailSource tap job jobName.step1 myEmailSource tap jobName.step1 tap job jobName notifications myEmailSink",NULL,"The syntax for both taps and job channels will be prefixed with the word tap. The parser will search both the all stream and job for now registries to find the name . If the name is present in only in one registry then that definition will be used. If the name is present both registries a syntax exception will be issued. The user then can optionally specify which registry to use by adding a second token to the definition that specifies the type of channel the user wants. i.e. tap stream streamName mySink If the user attempts to place a job on the greater left hand side without specifying a notification a syntax error will be thrown. Format tap type name tap type name Example tap streamName mySink tap stream streamName mySink mySource myProcessor tap stream mySink myEmailSource tap job jobName.step1 myEmailSource tap jobName.step1 tap job jobName notifications myEmailSink",NULL,"The syntax for both taps and job channels will be prefixed with the word tap<span class='highlight-text severity-high'>. The parser will search both the all stream and job for now registries to find the name . If the name is present in only in one registry then that definition will be used. If the name is present both registries a syntax exception will be issued. The user then can optionally specify which registry to use by adding a second token to the definition that specifies the type of channel the user wants. i.e. tap stream streamName mySink If the user attempts to place a job on the greater left hand side without specifying a notification a syntax error will be thrown. Format tap type name tap type name Example tap streamName mySink tap stream streamName mySink mySource myProcessor tap stream mySink myEmailSource tap job jobName.step1 myEmailSource tap jobName.step1 tap job jobName notifications myEmailSink</span>","minimal","punctuation","high",False
19332,"Currently, the way that we are accessing JobExecutions from batch admin, the JobParameters are not being filled in. We are using a org.springframework.batch.admin.service.JdbcSearchableJobExecutionDao to grab the job executions from the database. The database queries that it uses does not include the JobParameters which is stored in a different table . I think that this will require a change to batch admin in order to properly expose the parameters.",NULL,"Currently, the way that we are accessing JobExecutions from batch admin, the JobParameters are not being filled in. We are using a org.springframework.batch.admin.service.JdbcSearchableJobExecutionDao to grab the job executions from the database. The database queries that it uses does not include the JobParameters which is stored in a different table . I think that this will require a change to batch admin","in order to properly expose the parameters.","Add for who this story is","well_formed","no_role","high",False
19332,"Currently, the way that we are accessing JobExecutions from batch admin, the JobParameters are not being filled in. We are using a org.springframework.batch.admin.service.JdbcSearchableJobExecutionDao to grab the job executions from the database. The database queries that it uses does not include the JobParameters which is stored in a different table . I think that this will require a change to batch admin in order to properly expose the parameters.",NULL,"Currently, the way that we are accessing JobExecutions from batch admin, the JobParameters are not being filled in. We are using a org.springframework.batch.admin.service.JdbcSearchableJobExecutionDao to grab the job executions from the database. The database queries that it uses does not include the JobParameters which is stored in a different table . I think that this will require a change to batch admin","in order to properly expose the parameters.","Currently, the way that we are accessing JobExecutions from batch admin, the JobParameters are not being filled in<span class='highlight-text severity-high'>. We are using a org.springframework.batch.admin.service.JdbcSearchableJobExecutionDao to grab the job executions from the database. The database queries that it uses does not include the JobParameters which is stored in a different table . I think that this will require a change to batch admin in order to properly expose the parameters.</span>","minimal","punctuation","high",False
19332,"Currently, the way that we are accessing JobExecutions from batch admin, the JobParameters are not being filled in. We are using a org.springframework.batch.admin.service.JdbcSearchableJobExecutionDao to grab the job executions from the database. The database queries that it uses does not include the JobParameters which is stored in a different table . I think that this will require a change to batch admin in order to properly expose the parameters.",NULL,"Currently, the way that we are accessing JobExecutions from batch admin, the JobParameters are not being filled in. We are using a org.springframework.batch.admin.service.JdbcSearchableJobExecutionDao to grab the job executions from the database. The database queries that it uses does not include the JobParameters which is stored in a different table . I think that this will require a change to batch admin","in order to properly expose the parameters.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19236,"Create a Deployer class has methods RunningInstance deploySingleNode takes into account machine size as specified in properties file void destroyAllInstances or whatever JClouds returns from the destroy call ctor gets passed in the root boostrapping credentials. Install Script Steps Setup XD HOME variable Make sure privileges are set to ubuntu not root. start up redis and rabbit using ports as specified in xd ec2.properties Use port watch to make sure they started Start singlenode after configuration. Display hostname of singlenode server Report successful and failed startup Hit root of xd admin to see if there is a response on 9393 Integration Testing Verify that config files have been setup Verify XD has been started Verify XD can process a basic http post ",NULL,"Create a Deployer class has methods RunningInstance deploySingleNode takes into account machine size as specified in properties file void destroyAllInstances or whatever JClouds returns from the destroy call ctor gets passed in the root boostrapping credentials. Install Script Steps Setup XD HOME variable Make sure privileges are set to ubuntu not root. start up redis and rabbit using ports as specified in xd ec2.properties Use port watch to make sure they started Start singlenode after configuration. Display hostname of singlenode server Report successful and failed startup Hit root of xd admin to see if there is a response on 9393 Integration Testing Verify that config files have been setup Verify XD has been started Verify XD can process a basic http post ",NULL,"Add for who this story is","well_formed","no_role","high",False
19236,"Create a Deployer class has methods RunningInstance deploySingleNode takes into account machine size as specified in properties file void destroyAllInstances or whatever JClouds returns from the destroy call ctor gets passed in the root boostrapping credentials. Install Script Steps Setup XD HOME variable Make sure privileges are set to ubuntu not root. start up redis and rabbit using ports as specified in xd ec2.properties Use port watch to make sure they started Start singlenode after configuration. Display hostname of singlenode server Report successful and failed startup Hit root of xd admin to see if there is a response on 9393 Integration Testing Verify that config files have been setup Verify XD has been started Verify XD can process a basic http post ",NULL,"Create a Deployer class has methods RunningInstance deploySingleNode takes into account machine size as specified in properties file void destroyAllInstances or whatever JClouds returns from the destroy call ctor gets passed in the root boostrapping credentials. Install Script Steps Setup XD HOME variable Make sure privileges are set to ubuntu not root. start up redis and rabbit using ports as specified in xd ec2.properties Use port watch to make sure they started Start singlenode after configuration. Display hostname of singlenode server Report successful and failed startup Hit root of xd admin to see if there is a response on 9393 Integration Testing Verify that config files have been setup Verify XD has been started Verify XD can process a basic http post ",NULL,"Create a Deployer class has methods RunningInstance deploySingleNode takes into account machine size as specified in properties file void destroyAllInstances<span class='highlight-text severity-high'> or </span>whatever JClouds returns from the destroy call ctor gets passed in the root boostrapping credentials. Install Script Steps Setup XD HOME variable Make sure privileges are set to ubuntu not root. start up redis<span class='highlight-text severity-high'> and </span>rabbit using ports as specified in xd ec2.properties Use port watch to make sure they started Start singlenode after configuration. Display hostname of singlenode server Report successful and failed startup Hit root of xd admin to see if there is a response on 9393 Integration Testing Verify that config files have been setup Verify XD has been started Verify XD can process a basic http post ","atomic","conjunctions","high",False
19236,"Create a Deployer class has methods RunningInstance deploySingleNode takes into account machine size as specified in properties file void destroyAllInstances or whatever JClouds returns from the destroy call ctor gets passed in the root boostrapping credentials. Install Script Steps Setup XD HOME variable Make sure privileges are set to ubuntu not root. start up redis and rabbit using ports as specified in xd ec2.properties Use port watch to make sure they started Start singlenode after configuration. Display hostname of singlenode server Report successful and failed startup Hit root of xd admin to see if there is a response on 9393 Integration Testing Verify that config files have been setup Verify XD has been started Verify XD can process a basic http post ",NULL,"Create a Deployer class has methods RunningInstance deploySingleNode takes into account machine size as specified in properties file void destroyAllInstances or whatever JClouds returns from the destroy call ctor gets passed in the root boostrapping credentials. Install Script Steps Setup XD HOME variable Make sure privileges are set to ubuntu not root. start up redis and rabbit using ports as specified in xd ec2.properties Use port watch to make sure they started Start singlenode after configuration. Display hostname of singlenode server Report successful and failed startup Hit root of xd admin to see if there is a response on 9393 Integration Testing Verify that config files have been setup Verify XD has been started Verify XD can process a basic http post ",NULL,"Create a Deployer class has methods RunningInstance deploySingleNode takes into account machine size as specified in properties file void destroyAllInstances or whatever JClouds returns from the destroy call ctor gets passed in the root boostrapping credentials<span class='highlight-text severity-high'>. Install Script Steps Setup XD HOME variable Make sure privileges are set to ubuntu not root. start up redis and rabbit using ports as specified in xd ec2.properties Use port watch to make sure they started Start singlenode after configuration. Display hostname of singlenode server Report successful and failed startup Hit root of xd admin to see if there is a response on 9393 Integration Testing Verify that config files have been setup Verify XD has been started Verify XD can process a basic http post </span>","minimal","punctuation","high",False
19235,"Update the Deployer class to add the following methods RunningInstance deployContainer For each of the containers Using XD 977 install distribution Setup XD HOME variable Create configurator directory Copy the configurator to containers Run Configurators Verify that configuration files are setup correctly start container server Report if container started. using jmx Jolokia, if available If it didn t start report failure but continue. Report public DNS name of container Integration Testing Verify that config files have been setup properly For each container Verify container has been started Verify that container is working by creating a stream in admin trigger log . Verify that the log on the container is being updated. ",NULL,"Update the Deployer class to add the following methods RunningInstance deployContainer For each of the containers Using XD 977 install distribution Setup XD HOME variable Create configurator directory Copy the configurator to containers Run Configurators Verify that configuration files are setup correctly start container server Report if container started. using jmx Jolokia, if available If it didn t start report failure but continue. Report public DNS name of container Integration Testing Verify that config files have been setup properly For each container Verify container has been started Verify that container is working by creating a stream in admin trigger log . Verify that the log on the container is being updated. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19235,"Update the Deployer class to add the following methods RunningInstance deployContainer For each of the containers Using XD 977 install distribution Setup XD HOME variable Create configurator directory Copy the configurator to containers Run Configurators Verify that configuration files are setup correctly start container server Report if container started. using jmx Jolokia, if available If it didn t start report failure but continue. Report public DNS name of container Integration Testing Verify that config files have been setup properly For each container Verify container has been started Verify that container is working by creating a stream in admin trigger log . Verify that the log on the container is being updated. ",NULL,"Update the Deployer class to add the following methods RunningInstance deployContainer For each of the containers Using XD 977 install distribution Setup XD HOME variable Create configurator directory Copy the configurator to containers Run Configurators Verify that configuration files are setup correctly start container server Report if container started. using jmx Jolokia, if available If it didn t start report failure but continue. Report public DNS name of container Integration Testing Verify that config files have been setup properly For each container Verify container has been started Verify that container is working by creating a stream in admin trigger log . Verify that the log on the container is being updated. ",NULL,"Update the Deployer class to add the following methods RunningInstance deployContainer For each of the containers Using XD 977 install distribution Setup XD HOME variable Create configurator directory Copy the configurator to containers Run Configurators Verify that configuration files are setup correctly start container server Report if container started<span class='highlight-text severity-high'>. using jmx Jolokia, if available If it didn t start report failure but continue. Report public DNS name of container Integration Testing Verify that config files have been setup properly For each container Verify container has been started Verify that container is working by creating a stream in admin trigger log . Verify that the log on the container is being updated. </span>","minimal","punctuation","high",False
19237,"User specifies download distribution zip file from properties file. See XD 969 for key value pairs Copy the distribution from the shared EBS S3 the ebs volume assigned to the each node admin instance . If not on shared ebs s3 pull from http site specified by user and put on the shared EBS volume. Unzip the distribution from on the ebs volume for the instance to the home ubuntu directory make sure privileges are set to ubuntu not root. How to verify it works Create a JUNit style integration test that Deletes a known .zip distribution from EBS S3. Invoke the application functionality should be a 1 liner that will start up the instance and download the .zip distribution from the URI provided in xd ec2.properties. Verify the file is now in EBS S3 and also on the instance Tear down created instance Create new instance passing in the same URI of the .zip distribution. Verify as before ",NULL,"User specifies download distribution zip file from properties file. See XD 969 for key value pairs Copy the distribution from the shared EBS S3 the ebs volume assigned to the each node admin instance . If not on shared ebs s3 pull from http site specified by user and put on the shared EBS volume. Unzip the distribution from on the ebs volume for the instance to the home ubuntu directory make sure privileges are set to ubuntu not root. How to verify it works Create a JUNit style integration test that Deletes a known .zip distribution from EBS S3. Invoke the application functionality should be a 1 liner that will start up the instance and download the .zip distribution from the URI provided in xd ec2.properties. Verify the file is now in EBS S3 and also on the instance Tear down created instance Create new instance passing in the same URI of the .zip distribution. Verify as before ",NULL,"Add for who this story is","well_formed","no_role","high",False
19237,"User specifies download distribution zip file from properties file. See XD 969 for key value pairs Copy the distribution from the shared EBS S3 the ebs volume assigned to the each node admin instance . If not on shared ebs s3 pull from http site specified by user and put on the shared EBS volume. Unzip the distribution from on the ebs volume for the instance to the home ubuntu directory make sure privileges are set to ubuntu not root. How to verify it works Create a JUNit style integration test that Deletes a known .zip distribution from EBS S3. Invoke the application functionality should be a 1 liner that will start up the instance and download the .zip distribution from the URI provided in xd ec2.properties. Verify the file is now in EBS S3 and also on the instance Tear down created instance Create new instance passing in the same URI of the .zip distribution. Verify as before ",NULL,"User specifies download distribution zip file from properties file. See XD 969 for key value pairs Copy the distribution from the shared EBS S3 the ebs volume assigned to the each node admin instance . If not on shared ebs s3 pull from http site specified by user and put on the shared EBS volume. Unzip the distribution from on the ebs volume for the instance to the home ubuntu directory make sure privileges are set to ubuntu not root. How to verify it works Create a JUNit style integration test that Deletes a known .zip distribution from EBS S3. Invoke the application functionality should be a 1 liner that will start up the instance and download the .zip distribution from the URI provided in xd ec2.properties. Verify the file is now in EBS S3 and also on the instance Tear down created instance Create new instance passing in the same URI of the .zip distribution. Verify as before ",NULL,"User specifies download distribution zip file from properties file. See XD 969 for key value pairs Copy the distribution from the shared EBS S3 the ebs volume assigned to the each node admin instance . If not on shared ebs s3 pull from http site specified by user<span class='highlight-text severity-high'> and </span>put on the shared EBS volume. Unzip the distribution from on the ebs volume for the instance to the home ubuntu directory make sure privileges are set to ubuntu not root. How to verify it works Create a JUNit style integration test that Deletes a known .zip distribution from EBS S3. Invoke the application functionality should be a 1 liner that will start up the instance and download the .zip distribution from the URI provided in xd ec2.properties. Verify the file is now in EBS S3 and also on the instance Tear down created instance Create new instance passing in the same URI of the .zip distribution. Verify as before ","atomic","conjunctions","high",False
19237,"User specifies download distribution zip file from properties file. See XD 969 for key value pairs Copy the distribution from the shared EBS S3 the ebs volume assigned to the each node admin instance . If not on shared ebs s3 pull from http site specified by user and put on the shared EBS volume. Unzip the distribution from on the ebs volume for the instance to the home ubuntu directory make sure privileges are set to ubuntu not root. How to verify it works Create a JUNit style integration test that Deletes a known .zip distribution from EBS S3. Invoke the application functionality should be a 1 liner that will start up the instance and download the .zip distribution from the URI provided in xd ec2.properties. Verify the file is now in EBS S3 and also on the instance Tear down created instance Create new instance passing in the same URI of the .zip distribution. Verify as before ",NULL,"User specifies download distribution zip file from properties file. See XD 969 for key value pairs Copy the distribution from the shared EBS S3 the ebs volume assigned to the each node admin instance . If not on shared ebs s3 pull from http site specified by user and put on the shared EBS volume. Unzip the distribution from on the ebs volume for the instance to the home ubuntu directory make sure privileges are set to ubuntu not root. How to verify it works Create a JUNit style integration test that Deletes a known .zip distribution from EBS S3. Invoke the application functionality should be a 1 liner that will start up the instance and download the .zip distribution from the URI provided in xd ec2.properties. Verify the file is now in EBS S3 and also on the instance Tear down created instance Create new instance passing in the same URI of the .zip distribution. Verify as before ",NULL,"User specifies download distribution zip file from properties file<span class='highlight-text severity-high'>. See XD 969 for key value pairs Copy the distribution from the shared EBS S3 the ebs volume assigned to the each node admin instance . If not on shared ebs s3 pull from http site specified by user and put on the shared EBS volume. Unzip the distribution from on the ebs volume for the instance to the home ubuntu directory make sure privileges are set to ubuntu not root. How to verify it works Create a JUNit style integration test that Deletes a known .zip distribution from EBS S3. Invoke the application functionality should be a 1 liner that will start up the instance and download the .zip distribution from the URI provided in xd ec2.properties. Verify the file is now in EBS S3 and also on the instance Tear down created instance Create new instance passing in the same URI of the .zip distribution. Verify as before </span>","minimal","punctuation","high",False
19241,"Update the Deployer class to add the following methods RunningInstance deployAdminServer The install script steps Using XD 977 install distribution Setup XD HOME variable start up redis and rabbit using ports as specified in xd ec2.properties on admin server Create configurator directory Copy the configurator to containers Use port watch to make sure they started start admin server use port watch to make sure the admin started on 9393 Report if admin server started. If it didn t start abort install. Report public DNS name of admin server Integration Testing Verify XD admin has been started Create a basic stream trigger log and make sure we get a success code from xd admin was received. Query the redis to see if the stream was created. ",NULL,"Update the Deployer class to add the following methods RunningInstance deployAdminServer The install script steps Using XD 977 install distribution Setup XD HOME variable start up redis and rabbit using ports as specified in xd ec2.properties on admin server Create configurator directory Copy the configurator to containers Use port watch to make sure they started start admin server use port watch to make sure the admin started on 9393 Report if admin server started. If it didn t start abort install. Report public DNS name of admin server Integration Testing Verify XD admin has been started Create a basic stream trigger log and make sure we get a success code from xd admin was received. Query the redis to see if the stream was created. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19241,"Update the Deployer class to add the following methods RunningInstance deployAdminServer The install script steps Using XD 977 install distribution Setup XD HOME variable start up redis and rabbit using ports as specified in xd ec2.properties on admin server Create configurator directory Copy the configurator to containers Use port watch to make sure they started start admin server use port watch to make sure the admin started on 9393 Report if admin server started. If it didn t start abort install. Report public DNS name of admin server Integration Testing Verify XD admin has been started Create a basic stream trigger log and make sure we get a success code from xd admin was received. Query the redis to see if the stream was created. ",NULL,"Update the Deployer class to add the following methods RunningInstance deployAdminServer The install script steps Using XD 977 install distribution Setup XD HOME variable start up redis and rabbit using ports as specified in xd ec2.properties on admin server Create configurator directory Copy the configurator to containers Use port watch to make sure they started start admin server use port watch to make sure the admin started on 9393 Report if admin server started. If it didn t start abort install. Report public DNS name of admin server Integration Testing Verify XD admin has been started Create a basic stream trigger log and make sure we get a success code from xd admin was received. Query the redis to see if the stream was created. ",NULL,"Update the Deployer class to add the following methods RunningInstance deployAdminServer The install script steps Using XD 977 install distribution Setup XD HOME variable start up redis<span class='highlight-text severity-high'> and </span>rabbit using ports as specified in xd ec2.properties on admin server Create configurator directory Copy the configurator to containers Use port watch to make sure they started start admin server use port watch to make sure the admin started on 9393 Report if admin server started. If it didn t start abort install. Report public DNS name of admin server Integration Testing Verify XD admin has been started Create a basic stream trigger log and make sure we get a success code from xd admin was received. Query the redis to see if the stream was created. ","atomic","conjunctions","high",False
19241,"Update the Deployer class to add the following methods RunningInstance deployAdminServer The install script steps Using XD 977 install distribution Setup XD HOME variable start up redis and rabbit using ports as specified in xd ec2.properties on admin server Create configurator directory Copy the configurator to containers Use port watch to make sure they started start admin server use port watch to make sure the admin started on 9393 Report if admin server started. If it didn t start abort install. Report public DNS name of admin server Integration Testing Verify XD admin has been started Create a basic stream trigger log and make sure we get a success code from xd admin was received. Query the redis to see if the stream was created. ",NULL,"Update the Deployer class to add the following methods RunningInstance deployAdminServer The install script steps Using XD 977 install distribution Setup XD HOME variable start up redis and rabbit using ports as specified in xd ec2.properties on admin server Create configurator directory Copy the configurator to containers Use port watch to make sure they started start admin server use port watch to make sure the admin started on 9393 Report if admin server started. If it didn t start abort install. Report public DNS name of admin server Integration Testing Verify XD admin has been started Create a basic stream trigger log and make sure we get a success code from xd admin was received. Query the redis to see if the stream was created. ",NULL,"Update the Deployer class to add the following methods RunningInstance deployAdminServer The install script steps Using XD 977 install distribution Setup XD HOME variable start up redis and rabbit using ports as specified in xd ec2<span class='highlight-text severity-high'>.properties on admin server Create configurator directory Copy the configurator to containers Use port watch to make sure they started start admin server use port watch to make sure the admin started on 9393 Report if admin server started. If it didn t start abort install. Report public DNS name of admin server Integration Testing Verify XD admin has been started Create a basic stream trigger log and make sure we get a success code from xd admin was received. Query the redis to see if the stream was created. </span>","minimal","punctuation","high",False
19287,"We should support java.io.file payloads in order to support non textual file and large text file payloads being uploaded to HDFS. Currently text file payloads are converted to a text stream in memory and, non String payloads are converted to JSON first, using an object to json transformer . Ultimately we need to support streams such as file hdfs where the actually payload being copied to HDFS is not necessarily JSON or textual. Need to be able to support headers in the message that will indicate which HDFS file the data should be stored in. ",NULL,"We should support java.io.file payloads","in order to support non textual file and large text file payloads being uploaded to HDFS. Currently text file payloads are converted to a text stream in memory and, non String payloads are converted to JSON first, using an object to json transformer . Ultimately we need to support streams such as file hdfs where the actually payload being copied to HDFS is not necessarily JSON or textual. Need to be able to support headers in the message that will indicate which HDFS file the data should be stored in.","Add for who this story is","well_formed","no_role","high",False
19287,"We should support java.io.file payloads in order to support non textual file and large text file payloads being uploaded to HDFS. Currently text file payloads are converted to a text stream in memory and, non String payloads are converted to JSON first, using an object to json transformer . Ultimately we need to support streams such as file hdfs where the actually payload being copied to HDFS is not necessarily JSON or textual. Need to be able to support headers in the message that will indicate which HDFS file the data should be stored in. ",NULL,"We should support java.io.file payloads","in order to support non textual file and large text file payloads being uploaded to HDFS. Currently text file payloads are converted to a text stream in memory and, non String payloads are converted to JSON first, using an object to json transformer . Ultimately we need to support streams such as file hdfs where the actually payload being copied to HDFS is not necessarily JSON or textual. Need to be able to support headers in the message that will indicate which HDFS file the data should be stored in.","We should support java<span class='highlight-text severity-high'>.io.file payloads in order to support non textual file and large text file payloads being uploaded to HDFS. Currently text file payloads are converted to a text stream in memory and, non String payloads are converted to JSON first, using an object to json transformer . Ultimately we need to support streams such as file hdfs where the actually payload being copied to HDFS is not necessarily JSON or textual. Need to be able to support headers in the message that will indicate which HDFS file the data should be stored in. </span>","minimal","punctuation","high",False
19287,"We should support java.io.file payloads in order to support non textual file and large text file payloads being uploaded to HDFS. Currently text file payloads are converted to a text stream in memory and, non String payloads are converted to JSON first, using an object to json transformer . Ultimately we need to support streams such as file hdfs where the actually payload being copied to HDFS is not necessarily JSON or textual. Need to be able to support headers in the message that will indicate which HDFS file the data should be stored in. ",NULL,"We should support java.io.file payloads","in order to support non textual file and large text file payloads being uploaded to HDFS. Currently text file payloads are converted to a text stream in memory and, non String payloads are converted to JSON first, using an object to json transformer . Ultimately we need to support streams such as file hdfs where the actually payload being copied to HDFS is not necessarily JSON or textual. Need to be able to support headers in the message that will indicate which HDFS file the data should be stored in.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19293,"Currently, a lot of jars that are on the classpath of xd dirt are there to support modules. We should move those to the lib construct of a module and remove them from the CP of dirt. But this should not be done by simple mv , as we d lose version tracking and dependency management offered by gradle. Pending a dependency aware ModuleRegistry, we should be able to alter the build.gradle file so that it knows about individual modules maybe handles them as project copies its libs into the appropriate directory does not copy dependencies that are already legitimate dependencies of xd dirt this can be achieved by runtime introspection of the dependeny tree of both projects ",NULL,"Currently, a lot of jars that are on the classpath of xd dirt are there to support modules. We should move those to the lib construct of a module and remove them from the CP of dirt. But this should not be done by simple mv , as we d lose version tracking and dependency management offered by gradle. Pending a dependency aware ModuleRegistry, we should be able to alter the build.gradle file","so that it knows about individual modules maybe handles them as project copies its libs into the appropriate directory does not copy dependencies that are already legitimate dependencies of xd dirt this can be achieved by runtime introspection of the dependeny tree of both projects","Add for who this story is","well_formed","no_role","high",False
19293,"Currently, a lot of jars that are on the classpath of xd dirt are there to support modules. We should move those to the lib construct of a module and remove them from the CP of dirt. But this should not be done by simple mv , as we d lose version tracking and dependency management offered by gradle. Pending a dependency aware ModuleRegistry, we should be able to alter the build.gradle file so that it knows about individual modules maybe handles them as project copies its libs into the appropriate directory does not copy dependencies that are already legitimate dependencies of xd dirt this can be achieved by runtime introspection of the dependeny tree of both projects ",NULL,"Currently, a lot of jars that are on the classpath of xd dirt are there to support modules. We should move those to the lib construct of a module and remove them from the CP of dirt. But this should not be done by simple mv , as we d lose version tracking and dependency management offered by gradle. Pending a dependency aware ModuleRegistry, we should be able to alter the build.gradle file","so that it knows about individual modules maybe handles them as project copies its libs into the appropriate directory does not copy dependencies that are already legitimate dependencies of xd dirt this can be achieved by runtime introspection of the dependeny tree of both projects","Currently, a lot of jars that are on the classpath of xd dirt are there to support modules. We should move those to the lib construct of a module<span class='highlight-text severity-high'> and </span>remove them from the CP of dirt. But this should not be done by simple mv , as we d lose version tracking and dependency management offered by gradle. Pending a dependency aware ModuleRegistry, we should be able to alter the build.gradle file so that it knows about individual modules maybe handles them as project copies its libs into the appropriate directory does not copy dependencies that are already legitimate dependencies of xd dirt this can be achieved by runtime introspection of the dependeny tree of both projects ","atomic","conjunctions","high",False
19293,"Currently, a lot of jars that are on the classpath of xd dirt are there to support modules. We should move those to the lib construct of a module and remove them from the CP of dirt. But this should not be done by simple mv , as we d lose version tracking and dependency management offered by gradle. Pending a dependency aware ModuleRegistry, we should be able to alter the build.gradle file so that it knows about individual modules maybe handles them as project copies its libs into the appropriate directory does not copy dependencies that are already legitimate dependencies of xd dirt this can be achieved by runtime introspection of the dependeny tree of both projects ",NULL,"Currently, a lot of jars that are on the classpath of xd dirt are there to support modules. We should move those to the lib construct of a module and remove them from the CP of dirt. But this should not be done by simple mv , as we d lose version tracking and dependency management offered by gradle. Pending a dependency aware ModuleRegistry, we should be able to alter the build.gradle file","so that it knows about individual modules maybe handles them as project copies its libs into the appropriate directory does not copy dependencies that are already legitimate dependencies of xd dirt this can be achieved by runtime introspection of the dependeny tree of both projects","Currently, a lot of jars that are on the classpath of xd dirt are there to support modules<span class='highlight-text severity-high'>. We should move those to the lib construct of a module and remove them from the CP of dirt. But this should not be done by simple mv , as we d lose version tracking and dependency management offered by gradle. Pending a dependency aware ModuleRegistry, we should be able to alter the build.gradle file so that it knows about individual modules maybe handles them as project copies its libs into the appropriate directory does not copy dependencies that are already legitimate dependencies of xd dirt this can be achieved by runtime introspection of the dependeny tree of both projects </span>","minimal","punctuation","high",False
19293,"Currently, a lot of jars that are on the classpath of xd dirt are there to support modules. We should move those to the lib construct of a module and remove them from the CP of dirt. But this should not be done by simple mv , as we d lose version tracking and dependency management offered by gradle. Pending a dependency aware ModuleRegistry, we should be able to alter the build.gradle file so that it knows about individual modules maybe handles them as project copies its libs into the appropriate directory does not copy dependencies that are already legitimate dependencies of xd dirt this can be achieved by runtime introspection of the dependeny tree of both projects ",NULL,"Currently, a lot of jars that are on the classpath of xd dirt are there to support modules. We should move those to the lib construct of a module and remove them from the CP of dirt. But this should not be done by simple mv , as we d lose version tracking and dependency management offered by gradle. Pending a dependency aware ModuleRegistry, we should be able to alter the build.gradle file","so that it knows about individual modules maybe handles them as project copies its libs into the appropriate directory does not copy dependencies that are already legitimate dependencies of xd dirt this can be achieved by runtime introspection of the dependeny tree of both projects","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19301,"It looks like we don t handle deletion of source files currently. We should provide some support for that Maybe there is a way to into Spring Integration s PseudoTransactionManager support http docs.spring.io spring integration api org springframework integration transaction PseudoTransactionManager.html The File Source should possibly also support File archival functionality But that might also be a dedicated processor? . Not sure where we want to set the semantic boundaries for the File Source. ",NULL,"It looks like we don t handle deletion of source files currently. We should provide some support for that Maybe there is a way to into Spring Integration s PseudoTransactionManager support http docs.spring.io spring integration api org springframework integration transaction PseudoTransactionManager.html The File Source should possibly also support File archival functionality But that might also be a dedicated processor? . Not sure where we want to set the semantic boundaries for the File Source. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19301,"It looks like we don t handle deletion of source files currently. We should provide some support for that Maybe there is a way to into Spring Integration s PseudoTransactionManager support http docs.spring.io spring integration api org springframework integration transaction PseudoTransactionManager.html The File Source should possibly also support File archival functionality But that might also be a dedicated processor? . Not sure where we want to set the semantic boundaries for the File Source. ",NULL,"It looks like we don t handle deletion of source files currently. We should provide some support for that Maybe there is a way to into Spring Integration s PseudoTransactionManager support http docs.spring.io spring integration api org springframework integration transaction PseudoTransactionManager.html The File Source should possibly also support File archival functionality But that might also be a dedicated processor? . Not sure where we want to set the semantic boundaries for the File Source. ",NULL,"It looks like we don t handle deletion of source files currently<span class='highlight-text severity-high'>. We should provide some support for that Maybe there is a way to into Spring Integration s PseudoTransactionManager support http docs.spring.io spring integration api org springframework integration transaction PseudoTransactionManager.html The File Source should possibly also support File archival functionality But that might also be a dedicated processor? . Not sure where we want to set the semantic boundaries for the File Source. </span>","minimal","punctuation","high",False
19304,"Commands that pair up with the functionality described in XD 859 module list would list all modules in a table format module list type source would list only source modules and so on.",NULL,"Commands that pair up with the functionality described in XD 859 module list would list all modules in a table format module list type","source would list only source modules and so on.","Add for who this story is","well_formed","no_role","high",False
19304,"Commands that pair up with the functionality described in XD 859 module list would list all modules in a table format module list type source would list only source modules and so on.",NULL,"Commands that pair up with the functionality described in XD 859 module list would list all modules in a table format module list type","source would list only source modules and so on.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19313,"This story will need to be broken down further. The current code mixes together the type conversion that happens within a single JVM for data that is passed on a local transport between modules and serialization deserialization between JVMs. This should be separated. There was a suggestion that we could perhaps use typed data channels in SI as a means implement the type conversion between modules. The media type conversion support in Spring 4 is another part of this solution.",NULL,"This story will need to be broken down further. The current code mixes together the type conversion that happens within a single JVM for data that is passed on a local transport between modules and serialization deserialization between JVMs. This should be separated. There was a suggestion that we could perhaps use typed data channels in SI as a means implement the type conversion between modules. The media type conversion support in Spring 4 is another part of this solution.",NULL,"Add for who this story is","well_formed","no_role","high",False
19313,"This story will need to be broken down further. The current code mixes together the type conversion that happens within a single JVM for data that is passed on a local transport between modules and serialization deserialization between JVMs. This should be separated. There was a suggestion that we could perhaps use typed data channels in SI as a means implement the type conversion between modules. The media type conversion support in Spring 4 is another part of this solution.",NULL,"This story will need to be broken down further. The current code mixes together the type conversion that happens within a single JVM for data that is passed on a local transport between modules and serialization deserialization between JVMs. This should be separated. There was a suggestion that we could perhaps use typed data channels in SI as a means implement the type conversion between modules. The media type conversion support in Spring 4 is another part of this solution.",NULL,"This story will need to be broken down further<span class='highlight-text severity-high'>. The current code mixes together the type conversion that happens within a single JVM for data that is passed on a local transport between modules and serialization deserialization between JVMs. This should be separated. There was a suggestion that we could perhaps use typed data channels in SI as a means implement the type conversion between modules. The media type conversion support in Spring 4 is another part of this solution.</span>","minimal","punctuation","high",False
19348,"As of XD 685, we no longer have the ability to 1 Tap a named channel, ala stream1 foo sink stream2 tap foo sink 2 Tap a stream whose source is a named channel ala stream2 tap stream1 sink 3 Tap a label ala stream1 http obfuscator transform expression payload.replaceAll password , file stream2 tap stream1.obfuscator sink Loss of named channel support is due to the fact that we are creating a WireTap on a module s local output channel only thus we never tap named channels . We are not supporting labels because we only create a named channel called tap stream.module on stream creation, so later creation of tap on stream1.obfuscator is referring to a non existent named channel.","As of","XD 685, we no longer have the ability to 1 Tap a named channel, ala stream1 foo sink stream2 tap foo sink 2 Tap a stream whose","source is a named channel ala stream2 tap stream1 sink 3 Tap a label ala stream1 http obfuscator transform expression payload.replaceAll password , file stream2 tap stream1.obfuscator sink Loss of named channel support is due to the fact that we are creating a WireTap on a module s local output channel only thus we never tap named channels . We are not supporting labels because we only create a named channel called tap stream.module on stream creation, so later creation of tap on stream1.obfuscator is referring to a non existent named channel.","As of XD 685, we no longer have the ability to 1 Tap a named channel, ala stream1 foo sink stream2 tap foo sink 2 Tap a stream whose source is a named channel ala stream2 tap stream1 sink 3 Tap a label ala stream1 http obfuscator transform expression payload<span class='highlight-text severity-high'>.replaceAll password , file stream2 tap stream1.obfuscator sink Loss of named channel support is due to the fact that we are creating a WireTap on a module s local output channel only thus we never tap named channels . We are not supporting labels because we only create a named channel called tap stream.module on stream creation, so later creation of tap on stream1.obfuscator is referring to a non existent named channel.</span>","minimal","punctuation","high",False
19348,"As of XD 685, we no longer have the ability to 1 Tap a named channel, ala stream1 foo sink stream2 tap foo sink 2 Tap a stream whose source is a named channel ala stream2 tap stream1 sink 3 Tap a label ala stream1 http obfuscator transform expression payload.replaceAll password , file stream2 tap stream1.obfuscator sink Loss of named channel support is due to the fact that we are creating a WireTap on a module s local output channel only thus we never tap named channels . We are not supporting labels because we only create a named channel called tap stream.module on stream creation, so later creation of tap on stream1.obfuscator is referring to a non existent named channel.","As of","XD 685, we no longer have the ability to 1 Tap a named channel, ala stream1 foo sink stream2 tap foo sink 2 Tap a stream whose","source is a named channel ala stream2 tap stream1 sink 3 Tap a label ala stream1 http obfuscator transform expression payload.replaceAll password , file stream2 tap stream1.obfuscator sink Loss of named channel support is due to the fact that we are creating a WireTap on a module s local output channel only thus we never tap named channels . We are not supporting labels because we only create a named channel called tap stream.module on stream creation, so later creation of tap on stream1.obfuscator is referring to a non existent named channel.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19359,"Commands like stream deploy have changed over time to allow passing a all option. So it s either stream deploy foo or stream deploy all . This has a number of drawbacks, given that these are the only 2 alternatives Implementation code is cumbersome None of the options can be marked mandatory, yet one of them is required. This has to be checked in the command code itself TAB completion is less powerful as the shell doesn t know if we want the first or the second form. Consider splitting those commands into two distinct commands, one as before and one literally named stream deploy all .",NULL,"Commands like stream deploy have changed over time to allow passing a all option.","So it s either stream deploy foo or stream deploy all . This has a number of drawbacks, given that these are the only 2 alternatives Implementation code is cumbersome None of the options can be marked mandatory, yet one of them is required. This has to be checked in the command code itself TAB completion is less powerful as the shell doesn t know if we want the first or the second form. Consider splitting those commands into two distinct commands, one as before and one literally named stream deploy all .","Add for who this story is","well_formed","no_role","high",False
19359,"Commands like stream deploy have changed over time to allow passing a all option. So it s either stream deploy foo or stream deploy all . This has a number of drawbacks, given that these are the only 2 alternatives Implementation code is cumbersome None of the options can be marked mandatory, yet one of them is required. This has to be checked in the command code itself TAB completion is less powerful as the shell doesn t know if we want the first or the second form. Consider splitting those commands into two distinct commands, one as before and one literally named stream deploy all .",NULL,"Commands like stream deploy have changed over time to allow passing a all option.","So it s either stream deploy foo or stream deploy all . This has a number of drawbacks, given that these are the only 2 alternatives Implementation code is cumbersome None of the options can be marked mandatory, yet one of them is required. This has to be checked in the command code itself TAB completion is less powerful as the shell doesn t know if we want the first or the second form. Consider splitting those commands into two distinct commands, one as before and one literally named stream deploy all .","Commands like stream deploy have changed over time to allow passing a all option<span class='highlight-text severity-high'>. So it s either stream deploy foo or stream deploy all . This has a number of drawbacks, given that these are the only 2 alternatives Implementation code is cumbersome None of the options can be marked mandatory, yet one of them is required. This has to be checked in the command code itself TAB completion is less powerful as the shell doesn t know if we want the first or the second form. Consider splitting those commands into two distinct commands, one as before and one literally named stream deploy all .</span>","minimal","punctuation","high",False
19359,"Commands like stream deploy have changed over time to allow passing a all option. So it s either stream deploy foo or stream deploy all . This has a number of drawbacks, given that these are the only 2 alternatives Implementation code is cumbersome None of the options can be marked mandatory, yet one of them is required. This has to be checked in the command code itself TAB completion is less powerful as the shell doesn t know if we want the first or the second form. Consider splitting those commands into two distinct commands, one as before and one literally named stream deploy all .",NULL,"Commands like stream deploy have changed over time to allow passing a all option.","So it s either stream deploy foo or stream deploy all . This has a number of drawbacks, given that these are the only 2 alternatives Implementation code is cumbersome None of the options can be marked mandatory, yet one of them is required. This has to be checked in the command code itself TAB completion is less powerful as the shell doesn t know if we want the first or the second form. Consider splitting those commands into two distinct commands, one as before and one literally named stream deploy all .","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19377,"Triggers will be a source and no longer as a unique module. The following have to be removed spring xd dirt package org.springframework.xd.dirt.plugins.trigger META INF spring xd plugins triggers.xml org.springframework.xd.dirt.stream.TriggerPlugin The following beans will require updates to remove the trigger code spring xd dirt META INF spring xd internal deployers.xml Remove Triggerdeployer org.springframework.xd.dirt.plugins.job.JobPlugin Remove the registrars for fixedDelay, fixedRate, Cron. As well as the component selection, only need the job modules bean Update the tests to use the trigger as a source, instead of the trigger module. spring xd shell Remove trigger commands and associated tests xd controllers Remove trigger controllers and their associated tests This list cover most but not all the components affected. Success criteria Successful unit and integration tests. ",NULL,"Triggers will be a source and no longer as a unique module. The following have to be removed spring xd dirt package org.springframework.xd.dirt.plugins.trigger META INF spring xd plugins triggers.xml org.springframework.xd.dirt.stream.TriggerPlugin The following beans will require updates to remove the trigger code spring xd dirt META INF spring xd internal deployers.xml Remove Triggerdeployer org.springframework.xd.dirt.plugins.job.JobPlugin Remove the registrars for fixedDelay, fixedRate, Cron. As well as the component selection, only need the job modules bean Update the tests to use the trigger as a source, instead of the trigger module. spring xd shell Remove trigger commands and associated tests xd controllers Remove trigger controllers and their associated tests This list cover most but not all the components affected. Success criteria Successful unit and integration tests. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19552,"If we want spring xd to support interactions through a UI, then it would make sense that we should support the UI coming from a separate origin. This way, the UI could remain a separate project and be served from wherever we want. So, we would need to add the header to all outgoing REST responses. We may also need to add a Access Control Allow Methods header as well. In the short term, the Access Control Allow Origin header could be hard coded to a specific url I m using http localhost 9889 for now , but in the long term we would need this configurable.",NULL,"If we want spring xd to support interactions through a UI, then it would make sense that we should support the UI coming from a separate origin. This way, the UI could remain a separate project and be served from wherever we want.","So, we would need to add the header to all outgoing REST responses. We may also need to add a Access Control Allow Methods header as well. In the short term, the Access Control Allow Origin header could be hard coded to a specific url I m using http localhost 9889 for now , but in the long term we would need this configurable.","If we want spring xd to support interactions through a UI, then it would make sense that we should support the UI coming from a separate origin. This way, the UI could remain a separate project<span class='highlight-text severity-high'> and </span>be served from wherever we want. So, we would need to add the header to all outgoing REST responses. We may also need to add a Access Control Allow Methods header as well. In the short term, the Access Control Allow Origin header could be hard coded to a specific url I m using http localhost 9889 for now , but in the long term we would need this configurable.","atomic","conjunctions","high",False
19377,"Triggers will be a source and no longer as a unique module. The following have to be removed spring xd dirt package org.springframework.xd.dirt.plugins.trigger META INF spring xd plugins triggers.xml org.springframework.xd.dirt.stream.TriggerPlugin The following beans will require updates to remove the trigger code spring xd dirt META INF spring xd internal deployers.xml Remove Triggerdeployer org.springframework.xd.dirt.plugins.job.JobPlugin Remove the registrars for fixedDelay, fixedRate, Cron. As well as the component selection, only need the job modules bean Update the tests to use the trigger as a source, instead of the trigger module. spring xd shell Remove trigger commands and associated tests xd controllers Remove trigger controllers and their associated tests This list cover most but not all the components affected. Success criteria Successful unit and integration tests. ",NULL,"Triggers will be a source and no longer as a unique module. The following have to be removed spring xd dirt package org.springframework.xd.dirt.plugins.trigger META INF spring xd plugins triggers.xml org.springframework.xd.dirt.stream.TriggerPlugin The following beans will require updates to remove the trigger code spring xd dirt META INF spring xd internal deployers.xml Remove Triggerdeployer org.springframework.xd.dirt.plugins.job.JobPlugin Remove the registrars for fixedDelay, fixedRate, Cron. As well as the component selection, only need the job modules bean Update the tests to use the trigger as a source, instead of the trigger module. spring xd shell Remove trigger commands and associated tests xd controllers Remove trigger controllers and their associated tests This list cover most but not all the components affected. Success criteria Successful unit and integration tests. ",NULL,"Triggers will be a source<span class='highlight-text severity-high'> and </span>no longer as a unique module. The following have to be removed spring xd dirt package org.springframework.xd.dirt.plugins.trigger META INF spring xd plugins triggers.xml org.springframework.xd.dirt.stream.TriggerPlugin The following beans will require updates to remove the trigger code spring xd dirt META INF spring xd internal deployers.xml Remove Triggerdeployer org.springframework.xd.dirt.plugins.job.JobPlugin Remove the registrars for fixedDelay, fixedRate, Cron. As well as the component selection, only need the job modules bean Update the tests to use the trigger as a source, instead of the trigger module. spring xd shell Remove trigger commands and associated tests xd controllers Remove trigger controllers and their associated tests This list cover most but not all the components affected. Success criteria Successful unit and integration tests. ","atomic","conjunctions","high",False
19377,"Triggers will be a source and no longer as a unique module. The following have to be removed spring xd dirt package org.springframework.xd.dirt.plugins.trigger META INF spring xd plugins triggers.xml org.springframework.xd.dirt.stream.TriggerPlugin The following beans will require updates to remove the trigger code spring xd dirt META INF spring xd internal deployers.xml Remove Triggerdeployer org.springframework.xd.dirt.plugins.job.JobPlugin Remove the registrars for fixedDelay, fixedRate, Cron. As well as the component selection, only need the job modules bean Update the tests to use the trigger as a source, instead of the trigger module. spring xd shell Remove trigger commands and associated tests xd controllers Remove trigger controllers and their associated tests This list cover most but not all the components affected. Success criteria Successful unit and integration tests. ",NULL,"Triggers will be a source and no longer as a unique module. The following have to be removed spring xd dirt package org.springframework.xd.dirt.plugins.trigger META INF spring xd plugins triggers.xml org.springframework.xd.dirt.stream.TriggerPlugin The following beans will require updates to remove the trigger code spring xd dirt META INF spring xd internal deployers.xml Remove Triggerdeployer org.springframework.xd.dirt.plugins.job.JobPlugin Remove the registrars for fixedDelay, fixedRate, Cron. As well as the component selection, only need the job modules bean Update the tests to use the trigger as a source, instead of the trigger module. spring xd shell Remove trigger commands and associated tests xd controllers Remove trigger controllers and their associated tests This list cover most but not all the components affected. Success criteria Successful unit and integration tests. ",NULL,"Triggers will be a source and no longer as a unique module<span class='highlight-text severity-high'>. The following have to be removed spring xd dirt package org.springframework.xd.dirt.plugins.trigger META INF spring xd plugins triggers.xml org.springframework.xd.dirt.stream.TriggerPlugin The following beans will require updates to remove the trigger code spring xd dirt META INF spring xd internal deployers.xml Remove Triggerdeployer org.springframework.xd.dirt.plugins.job.JobPlugin Remove the registrars for fixedDelay, fixedRate, Cron. As well as the component selection, only need the job modules bean Update the tests to use the trigger as a source, instead of the trigger module. spring xd shell Remove trigger commands and associated tests xd controllers Remove trigger controllers and their associated tests This list cover most but not all the components affected. Success criteria Successful unit and integration tests. </span>","minimal","punctuation","high",False
19398,"When creating a job, a named channel will be created with a name of job. your job name i.e. job.foo. Required components A Transform See XD 733 JobPlugin needs to create the NamedChannel for the job and associate the transform. Registrar.xml will need an input channel? Name channel support will be required. Add channel to job rest apis to notify system that a named channel is requested. Unit Test ",NULL,"When creating a job, a named channel will be created with a name of job. your job name i.e. job.foo. Required components A Transform See XD 733 JobPlugin needs to create the NamedChannel for the job and associate the transform. Registrar.xml will need an input channel? Name channel support will be required. Add channel to job rest apis to notify system that a named channel is requested. Unit Test ",NULL,"Add for who this story is","well_formed","no_role","high",False
19398,"When creating a job, a named channel will be created with a name of job. your job name i.e. job.foo. Required components A Transform See XD 733 JobPlugin needs to create the NamedChannel for the job and associate the transform. Registrar.xml will need an input channel? Name channel support will be required. Add channel to job rest apis to notify system that a named channel is requested. Unit Test ",NULL,"When creating a job, a named channel will be created with a name of job. your job name i.e. job.foo. Required components A Transform See XD 733 JobPlugin needs to create the NamedChannel for the job and associate the transform. Registrar.xml will need an input channel? Name channel support will be required. Add channel to job rest apis to notify system that a named channel is requested. Unit Test ",NULL,"When creating a job, a named channel will be created with a name of job. your job name i.e. job.foo. Required components A Transform See XD 733 JobPlugin needs to create the NamedChannel for the job<span class='highlight-text severity-high'> and </span>associate the transform. Registrar.xml will need an input channel? Name channel support will be required. Add channel to job rest apis to notify system that a named channel is requested. Unit Test ","atomic","conjunctions","high",False
19398,"When creating a job, a named channel will be created with a name of job. your job name i.e. job.foo. Required components A Transform See XD 733 JobPlugin needs to create the NamedChannel for the job and associate the transform. Registrar.xml will need an input channel? Name channel support will be required. Add channel to job rest apis to notify system that a named channel is requested. Unit Test ",NULL,"When creating a job, a named channel will be created with a name of job. your job name i.e. job.foo. Required components A Transform See XD 733 JobPlugin needs to create the NamedChannel for the job and associate the transform. Registrar.xml will need an input channel? Name channel support will be required. Add channel to job rest apis to notify system that a named channel is requested. Unit Test ",NULL,"When creating a job, a named channel will be created with a name of job<span class='highlight-text severity-high'>. your job name i.e. job.foo. Required components A Transform See XD 733 JobPlugin needs to create the NamedChannel for the job and associate the transform. Registrar.xml will need an input channel? Name channel support will be required. Add channel to job rest apis to notify system that a named channel is requested. Unit Test </span>","minimal","punctuation","high",False
19437,"Because StringToJsonNodeTransformer expects a String as input, one cannot chain json related processors. A simple solution would be to also accept Jackson IN and forward it directly in that case.",NULL,"Because StringToJsonNodeTransformer expects a String as input, one cannot chain json related processors. A simple solution would be to also accept Jackson IN and forward it directly in that case.",NULL,"Add for who this story is","well_formed","no_role","high",False
19437,"Because StringToJsonNodeTransformer expects a String as input, one cannot chain json related processors. A simple solution would be to also accept Jackson IN and forward it directly in that case.",NULL,"Because StringToJsonNodeTransformer expects a String as input, one cannot chain json related processors. A simple solution would be to also accept Jackson IN and forward it directly in that case.",NULL,"Because StringToJsonNodeTransformer expects a String as input, one cannot chain json related processors<span class='highlight-text severity-high'>. A simple solution would be to also accept Jackson IN and forward it directly in that case.</span>","minimal","punctuation","high",False
19411,"After the xd singlenode process has started, create a new job that has dependencies not already in the parent application context, and then create and run a new job that uses the new module. Implementation Suggestions Develop in the test tree, we can put in the jar and config from https github.com SpringSource spring xd samples tree master batch simple How to verify it works. In a JUnit test case copy in a new job that has new dependencies, ..copy the lib and config directories into the location where the ModuleRegisty will pick it up. Deploy a new job and check in the job repository that the job ran and was successful. ",NULL,"After the xd singlenode process has started, create a new job that has dependencies not already in the parent application context, and then create and run a new job that uses the new module. Implementation Suggestions Develop in the test tree, we can put in the jar and config from https github.com SpringSource spring xd samples tree master batch simple How to verify it works. In a JUnit test case copy in a new job that has new dependencies, ..copy the lib and config directories into the location where the ModuleRegisty will pick it up. Deploy a new job and check in the job repository that the job ran and was successful. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19411,"After the xd singlenode process has started, create a new job that has dependencies not already in the parent application context, and then create and run a new job that uses the new module. Implementation Suggestions Develop in the test tree, we can put in the jar and config from https github.com SpringSource spring xd samples tree master batch simple How to verify it works. In a JUnit test case copy in a new job that has new dependencies, ..copy the lib and config directories into the location where the ModuleRegisty will pick it up. Deploy a new job and check in the job repository that the job ran and was successful. ",NULL,"After the xd singlenode process has started, create a new job that has dependencies not already in the parent application context, and then create and run a new job that uses the new module. Implementation Suggestions Develop in the test tree, we can put in the jar and config from https github.com SpringSource spring xd samples tree master batch simple How to verify it works. In a JUnit test case copy in a new job that has new dependencies, ..copy the lib and config directories into the location where the ModuleRegisty will pick it up. Deploy a new job and check in the job repository that the job ran and was successful. ",NULL,"After the xd singlenode process has started, create a new job that has dependencies not already in the parent application context,<span class='highlight-text severity-high'> and </span>then create and run a new job that uses the new module. Implementation Suggestions Develop in the test tree, we can put in the jar and config from https github.com SpringSource spring xd samples tree master batch simple How to verify it works. In a JUnit test case copy in a new job that has new dependencies, ..copy the lib and config directories into the location where the ModuleRegisty will pick it up. Deploy a new job and check in the job repository that the job ran and was successful. ","atomic","conjunctions","high",False
19411,"After the xd singlenode process has started, create a new job that has dependencies not already in the parent application context, and then create and run a new job that uses the new module. Implementation Suggestions Develop in the test tree, we can put in the jar and config from https github.com SpringSource spring xd samples tree master batch simple How to verify it works. In a JUnit test case copy in a new job that has new dependencies, ..copy the lib and config directories into the location where the ModuleRegisty will pick it up. Deploy a new job and check in the job repository that the job ran and was successful. ",NULL,"After the xd singlenode process has started, create a new job that has dependencies not already in the parent application context, and then create and run a new job that uses the new module. Implementation Suggestions Develop in the test tree, we can put in the jar and config from https github.com SpringSource spring xd samples tree master batch simple How to verify it works. In a JUnit test case copy in a new job that has new dependencies, ..copy the lib and config directories into the location where the ModuleRegisty will pick it up. Deploy a new job and check in the job repository that the job ran and was successful. ",NULL,"After the xd singlenode process has started, create a new job that has dependencies not already in the parent application context, and then create and run a new job that uses the new module<span class='highlight-text severity-high'>. Implementation Suggestions Develop in the test tree, we can put in the jar and config from https github.com SpringSource spring xd samples tree master batch simple How to verify it works. In a JUnit test case copy in a new job that has new dependencies, ..copy the lib and config directories into the location where the ModuleRegisty will pick it up. Deploy a new job and check in the job repository that the job ran and was successful. </span>","minimal","punctuation","high",False
19412,"After the xd singlenode process has started, create a new module that has dependencies not already in the parent application context, and then create a stream that uses the new module. Implementation Suggestions Develop in the test tree, maybe of the xd shell project, a new module. the lib and config should be sititng around in a directory waiting to be copied into the appropriate spot. Could try http www.date4j.net ..The config file would be similar to time.xml but use the date4j class. How to verify it works In a JUnit test case copy in a new module that has new dependencies, ..copy the lib and config directories into the location where the ModuleRegisty will pick it up. Deploy a new stream, date4j file and see if there are contents in the file. ",NULL,"After the xd singlenode process has started, create a new module that has dependencies not already in the parent application context, and then create a stream that uses the new module. Implementation Suggestions Develop in the test tree, maybe of the xd shell project, a new module. the lib and config should be sititng around in a directory waiting to be copied into the appropriate spot. Could try http www.date4j.net ..The config file would be similar to time.xml but use the date4j class. How to verify it works In a JUnit test case copy in a new module that has new dependencies, ..copy the lib and config directories into the location where the ModuleRegisty will pick it up. Deploy a new stream, date4j file and see if there are contents in the file. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19412,"After the xd singlenode process has started, create a new module that has dependencies not already in the parent application context, and then create a stream that uses the new module. Implementation Suggestions Develop in the test tree, maybe of the xd shell project, a new module. the lib and config should be sititng around in a directory waiting to be copied into the appropriate spot. Could try http www.date4j.net ..The config file would be similar to time.xml but use the date4j class. How to verify it works In a JUnit test case copy in a new module that has new dependencies, ..copy the lib and config directories into the location where the ModuleRegisty will pick it up. Deploy a new stream, date4j file and see if there are contents in the file. ",NULL,"After the xd singlenode process has started, create a new module that has dependencies not already in the parent application context, and then create a stream that uses the new module. Implementation Suggestions Develop in the test tree, maybe of the xd shell project, a new module. the lib and config should be sititng around in a directory waiting to be copied into the appropriate spot. Could try http www.date4j.net ..The config file would be similar to time.xml but use the date4j class. How to verify it works In a JUnit test case copy in a new module that has new dependencies, ..copy the lib and config directories into the location where the ModuleRegisty will pick it up. Deploy a new stream, date4j file and see if there are contents in the file. ",NULL,"After the xd singlenode process has started, create a new module that has dependencies not already in the parent application context,<span class='highlight-text severity-high'> and </span>then create a stream that uses the new module. Implementation Suggestions Develop in the test tree, maybe of the xd shell project, a new module. the lib and config should be sititng around in a directory waiting to be copied into the appropriate spot. Could try http www.date4j.net ..The config file would be similar to time.xml but use the date4j class. How to verify it works In a JUnit test case copy in a new module that has new dependencies, ..copy the lib and config directories into the location where the ModuleRegisty will pick it up. Deploy a new stream, date4j file and see if there are contents in the file. ","atomic","conjunctions","high",False
19412,"After the xd singlenode process has started, create a new module that has dependencies not already in the parent application context, and then create a stream that uses the new module. Implementation Suggestions Develop in the test tree, maybe of the xd shell project, a new module. the lib and config should be sititng around in a directory waiting to be copied into the appropriate spot. Could try http www.date4j.net ..The config file would be similar to time.xml but use the date4j class. How to verify it works In a JUnit test case copy in a new module that has new dependencies, ..copy the lib and config directories into the location where the ModuleRegisty will pick it up. Deploy a new stream, date4j file and see if there are contents in the file. ",NULL,"After the xd singlenode process has started, create a new module that has dependencies not already in the parent application context, and then create a stream that uses the new module. Implementation Suggestions Develop in the test tree, maybe of the xd shell project, a new module. the lib and config should be sititng around in a directory waiting to be copied into the appropriate spot. Could try http www.date4j.net ..The config file would be similar to time.xml but use the date4j class. How to verify it works In a JUnit test case copy in a new module that has new dependencies, ..copy the lib and config directories into the location where the ModuleRegisty will pick it up. Deploy a new stream, date4j file and see if there are contents in the file. ",NULL,"After the xd singlenode process has started, create a new module that has dependencies not already in the parent application context, and then create a stream that uses the new module<span class='highlight-text severity-high'>. Implementation Suggestions Develop in the test tree, maybe of the xd shell project, a new module. the lib and config should be sititng around in a directory waiting to be copied into the appropriate spot. Could try http www.date4j.net ..The config file would be similar to time.xml but use the date4j class. How to verify it works In a JUnit test case copy in a new module that has new dependencies, ..copy the lib and config directories into the location where the ModuleRegisty will pick it up. Deploy a new stream, date4j file and see if there are contents in the file. </span>","minimal","punctuation","high",False
19421,"When trying to undeploy destroy a tap that has reference to an already deleted stream fails with the following exception Command failed org.springframework.xd.rest.client.impl.SpringXDException XD116E pos 4 unrecognized stream reference stream name at the tap defintion . As expected, the StreamConfigParser s lookupStream fails to find the stream name as the stream doesn t exist in the repository. In this scenario, what is a better way to handle the tap operations. Should we undeploy the tap when the stream is destroyed? though I'don t see an easy way to find the taps that use a specific stream .",NULL,"When trying to undeploy destroy a tap that has reference to an already deleted stream fails with the following exception Command failed org.springframework.xd.rest.client.impl.SpringXDException XD116E pos 4 unrecognized stream reference stream name at the tap defintion . As expected, the StreamConfigParser s lookupStream fails to find the stream name as the stream doesn t exist in the repository. In this scenario, what is a better way to handle the tap operations. Should we undeploy the tap when the stream is destroyed? though I'don t see an easy way to find the taps that use a specific stream .",NULL,"Add for who this story is","well_formed","no_role","high",False
19421,"When trying to undeploy destroy a tap that has reference to an already deleted stream fails with the following exception Command failed org.springframework.xd.rest.client.impl.SpringXDException XD116E pos 4 unrecognized stream reference stream name at the tap defintion . As expected, the StreamConfigParser s lookupStream fails to find the stream name as the stream doesn t exist in the repository. In this scenario, what is a better way to handle the tap operations. Should we undeploy the tap when the stream is destroyed? though I'don t see an easy way to find the taps that use a specific stream .",NULL,"When trying to undeploy destroy a tap that has reference to an already deleted stream fails with the following exception Command failed org.springframework.xd.rest.client.impl.SpringXDException XD116E pos 4 unrecognized stream reference stream name at the tap defintion . As expected, the StreamConfigParser s lookupStream fails to find the stream name as the stream doesn t exist in the repository. In this scenario, what is a better way to handle the tap operations. Should we undeploy the tap when the stream is destroyed? though I'don t see an easy way to find the taps that use a specific stream .",NULL,"When trying to undeploy destroy a tap that has reference to an already deleted stream fails with the following exception Command failed org<span class='highlight-text severity-high'>.springframework.xd.rest.client.impl.SpringXDException XD116E pos 4 unrecognized stream reference stream name at the tap defintion . As expected, the StreamConfigParser s lookupStream fails to find the stream name as the stream doesn t exist in the repository. In this scenario, what is a better way to handle the tap operations. Should we undeploy the tap when the stream is destroyed? though I'don t see an easy way to find the taps that use a specific stream .</span>","minimal","punctuation","high",False
19552,"If we want spring xd to support interactions through a UI, then it would make sense that we should support the UI coming from a separate origin. This way, the UI could remain a separate project and be served from wherever we want. So, we would need to add the header to all outgoing REST responses. We may also need to add a Access Control Allow Methods header as well. In the short term, the Access Control Allow Origin header could be hard coded to a specific url I m using http localhost 9889 for now , but in the long term we would need this configurable.",NULL,"If we want spring xd to support interactions through a UI, then it would make sense that we should support the UI coming from a separate origin. This way, the UI could remain a separate project and be served from wherever we want.","So, we would need to add the header to all outgoing REST responses. We may also need to add a Access Control Allow Methods header as well. In the short term, the Access Control Allow Origin header could be hard coded to a specific url I m using http localhost 9889 for now , but in the long term we would need this configurable.","If we want spring xd to support interactions through a UI, then it would make sense that we should support the UI coming from a separate origin<span class='highlight-text severity-high'>. This way, the UI could remain a separate project and be served from wherever we want. So, we would need to add the header to all outgoing REST responses. We may also need to add a Access Control Allow Methods header as well. In the short term, the Access Control Allow Origin header could be hard coded to a specific url I m using http localhost 9889 for now , but in the long term we would need this configurable.</span>","minimal","punctuation","high",False
19552,"If we want spring xd to support interactions through a UI, then it would make sense that we should support the UI coming from a separate origin. This way, the UI could remain a separate project and be served from wherever we want. So, we would need to add the header to all outgoing REST responses. We may also need to add a Access Control Allow Methods header as well. In the short term, the Access Control Allow Origin header could be hard coded to a specific url I m using http localhost 9889 for now , but in the long term we would need this configurable.",NULL,"If we want spring xd to support interactions through a UI, then it would make sense that we should support the UI coming from a separate origin. This way, the UI could remain a separate project and be served from wherever we want.","So, we would need to add the header to all outgoing REST responses. We may also need to add a Access Control Allow Methods header as well. In the short term, the Access Control Allow Origin header could be hard coded to a specific url I m using http localhost 9889 for now , but in the long term we would need this configurable.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19448,"Consider 2 Admins Containers that are using discrete Redis instances but a shared Rabbit instance for the transport. If two different streams are deployed on each, but with the same stream name, the Rabbit queues will be common e.g. foo.0 , causing crosstalk. Stream names must be unique across all container instances sharing Rabbit infrastructure. I am not sure what the solution is; two instances of the same stream do need common queues but instances of different streams need a qualifier of some kind container name? . I guess it s not that big of an issue because, if they re sharing infrastructure, they re likely to be sharing a stream repo too in which case you d need unique stream names.",NULL,"Consider 2 Admins Containers that are using discrete Redis instances but a shared Rabbit instance for the transport. If two different streams are deployed on each, but with the same stream name, the Rabbit queues will be common e.g. foo.0 , causing crosstalk. Stream names must be unique across all container instances sharing Rabbit infrastructure. I am not sure what the solution is; two instances of the same stream do need common queues but instances of different streams need a qualifier of some kind container name? . I guess it s not that big of an issue because, if they re sharing infrastructure, they re likely to be sharing a stream repo too in which case you d need unique stream names.",NULL,"Add for who this story is","well_formed","no_role","high",False
19448,"Consider 2 Admins Containers that are using discrete Redis instances but a shared Rabbit instance for the transport. If two different streams are deployed on each, but with the same stream name, the Rabbit queues will be common e.g. foo.0 , causing crosstalk. Stream names must be unique across all container instances sharing Rabbit infrastructure. I am not sure what the solution is; two instances of the same stream do need common queues but instances of different streams need a qualifier of some kind container name? . I guess it s not that big of an issue because, if they re sharing infrastructure, they re likely to be sharing a stream repo too in which case you d need unique stream names.",NULL,"Consider 2 Admins Containers that are using discrete Redis instances but a shared Rabbit instance for the transport. If two different streams are deployed on each, but with the same stream name, the Rabbit queues will be common e.g. foo.0 , causing crosstalk. Stream names must be unique across all container instances sharing Rabbit infrastructure. I am not sure what the solution is; two instances of the same stream do need common queues but instances of different streams need a qualifier of some kind container name? . I guess it s not that big of an issue because, if they re sharing infrastructure, they re likely to be sharing a stream repo too in which case you d need unique stream names.",NULL,"Consider 2 Admins Containers that are using discrete Redis instances but a shared Rabbit instance for the transport<span class='highlight-text severity-high'>. If two different streams are deployed on each, but with the same stream name, the Rabbit queues will be common e.g. foo.0 , causing crosstalk. Stream names must be unique across all container instances sharing Rabbit infrastructure. I am not sure what the solution is; two instances of the same stream do need common queues but instances of different streams need a qualifier of some kind container name? . I guess it s not that big of an issue because, if they re sharing infrastructure, they re likely to be sharing a stream repo too in which case you d need unique stream names.</span>","minimal","punctuation","high",False
19449,"Instead of xd http post target http localhost 9898 data hello world POST text plain;charset UTF 8 http localhost 9898 hello world 200 OK Content Length 0 Connection keep alive Success sending data hello world to target http localhost 9898 have xd http post target http localhost 9898 data hello world POST text plain;charset UTF 8 http localhost 9898 hello world 200 OK or better yet xd http post target http localhost 9898 data hello world 200 OK POST text plain;charset UTF 8 http localhost 9898 hello world ",NULL,"Instead of xd http post target http localhost 9898 data hello world POST text plain;charset UTF 8 http localhost 9898 hello world 200 OK Content Length 0 Connection keep alive Success sending data hello world to target http localhost 9898 have xd http post target http localhost 9898 data hello world POST text plain;charset UTF 8 http localhost 9898 hello world 200 OK or better yet xd http post target http localhost 9898 data hello world 200 OK POST text plain;charset UTF 8 http localhost 9898 hello world ",NULL,"Add for who this story is","well_formed","no_role","high",False
19449,"Instead of xd http post target http localhost 9898 data hello world POST text plain;charset UTF 8 http localhost 9898 hello world 200 OK Content Length 0 Connection keep alive Success sending data hello world to target http localhost 9898 have xd http post target http localhost 9898 data hello world POST text plain;charset UTF 8 http localhost 9898 hello world 200 OK or better yet xd http post target http localhost 9898 data hello world 200 OK POST text plain;charset UTF 8 http localhost 9898 hello world ",NULL,"Instead of xd http post target http localhost 9898 data hello world POST text plain;charset UTF 8 http localhost 9898 hello world 200 OK Content Length 0 Connection keep alive Success sending data hello world to target http localhost 9898 have xd http post target http localhost 9898 data hello world POST text plain;charset UTF 8 http localhost 9898 hello world 200 OK or better yet xd http post target http localhost 9898 data hello world 200 OK POST text plain;charset UTF 8 http localhost 9898 hello world ",NULL,"Instead of xd http post target http localhost 9898 data hello world POST text plain;charset UTF 8 http localhost 9898 hello world 200 OK Content Length 0 Connection keep alive Success sending data hello world to target http localhost 9898 have xd http post target http localhost 9898 data hello world POST text plain;charset UTF 8 http localhost 9898 hello world 200 OK<span class='highlight-text severity-high'> or </span>better yet xd http post target http localhost 9898 data hello world 200 OK POST text plain;charset UTF 8 http localhost 9898 hello world ","atomic","conjunctions","high",False
19469,"Content Type during transport transit is not the same as the content type within modules. Real transports always use byte which may contain raw byte from a source, a byte converted from a String which may or may not already contain JSON , or a byte containing JSON converted by the transport on the outbound side. The transport needs to convey which of these was applied on the outbound side so it can properly reconstruct the message. Retain any content type header that already exists in the message, and restore it. For Rabbit, use normal SI Rabbit headers to convey this information. For Redis, add the information to the byte .",NULL,"Content Type during transport transit is not the same as the content type within modules. Real transports always use byte which may contain raw byte from a","source, a byte converted from a String which may or may not already contain JSON , or a byte containing JSON converted by the transport on the outbound side. The transport needs to convey which of these was applied on the outbound side so it can properly reconstruct the message. Retain any content type header that already exists in the message, and restore it. For Rabbit, use normal SI Rabbit headers to convey this information. For Redis, add the information to the byte .","Add for who this story is","well_formed","no_role","high",False
19469,"Content Type during transport transit is not the same as the content type within modules. Real transports always use byte which may contain raw byte from a source, a byte converted from a String which may or may not already contain JSON , or a byte containing JSON converted by the transport on the outbound side. The transport needs to convey which of these was applied on the outbound side so it can properly reconstruct the message. Retain any content type header that already exists in the message, and restore it. For Rabbit, use normal SI Rabbit headers to convey this information. For Redis, add the information to the byte .",NULL,"Content Type during transport transit is not the same as the content type within modules. Real transports always use byte which may contain raw byte from a","source, a byte converted from a String which may or may not already contain JSON , or a byte containing JSON converted by the transport on the outbound side. The transport needs to convey which of these was applied on the outbound side so it can properly reconstruct the message. Retain any content type header that already exists in the message, and restore it. For Rabbit, use normal SI Rabbit headers to convey this information. For Redis, add the information to the byte .","Content Type during transport transit is not the same as the content type within modules<span class='highlight-text severity-high'>. Real transports always use byte which may contain raw byte from a source, a byte converted from a String which may or may not already contain JSON , or a byte containing JSON converted by the transport on the outbound side. The transport needs to convey which of these was applied on the outbound side so it can properly reconstruct the message. Retain any content type header that already exists in the message, and restore it. For Rabbit, use normal SI Rabbit headers to convey this information. For Redis, add the information to the byte .</span>","minimal","punctuation","high",False
19469,"Content Type during transport transit is not the same as the content type within modules. Real transports always use byte which may contain raw byte from a source, a byte converted from a String which may or may not already contain JSON , or a byte containing JSON converted by the transport on the outbound side. The transport needs to convey which of these was applied on the outbound side so it can properly reconstruct the message. Retain any content type header that already exists in the message, and restore it. For Rabbit, use normal SI Rabbit headers to convey this information. For Redis, add the information to the byte .",NULL,"Content Type during transport transit is not the same as the content type within modules. Real transports always use byte which may contain raw byte from a","source, a byte converted from a String which may or may not already contain JSON , or a byte containing JSON converted by the transport on the outbound side. The transport needs to convey which of these was applied on the outbound side so it can properly reconstruct the message. Retain any content type header that already exists in the message, and restore it. For Rabbit, use normal SI Rabbit headers to convey this information. For Redis, add the information to the byte .","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19493,"DSL should be able to parse what is below, need tests CLI integration tests to ensure it is being mapped to an executeable stream. myIntricateFlow transform expression payload.toUppercase transform expression payload.toLowercase http myIntricateFlow file obfuscateName transform expression payload.replaceAll name ,XXX twitter query Bieber obfuscateName name Justin file ",NULL,"DSL should be able to parse what is below, need tests CLI integration tests to ensure it is being mapped to an executeable stream. myIntricateFlow transform expression payload.toUppercase transform expression payload.toLowercase http myIntricateFlow file obfuscateName transform expression payload.replaceAll name ,XXX twitter query Bieber obfuscateName name Justin file ",NULL,"Add for who this story is","well_formed","no_role","high",False
19493,"DSL should be able to parse what is below, need tests CLI integration tests to ensure it is being mapped to an executeable stream. myIntricateFlow transform expression payload.toUppercase transform expression payload.toLowercase http myIntricateFlow file obfuscateName transform expression payload.replaceAll name ,XXX twitter query Bieber obfuscateName name Justin file ",NULL,"DSL should be able to parse what is below, need tests CLI integration tests to ensure it is being mapped to an executeable stream. myIntricateFlow transform expression payload.toUppercase transform expression payload.toLowercase http myIntricateFlow file obfuscateName transform expression payload.replaceAll name ,XXX twitter query Bieber obfuscateName name Justin file ",NULL,"DSL should be able to parse what is below, need tests CLI integration tests to ensure it is being mapped to an executeable stream<span class='highlight-text severity-high'>. myIntricateFlow transform expression payload.toUppercase transform expression payload.toLowercase http myIntricateFlow file obfuscateName transform expression payload.replaceAll name ,XXX twitter query Bieber obfuscateName name Justin file </span>","minimal","punctuation","high",False
19508,"This story corresponds to 3 4 in discussion below. Currently, sink xml files define the Repository hardcoded to be redis and the services tied to Redis as well! I want to 1 Move those definitions out of the sink files, to somewhere shared by both admin and container 2 Make them aware of some configuration parameter similar to the store command line option 3 Get rid of so called Service layer doesn t do much right now, and logic would better live in the Handler IMO 4 Have REST controllers depend on XRepository in all cases",NULL,"This story corresponds to 3 4 in discussion below. Currently, sink xml files define the Repository hardcoded to be redis and the services tied to Redis as well! I want to 1 Move those definitions out of the sink files, to","somewhere shared by both admin and container 2 Make them aware of some configuration parameter similar to the store command line option 3 Get rid of so called Service layer doesn t do much right now, and logic would better live in the Handler IMO 4 Have REST controllers depend on XRepository in all cases","Add for who this story is","well_formed","no_role","high",False
19508,"This story corresponds to 3 4 in discussion below. Currently, sink xml files define the Repository hardcoded to be redis and the services tied to Redis as well! I want to 1 Move those definitions out of the sink files, to somewhere shared by both admin and container 2 Make them aware of some configuration parameter similar to the store command line option 3 Get rid of so called Service layer doesn t do much right now, and logic would better live in the Handler IMO 4 Have REST controllers depend on XRepository in all cases",NULL,"This story corresponds to 3 4 in discussion below. Currently, sink xml files define the Repository hardcoded to be redis and the services tied to Redis as well! I want to 1 Move those definitions out of the sink files, to","somewhere shared by both admin and container 2 Make them aware of some configuration parameter similar to the store command line option 3 Get rid of so called Service layer doesn t do much right now, and logic would better live in the Handler IMO 4 Have REST controllers depend on XRepository in all cases","This story corresponds to 3 4 in discussion below. Currently, sink xml files define the Repository hardcoded to be redis<span class='highlight-text severity-high'> and </span>the services tied to Redis as well! I want to 1 Move those definitions out of the sink files, to somewhere shared by both admin and container 2 Make them aware of some configuration parameter similar to the store command line option 3 Get rid of so called Service layer doesn t do much right now, and logic would better live in the Handler IMO 4 Have REST controllers depend on XRepository in all cases","atomic","conjunctions","high",False
19508,"This story corresponds to 3 4 in discussion below. Currently, sink xml files define the Repository hardcoded to be redis and the services tied to Redis as well! I want to 1 Move those definitions out of the sink files, to somewhere shared by both admin and container 2 Make them aware of some configuration parameter similar to the store command line option 3 Get rid of so called Service layer doesn t do much right now, and logic would better live in the Handler IMO 4 Have REST controllers depend on XRepository in all cases",NULL,"This story corresponds to 3 4 in discussion below. Currently, sink xml files define the Repository hardcoded to be redis and the services tied to Redis as well! I want to 1 Move those definitions out of the sink files, to","somewhere shared by both admin and container 2 Make them aware of some configuration parameter similar to the store command line option 3 Get rid of so called Service layer doesn t do much right now, and logic would better live in the Handler IMO 4 Have REST controllers depend on XRepository in all cases","This story corresponds to 3 4 in discussion below<span class='highlight-text severity-high'>. Currently, sink xml files define the Repository hardcoded to be redis and the services tied to Redis as well! I want to 1 Move those definitions out of the sink files, to somewhere shared by both admin and container 2 Make them aware of some configuration parameter similar to the store command line option 3 Get rid of so called Service layer doesn t do much right now, and logic would better live in the Handler IMO 4 Have REST controllers depend on XRepository in all cases</span>","minimal","punctuation","high",False
19508,"This story corresponds to 3 4 in discussion below. Currently, sink xml files define the Repository hardcoded to be redis and the services tied to Redis as well! I want to 1 Move those definitions out of the sink files, to somewhere shared by both admin and container 2 Make them aware of some configuration parameter similar to the store command line option 3 Get rid of so called Service layer doesn t do much right now, and logic would better live in the Handler IMO 4 Have REST controllers depend on XRepository in all cases",NULL,"This story corresponds to 3 4 in discussion below. Currently, sink xml files define the Repository hardcoded to be redis and the services tied to Redis as well! I want to 1 Move those definitions out of the sink files, to","somewhere shared by both admin and container 2 Make them aware of some configuration parameter similar to the store command line option 3 Get rid of so called Service layer doesn t do much right now, and logic would better live in the Handler IMO 4 Have REST controllers depend on XRepository in all cases","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19529,"DefaultFormattingConversionService provides Collection Object conversion which will produce the first item if the target type matches. Here, this results in an unfortunate side effect, getTuple List Tuple list would return a Tuple which is misleading. In this case it is preferable to treat it as an error if the argument is not a Tuple.",NULL,"DefaultFormattingConversionService provides Collection Object conversion which will produce the first item if the target type matches. Here, this results in an unfortunate side effect, getTuple List Tuple list would return a Tuple which is misleading. In this case it is preferable to treat it as an error if the argument is not a Tuple.",NULL,"Add for who this story is","well_formed","no_role","high",False
19529,"DefaultFormattingConversionService provides Collection Object conversion which will produce the first item if the target type matches. Here, this results in an unfortunate side effect, getTuple List Tuple list would return a Tuple which is misleading. In this case it is preferable to treat it as an error if the argument is not a Tuple.",NULL,"DefaultFormattingConversionService provides Collection Object conversion which will produce the first item if the target type matches. Here, this results in an unfortunate side effect, getTuple List Tuple list would return a Tuple which is misleading. In this case it is preferable to treat it as an error if the argument is not a Tuple.",NULL,"DefaultFormattingConversionService provides Collection Object conversion which will produce the first item if the target type matches<span class='highlight-text severity-high'>. Here, this results in an unfortunate side effect, getTuple List Tuple list would return a Tuple which is misleading. In this case it is preferable to treat it as an error if the argument is not a Tuple.</span>","minimal","punctuation","high",False
19558,"With support for substreams parameterized streams now in the parser it will be possible to create a stream that cannot be deployed it may not fit the source processor sink structure or it is a parameterized stream with no default values for parameters. Need to check how XD is going to handle these after creating them, attempting to deploy them should return appropriate errors. They should exist in the stream directory .",NULL,"With support for substreams parameterized streams now in the parser it will be possible to create a stream that cannot be deployed it may not fit the source processor sink structure or it is a parameterized stream with no default values for parameters. Need to check how XD is going to handle these after creating them, attempting to deploy them should return appropriate errors. They should exist in the stream directory .",NULL,"Add for who this story is","well_formed","no_role","high",False
19558,"With support for substreams parameterized streams now in the parser it will be possible to create a stream that cannot be deployed it may not fit the source processor sink structure or it is a parameterized stream with no default values for parameters. Need to check how XD is going to handle these after creating them, attempting to deploy them should return appropriate errors. They should exist in the stream directory .",NULL,"With support for substreams parameterized streams now in the parser it will be possible to create a stream that cannot be deployed it may not fit the source processor sink structure or it is a parameterized stream with no default values for parameters. Need to check how XD is going to handle these after creating them, attempting to deploy them should return appropriate errors. They should exist in the stream directory .",NULL,"With support for substreams parameterized streams now in the parser it will be possible to create a stream that cannot be deployed it may not fit the source processor sink structure<span class='highlight-text severity-high'> or </span>it is a parameterized stream with no default values for parameters. Need to check how XD is going to handle these after creating them, attempting to deploy them should return appropriate errors. They should exist in the stream directory .","atomic","conjunctions","high",False
19558,"With support for substreams parameterized streams now in the parser it will be possible to create a stream that cannot be deployed it may not fit the source processor sink structure or it is a parameterized stream with no default values for parameters. Need to check how XD is going to handle these after creating them, attempting to deploy them should return appropriate errors. They should exist in the stream directory .",NULL,"With support for substreams parameterized streams now in the parser it will be possible to create a stream that cannot be deployed it may not fit the source processor sink structure or it is a parameterized stream with no default values for parameters. Need to check how XD is going to handle these after creating them, attempting to deploy them should return appropriate errors. They should exist in the stream directory .",NULL,"With support for substreams parameterized streams now in the parser it will be possible to create a stream that cannot be deployed it may not fit the source processor sink structure or it is a parameterized stream with no default values for parameters<span class='highlight-text severity-high'>. Need to check how XD is going to handle these after creating them, attempting to deploy them should return appropriate errors. They should exist in the stream directory .</span>","minimal","punctuation","high",False
19560,"Following stream parsing there is now a stream resolution stage that chases down substream references and fills in parameterization. The lookup of streams is done through implementors of the StreamLookupEnvironment interface. Currently the parser implements this itself but it is really a job for the stream directory. The parser implementation doesn t know about stream deletions, for example, so may still resolve streams that no longer exist.",NULL,"Following stream parsing there is now a stream re","solution stage that chases down substream references and fills in parameterization. The lookup of streams is done through implementors of the StreamLookupEnvironment interface. Currently the parser implements this itself but it is really a job for the stream directory. The parser implementation doesn t know about stream deletions, for example, so may still resolve streams that no longer exist.","Add for who this story is","well_formed","no_role","high",False
19720,"This will allow for a simple way to shutdown the server via an HTTP call. Support for security is a separate story. The end goal is to have some shell scripts distributed that can issue HTTP requests to shutdown the xd admin and xd container servers. The newest version of Jolokia has the ability to boostrap itself inside an application context vs. requiring a java agent. I suspect using the application context approach will provide us with more flexibility e.g. property replacement etc but not sure.",NULL,"This will allow for a simple way to shutdown the server via an HTTP call. Support for security is a separate story. The end goal is to have some shell scripts distributed that can issue HTTP requests to shutdown the xd admin and xd container servers. The newest version of Jolokia has the ability to boostrap itself inside an application context vs. requiring a java agent. I suspect using the application context approach will provide us with more flexibility e.g. property replacement etc but not sure.",NULL,"Add for who this story is","well_formed","no_role","high",False
19560,"Following stream parsing there is now a stream resolution stage that chases down substream references and fills in parameterization. The lookup of streams is done through implementors of the StreamLookupEnvironment interface. Currently the parser implements this itself but it is really a job for the stream directory. The parser implementation doesn t know about stream deletions, for example, so may still resolve streams that no longer exist.",NULL,"Following stream parsing there is now a stream re","solution stage that chases down substream references and fills in parameterization. The lookup of streams is done through implementors of the StreamLookupEnvironment interface. Currently the parser implements this itself but it is really a job for the stream directory. The parser implementation doesn t know about stream deletions, for example, so may still resolve streams that no longer exist.","Following stream parsing there is now a stream resolution stage that chases down substream references and fills in parameterization<span class='highlight-text severity-high'>. The lookup of streams is done through implementors of the StreamLookupEnvironment interface. Currently the parser implements this itself but it is really a job for the stream directory. The parser implementation doesn t know about stream deletions, for example, so may still resolve streams that no longer exist.</span>","minimal","punctuation","high",False
19560,"Following stream parsing there is now a stream resolution stage that chases down substream references and fills in parameterization. The lookup of streams is done through implementors of the StreamLookupEnvironment interface. Currently the parser implements this itself but it is really a job for the stream directory. The parser implementation doesn t know about stream deletions, for example, so may still resolve streams that no longer exist.",NULL,"Following stream parsing there is now a stream re","solution stage that chases down substream references and fills in parameterization. The lookup of streams is done through implementors of the StreamLookupEnvironment interface. Currently the parser implements this itself but it is really a job for the stream directory. The parser implementation doesn t know about stream deletions, for example, so may still resolve streams that no longer exist.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19589,"the current streams chapter http static.springsource.org spring xd docs 1.0.0.M1 reference html streams shows using curl to post some data to a http source module, curl d hello http localhost 9000 create a shell command so curl doesn t have to be used. https github.com SpringSource rest shell has a command already developed for this.",NULL,"the current streams chapter http static.spring","source.org spring xd docs 1.0.0.M1 reference html streams shows using curl to post some data to a http source module, curl d hello http localhost 9000 create a shell command so curl doesn t have to be used. https github.com SpringSource rest shell has a command already developed for this.","Add for who this story is","well_formed","no_role","high",False
19589,"the current streams chapter http static.springsource.org spring xd docs 1.0.0.M1 reference html streams shows using curl to post some data to a http source module, curl d hello http localhost 9000 create a shell command so curl doesn t have to be used. https github.com SpringSource rest shell has a command already developed for this.",NULL,"the current streams chapter http static.spring","source.org spring xd docs 1.0.0.M1 reference html streams shows using curl to post some data to a http source module, curl d hello http localhost 9000 create a shell command so curl doesn t have to be used. https github.com SpringSource rest shell has a command already developed for this.","the current streams chapter http static<span class='highlight-text severity-high'>.springsource.org spring xd docs 1.0.0.M1 reference html streams shows using curl to post some data to a http source module, curl d hello http localhost 9000 create a shell command so curl doesn t have to be used. https github.com SpringSource rest shell has a command already developed for this.</span>","minimal","punctuation","high",False
19589,"the current streams chapter http static.springsource.org spring xd docs 1.0.0.M1 reference html streams shows using curl to post some data to a http source module, curl d hello http localhost 9000 create a shell command so curl doesn t have to be used. https github.com SpringSource rest shell has a command already developed for this.",NULL,"the current streams chapter http static.spring","source.org spring xd docs 1.0.0.M1 reference html streams shows using curl to post some data to a http source module, curl d hello http localhost 9000 create a shell command so curl doesn t have to be used. https github.com SpringSource rest shell has a command already developed for this.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19596,"Some java files are currently missing headers. The plugin at https github.com hierynomus license gradle plugin can help, but initial trial revealed that skipExistingHeaders does not seem to be honored. We may then need to use a year construction like 2001 current or force all files to have current year. Don t know the legal implications of this Default source sets encompass all files in the classpath basically, so that means .xml as well as .properties files for example. It would seem logical to add header to those as well, but I'don t think this is what we do on other projetcs.",NULL,"Some java files are currently missing headers. The plugin at https github.com hierynomus license gradle plugin can help, but initial trial revealed that skipExistingHeaders does not seem to be honored. We may then need to use a year construction like 2001 current or force all files to have current year. Don t know the legal implications of this Default source sets encompass all files in the classpath basically,","so that means .xml as well as .properties files for example. It would seem logical to add header to those as well, but I'don t think this is what we do on other projetcs.","Add for who this story is","well_formed","no_role","high",False
19596,"Some java files are currently missing headers. The plugin at https github.com hierynomus license gradle plugin can help, but initial trial revealed that skipExistingHeaders does not seem to be honored. We may then need to use a year construction like 2001 current or force all files to have current year. Don t know the legal implications of this Default source sets encompass all files in the classpath basically, so that means .xml as well as .properties files for example. It would seem logical to add header to those as well, but I'don t think this is what we do on other projetcs.",NULL,"Some java files are currently missing headers. The plugin at https github.com hierynomus license gradle plugin can help, but initial trial revealed that skipExistingHeaders does not seem to be honored. We may then need to use a year construction like 2001 current or force all files to have current year. Don t know the legal implications of this Default source sets encompass all files in the classpath basically,","so that means .xml as well as .properties files for example. It would seem logical to add header to those as well, but I'don t think this is what we do on other projetcs.","Some java files are currently missing headers. The plugin at https github.com hierynomus license gradle plugin can help, but initial trial revealed that skipExistingHeaders does not seem to be honored. We may then need to use a year construction like 2001 current<span class='highlight-text severity-high'> or </span>force all files to have current year. Don t know the legal implications of this Default source sets encompass all files in the classpath basically, so that means .xml as well as .properties files for example. It would seem logical to add header to those as well, but I'don t think this is what we do on other projetcs.","atomic","conjunctions","high",False
19596,"Some java files are currently missing headers. The plugin at https github.com hierynomus license gradle plugin can help, but initial trial revealed that skipExistingHeaders does not seem to be honored. We may then need to use a year construction like 2001 current or force all files to have current year. Don t know the legal implications of this Default source sets encompass all files in the classpath basically, so that means .xml as well as .properties files for example. It would seem logical to add header to those as well, but I'don t think this is what we do on other projetcs.",NULL,"Some java files are currently missing headers. The plugin at https github.com hierynomus license gradle plugin can help, but initial trial revealed that skipExistingHeaders does not seem to be honored. We may then need to use a year construction like 2001 current or force all files to have current year. Don t know the legal implications of this Default source sets encompass all files in the classpath basically,","so that means .xml as well as .properties files for example. It would seem logical to add header to those as well, but I'don t think this is what we do on other projetcs.","Some java files are currently missing headers<span class='highlight-text severity-high'>. The plugin at https github.com hierynomus license gradle plugin can help, but initial trial revealed that skipExistingHeaders does not seem to be honored. We may then need to use a year construction like 2001 current or force all files to have current year. Don t know the legal implications of this Default source sets encompass all files in the classpath basically, so that means .xml as well as .properties files for example. It would seem logical to add header to those as well, but I'don t think this is what we do on other projetcs.</span>","minimal","punctuation","high",False
19596,"Some java files are currently missing headers. The plugin at https github.com hierynomus license gradle plugin can help, but initial trial revealed that skipExistingHeaders does not seem to be honored. We may then need to use a year construction like 2001 current or force all files to have current year. Don t know the legal implications of this Default source sets encompass all files in the classpath basically, so that means .xml as well as .properties files for example. It would seem logical to add header to those as well, but I'don t think this is what we do on other projetcs.",NULL,"Some java files are currently missing headers. The plugin at https github.com hierynomus license gradle plugin can help, but initial trial revealed that skipExistingHeaders does not seem to be honored. We may then need to use a year construction like 2001 current or force all files to have current year. Don t know the legal implications of this Default source sets encompass all files in the classpath basically,","so that means .xml as well as .properties files for example. It would seem logical to add header to those as well, but I'don t think this is what we do on other projetcs.","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19646,"Currently, the RedisQueueInboundChannelAdapter has blocking operation when pulling the messages out of redis queue and this is not performant. There are few ideas from the discussion to make it better 1 Get more items from the redis queue per connection 2 We will also have compression of messages at the channel registry before being sent to the redis queue We also need to investigate what redis connection strategy makes the RedisQueueInboundAdapter better. ",NULL,"Currently, the RedisQueueInboundChannelAdapter has blocking operation when pulling the messages out of redis queue and this is not performant. There are few ideas from the discussion to make it better 1 Get more items from the redis queue per connection 2 We will also have compression of messages at the channel registry before being sent to the redis queue We also need to investigate what redis connection strategy makes the RedisQueueInboundAdapter better. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19646,"Currently, the RedisQueueInboundChannelAdapter has blocking operation when pulling the messages out of redis queue and this is not performant. There are few ideas from the discussion to make it better 1 Get more items from the redis queue per connection 2 We will also have compression of messages at the channel registry before being sent to the redis queue We also need to investigate what redis connection strategy makes the RedisQueueInboundAdapter better. ",NULL,"Currently, the RedisQueueInboundChannelAdapter has blocking operation when pulling the messages out of redis queue and this is not performant. There are few ideas from the discussion to make it better 1 Get more items from the redis queue per connection 2 We will also have compression of messages at the channel registry before being sent to the redis queue We also need to investigate what redis connection strategy makes the RedisQueueInboundAdapter better. ",NULL,"Currently, the RedisQueueInboundChannelAdapter has blocking operation when pulling the messages out of redis queue<span class='highlight-text severity-high'> and </span>this is not performant. There are few ideas from the discussion to make it better 1 Get more items from the redis queue per connection 2 We will also have compression of messages at the channel registry before being sent to the redis queue We also need to investigate what redis connection strategy makes the RedisQueueInboundAdapter better. ","atomic","conjunctions","high",False
19646,"Currently, the RedisQueueInboundChannelAdapter has blocking operation when pulling the messages out of redis queue and this is not performant. There are few ideas from the discussion to make it better 1 Get more items from the redis queue per connection 2 We will also have compression of messages at the channel registry before being sent to the redis queue We also need to investigate what redis connection strategy makes the RedisQueueInboundAdapter better. ",NULL,"Currently, the RedisQueueInboundChannelAdapter has blocking operation when pulling the messages out of redis queue and this is not performant. There are few ideas from the discussion to make it better 1 Get more items from the redis queue per connection 2 We will also have compression of messages at the channel registry before being sent to the redis queue We also need to investigate what redis connection strategy makes the RedisQueueInboundAdapter better. ",NULL,"Currently, the RedisQueueInboundChannelAdapter has blocking operation when pulling the messages out of redis queue and this is not performant<span class='highlight-text severity-high'>. There are few ideas from the discussion to make it better 1 Get more items from the redis queue per connection 2 We will also have compression of messages at the channel registry before being sent to the redis queue We also need to investigate what redis connection strategy makes the RedisQueueInboundAdapter better. </span>","minimal","punctuation","high",False
19702,"launcher.xml can make use of the system property xd.pipeProtocol inside an import statement. This determines which version of the XD infrastructure to load, for example what ChannelRegistry implementation, Local or Redis based, or specific message listener containers. File name conventions should be used, so if the option passed in from the command line is pipeProtocol localChannel then the XML filename looked for has the Protocol suffix applied, e.g. localChannelProtocol, and is loaded via the classpath. Redis and Local will not be the only options, other implementations will be provided in the future, e.g. Rabbit, and the user may be able to provide their own implementations of these infrastructure classes an advanced task . ",NULL,"launcher.xml can make use of the system property xd.pipeProtocol inside an import statement. This determines which version of the XD infrastructure to load, for example what ChannelRegistry implementation, Local or Redis based, or specific message listener containers. File name conventions should be used,","so if the option passed in from the command line is pipeProtocol localChannel then the XML filename looked for has the Protocol suffix applied, e.g. localChannelProtocol, and is loaded via the classpath. Redis and Local will not be the only options, other implementations will be provided in the future, e.g. Rabbit, and the user may be able to provide their own implementations of these infrastructure classes an advanced task .","Add for who this story is","well_formed","no_role","high",False
19702,"launcher.xml can make use of the system property xd.pipeProtocol inside an import statement. This determines which version of the XD infrastructure to load, for example what ChannelRegistry implementation, Local or Redis based, or specific message listener containers. File name conventions should be used, so if the option passed in from the command line is pipeProtocol localChannel then the XML filename looked for has the Protocol suffix applied, e.g. localChannelProtocol, and is loaded via the classpath. Redis and Local will not be the only options, other implementations will be provided in the future, e.g. Rabbit, and the user may be able to provide their own implementations of these infrastructure classes an advanced task . ",NULL,"launcher.xml can make use of the system property xd.pipeProtocol inside an import statement. This determines which version of the XD infrastructure to load, for example what ChannelRegistry implementation, Local or Redis based, or specific message listener containers. File name conventions should be used,","so if the option passed in from the command line is pipeProtocol localChannel then the XML filename looked for has the Protocol suffix applied, e.g. localChannelProtocol, and is loaded via the classpath. Redis and Local will not be the only options, other implementations will be provided in the future, e.g. Rabbit, and the user may be able to provide their own implementations of these infrastructure classes an advanced task .","launcher.xml can make use of the system property xd.pipeProtocol inside an import statement. This determines which version of the XD infrastructure to load, for example what ChannelRegistry implementation, Local<span class='highlight-text severity-high'> or </span>Redis based, or specific message listener containers. File name conventions should be used, so if the option passed in from the command line is pipeProtocol localChannel then the XML filename looked for has the Protocol suffix applied, e.g. localChannelProtocol,<span class='highlight-text severity-high'> and </span>is loaded via the classpath. Redis and Local will not be the only options, other implementations will be provided in the future, e.g. Rabbit, and the user may be able to provide their own implementations of these infrastructure classes an advanced task . ","atomic","conjunctions","high",False
19702,"launcher.xml can make use of the system property xd.pipeProtocol inside an import statement. This determines which version of the XD infrastructure to load, for example what ChannelRegistry implementation, Local or Redis based, or specific message listener containers. File name conventions should be used, so if the option passed in from the command line is pipeProtocol localChannel then the XML filename looked for has the Protocol suffix applied, e.g. localChannelProtocol, and is loaded via the classpath. Redis and Local will not be the only options, other implementations will be provided in the future, e.g. Rabbit, and the user may be able to provide their own implementations of these infrastructure classes an advanced task . ",NULL,"launcher.xml can make use of the system property xd.pipeProtocol inside an import statement. This determines which version of the XD infrastructure to load, for example what ChannelRegistry implementation, Local or Redis based, or specific message listener containers. File name conventions should be used,","so if the option passed in from the command line is pipeProtocol localChannel then the XML filename looked for has the Protocol suffix applied, e.g. localChannelProtocol, and is loaded via the classpath. Redis and Local will not be the only options, other implementations will be provided in the future, e.g. Rabbit, and the user may be able to provide their own implementations of these infrastructure classes an advanced task .","launcher<span class='highlight-text severity-high'>.xml can make use of the system property xd.pipeProtocol inside an import statement. This determines which version of the XD infrastructure to load, for example what ChannelRegistry implementation, Local or Redis based, or specific message listener containers. File name conventions should be used, so if the option passed in from the command line is pipeProtocol localChannel then the XML filename looked for has the Protocol suffix applied, e.g. localChannelProtocol, and is loaded via the classpath. Redis and Local will not be the only options, other implementations will be provided in the future, e.g. Rabbit, and the user may be able to provide their own implementations of these infrastructure classes an advanced task . </span>","minimal","punctuation","high",False
19702,"launcher.xml can make use of the system property xd.pipeProtocol inside an import statement. This determines which version of the XD infrastructure to load, for example what ChannelRegistry implementation, Local or Redis based, or specific message listener containers. File name conventions should be used, so if the option passed in from the command line is pipeProtocol localChannel then the XML filename looked for has the Protocol suffix applied, e.g. localChannelProtocol, and is loaded via the classpath. Redis and Local will not be the only options, other implementations will be provided in the future, e.g. Rabbit, and the user may be able to provide their own implementations of these infrastructure classes an advanced task . ",NULL,"launcher.xml can make use of the system property xd.pipeProtocol inside an import statement. This determines which version of the XD infrastructure to load, for example what ChannelRegistry implementation, Local or Redis based, or specific message listener containers. File name conventions should be used,","so if the option passed in from the command line is pipeProtocol localChannel then the XML filename looked for has the Protocol suffix applied, e.g. localChannelProtocol, and is loaded via the classpath. Redis and Local will not be the only options, other implementations will be provided in the future, e.g. Rabbit, and the user may be able to provide their own implementations of these infrastructure classes an advanced task .","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19714,"We need to have the XD container admin reading the registry specific property based on the registry type selected. From Mark F, on one of the code review comments Maybe rather than having redis, rabbit, etc. properties all within a container.properties we should rely upon naming conventions instead. Specifically, we could have a single configurable property for the type of channel registry redis , rabbit , or local being possible values , and then we could use something like import resource config registry.type .xml context property placeholder location config registry.type .properties ",NULL,"We need to have the XD container admin reading the registry specific property based on the registry type selected. From Mark F, on one of the code review comments Maybe rather than having redis, rabbit, etc. properties all within a container.properties we should rely upon naming conventions instead. Specifically, we could have a single configurable property for the type of channel registry redis , rabbit , or local being possible values , and then we could use something like import resource config registry.type .xml context property placeholder location config registry.type .properties ",NULL,"Add for who this story is","well_formed","no_role","high",False
19714,"We need to have the XD container admin reading the registry specific property based on the registry type selected. From Mark F, on one of the code review comments Maybe rather than having redis, rabbit, etc. properties all within a container.properties we should rely upon naming conventions instead. Specifically, we could have a single configurable property for the type of channel registry redis , rabbit , or local being possible values , and then we could use something like import resource config registry.type .xml context property placeholder location config registry.type .properties ",NULL,"We need to have the XD container admin reading the registry specific property based on the registry type selected. From Mark F, on one of the code review comments Maybe rather than having redis, rabbit, etc. properties all within a container.properties we should rely upon naming conventions instead. Specifically, we could have a single configurable property for the type of channel registry redis , rabbit , or local being possible values , and then we could use something like import resource config registry.type .xml context property placeholder location config registry.type .properties ",NULL,"We need to have the XD container admin reading the registry specific property based on the registry type selected. From Mark F, on one of the code review comments Maybe rather than having redis, rabbit, etc. properties all within a container.properties we should rely upon naming conventions instead. Specifically, we could have a single configurable property for the type of channel registry redis , rabbit ,<span class='highlight-text severity-high'> or </span>local being possible values ,<span class='highlight-text severity-high'> and </span>then we could use something like import resource config registry.type .xml context property placeholder location config registry.type .properties ","atomic","conjunctions","high",False
19714,"We need to have the XD container admin reading the registry specific property based on the registry type selected. From Mark F, on one of the code review comments Maybe rather than having redis, rabbit, etc. properties all within a container.properties we should rely upon naming conventions instead. Specifically, we could have a single configurable property for the type of channel registry redis , rabbit , or local being possible values , and then we could use something like import resource config registry.type .xml context property placeholder location config registry.type .properties ",NULL,"We need to have the XD container admin reading the registry specific property based on the registry type selected. From Mark F, on one of the code review comments Maybe rather than having redis, rabbit, etc. properties all within a container.properties we should rely upon naming conventions instead. Specifically, we could have a single configurable property for the type of channel registry redis , rabbit , or local being possible values , and then we could use something like import resource config registry.type .xml context property placeholder location config registry.type .properties ",NULL,"We need to have the XD container admin reading the registry specific property based on the registry type selected<span class='highlight-text severity-high'>. From Mark F, on one of the code review comments Maybe rather than having redis, rabbit, etc. properties all within a container.properties we should rely upon naming conventions instead. Specifically, we could have a single configurable property for the type of channel registry redis , rabbit , or local being possible values , and then we could use something like import resource config registry.type .xml context property placeholder location config registry.type .properties </span>","minimal","punctuation","high",False
19720,"This will allow for a simple way to shutdown the server via an HTTP call. Support for security is a separate story. The end goal is to have some shell scripts distributed that can issue HTTP requests to shutdown the xd admin and xd container servers. The newest version of Jolokia has the ability to boostrap itself inside an application context vs. requiring a java agent. I suspect using the application context approach will provide us with more flexibility e.g. property replacement etc but not sure.",NULL,"This will allow for a simple way to shutdown the server via an HTTP call. Support for security is a separate story. The end goal is to have some shell scripts distributed that can issue HTTP requests to shutdown the xd admin and xd container servers. The newest version of Jolokia has the ability to boostrap itself inside an application context vs. requiring a java agent. I suspect using the application context approach will provide us with more flexibility e.g. property replacement etc but not sure.",NULL,"This will allow for a simple way to shutdown the server via an HTTP call<span class='highlight-text severity-high'>. Support for security is a separate story. The end goal is to have some shell scripts distributed that can issue HTTP requests to shutdown the xd admin and xd container servers. The newest version of Jolokia has the ability to boostrap itself inside an application context vs. requiring a java agent. I suspect using the application context approach will provide us with more flexibility e.g. property replacement etc but not sure.</span>","minimal","punctuation","high",False
19724,"Shouldn t we have something like a ContextRefreshedEvent Listener and output some informational messages to the console, so the user knows the Container is up Which contexts. Maybe even print a link to the docs ? Maybe even some simple ascii art for demos ? Right now it looks somewhat barren. Redis provides something similar. This may even go hand in hand to provided a better configuration model storing common config parameters centrally code V . . , v1.0.0.M1 eXtreme Data Using Redis at localhost 6379 The Server PID 12345 is now ready on http myserver 123 streams Documentation https github.com SpringSource spring xd wiki code ",NULL,"Shouldn t we have","something like a ContextRefreshedEvent Listener and output some informational messages to the console, so the user knows the Container is up Which contexts. Maybe even print a link to the docs ? Maybe even some simple ascii art for demos ? Right now it looks somewhat barren. Redis provides something similar. This may even go hand in hand to provided a better configuration model storing common config parameters centrally code V . . , v1.0.0.M1 eXtreme Data Using Redis at localhost 6379 The Server PID 12345 is now ready on http myserver 123 streams Documentation https github.com SpringSource spring xd wiki code","Add for who this story is","well_formed","no_role","high",False
19724,"Shouldn t we have something like a ContextRefreshedEvent Listener and output some informational messages to the console, so the user knows the Container is up Which contexts. Maybe even print a link to the docs ? Maybe even some simple ascii art for demos ? Right now it looks somewhat barren. Redis provides something similar. This may even go hand in hand to provided a better configuration model storing common config parameters centrally code V . . , v1.0.0.M1 eXtreme Data Using Redis at localhost 6379 The Server PID 12345 is now ready on http myserver 123 streams Documentation https github.com SpringSource spring xd wiki code ",NULL,"Shouldn t we have","something like a ContextRefreshedEvent Listener and output some informational messages to the console, so the user knows the Container is up Which contexts. Maybe even print a link to the docs ? Maybe even some simple ascii art for demos ? Right now it looks somewhat barren. Redis provides something similar. This may even go hand in hand to provided a better configuration model storing common config parameters centrally code V . . , v1.0.0.M1 eXtreme Data Using Redis at localhost 6379 The Server PID 12345 is now ready on http myserver 123 streams Documentation https github.com SpringSource spring xd wiki code","Shouldn t we have something like a ContextRefreshedEvent Listener and output some informational messages to the console, so the user knows the Container is up Which contexts<span class='highlight-text severity-high'>. Maybe even print a link to the docs ? Maybe even some simple ascii art for demos ? Right now it looks somewhat barren. Redis provides something similar. This may even go hand in hand to provided a better configuration model storing common config parameters centrally code V . . , v1.0.0.M1 eXtreme Data Using Redis at localhost 6379 The Server PID 12345 is now ready on http myserver 123 streams Documentation https github.com SpringSource spring xd wiki code </span>","minimal","punctuation","high",False
19724,"Shouldn t we have something like a ContextRefreshedEvent Listener and output some informational messages to the console, so the user knows the Container is up Which contexts. Maybe even print a link to the docs ? Maybe even some simple ascii art for demos ? Right now it looks somewhat barren. Redis provides something similar. This may even go hand in hand to provided a better configuration model storing common config parameters centrally code V . . , v1.0.0.M1 eXtreme Data Using Redis at localhost 6379 The Server PID 12345 is now ready on http myserver 123 streams Documentation https github.com SpringSource spring xd wiki code ",NULL,"Shouldn t we have","something like a ContextRefreshedEvent Listener and output some informational messages to the console, so the user knows the Container is up Which contexts. Maybe even print a link to the docs ? Maybe even some simple ascii art for demos ? Right now it looks somewhat barren. Redis provides something similar. This may even go hand in hand to provided a better configuration model storing common config parameters centrally code V . . , v1.0.0.M1 eXtreme Data Using Redis at localhost 6379 The Server PID 12345 is now ready on http myserver 123 streams Documentation https github.com SpringSource spring xd wiki code","Use the most common template: As a, I want to, So","uniform","uniform","medium",False
19752,"The final directory structure should look like install dir xd install dir redis install dir gemfire inside the XD directory xd bin which has xd container and xd admin scripts xd lib inside the gemfire directory gemfire bin has the gemfire server script gemfire lib inside the redis directory is redis redis latest v.x.y.z.tar redis README readis install redis script that does the basic 4 commands to install redis. There should be a gradle task that runs after the distZip task, that will take the contents of different project directories, script diretories and redis binary directories and creates the final layout for the distribution.",NULL,"The final directory structure should look like install dir xd install dir redis install dir gemfire inside the XD directory xd bin which has xd container and xd admin scripts xd lib inside the gemfire directory gemfire bin has the gemfire server script gemfire lib inside the redis directory is redis redis latest v.x.y.z.tar redis README readis install redis script that does the basic 4 commands to install redis. There should be a gradle task that runs after the distZip task, that will take the contents of different project directories, script diretories and redis binary directories and creates the final layout for the distribution.",NULL,"Add for who this story is","well_formed","no_role","high",False
19752,"The final directory structure should look like install dir xd install dir redis install dir gemfire inside the XD directory xd bin which has xd container and xd admin scripts xd lib inside the gemfire directory gemfire bin has the gemfire server script gemfire lib inside the redis directory is redis redis latest v.x.y.z.tar redis README readis install redis script that does the basic 4 commands to install redis. There should be a gradle task that runs after the distZip task, that will take the contents of different project directories, script diretories and redis binary directories and creates the final layout for the distribution.",NULL,"The final directory structure should look like install dir xd install dir redis install dir gemfire inside the XD directory xd bin which has xd container and xd admin scripts xd lib inside the gemfire directory gemfire bin has the gemfire server script gemfire lib inside the redis directory is redis redis latest v.x.y.z.tar redis README readis install redis script that does the basic 4 commands to install redis. There should be a gradle task that runs after the distZip task, that will take the contents of different project directories, script diretories and redis binary directories and creates the final layout for the distribution.",NULL,"The final directory structure should look like install dir xd install dir redis install dir gemfire inside the XD directory xd bin which has xd container<span class='highlight-text severity-high'> and </span>xd admin scripts xd lib inside the gemfire directory gemfire bin has the gemfire server script gemfire lib inside the redis directory is redis redis latest v.x.y.z.tar redis README readis install redis script that does the basic 4 commands to install redis. There should be a gradle task that runs after the distZip task, that will take the contents of different project directories, script diretories and redis binary directories and creates the final layout for the distribution.","atomic","conjunctions","high",False
19752,"The final directory structure should look like install dir xd install dir redis install dir gemfire inside the XD directory xd bin which has xd container and xd admin scripts xd lib inside the gemfire directory gemfire bin has the gemfire server script gemfire lib inside the redis directory is redis redis latest v.x.y.z.tar redis README readis install redis script that does the basic 4 commands to install redis. There should be a gradle task that runs after the distZip task, that will take the contents of different project directories, script diretories and redis binary directories and creates the final layout for the distribution.",NULL,"The final directory structure should look like install dir xd install dir redis install dir gemfire inside the XD directory xd bin which has xd container and xd admin scripts xd lib inside the gemfire directory gemfire bin has the gemfire server script gemfire lib inside the redis directory is redis redis latest v.x.y.z.tar redis README readis install redis script that does the basic 4 commands to install redis. There should be a gradle task that runs after the distZip task, that will take the contents of different project directories, script diretories and redis binary directories and creates the final layout for the distribution.",NULL,"The final directory structure should look like install dir xd install dir redis install dir gemfire inside the XD directory xd bin which has xd container and xd admin scripts xd lib inside the gemfire directory gemfire bin has the gemfire server script gemfire lib inside the redis directory is redis redis latest v<span class='highlight-text severity-high'>.x.y.z.tar redis README readis install redis script that does the basic 4 commands to install redis. There should be a gradle task that runs after the distZip task, that will take the contents of different project directories, script diretories and redis binary directories and creates the final layout for the distribution.</span>","minimal","punctuation","high",False
19770,"Based on a single process Spring Batch job, ETL of data from HDFS to MongoDB.",NULL,"Based on a single process Spring Batch job, ETL of data from HDFS to MongoDB.",NULL,"Add for who this story is","well_formed","no_role","high",False
19775,"The gradle application task should get us most of the way to create a distributable artifact akin to what you see when downloading tomcat jetty etc. Now there is a launch task task launch, dependsOn classes , type JavaExec main org.springframework.xd.dirt.stream.StreamServer classpath sourceSets.test.runtimeClasspath The same main should be referenced in the application plugin, a task to create a .zip distributable is needed. Ideally would be nice to 1. download .zip 2. unzip 3. cd spring xd bin 4. xdserver start and gracefully shutdown later with 5. xdserver stop I'don t know if we can should bundle redis, I think we should bundle it. The scripts can be for unix linux and for windows. Discuss a brew based install as well. ",NULL,"The gradle application task should get us most of the way to create a distributable artifact akin to what you see when downloading tomcat jetty etc. Now there is a launch task task launch, dependsOn classes , type JavaExec main org.springframework.xd.dirt.stream.StreamServer classpath sourceSets.test.runtimeClasspath The same main should be referenced in the application plugin, a task to create a .zip distributable is needed. Ideally would be nice to 1. download .zip 2. unzip 3. cd spring xd bin 4. xdserver start and gracefully shutdown later with 5. xdserver stop I'don t know if we can should bundle redis, I think we should bundle it. The scripts can be for unix linux and for windows. Discuss a brew based install as well. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19775,"The gradle application task should get us most of the way to create a distributable artifact akin to what you see when downloading tomcat jetty etc. Now there is a launch task task launch, dependsOn classes , type JavaExec main org.springframework.xd.dirt.stream.StreamServer classpath sourceSets.test.runtimeClasspath The same main should be referenced in the application plugin, a task to create a .zip distributable is needed. Ideally would be nice to 1. download .zip 2. unzip 3. cd spring xd bin 4. xdserver start and gracefully shutdown later with 5. xdserver stop I'don t know if we can should bundle redis, I think we should bundle it. The scripts can be for unix linux and for windows. Discuss a brew based install as well. ",NULL,"The gradle application task should get us most of the way to create a distributable artifact akin to what you see when downloading tomcat jetty etc. Now there is a launch task task launch, dependsOn classes , type JavaExec main org.springframework.xd.dirt.stream.StreamServer classpath sourceSets.test.runtimeClasspath The same main should be referenced in the application plugin, a task to create a .zip distributable is needed. Ideally would be nice to 1. download .zip 2. unzip 3. cd spring xd bin 4. xdserver start and gracefully shutdown later with 5. xdserver stop I'don t know if we can should bundle redis, I think we should bundle it. The scripts can be for unix linux and for windows. Discuss a brew based install as well. ",NULL,"The gradle application task should get us most of the way to create a distributable artifact akin to what you see when downloading tomcat jetty etc<span class='highlight-text severity-high'>. Now there is a launch task task launch, dependsOn classes , type JavaExec main org.springframework.xd.dirt.stream.StreamServer classpath sourceSets.test.runtimeClasspath The same main should be referenced in the application plugin, a task to create a .zip distributable is needed. Ideally would be nice to 1. download .zip 2. unzip 3. cd spring xd bin 4. xdserver start and gracefully shutdown later with 5. xdserver stop I'don t know if we can should bundle redis, I think we should bundle it. The scripts can be for unix linux and for windows. Discuss a brew based install as well. </span>","minimal","punctuation","high",False
19786,"This provides common CRUD behavior and a shared interface that can be useful in testing scenarios. ",NULL,"This provides common CRUD behavior and a shared interface that can be useful in testing scenarios. ",NULL,"Add for who this story is","well_formed","no_role","high",False
19786,"This provides common CRUD behavior and a shared interface that can be useful in testing scenarios. ",NULL,"This provides common CRUD behavior and a shared interface that can be useful in testing scenarios. ",NULL,"This provides common CRUD behavior<span class='highlight-text severity-high'> and </span>a shared interface that can be useful in testing scenarios. ","atomic","conjunctions","high",False
